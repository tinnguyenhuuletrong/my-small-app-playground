This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)

# Directory Structure
```
.github/
  workflows/
    nightly.yml
    test.yml
client/
  session_test.go
  session.pb.go
config/
  config_test.go
  config.go
docs/
  COPYRIGHT
  devops.CHS.md
  devops.md
  overview.CHS.md
  storage.CHS.md
  storage.md
  test.md
examples/
  README.md
internal/
  fileutil/
    go114.go
    go116.go
    utils_test.go
    utils.go
    vfs_test.go
  id/
    nhid_test.go
    nhid.go
  invariants/
    arch.go
    memfs.go
    monkey.go
    nomemfs.go
    nomonkey.go
    norace.go
    race.go
  logdb/
    kv/
      pebble/
        kv_pebble.go
        monkey.go
        monkeynoop.go
      kv.go
    tee/
      tee.go
    batch_test.go
    batch.go
    cache_test.go
    cache.go
    compaction_test.go
    compaction.go
    context_test.go
    context.go
    db_test.go
    db.go
    key_test.go
    key.go
    kv_default.go
    kv_pebble_memfs.go
    kv_test.go
    log_logdb_test.go
    logdb.go
    logreader_etcd_test.go
    logreader_test.go
    logreader.go
    plain.go
    sharded.go
  raft/
    entryutils_test.go
    entryutils.go
    inmemory_etcd_test.go
    inmemory_test.go
    inmemory.go
    logdb_etcd_test.go
    logdb_test.go
    logentry_etcd_test.go
    logentry_helper.go
    logentry_test.go
    logentry.go
    monkey.go
    NOTICE
    NOTICE.etcd
    peer_test.go
    peer.go
    raft_etcd_paper_test.go
    raft_etcd_test.go
    raft_test.go
    raft.go
    readindex_test.go
    readindex.go
    remote_test.go
    remote.go
  registry/
    event.go
    gossip_test.go
    gossip.go
    logger.go
    nodehost.go
    registry_test.go
    registry.go
    view_test.go
    view.go
  rsm/
    adapter_test.go
    adapter.go
    chunkwriter_test.go
    chunkwriter.go
    encoded_test.go
    encoded.go
    files_test.go
    files.go
    lrusession_test.go
    lrusession.go
    managed_test.go
    managed.go
    membership_test.go
    membership.go
    offload_test.go
    offload.go
    rwv_test.go
    rwv.go
    session_test.go
    session.go
    sessionmanager_test.go
    sessionmanager.go
    snapshotio_test.go
    snapshotio.go
    statemachine_test.go
    statemachine.go
    taskqueue_test.go
    taskqueue.go
  server/
    environment_test.go
    environment.go
    event.go
    message_test.go
    message.go
    partition.go
    rate_test.go
    rate.go
    snapshotenv_test.go
    snapshotenv.go
  settings/
    hard.go
    overwrite.go
    soft.go
  tan/
    benchmark_test.go
    bootstrap.go
    compaction_test.go
    compaction.go
    crc.go
    db_keeper.go
    db_test.go
    db.go
    filename_test.go
    filename.go
    index_test.go
    index.go
    LICENSE.pebble
    logdb_test.go
    logdb.go
    node_states.go
    open.go
    options.go
    prealloc_generic.go
    prealloc_linux.go
    read_state.go
    README.md
    record_test.go
    record.go
    utils.go
    version_edit_test.go
    version_edit.go
    version_set_test.go
    version_set.go
    version_test.go
    version.go
  tests/
    kvpb/
      kv.go
    concurrent.go
    concurrentkv.go
    error.go
    fakedisk_test.go
    fakedisk.go
    kvtest.go
    noop.go
  transport/
    tests/
      localhost.crt
      localhost.csr
      localhost.key
      README.md
      test-root-ca.crt
    chunk_test.go
    chunk.go
    chunkfile.go
    fuzz.go
    job_test.go
    job.go
    metrics.go
    monkey.go
    noop.go
    snapshot.go
    tcp_test.go
    tcp.go
    transport_test.go
    transport.go
  utils/
    dio/
      io_test.go
      io.go
    error.go
  vfs/
    defaultfs.go
    error.go
    memfs.go
    vfs.go
logger/
  capnslogger.go
  logger.go
plugin/
  chan/
    chan.go
  tan/
    tan.go
  tee/
    tee.go
raftio/
  binversion.go
  listener.go
  logdb.go
  registry.go
  transport.go
raftpb/
  bootstrap.go
  chunk.go
  common.go
  configchange.go
  entry.go
  entrybatch.go
  fuzz.go
  membership.go
  message.go
  messagebatch.go
  raft_optimized.go
  raft_test.go
  raft.go
  raftdatastatus.go
  snapshot.go
  snapshotfile.go
  snapshotheader.go
  state.go
  types.go
  update_test.go
  update.go
statemachine/
  concurrent.go
  disk.go
  extension.go
  rsm.go
tools/
  fsync/
    main.go
    README.md
  upgrade310/
    upgrade.go
  import_test.go
  import.go
.codecov.yml
.gitignore
AUTHORS
benchmark_test.go
CHANGELOG.md
CONTRIBUTING.md
engine_test.go
engine.go
event.go
go.mod
issue_template.md
LICENSE
Makefile
monkey.go
monkeynoop.go
node_test.go
node.go
nodehost_test.go
nodehost.go
NOTICE
queue_test.go
queue.go
quiesce_test.go
quiesce.go
README.CHS.md
README.md
registry.go
request_test.go
request.go
snapshotstate_test.go
snapshotstate.go
snapshotter_test.go
snapshotter.go
```

# Files

## File: .github/workflows/nightly.yml
````yaml
on:
  schedule:
  - cron: '30 19 * * *'
name: Nightly
jobs:
  monkey-test:
    runs-on: ubuntu-22.04
    strategy:
      matrix:
        target: [race-monkey-test, race-ondisk-monkey-test, memfs-monkey-test, memfs-ondisk-monkey-test]
    steps:
    - name: Install Go
      uses: actions/setup-go@v3
      with:
        go-version: 1.24.x
    - name: Test
      env:
        TARGET: ${{ matrix.target }}
      run: |
        git clone https://github.com/lni/drummer
        sudo apt-get install librocksdb-dev
        cd drummer
        go get github.com/lni/dragonboat/v4@master
        make test
        make $TARGET
````

## File: .github/workflows/test.yml
````yaml
on: [push, pull_request]
name: Test
jobs:
  unit-test:
    runs-on: ubuntu-22.04
    steps:
    - name: Install Go
      uses: actions/setup-go@v3
      with:
        go-version: 1.24.x
    - name: Checkout code
      uses: actions/checkout@v3
    - name: Test
      env:
        CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
      run: |
       make actions-test
       bash <(curl -s https://codecov.io/bash) -t $CODECOV_TOKEN
  race-unit-test:
    runs-on: ubuntu-22.04
    steps:
    - name: Install Go
      uses: actions/setup-go@v3
      with:
        go-version: 1.24.x
    - name: Checkout code
      uses: actions/checkout@v3
    - name: Test
      run: RACE=1 make test
  memfs-unit-test:
    runs-on: ubuntu-22.04
    steps:
    - name: Install Go
      uses: actions/setup-go@v3
      with:
        go-version: 1.24.x
    - name: Checkout code
      uses: actions/checkout@v3
    - name: Test
      run: MEMFS_TEST=1 make test
  macos-unit-test:
    runs-on: macos-latest
    steps:
    - name: Install Go
      uses: actions/setup-go@v3
      with:
        go-version: 1.24.x
    - name: Checkout code
      uses: actions/checkout@v3
    - name: Test
      run: make test
  build-tools:
    runs-on: ubuntu-22.04
    steps:
    - name: Install Go
      uses: actions/setup-go@v3
      with:
        go-version: 1.24.x
    - name: Checkout code
      uses: actions/checkout@v3
    - name: Build
      run: make tools
  build-examples:
    runs-on: ubuntu-22.04
    steps:
    - name: Install Go
      uses: actions/setup-go@v3
      with:
        go-version: 1.24.x
    - name: Build
      run: |
       git clone https://github.com/lni/dragonboat-example
       cd dragonboat-example
       GOPROXY=direct go get github.com/lni/dragonboat/v4@master
       make
  static-check:
    runs-on: ubuntu-22.04
    steps: 
     - name: Install Go
       uses: actions/setup-go@v3
       with:
         go-version: 1.24.x
     - name: Checkout code
       uses: actions/checkout@v3
     - name: Static check
       run: |
        make install-static-check-tools
        make static-check
````

## File: client/session_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package client

import (
	"testing"

	"github.com/lni/goutils/random"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

func TestNoOPSessionHasExpectedSeriesID(t *testing.T) {
	cs := NewNoOPSession(120, random.LockGuardedRand)
	assert.Equal(t, NoOPSeriesID, cs.SeriesID, "series id unexpected")
	assert.Equal(t, uint64(120), cs.ShardID, "shard id unexpected")
}

func TestNoOPSessionNotAllowedForSessionOps(t *testing.T) {
	cs := NewNoOPSession(120, random.LockGuardedRand)
	require.Panics(t, func() { cs.PrepareForRegister() })
	require.Panics(t, func() { cs.PrepareForUnregister() })
	require.Panics(t, func() { cs.PrepareForPropose() })
}

func TestProposalCompleted(t *testing.T) {
	cs := NewSession(120, random.LockGuardedRand)
	cs.PrepareForPropose()
	for i := 0; i < 128; i++ {
		cs.ProposalCompleted()
	}
	expectedSeriesID := SeriesIDFirstProposal + 128
	assert.Equal(t, expectedSeriesID, cs.SeriesID, "unexpected series id")
	assert.Equal(t, cs.SeriesID, cs.RespondedTo+1,
		"unexpected responded to value")
}

func TestInvalidNoOPSessionIsReported(t *testing.T) {
	cs := Session{
		SeriesID: NoOPSeriesID,
		ClientID: NotSessionManagedClientID,
	}
	assert.False(t, cs.ValidForProposal(0),
		"failed to identify invalid client session")
}

func TestValidForProposal(t *testing.T) {
	cs := NewSession(120, random.LockGuardedRand)
	cs.PrepareForRegister()
	assert.False(t, cs.ValidForProposal(120), "bad ValidForProposal result")
	cs.PrepareForUnregister()
	assert.False(t, cs.ValidForProposal(120), "bad ValidForProposal result")
	cs.RespondedTo = 200
	cs.SeriesID = 100
	require.Panics(t, func() { cs.ValidForProposal(120) })
}

func TestValidForSessionOp(t *testing.T) {
	cs := NewNoOPSession(120, random.LockGuardedRand)
	assert.False(t, cs.ValidForSessionOp(120), "bad ValidForSessionOp result")

	cs = NewSession(120, random.LockGuardedRand)
	cs.ClientID = NotSessionManagedClientID
	assert.False(t, cs.ValidForSessionOp(120), "bad ValidForSessionOp result")

	cs = NewSession(120, random.LockGuardedRand)
	cs.PrepareForPropose()
	assert.False(t, cs.ValidForSessionOp(120), "bad ValidForSessionOp result")

	cs.PrepareForUnregister()
	assert.True(t, cs.ValidForSessionOp(120), "bad ValidForSessionOp result")

	cs.PrepareForRegister()
	assert.True(t, cs.ValidForSessionOp(120), "bad ValidForSessionOp result")
}

func TestIsNoOPSession(t *testing.T) {
	s := NewNoOPSession(1, random.LockGuardedRand)
	assert.True(t, s.IsNoOPSession(), "not considered as a noop session")

	s.ClientID++
	assert.True(t, s.IsNoOPSession(), "not considered as a noop session")

	s.SeriesID++
	assert.False(t, s.IsNoOPSession(), "still considered as a noop session")
}
````

## File: client/session.pb.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: session.proto
/*
	Package client implements the client Session struct for identifying proposal
	clients and their progress. Together with the LRU based session store on the
	server side, client Session helps to implement the at-most-once semantic. It
	ensures that when retrying a prviously timed out proposal, the retried
	proposal will not be applied twice if the previous proposal attempt actually
	succeed. See section 6.3 of Diego Ongaro's PhD thesis for more details.
*/
package client

import (
	"fmt"
	"io"
	"math"

	"github.com/lni/goutils/random"
)

const (
	// NotSessionManagedClientID is a special client id used to indicate
	// that the entry is not managed by a client session. This is used when
	// the entry is not generated by the user, e.g. the blank entry proposed
	// after becoming the leader.
	NotSessionManagedClientID uint64 = 0
	// NoOPSeriesID is a special series ID used to indicate that the client
	// session is a NOOP client session.
	NoOPSeriesID uint64 = 0
	// SeriesIDForRegister is the special series id used for registering
	// a new client session.
	SeriesIDForRegister uint64 = math.MaxUint64 - 1
	// SeriesIDForUnregister is the special series id used for unregistering
	// client session.
	SeriesIDForUnregister uint64 = math.MaxUint64
	// SeriesIDFirstProposal is the first series id to be used for making
	// proposals.
	SeriesIDFirstProposal uint64 = 1
)

var (
	ErrInvalidLengthSession = fmt.Errorf("proto: negative length found during unmarshaling")
	ErrIntOverflowSession   = fmt.Errorf("proto: integer overflow")
)

// Session is the struct used to keep tracking the progress of a client.
type Session struct {
	ShardID     uint64
	ClientID    uint64
	SeriesID    uint64
	RespondedTo uint64
}

func (*Session) ProtoMessage() {}

func (m *Session) Reset() { *m = Session{} }
func (m *Session) String() string {
	return fmt.Sprintf("%d:%d:%d:%d", m.ShardID, m.ClientID, m.SeriesID, m.RespondedTo)
}

// NewSession returns a new client session not registered yet. This function
// is not expected to be directly invoked by application.
func NewSession(shardID uint64, rng random.Source) *Session {
	for {
		cid := rng.Uint64()
		if cid != NotSessionManagedClientID {
			return &Session{
				ShardID:  shardID,
				ClientID: cid,
				SeriesID: NoOPSeriesID + 1,
			}
		}
	}
}

// NewNoOPSession creates a new NoOP client session ready to be used for
// making proposals. This function is not expected to be directly invoked by
// application.
func NewNoOPSession(shardID uint64, rng random.Source) *Session {
	for {
		cid := rng.Uint64()
		if cid != NotSessionManagedClientID {
			return &Session{
				ShardID:  shardID,
				ClientID: cid,
				SeriesID: NoOPSeriesID,
			}
		}
	}
}

func (m *Session) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *Session) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	dAtA[i] = 0x8
	i++
	i = encodeVarintSession(dAtA, i, uint64(m.ShardID))
	dAtA[i] = 0x10
	i++
	i = encodeVarintSession(dAtA, i, uint64(m.ClientID))
	dAtA[i] = 0x18
	i++
	i = encodeVarintSession(dAtA, i, uint64(m.SeriesID))
	dAtA[i] = 0x20
	i++
	i = encodeVarintSession(dAtA, i, uint64(m.RespondedTo))
	return i, nil
}

func encodeVarintSession(dAtA []byte, offset int, v uint64) int {
	for v >= 1<<7 {
		dAtA[offset] = uint8(v&0x7f | 0x80)
		v >>= 7
		offset++
	}
	dAtA[offset] = uint8(v)
	return offset + 1
}
func (m *Session) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	n += 1 + sovSession(uint64(m.ShardID))
	n += 1 + sovSession(uint64(m.ClientID))
	n += 1 + sovSession(uint64(m.SeriesID))
	n += 1 + sovSession(uint64(m.RespondedTo))
	return n
}

func sovSession(x uint64) (n int) {
	for {
		n++
		x >>= 7
		if x == 0 {
			break
		}
	}
	return n
}

func (m *Session) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowSession
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: Session: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: Session: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ShardID", wireType)
			}
			m.ShardID = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowSession
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ShardID |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ClientID", wireType)
			}
			m.ClientID = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowSession
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ClientID |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field SeriesID", wireType)
			}
			m.SeriesID = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowSession
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.SeriesID |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field RespondedTo", wireType)
			}
			m.RespondedTo = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowSession
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.RespondedTo |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		default:
			iNdEx = preIndex
			skippy, err := skipSession(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthSession
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}

func skipSession(dAtA []byte) (n int, err error) {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return 0, ErrIntOverflowSession
			}
			if iNdEx >= l {
				return 0, io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		wireType := int(wire & 0x7)
		switch wireType {
		case 0:
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowSession
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				iNdEx++
				if dAtA[iNdEx-1] < 0x80 {
					break
				}
			}
			return iNdEx, nil
		case 1:
			iNdEx += 8
			return iNdEx, nil
		case 2:
			var length int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowSession
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				length |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			iNdEx += length
			if length < 0 {
				return 0, ErrInvalidLengthSession
			}
			return iNdEx, nil
		case 3:
			for {
				var innerWire uint64
				var start int = iNdEx
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return 0, ErrIntOverflowSession
					}
					if iNdEx >= l {
						return 0, io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					innerWire |= (uint64(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				innerWireType := int(innerWire & 0x7)
				if innerWireType == 4 {
					break
				}
				next, err := skipSession(dAtA[start:])
				if err != nil {
					return 0, err
				}
				iNdEx = start + next
			}
			return iNdEx, nil
		case 4:
			return iNdEx, nil
		case 5:
			iNdEx += 4
			return iNdEx, nil
		default:
			return 0, fmt.Errorf("proto: illegal wireType %d", wireType)
		}
	}
	panic("unreachable")
}

// IsNoOPSession returns a boolean flag indicating whether the session instance
// is a NoOP session.
func (m *Session) IsNoOPSession() bool {
	return m.SeriesID == NoOPSeriesID
}

// ShardIDMustMatch asserts that the input shard id matches the shard id
// of the client session.
func (m *Session) ShardIDMustMatch(shardID uint64) {
	if m.ShardID != shardID {
		panic("shard id do not match")
	}
}

// PrepareForRegister sets the series id to the special series id for
// registering client session.
func (m *Session) PrepareForRegister() {
	m.assertRegularSession()
	m.SeriesID = SeriesIDForRegister
}

// PrepareForUnregister sets the series id to the special series id for
// unregistering client session.
func (m *Session) PrepareForUnregister() {
	m.assertRegularSession()
	m.SeriesID = SeriesIDForUnregister
}

// PrepareForPropose sets the series id to the first series id that can be used
// for making proposals.
func (m *Session) PrepareForPropose() {
	m.assertRegularSession()
	m.SeriesID = SeriesIDFirstProposal
}

// ProposalCompleted increases the series id and the RespondedTo value.
// ProposalCompleted is expected to be called by the application every time
// when a proposal is completed or aborted by the application.
func (m *Session) ProposalCompleted() {
	m.assertRegularSession()
	if m.SeriesID == m.RespondedTo+1 {
		m.RespondedTo = m.SeriesID
		m.SeriesID++
	} else {
		panic("invalid responded to/series id values")
	}
}

func (m *Session) assertRegularSession() {
	if m.ClientID == NotSessionManagedClientID ||
		m.SeriesID == NoOPSeriesID {
		panic("not a regular session")
	}
}

// ValidForProposal checks whether the client session object is valid for
// making proposals.
func (m *Session) ValidForProposal(shardID uint64) bool {
	if m.SeriesID == NoOPSeriesID && m.ClientID == NotSessionManagedClientID {
		return false
	}
	if m.ShardID != shardID {
		return false
	}
	if m.ClientID == NotSessionManagedClientID {
		return false
	}
	if m.SeriesID == SeriesIDForRegister ||
		m.SeriesID == SeriesIDForUnregister {
		return false
	}
	if m.RespondedTo > m.SeriesID {
		panic("m.RespondedTo > m.SeriesID")
	}
	return true
}

// ValidForSessionOp checks whether the client session is valid for
// making client session related proposals, e.g. registering or unregistering
// a client session.
func (m *Session) ValidForSessionOp(shardID uint64) bool {
	if m.ShardID != shardID {
		return false
	}
	if m.ClientID == NotSessionManagedClientID ||
		m.SeriesID == NoOPSeriesID {
		return false
	}
	if m.SeriesID == SeriesIDForRegister ||
		m.SeriesID == SeriesIDForUnregister {
		return true
	}
	return false
}
````

## File: config/config_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package config

import (
	"reflect"
	"testing"

	"github.com/lni/dragonboat/v4/raftio"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

func ExampleNodeHostConfig() {
	nhc := NodeHostConfig{
		WALDir:         "/data/wal",
		NodeHostDir:    "/data/dragonboat-data",
		RTTMillisecond: 200,
		// RaftAddress is the public address that will be used by others to contact
		// this NodeHost instance.
		RaftAddress: "node01.raft.company.com:5012",
		// ListenAddress is the local address to listen on. This field is typically
		// set when there is port forwarding involved, e.g. your docker container
		// might has a private address of 172.17.0.2 when the public address of the
		// host is node01.raft.company.com and tcp port 5012 has been published.
		ListenAddress: "172.17.0.2:5012",
	}
	_ = nhc
}

func checkValidAddress(t *testing.T, addr string) {
	assert.True(t, IsValidAddress(addr),
		"valid addr %s considreed as invalid", addr)
}

func checkInvalidAddress(t *testing.T, addr string) {
	assert.False(t, IsValidAddress(addr),
		"invalid addr %s considered as valid", addr)
}

func TestListenAddress(t *testing.T) {
	nhc := NodeHostConfig{
		ListenAddress: "listen.address:12345",
		RaftAddress:   "raft.address:23456",
	}
	assert.Equal(t, nhc.ListenAddress, nhc.GetListenAddress(),
		"unexpected listen address %s, want %s",
		nhc.GetListenAddress(), nhc.ListenAddress)
	nhc.ListenAddress = ""
	assert.Equal(t, nhc.RaftAddress, nhc.GetListenAddress(),
		"unexpected listen address %s, want %s",
		nhc.GetListenAddress(), nhc.RaftAddress)
}

func TestIsValidAddress(t *testing.T) {
	va := []string{
		"192.0.0.1:12345",
		"202.96.1.23:1234",
		"myhost:214",
		"0.0.0.0:12345",
		"node1.mydomain.com.cn:12345",
		"myhost.test:12345",
		"    myhost.test:12345 ",
	}
	for _, v := range va {
		checkValidAddress(t, v)
	}
	iva := []string{
		"192.168.0.1",
		"myhost",
		"192.168.0.1:",
		"192.168.0.1:0",
		"192.168.0.1:65536",
		"192.168.0.1:-1",
		":12345",
		":",
		"#$:%",
		"mytest:again",
		"myhost:",
		// FIXME:
		// current validator consider the below two as valid
		// "345.168.0.1:12345",
		// "192.345.0.1:12345",
		// "192.168.345.1:12345",
		// "192.168.1.345:12345",
		"192 .168.0.1:12345",
		"myhost :12345",
		"",
		"    ",
	}
	for _, v := range iva {
		checkInvalidAddress(t, v)
	}
}

func TestWitnessNodeCanNotBeNonVoting(t *testing.T) {
	cfg := Config{IsWitness: true, IsNonVoting: true}
	err := cfg.Validate()
	require.Error(t, err, "witness node can not be an observer")
}

func TestWitnessCanNotTakeSnapshot(t *testing.T) {
	cfg := Config{IsWitness: true, SnapshotEntries: 100}
	err := cfg.Validate()
	require.Error(t, err, "witness node can not take snapshot")
}

func TestLogDBConfigIsEmpty(t *testing.T) {
	cfg := LogDBConfig{}
	assert.True(t, cfg.IsEmpty(), "not empty")
	cfg.KVMaxBackgroundCompactions = 1
	assert.False(t, cfg.IsEmpty(), "still empty")
}

func TestLogDBConfigMemSize(t *testing.T) {
	c := GetDefaultLogDBConfig()
	assert.Equal(t, uint64(8192), c.MemorySizeMB(),
		"unexpected default memory size")
	c1 := GetTinyMemLogDBConfig()
	assert.Equal(t, uint64(256), c1.MemorySizeMB(),
		"size %d, want 256", c1.MemorySizeMB())
	c2 := GetSmallMemLogDBConfig()
	assert.Equal(t, uint64(1024), c2.MemorySizeMB(),
		"size %d, want 1024", c2.MemorySizeMB())
	c3 := GetMediumMemLogDBConfig()
	assert.Equal(t, uint64(4096), c3.MemorySizeMB(),
		"size %d, want 4096", c3.MemorySizeMB())
	c4 := GetLargeMemLogDBConfig()
	assert.Equal(t, uint64(8192), c4.MemorySizeMB(),
		"size %d, want 8192", c4.MemorySizeMB())
}

func TestTransportFactoryAndModuleCanNotBeSetTogether(t *testing.T) {
	m := &defaultTransport{}
	c := NodeHostConfig{
		RaftAddress:    "localhost:9010",
		RTTMillisecond: 100,
		NodeHostDir:    "/data",
		RaftRPCFactory: m.Create,
	}
	err := c.Validate()
	require.NoError(t, err, "cfg not valid")
	c.Expert.TransportFactory = m
	err = c.Validate()
	require.Error(t, err, "cfg not considered as invalid")
}

func TestLogDBFactoryAndExpertLogDBFactoryCanNotBeSetTogether(t *testing.T) {
	f := func(NodeHostConfig,
		LogDBCallback, []string, []string) (raftio.ILogDB, error) {
		return nil, nil
	}
	c := NodeHostConfig{
		RaftAddress:    "localhost:9010",
		RTTMillisecond: 100,
		NodeHostDir:    "/data",
		LogDBFactory:   LogDBFactoryFunc(f),
	}
	err := c.Validate()
	require.NoError(t, err, "cfg not valid")
	c.Expert.LogDBFactory = &defaultLogDB{}
	err = c.Validate()
	require.Error(t, err, "cfg not considered as invalid")
}

func TestGossipMustBeConfiguredWhenDefaultNodeRegistryEnabled(t *testing.T) {
	c := NodeHostConfig{
		RaftAddress:    "localhost:9010",
		RTTMillisecond: 100,
		NodeHostDir:    "/data",
	}
	err := c.Validate()
	require.NoError(t, err, "invalid config")
	c.DefaultNodeRegistryEnabled = true
	err = c.Validate()
	require.Error(t, err, "unexpectedly considreed as valid config")
	c.Gossip = GossipConfig{
		BindAddress: "localhost:12345",
		Seed:        []string{"localhost:23456"},
	}
	err = c.Validate()
	require.NoError(t, err, "invalid config")
}

func TestGossipConfigIsEmtpy(t *testing.T) {
	gc := &GossipConfig{}
	assert.True(t, gc.IsEmpty(), "not empty")
	tests := []struct {
		bindAddr      string
		advertiseAddr string
		seed          []string
		empty         bool
	}{
		{"localhost:12345", "", []string{}, false},
		{"", "localhost:12345", []string{}, false},
		{"", "", []string{}, true},
		{"", "", []string{"127.0.0.1:12345"}, false},
	}
	for idx, tt := range tests {
		gc := &GossipConfig{
			BindAddress:      tt.bindAddr,
			AdvertiseAddress: tt.advertiseAddr,
			Seed:             tt.seed,
		}
		assert.Equal(t, tt.empty, gc.IsEmpty(),
			"%d, got %t, want %t", idx, gc.IsEmpty(), tt.empty)
	}
}

func TestGossipConfigValidate(t *testing.T) {
	tests := []struct {
		bindAddr      string
		advertiseAddr string
		seed          []string
		valid         bool
	}{
		{"114.1.1.1:12345", "202.23.45.1:12345", []string{"128.0.0.1:1234"}, true},
		{"myhost.com:12345", "202.23.45.1:12345", []string{"128.0.0.1:1234"}, true},
		{"myhost.com:12345", "", []string{"128.0.0.1:1234"}, true},
		{"", "202.23.45.1:12345", []string{"128.0.0.1:1234"}, false},
		{"myhost.com", "202.23.45.1:12345", []string{"128.0.0.1:1234"}, false},
		{"myhost.com:12345", "myhost2.net:12345", []string{"128.0.0.1:1234"}, false},
		{"myhost.com:12345", "202.23.45.1:12345", []string{}, false},
		{"myhost.com:12345", "202.23.45.1:12345", []string{"myhost.com:12345"}, false},
		{"myhost.com:12345", "202.23.45.1:12345", []string{"202.23.45.1:12345"}, false},
		{"myhost.com:12345", "202.23.45.1", []string{"128.0.0.1:1234"}, false},
		{"myhost.com:12345", "202.23.45.1:12345", []string{"128.0.0.1"}, false},
		{"myhost.com:12345", ":12345", []string{"128.0.0.1:12345"}, false},
		// FIXME:
		// current validator consider this as valid
		// {"300.0.0.1:12345", "202.23.45.1:12345", []string{"128.0.0.1:12345"}, false},
		{"myhost.com:66345", "202.23.45.1:12345", []string{"128.0.0.1:12345"}, false},
		{"myhost.com:12345", "302.23.45.1:12345", []string{"128.0.0.1:12345"}, false},
		{"myhost.com:12345", "202.23.45.1:72345", []string{"128.0.0.1:12345"}, false},
		// FIXME:
		// current validator consider this as valid
		// {"myhost.com:12345", "202.23.45.1:12345", []string{"328.0.0.1:12345"}, false},
		{"myhost.com:12345", "202.23.45.1:12345", []string{"128.0.0.1:65536"}, false},
		{"myhost.com:12345", "202.23.45.1:12345", []string{"128.0.0.1::12345"}, false},
		{"myhost.com:12345", "202.23.45.1::12345", []string{"128.0.0.1:12345"}, false},
		{"myhost.com::12345", "202.23.45.1:12345", []string{"128.0.0.1:12345"}, false},
		{"node1:12345", "202.96.23.1:12345", []string{"node3:12345", "node4:12345"}, true},
	}
	for idx, tt := range tests {
		gc := &GossipConfig{
			BindAddress:      tt.bindAddr,
			AdvertiseAddress: tt.advertiseAddr,
			Seed:             tt.seed,
		}
		err := gc.Validate()
		if tt.valid {
			assert.NoError(t, err, "%d, err: %v, valid: %t", idx, err, tt.valid)
		} else {
			assert.Error(t, err, "%d, err: %v, valid: %t", idx, err, tt.valid)
		}
	}
}

func TestDefaultEngineConfig(t *testing.T) {
	nhc := &NodeHostConfig{}
	err := nhc.Prepare()
	require.NoError(t, err, "prepare failed, %v", err)
	ec := GetDefaultEngineConfig()
	assert.True(t, reflect.DeepEqual(&nhc.Expert.Engine, &ec),
		"default engine configure not set")
}
````

## File: config/config.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

/*
Package config contains functions and types used for managing dragonboat's
configurations.
*/
package config

import (
	"crypto/tls"
	"net"
	"path/filepath"
	"reflect"
	"strconv"
	"time"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/netutil"
	"github.com/lni/goutils/stringutil"

	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/id"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/logger"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

var (
	plog = logger.GetLogger("config")
)

const (
	// don't change these, see the comments on ExpertConfig.
	defaultExecShards  uint64 = 16
	defaultLogDBShards uint64 = 16
)

// CompressionType is the type of the compression.
type CompressionType = pb.CompressionType

const (
	// NoCompression is the CompressionType value used to indicate not to use
	// any compression.
	NoCompression CompressionType = pb.NoCompression
	// Snappy is the CompressionType value used to indicate that google snappy
	// is used for data compression.
	Snappy CompressionType = pb.Snappy
)

// Config is used to configure Raft nodes.
type Config struct {
	// ReplicaID is a non-zero value used to identify a node within a Raft shard.
	ReplicaID uint64
	// ShardID is the unique value used to identify a Raft group that contains
	// multiple replicas.
	ShardID uint64
	// CheckQuorum specifies whether the leader node should periodically check
	// non-leader node status and step down to become a follower node when it no
	// longer has the quorum.
	CheckQuorum bool
	// Whether to use PreVote for this node. PreVote is described in the section
	// 9.7 of the raft thesis.
	PreVote bool
	// ElectionRTT is the minimum number of message RTT between elections. Message
	// RTT is defined by NodeHostConfig.RTTMillisecond. The Raft paper suggests it
	// to be a magnitude greater than HeartbeatRTT, which is the interval between
	// two heartbeats. In Raft, the actual interval between elections is
	// randomized to be between ElectionRTT and 2 * ElectionRTT.
	//
	// As an example, assuming NodeHostConfig.RTTMillisecond is 100 millisecond,
	// to set the election interval to be 1 second, then ElectionRTT should be set
	// to 10.
	//
	// When CheckQuorum is enabled, ElectionRTT also defines the interval for
	// checking leader quorum.
	ElectionRTT uint64
	// HeartbeatRTT is the number of message RTT between heartbeats. Message
	// RTT is defined by NodeHostConfig.RTTMillisecond. The Raft paper suggest the
	// heartbeat interval to be close to the average RTT between nodes.
	//
	// As an example, assuming NodeHostConfig.RTTMillisecond is 100 millisecond,
	// to set the heartbeat interval to be every 200 milliseconds, then
	// HeartbeatRTT should be set to 2.
	HeartbeatRTT uint64
	// SnapshotEntries defines how often the state machine should be snapshotted
	// automatically. It is defined in terms of the number of applied Raft log
	// entries. SnapshotEntries can be set to 0 to disable such automatic
	// snapshotting.
	//
	// When SnapshotEntries is set to N, it means a snapshot is created for
	// roughly every N applied Raft log entries (proposals). This also implies
	// that sending N log entries to a follower is more expensive than sending a
	// snapshot.
	//
	// Once a snapshot is generated, Raft log entries covered by the new snapshot
	// can be compacted. This involves two steps, redundant log entries are first
	// marked as deleted, then they are physically removed from the underlying
	// storage when a LogDB compaction is issued at a later stage. See the godoc
	// on CompactionOverhead for details on what log entries are actually removed
	// and compacted after generating a snapshot.
	//
	// Once automatic snapshotting is disabled by setting the SnapshotEntries
	// field to 0, users can still use NodeHost's RequestSnapshot or
	// SyncRequestSnapshot methods to manually request snapshots.
	SnapshotEntries uint64
	// CompactionOverhead defines the number of most recent entries to keep after
	// each Raft log compaction. Raft log compaction is performed automatically
	// every time a snapshot is created.
	//
	// For example, when a snapshot is created at let's say index 10,000, then all
	// Raft log entries with index <= 10,000 can be removed from that node as they
	// have already been covered by the created snapshot image. This frees up the
	// maximum storage space but comes at the cost that the full snapshot will
	// have to be sent to the follower if the follower requires any Raft log entry
	// at index <= 10,000. When CompactionOverhead is set to say 500, Dragonboat
	// then compacts the Raft log up to index 9,500 and keeps Raft log entries
	// between index (9,500, 10,000]. As a result, the node can still replicate
	// Raft log entries between index (9,500, 10,000] to other peers and only fall
	// back to stream the full snapshot if any Raft log entry with index <= 9,500
	// is required to be replicated.
	CompactionOverhead uint64
	// OrderedConfigChange determines whether Raft membership change is enforced
	// with ordered config change ID.
	//
	// When set to true, ConfigChangeIndex is required for membership change
	// requests. This behaves like an optimistic write lock forcing clients to
	// linearize membership change requests explicitly. (recommended)
	//
	// When set to false (default), ConfigChangeIndex is ignored for membership
	// change requests. This may cause a client to request a membership change
	// based on stale membership data.
	OrderedConfigChange bool
	// MaxInMemLogSize is the target size in bytes allowed for storing in memory
	// Raft logs on each Raft node. In memory Raft logs are the ones that have
	// not been applied yet.
	// MaxInMemLogSize is a target value implemented to prevent unbounded memory
	// growth, it is not for precisely limiting the exact memory usage.
	// When MaxInMemLogSize is 0, the target is set to math.MaxUint64. When
	// MaxInMemLogSize is set and the target is reached, error will be returned
	// when clients try to make new proposals.
	// MaxInMemLogSize is recommended to be significantly larger than the biggest
	// proposal you are going to use.
	MaxInMemLogSize uint64
	// SnapshotCompressionType is the compression type to use for compressing
	// generated snapshot data. No compression is used by default.
	SnapshotCompressionType CompressionType
	// EntryCompressionType is the compression type to use for compressing the
	// payload of user proposals. When Snappy is used, the maximum proposal
	// payload allowed is roughly limited to 3.42GBytes. No compression is used
	// by default.
	EntryCompressionType CompressionType
	// DisableAutoCompactions disables auto compaction used for reclaiming Raft
	// log entry storage spaces. By default, compaction request is issued every
	// time when a snapshot is created, this helps to reclaim disk spaces as
	// soon as possible at the cost of immediate higher IO overhead. Users can
	// disable such auto compactions and use NodeHost.RequestCompaction to
	// manually request such compactions when necessary.
	DisableAutoCompactions bool
	// IsNonVoting indicates whether this is a non-voting Raft node. Described as
	// non-voting members in the section 4.2.1 of Diego Ongaro's thesis, they are
	// used to allow a new node to join the shard and catch up with other
	// existing ndoes without impacting the availability. Extra non-voting nodes
	// can also be introduced to serve read-only requests.
	IsNonVoting bool
	// IsObserver indicates whether this is a non-voting Raft node without voting
	// power.
	//
	// Deprecated: use IsNonVoting instead.
	IsObserver bool
	// IsWitness indicates whether this is a witness Raft node without actual log
	// replication and do not have state machine. It is mentioned in the section
	// 11.7.2 of Diego Ongaro's thesis.
	//
	// Witness support is currently experimental.
	IsWitness bool
	// Quiesce specifies whether to let the Raft shard enter quiesce mode when
	// there is no shard activity. Shards in quiesce mode do not exchange
	// heartbeat messages to minimize bandwidth consumption.
	//
	// Quiesce support is currently experimental.
	Quiesce bool
	// WaitReady specifies whether to wait for the node to transition
	// from recovering to ready state before returning from StartReplica.
	WaitReady bool
}

// Validate validates the Config instance and return an error when any member
// field is considered as invalid.
func (c *Config) Validate() error {
	if c.ReplicaID == 0 {
		return errors.New("invalid ReplicaID, it must be >= 1")
	}
	if c.HeartbeatRTT == 0 {
		return errors.New("HeartbeatRTT must be > 0")
	}
	if c.ElectionRTT == 0 {
		return errors.New("ElectionRTT must be > 0")
	}
	if c.ElectionRTT <= 2*c.HeartbeatRTT {
		return errors.New("invalid election rtt")
	}
	if c.ElectionRTT < 10*c.HeartbeatRTT {
		plog.Warningf("ElectionRTT is not a magnitude larger than HeartbeatRTT")
	}
	if c.MaxInMemLogSize > 0 &&
		c.MaxInMemLogSize < settings.EntryNonCmdFieldsSize+1 {
		return errors.New("MaxInMemLogSize is too small")
	}
	if c.SnapshotCompressionType != Snappy &&
		c.SnapshotCompressionType != NoCompression {
		return errors.New("unknown compression type")
	}
	if c.EntryCompressionType != Snappy &&
		c.EntryCompressionType != NoCompression {
		return errors.New("unknown compression type")
	}
	if c.IsWitness && c.SnapshotEntries > 0 {
		return errors.New("witness node can not take snapshot")
	}
	if c.IsObserver {
		c.IsNonVoting = true
	}
	if c.IsWitness && c.IsNonVoting {
		return errors.New("witness node can not be a non-voting node")
	}
	return nil
}

// NodeHostConfig is the configuration used to configure NodeHost instances.
type NodeHostConfig struct {
	// DeploymentID is used to determine whether two NodeHost instances belong to
	// the same deployment and thus allowed to communicate with each other. This
	// helps to prvent accidentially misconfigured NodeHost instances to cause
	// data corruption errors by sending out of context messages to unrelated
	// Raft nodes.
	// For a particular dragonboat based application, you can set DeploymentID
	// to the same uint64 value on all production NodeHost instances, then use
	// different DeploymentID values on your staging and dev environment. It is
	// also recommended to use different DeploymentID values for different
	// dragonboat based applications.
	// When not set, the default value 0 will be used as the deployment ID and
	// thus allowing all NodeHost instances with deployment ID 0 to communicate
	// with each other.
	DeploymentID uint64
	// NodeHostID specifies what NodeHostID to use. By default, when NodeHostID
	// is empty, a random UUID will be generated and recorded by the system.
	// Specifying a concrete NodeHostID here will cause the specified NodeHostID
	// value to be used. NodeHostID is only used when DefaultNodeRegistryEnabled is
	// set to true.
	NodeHostID string
	// WALDir is the directory used for storing the WAL of Raft entries. It is
	// recommended to use low latency storage such as NVME SSD with power loss
	// protection to store such WAL data. Leave WALDir to have zero value will
	// have everything stored in NodeHostDir.
	WALDir string
	// NodeHostDir is where everything else is stored.
	NodeHostDir string
	// RTTMillisecond defines the average Round Trip Time (RTT) in milliseconds
	// between two NodeHost instances. Such a RTT interval is internally used as
	// a logical clock tick, Raft heartbeat and election intervals are both
	// defined in terms of how many such logical clock ticks (RTT intervals).
	// Note that RTTMillisecond is the combined delays between two NodeHost
	// instances including all delays caused by network transmission, delays
	// caused by NodeHost queuing and processing. As an example, when fully
	// loaded, the average Round Trip Time between two of our NodeHost instances
	// used for benchmarking purposes is up to 500 microseconds when the ping time
	// between them is 100 microseconds. Set RTTMillisecond to 1 when it is less
	// than 1 million in your environment.
	RTTMillisecond uint64
	// RaftAddress is a DNS name:port or IP:port address used by the transport
	// module for exchanging Raft messages, snapshots and metadata between
	// NodeHost instances. It should be set to the public address that can be
	// accessed from remote NodeHost instances.
	//
	// When the NodeHostConfig.ListenAddress field is empty, NodeHost listens on
	// RaftAddress for incoming Raft messages. When hostname or domain name is
	// used, it will be resolved to IPv4 addresses first and Dragonboat listens
	// to all resolved IPv4 addresses.
	//
	// By default, the RaftAddress value is not allowed to change between NodeHost
	// restarts. DefaultNodeRegistryEnabled should be set to true when the RaftAddress
	// value might change after restart.
	RaftAddress string
	// DefaultNodeRegistryEnabled indicates that NodeHost instances should be addressed
	// by their NodeHostID values. This feature is usually used when only dynamic
	// addresses are available. When enabled, NodeHostID values should be used
	// as the target parameter when calling NodeHost's StartReplica,
	// RequestAddReplica, RequestAddNonVoting and RequestAddWitness methods.
	//
	// Enabling DefaultNodeRegistryEnabled also enables the internal gossip service,
	// NodeHostConfig.Gossip must be configured to control the behaviors of the
	// gossip service.
	//
	// Note that once enabled, the DefaultNodeRegistryEnabled setting can not be later
	// disabled after restarts.
	//
	// Please see the godocs of the NodeHostConfig.Gossip field for a detailed
	// example on how DefaultNodeRegistryEnabled and gossip works.
	DefaultNodeRegistryEnabled bool
	// ListenAddress is an optional field in the hostname:port or IP:port address
	// form used by the transport module to listen on for Raft message and
	// snapshots. When the ListenAddress field is not set, The transport module
	// listens on RaftAddress. If 0.0.0.0 is specified as the IP of the
	// ListenAddress, Dragonboat listens to the specified port on all network
	// interfaces. When hostname or domain name is used, it will be resolved to
	// IPv4 addresses first and Dragonboat listens to all resolved IPv4 addresses.
	ListenAddress string
	// MutualTLS defines whether to use mutual TLS for authenticating servers
	// and clients. Insecure communication is used when MutualTLS is set to
	// False.
	// See https://github.com/lni/dragonboat/wiki/TLS-in-Dragonboat for more
	// details on how to use Mutual TLS.
	MutualTLS bool
	// CAFile is the path of the CA certificate file. This field is ignored when
	// MutualTLS is false.
	CAFile string
	// CertFile is the path of the node certificate file. This field is ignored
	// when MutualTLS is false.
	CertFile string
	// KeyFile is the path of the node key file. This field is ignored when
	// MutualTLS is false.
	KeyFile string
	// LogDBFactory is the factory function used for creating the Log DB instance
	// used by NodeHost. The default zero value causes the default built-in RocksDB
	// based Log DB implementation to be used.
	//
	// Deprecated: Use NodeHostConfig.Expert.LogDBFactory instead.
	LogDBFactory LogDBFactoryFunc
	// RaftRPCFactory is the factory function used for creating the transport
	// instance for exchanging Raft message between NodeHost instances. The default
	// zero value causes the built-in TCP based transport to be used.
	//
	// Deprecated: Use NodeHostConfig.Expert.TransportFactory instead.
	RaftRPCFactory RaftRPCFactoryFunc
	// EnableMetrics determines whether health metrics in Prometheus format should
	// be enabled.
	EnableMetrics bool
	// RaftEventListener is the listener for Raft events, such as Raft leadership
	// change, exposed to user space. NodeHost uses a single dedicated goroutine
	// to invoke all RaftEventListener methods one by one, CPU intensive or IO
	// related procedures that can cause long delays should be offloaded to worker
	// goroutines managed by users. See the raftio.IRaftEventListener definition
	// for more details.
	RaftEventListener raftio.IRaftEventListener
	// SystemEventsListener allows users to be notified for system events such
	// as snapshot creation, log compaction and snapshot streaming. It is usually
	// used for testing purposes or for other advanced usages, Dragonboat
	// applications are not required to explicitly set this field.
	SystemEventListener raftio.ISystemEventListener
	// MaxSendQueueSize is the maximum size in bytes of each send queue.
	// Once the maximum size is reached, further replication messages will be
	// dropped to restrict memory usage. When set to 0, it means the send queue
	// size is unlimited.
	MaxSendQueueSize uint64
	// MaxReceiveQueueSize is the maximum size in bytes of each receive queue.
	// Once the maximum size is reached, further replication messages will be
	// dropped to restrict memory usage. When set to 0, it means the queue size
	// is unlimited.
	MaxReceiveQueueSize uint64
	// NotifyCommit specifies whether clients should be notified when their
	// regular proposals and config change requests are committed. By default,
	// commits are not notified, clients are only notified when their proposals
	// are both committed and applied.
	NotifyCommit bool
	// Gossip contains configurations for the gossip service. When the
	// DefaultNodeRegistryEnabled field is set to true, each NodeHost instance will use
	// an internal gossip service to exchange knowledges of known NodeHost
	// instances including their RaftAddress and NodeHostID values. This Gossip
	// field contains configurations that controls how the gossip service works.
	//
	// As an detailed example on how to use the gossip service in the situation
	// where all available machines have dynamically assigned IPs on reboot -
	//
	// Consider that there are three NodeHost instances on three machines, each
	// of them has a dynamically assigned IP address which will change on reboot.
	// NodeHostConfig.RaftAddress should be set to the current address that can be
	// reached by remote NodeHost instance. In this example, we will assume they
	// are
	//
	// 10.0.0.100:24000
	// 10.0.0.200:24000
	// 10.0.0.300:24000
	//
	// To use these machines, first enable the NodeHostConfig.DefaultNodeRegistryEnabled
	// field and start the NodeHost instances. The NodeHostID value of each
	// NodeHost instance can be obtained by calling NodeHost.ID(). Let's say they
	// are
	//
	// "nhid-xxxxx",
	// "nhid-yyyyy",
	// "nhid-zzzzz".
	//
	// All these NodeHostID are fixed, they will never change after reboots.
	//
	// When starting Raft nodes or requesting new nodes to be added, use the above
	// mentioned NodeHostID values as the target parameters (which are of the
	// Target type). Let's say we want to start a Raft Node as a part of a three
	// replica Raft shard, the initialMembers parameter of the StartReplica
	// method can be set to
	//
	// initialMembers := map[uint64]Target {
	// 	 1: "nhid-xxxxx",
	//   2: "nhid-yyyyy",
	//   3: "nhid-zzzzz",
	// }
	//
	// This indicates that node 1 of the shard will be running on the NodeHost
	// instance identified by the NodeHostID value "nhid-xxxxx", node 2 of the
	// same shard will be running on the NodeHost instance identified by the
	// NodeHostID value of "nhid-yyyyy" and so on.
	//
	// The internal gossip service exchanges NodeHost details, including their
	// NodeHostID and RaftAddress values, with all other known NodeHost instances.
	// Thanks to the nature of gossip, it will eventually allow each NodeHost
	// instance to be aware of the current details of all NodeHost instances.
	// As a result, let's say when Raft node 1 wants to send a Raft message to
	// node 2, it first figures out that node 2 is running on the NodeHost
	// identified by the NodeHostID value "nhid-yyyyy", RaftAddress information
	// from the gossip service further shows that "nhid-yyyyy" maps to a machine
	// currently reachable at 10.0.0.200:24000. Raft messages can thus be
	// delivered.
	//
	// The Gossip field here is used to configure how the gossip service works.
	// In this example, let's say we choose to use the following configurations
	// for those three NodeHost instaces.
	//
	// GossipConfig {
	//   BindAddress: "10.0.0.100:24001",
	//   Seed: []string{10.0.0.200:24001},
	// }
	//
	// GossipConfig {
	//   BindAddress: "10.0.0.200:24001",
	//   Seed: []string{10.0.0.300:24001},
	// }
	//
	// GossipConfig {
	//   BindAddress: "10.0.0.300:24001",
	//   Seed: []string{10.0.0.100:24001},
	// }
	//
	// For those three machines, the gossip component listens on
	// "10.0.0.100:24001", "10.0.0.200:24001" and "10.0.0.300:24001" respectively
	// for incoming gossip messages. The Seed field is a list of known gossip end
	// points the local gossip service will try to talk to. The Seed field doesn't
	// need to include all gossip end points, a few well connected nodes in the
	// gossip network is enough.
	//
	// Alternatively, if you wish to use a custom registry but manage it yourself,
	// the Expert.NodeRegistryFactory field can be set to provide a registry that
	// implements the raftio.INodeRegistry interface. A registry is simply a common
	// channel shared between all nodes that allows them to identify each other.
	Gossip GossipConfig

	// Expert contains options for expert users who are familiar with the internals
	// of Dragonboat. Users are recommended not to use this field unless
	// absolutely necessary. It is important to note that any change to this field
	// may cause an existing instance unable to restart, it may also cause negative
	// performance impacts.
	Expert ExpertConfig
}

// IFS is the filesystem interface used by tests.
type IFS = vfs.IFS

// TargetValidator is the validtor used to validate user specified target values.
type TargetValidator func(string) bool

// RaftAddressValidator is the validator used to validate user specified
// RaftAddress values.
type RaftAddressValidator func(string) bool

// LogDBFactory is the interface used for creating custom logdb modules.
type LogDBFactory interface {
	// Create creates a logdb module.
	Create(NodeHostConfig,
		LogDBCallback, []string, []string) (raftio.ILogDB, error)
	// Name returns the type name of the logdb module.
	Name() string
}

// NodeRegistryFactory is the interface used for providing a custom node registry.
// For a short example of how to implement a custom node registry, please see
// TestExternalNodeRegistryFunction in nodehost_test.go.
type NodeRegistryFactory interface {
	Create(nhid string, streamConnections uint64, v TargetValidator) (raftio.INodeRegistry, error)
}

// TransportFactory is the interface used for creating custom transport modules.
type TransportFactory interface {
	// Create creates a transport module.
	Create(NodeHostConfig,
		raftio.MessageHandler, raftio.ChunkHandler) raftio.ITransport
	// Validate validates the RaftAddress of the NodeHost. When using a custom
	// transport module, users are granted full control on what address type to
	// use for the NodeHostConfig.RaftAddress field, it can be of the traditional
	// IP:Port format or any other form. The Validate method is used to validate
	// that a received address is of the valid form.
	Validate(string) bool
}

// LogDBInfo is the info provided when LogDBCallback is invoked.
type LogDBInfo struct {
	Shard uint64
	Busy  bool
}

// LogDBCallback is called by the LogDB layer whenever NodeHost is required to
// be notified for the status change of the LogDB.
type LogDBCallback func(LogDBInfo)

// RaftRPCFactoryFunc is the factory function that creates the transport module
// instance for exchanging Raft messages between NodeHosts.
//
// Deprecated: Use TransportFactory instead.
type RaftRPCFactoryFunc func(NodeHostConfig,
	raftio.MessageHandler, raftio.ChunkHandler) raftio.ITransport

// LogDBFactoryFunc is the factory function that creates NodeHost's persistent
// storage module known as Log DB.
//
// Deprecated: Use LogDBFactory instead.
type LogDBFactoryFunc func(NodeHostConfig,
	LogDBCallback, []string, []string) (raftio.ILogDB, error)

// Validate validates the NodeHostConfig instance and return an error when
// the configuration is considered as invalid.
func (c *NodeHostConfig) Validate() error {
	if c.RTTMillisecond == 0 {
		return errors.New("invalid RTTMillisecond")
	}
	if len(c.NodeHostDir) == 0 {
		return errors.New("NodeHostConfig.NodeHostDir is empty")
	}
	if !c.MutualTLS {
		plog.Warningf("mutual TLS disabled, communication is insecure")
		if len(c.CAFile) > 0 || len(c.CertFile) > 0 || len(c.KeyFile) > 0 {
			plog.Warningf("CAFile/CertFile/KeyFile specified when MutualTLS is disabled")
		}
	}
	if c.MutualTLS {
		if len(c.CAFile) == 0 {
			return errors.New("CA file not specified")
		}
		if len(c.CertFile) == 0 {
			return errors.New("cert file not specified")
		}
		if len(c.KeyFile) == 0 {
			return errors.New("key file not specified")
		}
	}
	if c.MaxSendQueueSize > 0 &&
		c.MaxSendQueueSize < settings.EntryNonCmdFieldsSize+1 {
		return errors.New("MaxSendQueueSize value is too small")
	}
	if c.MaxReceiveQueueSize > 0 &&
		c.MaxReceiveQueueSize < settings.EntryNonCmdFieldsSize+1 {
		return errors.New("MaxReceiveSize value is too small")
	}
	if c.RaftRPCFactory != nil && c.Expert.TransportFactory != nil {
		return errors.New("both TransportFactory and RaftRPCFactory specified")
	}
	if c.LogDBFactory != nil && c.Expert.LogDBFactory != nil {
		return errors.New("both LogDBFactory and Expert.LogDBFactory specified")
	}
	if c.DefaultNodeRegistryEnabled && c.Gossip.IsEmpty() {
		return errors.New("gossip service not configured")
	}
	validate := c.GetRaftAddressValidator()
	if !validate(c.RaftAddress) {
		return errors.New("invalid NodeHost address")
	}
	if len(c.ListenAddress) > 0 && !validate(c.ListenAddress) {
		return errors.New("invalid ListenAddress")
	}
	if !c.Gossip.IsEmpty() {
		if err := c.Gossip.Validate(); err != nil {
			return err
		}
	}
	if !c.Expert.Engine.IsEmpty() {
		if err := c.Expert.Engine.Validate(); err != nil {
			return err
		}
	}
	return nil
}

type defaultTransport struct {
	factory RaftRPCFactoryFunc
}

func (tm *defaultTransport) Create(nhConfig NodeHostConfig,
	handler raftio.MessageHandler,
	chunkHandler raftio.ChunkHandler) raftio.ITransport {
	return tm.factory(nhConfig, handler, chunkHandler)
}

func (tm *defaultTransport) Validate(addr string) bool {
	return stringutil.IsValidAddress(addr)
}

type defaultLogDB struct {
	factory LogDBFactoryFunc
}

func (l *defaultLogDB) Create(nhConfig NodeHostConfig,
	cb LogDBCallback, dirs []string, wals []string) (raftio.ILogDB, error) {
	return l.factory(nhConfig, cb, dirs, wals)
}

func (l *defaultLogDB) Name() string {
	fs := vfs.DefaultFS
	dir, err := fileutil.TempDir("", "dragonboat-logdb-test", fs)
	if err != nil {
		panic(err)
	}
	defer func() {
		if err := fs.RemoveAll(dir); err != nil {
			panic(err)
		}
	}()
	nhc := NodeHostConfig{
		Expert: ExpertConfig{
			LogDB: GetDefaultLogDBConfig(),
			FS:    fs,
		},
	}
	ldb, err := l.factory(nhc, nil, []string{dir}, []string{})
	if err != nil {
		plog.Panicf("failed to create ldb, %v", err)
	}
	defer func() {
		if err := ldb.Close(); err != nil {
			panic(err)
		}
	}()
	return ldb.Name()
}

// Prepare sets the default value for NodeHostConfig.
func (c *NodeHostConfig) Prepare() error {
	var err error
	c.NodeHostDir, err = filepath.Abs(c.NodeHostDir)
	if err != nil {
		return err
	}
	if len(c.WALDir) > 0 {
		c.WALDir, err = filepath.Abs(c.WALDir)
		if err != nil {
			return err
		}
	}
	if c.Expert.FS == nil {
		c.Expert.FS = vfs.DefaultFS
	}
	if c.Expert.Engine.IsEmpty() {
		plog.Infof("using default EngineConfig")
		c.Expert.Engine = GetDefaultEngineConfig()
	}
	if c.Expert.LogDB.IsEmpty() {
		plog.Infof("using default LogDBConfig")
		c.Expert.LogDB = GetDefaultLogDBConfig()
	}
	if c.RaftRPCFactory != nil && c.Expert.TransportFactory == nil {
		c.Expert.TransportFactory = &defaultTransport{factory: c.RaftRPCFactory}
		c.RaftRPCFactory = nil
	}
	if c.LogDBFactory != nil && c.Expert.LogDBFactory == nil {
		c.Expert.LogDBFactory = &defaultLogDB{factory: c.LogDBFactory}
		c.LogDBFactory = nil
	}
	return nil
}

// NodeRegistryEnabled returns a bool indicating if any node registry is enabled.
func (c *NodeHostConfig) NodeRegistryEnabled() bool {
	return c.DefaultNodeRegistryEnabled || c.Expert.NodeRegistryFactory != nil
}

// GetListenAddress returns the actual address the transport module is going to
// listen on.
func (c *NodeHostConfig) GetListenAddress() string {
	if len(c.ListenAddress) > 0 {
		return c.ListenAddress
	}
	return c.RaftAddress
}

// GetServerTLSConfig returns the server tls.Config instance based on the
// TLS settings in NodeHostConfig.
func (c *NodeHostConfig) GetServerTLSConfig() (*tls.Config, error) {
	if c.MutualTLS {
		return netutil.GetServerTLSConfig(c.CAFile, c.CertFile, c.KeyFile)
	}
	return nil, nil
}

// GetClientTLSConfig returns the client tls.Config instance for the specified
// target based on the TLS settings in NodeHostConfig.
func (c *NodeHostConfig) GetClientTLSConfig(target string) (*tls.Config, error) {
	if c.MutualTLS {
		tlsConfig, err := netutil.GetClientTLSConfig("",
			c.CAFile, c.CertFile, c.KeyFile)
		if err != nil {
			return nil, err
		}
		host, err := netutil.GetHost(target)
		if err != nil {
			return nil, err
		}
		return &tls.Config{
			ServerName:   host,
			Certificates: tlsConfig.Certificates,
			RootCAs:      tlsConfig.RootCAs,
		}, nil
	}
	return nil, nil
}

// GetDeploymentID returns the deployment ID to be used.
func (c *NodeHostConfig) GetDeploymentID() uint64 {
	if c.DeploymentID == 0 {
		return settings.UnmanagedDeploymentID
	}
	return c.DeploymentID
}

// GetTargetValidator returns a TargetValidator based on the specified
// NodeHostConfig instance.
func (c *NodeHostConfig) GetTargetValidator() TargetValidator {
	if c.NodeRegistryEnabled() {
		return id.IsNodeHostID
	} else if c.Expert.TransportFactory != nil {
		return c.Expert.TransportFactory.Validate
	}
	return stringutil.IsValidAddress
}

// GetRaftAddressValidator creates a RaftAddressValidator based on the specified
// NodeHostConfig instance.
func (c *NodeHostConfig) GetRaftAddressValidator() RaftAddressValidator {
	if c.Expert.TransportFactory != nil {
		return c.Expert.TransportFactory.Validate
	}
	return stringutil.IsValidAddress
}

// IsValidAddress returns a boolean value indicating whether the input address
// is valid.
func IsValidAddress(addr string) bool {
	return stringutil.IsValidAddress(addr)
}

// LogDBConfig is the configuration object for the LogDB storage engine. This
// config option is only for advanced users when tuning the balance of I/O
// performance and memory consumption.
//
// All KV* fields in LogDBConfig had their names derived from RocksDB options,
// please check RocksDB Tuning Guide wiki for more details.
//
// KVWriteBufferSize and KVMaxWriteBufferNumber are two parameters that directly
// affect the upper bound of memory size used by the built-in LogDB storage
// engine.
type LogDBConfig struct {
	Shards                             uint64
	KVKeepLogFileNum                   uint64
	KVMaxBackgroundCompactions         uint64
	KVMaxBackgroundFlushes             uint64
	KVLRUCacheSize                     uint64
	KVWriteBufferSize                  uint64
	KVMaxWriteBufferNumber             uint64
	KVLevel0FileNumCompactionTrigger   uint64
	KVLevel0SlowdownWritesTrigger      uint64
	KVLevel0StopWritesTrigger          uint64
	KVMaxBytesForLevelBase             uint64
	KVMaxBytesForLevelMultiplier       uint64
	KVTargetFileSizeBase               uint64
	KVTargetFileSizeMultiplier         uint64
	KVLevelCompactionDynamicLevelBytes uint64
	KVRecycleLogFileNum                uint64
	KVNumOfLevels                      uint64
	KVBlockSize                        uint64
	SaveBufferSize                     uint64
	MaxSaveBufferSize                  uint64
}

// GetDefaultLogDBConfig returns the default configurations for the LogDB
// storage engine. The default LogDB configuration use up to 8GBytes memory.
func GetDefaultLogDBConfig() LogDBConfig {
	return GetLargeMemLogDBConfig()
}

// GetTinyMemLogDBConfig returns a LogDB config aimed for minimizing memory
// size. When using the returned config, LogDB takes up to 256MBytes memory.
func GetTinyMemLogDBConfig() LogDBConfig {
	cfg := getDefaultLogDBConfig()
	cfg.KVWriteBufferSize = 4 * 1024 * 1024
	cfg.KVMaxWriteBufferNumber = 4
	return cfg
}

// GetSmallMemLogDBConfig returns a LogDB config aimed to keep memory size at
// low level. When using the returned config, LogDB takes up to 1GBytes memory.
func GetSmallMemLogDBConfig() LogDBConfig {
	cfg := getDefaultLogDBConfig()
	cfg.KVWriteBufferSize = 16 * 1024 * 1024
	cfg.KVMaxWriteBufferNumber = 4
	return cfg
}

// GetMediumMemLogDBConfig returns a LogDB config aimed to keep memory size at
// medium level. When using the returned config, LogDB takes up to 4GBytes
// memory.
func GetMediumMemLogDBConfig() LogDBConfig {
	cfg := getDefaultLogDBConfig()
	cfg.KVWriteBufferSize = 64 * 1024 * 1024
	cfg.KVMaxWriteBufferNumber = 4
	return cfg
}

// GetLargeMemLogDBConfig returns a LogDB config aimed to keep memory size to be
// large for good I/O performance. It is the default setting used by the system.
// When using the returned config, LogDB takes up to 8GBytes memory.
func GetLargeMemLogDBConfig() LogDBConfig {
	return getDefaultLogDBConfig()
}

func getDefaultLogDBConfig() LogDBConfig {
	return LogDBConfig{
		Shards:                             defaultLogDBShards,
		KVMaxBackgroundCompactions:         2,
		KVMaxBackgroundFlushes:             2,
		KVLRUCacheSize:                     0,
		KVKeepLogFileNum:                   16,
		KVWriteBufferSize:                  128 * 1024 * 1024,
		KVMaxWriteBufferNumber:             4,
		KVLevel0FileNumCompactionTrigger:   8,
		KVLevel0SlowdownWritesTrigger:      17,
		KVLevel0StopWritesTrigger:          24,
		KVMaxBytesForLevelBase:             4 * 1024 * 1024 * 1024,
		KVMaxBytesForLevelMultiplier:       2,
		KVTargetFileSizeBase:               16 * 1024 * 1024,
		KVTargetFileSizeMultiplier:         2,
		KVLevelCompactionDynamicLevelBytes: 0,
		KVRecycleLogFileNum:                0,
		KVNumOfLevels:                      7,
		KVBlockSize:                        32 * 1024,
		SaveBufferSize:                     32 * 1024,
		MaxSaveBufferSize:                  64 * 1024 * 1024,
	}
}

// MemorySizeMB returns the estimated upper bound memory size used by the LogDB
// storage engine. The returned value is in MBytes.
func (cfg *LogDBConfig) MemorySizeMB() uint64 {
	ss := cfg.KVWriteBufferSize * cfg.KVMaxWriteBufferNumber
	bs := ss * cfg.Shards
	return bs / (1024 * 1024)
}

// IsEmpty returns a boolean value indicating whether the LogDBConfig instance
// is empty.
func (cfg *LogDBConfig) IsEmpty() bool {
	return reflect.DeepEqual(cfg, &LogDBConfig{})
}

// EngineConfig is the configuration for the execution engine.
type EngineConfig struct {
	// ExecShards is the number of execution shards in the first stage of the
	// execution engine. Default value is 16. Once deployed, this value can not
	// be changed later.
	ExecShards uint64
	// CommitShards is the number of commit shards in the second stage of the
	// execution engine. Default value is 16.
	CommitShards uint64
	// ApplyShards is the number of apply shards in the third stage of the
	// execution engine. Default value is 16.
	ApplyShards uint64
	// SnapshotShards is the number of snapshot shards in the forth stage of the
	// execution engine. Default value is 48.
	SnapshotShards uint64
	// CloseShards is the number of close shards used for closing stopped
	// state machines. Default value is 32.
	CloseShards uint64
}

// GetDefaultEngineConfig returns the default EngineConfig instance.
func GetDefaultEngineConfig() EngineConfig {
	return EngineConfig{
		ExecShards:     defaultExecShards,
		CommitShards:   16,
		ApplyShards:    16,
		SnapshotShards: 48,
		CloseShards:    32,
	}
}

// IsEmpty returns a boolean value indicating whether EngineConfig is an empty
// one.
func (ec EngineConfig) IsEmpty() bool {
	return reflect.DeepEqual(&ec, &EngineConfig{})
}

// Validate return an error value when the EngineConfig is invalid.
func (ec EngineConfig) Validate() error {
	if ec.ExecShards == 0 || ec.CommitShards == 0 || ec.ApplyShards == 0 ||
		ec.SnapshotShards == 0 || ec.CloseShards == 0 {
		return errors.New("invalid engine configuration")
	}
	return nil
}

// GetDefaultExpertConfig returns the default ExpertConfig.
func GetDefaultExpertConfig() ExpertConfig {
	return ExpertConfig{
		Engine: GetDefaultEngineConfig(),
		LogDB:  getDefaultLogDBConfig(),
	}
}

// ExpertConfig contains options for expert users who are familiar with the
// internals of Dragonboat. Users are recommended not to set ExpertConfig
// unless it is absoloutely necessary.
type ExpertConfig struct {
	// LogDBFactory is the factory function used for creating the LogDB instance
	// used by NodeHost. When not set, the default built-in Pebble based LogDB
	// implementation is used.
	LogDBFactory LogDBFactory
	// TransportFactory is an optional factory type used for creating the custom
	// transport module to be used by dragonbaot. When not set, the built-in TCP
	// transport module is used.
	TransportFactory TransportFactory
	// Engine is the configuration for the execution engine.
	Engine EngineConfig
	// LogDB contains configuration options for the LogDB storage engine. LogDB
	// is used for storing Raft Logs and metadata. This optional option is used
	// by advanced users for tuning the balance of I/O performance, memory and
	// disk usages.
	LogDB LogDBConfig
	// FS is the filesystem instance used in tests.
	FS IFS
	// TestGossipProbeInterval defines the probe interval used by the gossip
	// service in tests.
	TestGossipProbeInterval time.Duration
	// NodeRegistryFactory defines a custom node registry function that can be used
	// instead of a static registry or the built in memberlist gossip mechanism.
	NodeRegistryFactory NodeRegistryFactory
}

// GossipConfig contains configurations for the gossip service. Gossip service
// is a fully distributed networked service for exchanging knowledge on
// NodeHost instances. When enabled by the NodeHostConfig.DefaultNodeRegistryEnabled
// field, it is employed to manage NodeHostID to RaftAddress mappings of known
// NodeHost instances.
type GossipConfig struct {
	// BindAddress is the address for the gossip service to bind to and listen on.
	// Both UDP and TCP ports are used by the gossip service. The local gossip
	// service should be able to receive gossip service related messages by
	// binding to and listening on this address. BindAddress is usually in the
	// format of IP:Port, Hostname:Port or DNS Name:Port.
	BindAddress string
	// AdvertiseAddress is the address to advertise to other NodeHost instances
	// used for NAT traversal. Gossip services running on remote NodeHost
	// instances will use AdvertiseAddress to exchange gossip service related
	// messages. AdvertiseAddress is in the format of IP:Port.
	AdvertiseAddress string
	// Seed is a list of AdvertiseAddress of remote NodeHost instances. Local
	// NodeHost instance will try to contact all of them to bootstrap the gossip
	// service. At least one reachable NodeHost instance is required to
	// successfully bootstrap the gossip service. Each seed address is in the
	// format of IP:Port, Hostname:Port or DNS Name:Port.
	//
	// It is ok to include seed addresses that are temporarily unreachable, e.g.
	// when launching the first NodeHost instance in your deployment, you can
	// include AdvertiseAddresses from other NodeHost instances that you plan to
	// launch shortly afterwards.
	Seed []string
	// Meta is the extra metadata to be included in gossip node's Meta field. It
	// will be propagated to all other NodeHost instances via gossip.
	Meta []byte
}

// IsEmpty returns a boolean flag indicating whether the GossipConfig instance
// is empty.
func (g *GossipConfig) IsEmpty() bool {
	return len(g.BindAddress) == 0 &&
		len(g.AdvertiseAddress) == 0 && len(g.Seed) == 0
}

// Validate validates the GossipConfig instance.
func (g *GossipConfig) Validate() error {
	if len(g.BindAddress) > 0 && !stringutil.IsValidAddress(g.BindAddress) {
		return errors.New("invalid GossipConfig.BindAddress")
	} else if len(g.BindAddress) == 0 {
		return errors.New("BindAddress not set")
	}
	if len(g.AdvertiseAddress) > 0 && !isValidAdvertiseAddress(g.AdvertiseAddress) {
		return errors.New("invalid GossipConfig.AdvertiseAddress")
	}
	if len(g.Seed) == 0 {
		return errors.New("seed nodes not set")
	}
	count := 0
	for _, v := range g.Seed {
		if v != g.BindAddress && v != g.AdvertiseAddress {
			count++
		}
		if !stringutil.IsValidAddress(v) {
			return errors.New("invalid GossipConfig.Seed value")
		}
	}
	if count == 0 {
		return errors.New("no valid seed node")
	}
	return nil
}

func isValidAdvertiseAddress(addr string) bool {
	host, sp, err := net.SplitHostPort(addr)
	if err != nil {
		return false
	}
	port, err := strconv.ParseUint(sp, 10, 16)
	if err != nil {
		return false
	}
	if port > 65535 {
		return false
	}
	// the memberlist package doesn't allow hostname or DNS name to be used in
	// advertise address
	return stringutil.IPV4Regex.MatchString(host)
}
````

## File: docs/COPYRIGHT
````
The Dragonboat project is licensed under Apache 2.0 terms.

Dragonboat contains source code from the following open source projects. 
Please raise an issue if anything is missing from the list below.

pebble's wal code, which is partially based on the go version leveldb. 

BSD License

Copyright 2011 The LevelDB-Go and Pebble Authors. All rights reserved. Use
of this source code is governed by a BSD-style license that can be found in
the LICENSE file.

Copyright (c) 2011 The LevelDB-Go Authors. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

   * Redistributions of source code must retain the above copyright
notice, this list of conditions and the following disclaimer.
   * Redistributions in binary form must reproduce the above
copyright notice, this list of conditions and the following disclaimer
in the documentation and/or other materials provided with the
distribution.
   * Neither the name of Google Inc. nor the names of its
contributors may be used to endorse or promote products derived from
this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


etcd Raft's test cases and some raft protocol implementation ideas (Apache2)
https://github.com/etcd-io/etcd/tree/master/raft

Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.


etcd's repo has a NOTICE file with the following content

CoreOS Project
Copyright 2014 CoreOS, Inc

This product includes software developed at CoreOS, Inc.
(http://www.coreos.com/).


zhreshold's zupply C++11 portable filesystem access used in tests (MIT)
https://github.com/zhreshold/zupply

The MIT License (MIT)

Copyright (c) <2015> <Joshua Z. Zhang>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


cockroach's async message send pattern used in the internal/transport package 
https://github.com/cockroachdb/cockroach

Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright {}

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
````

## File: docs/devops.CHS.md
````markdown
# 运维注意事项 #

本文档描述使用dragonboat的应用在部署上线以后的运维注意事项。请注意，不正确的运维方式将可能直接导致数据的永久性毁损。

* 建议使用ext4文件系统，其它文件系统未做任何测试。
* 必须使用本地硬盘，建议使用高写入寿命的企业级NVME固态硬盘，避免使用NFS、CIFS、Samba或Ceph等任何形式网络共享存储方式。
* Dragonboat所生成存储的数据绝不可通过直接文件、目录拷贝覆盖操作来进行备份与恢复。这将永久性损坏所涉及的Raft组。
* 每个Raft组已有多份副本，增加Raft组的副本数量是避免因部分节点故障失效而带来服务不可用及数据丢失的最好解决途径。比如，5个副本允许至少2个节点同时发生故障，它较3副本带来更高的数据安全与系统高可用性保障。
* 在个别节点发生故障后，如果多数派Quorum依旧存在、Raft组依旧可用，应该首先增加一个non-voting节点以开始立即同步Raft组状态，待同步完成后立刻将其升级为普通节点，然后通过成员变更移除故障节点。对于间歇性的可立即恢复的硬件故障，比如短暂的网络分区或系统掉电，可立即试图修复故障机器。
* 如发生磁盘损坏，比如发生磁盘相关数据的校验错，在排除系软件故障引起后，应立即停止该节点并替换磁盘，并通过上述组成员变更方法替换已永久故障的节点。在重启已确认磁盘故障的节点前，应确保已通过磁盘替换方式完全清空所有Dragonboat数据，且使用新磁盘的该节点应使用新的RaftAddress值，在IP或者DNS Name无法变更的情况下，可使用其它端口来确保RaftAddress被更新。
* 在极端情况下，当多数节点同时永久故障并无法修复时，Raft组将不可用。此时须使用github.com/lni/dragonboat/tools包提供的ImportSnapshot工具修复受损的Raft组。这需要用户日常定期使用NodeHost的ExportSnapshot方法导出并备份快照供此灾备用途。
* 在默认方式下，NodeHost每次重启后其RaftAddress不能变化，否则将报错。
* 如无法直接满足上述RaftAddress不变的要求，比如IP地址会在每次机器重启后因随机分配而变更，可考虑是否可以使用DNS Name并配合适当配置管理，来达到RaftAddress始终不变的要求。
* 如依旧无法按照上述方法满足RaftAddress不变的需求，可通过设置DefaultNodeRegistryEnabled项来开启gossip功能，它被设计用来处理动态的RaftAddress的场景，具体请参阅文档。
* 用户应该自行测试系统是否具备高可用性，并测试在不同数量规模与组合的节点失效情况下用户系统是否可以正确处理并保持Raft组的高可用。各类灾备维护应该是日常CI的一部分。
````

## File: docs/devops.md
````markdown
# DevOps #

This document describes the DevOps requirements for operating Dragonboat based applications in production. Please note that incorrect DevOps operations can potentially corrupt your Raft shards permanently.

* Please read the Raft paper first - you need to have some good understanding on the protocol before being able to use it in production.
* It is recommended to use the ext4 filesystem, other filesystems have never been tested.
* It is recommended to use enterprise NVME SSD with high write endurance rating. Must use local hard disks and avoid any NFS, CIFS, Samba, CEPH or other similar shared storage.
* Never try to backup or restore Dragonboat data by directly operating on Dragonboat data files or directories. It can immediately corrupt your Raft shards. 
* Each Raft shard has multiple replicas, the best way to safeguard the availability of your services and data is to increase the number of replicas. As an example, the Raft shard can tolerant 2 replica failures when there are 5 replicas, while it can only tolerant 1 replica failure when using 3 replicas. Host your replicas on separate physical nodes. 
* On replica failure, the Raft shard will be available when it still has the quorum. To handle such failures, you can add a non-voting replica first to start replicating data to it, once in sync with other replicas you can promote the non-voting replica to a regular replica and remove the failed node by using membership change APIs. For those failed replicas caused by intermittent failures such as short term network partition or power loss, you should resolve the network or power issue and try restarting the affected replicas.
* On disk failure, e.g. when experiencing data integrity check errors or write failures, it is important to immediately replace the failed disk and replace the failed replicas using the above described membership change method. To replace replicas with such disk failures, it is important to have the failed disk replaced first to ensure corrupted data is removed. As a refreshed replica with no existing data, the replica must be assigned a new replica ID and a new RaftAddress value to avoid confusing other nodes.
* When any of your physical node is permanently dead, use the same membership change procedure described above to replace all replicas on that dead node with brand new replicas that come with brand new replica IDs. 
* When the quorum is gone, e.g. 2 replicas out of your 3 replicas shard are dead, you will not be able to resolve it without losing data. The github.com/lni/dragonboat/tools package provides the ImportSnapshot method to import a previously exported snapshot to repair such failed Raft shard. Always setup fully automated tests for your system to ensure that you can recovery from such situations.
* By default, the RaftAddress value can not be changed between restarts, otherwise the system will panic with an error message.
* When you can't provide a static IP for your nodes, e.g. when IP is dynamically assigned on node restart, you may want to configure a static DNS name for that node and update it on each restart. 
* When it is not possible to do so, you can choose to set the DefaultNodeRegistryEnabled field to enable the gossip feature which is designed to handle dynamic RaftAddress. Check godocs for more details on the gossip feature. 
* The most important thing - always test your system to ensure that it has high availability by design, fully automated disaster recovery tests should always be a part of the CI.
````

## File: docs/overview.CHS.md
````markdown
#  Dragonboat综述 #

This document is an overview of the dragonboat library, it is in Chinese. The English translation is being worked on and it will be provided soon.   

本文档提供Dragonboat的使用综述信息，阅读完后您将可以自如的使用Dragonboat库在您的项目中实现您所需要的各类有强一致需求的功能与组件。本文档假设您已掌握Go编程，但您无需对Raft或其它任何分布式共识算法有详细理解。作为入门类工程性文档，本文刻意避免理论、原理型的描述，对这方面有兴趣的用户请参考各类分布式算法相关论文。

## 分布式共识算法 ##

如果您对Raft等分布式共识算法已经有足够了解，可以跳过本节。

为了避免单点问题，我们希望数据有多个副本且被保存在多个地域分布的服务器上，这样当某一台或几台服务器出现当机时，根据副本数和位置可以有机会使得数据依旧在线进而服务继续可用，这也就是常说的高可用。存储在多个服务器上的多个副本同时提供了更高的整体读带宽，这对互联网应用普遍具有的高读写比特性也具备较高实际应用的意义。

以三个副本的数据为例，如果不考虑三个副本是否一致，只要任何一个副本在线，我们就可以认为数据可用。但这样的做法引入了副本间数据不一致的问题，把巨大的应用设计和实现上的麻烦强行推给了应用开发者，对于很多后台应用，这种做法已经被大面积淘汰接近消亡。在多数副本在线这一前提下，即三个副本中的至少两个可用时，共识算法可以使得这三个副本在外界看起来始终数据一致并提供高可用。几乎所有主要互联网公司均已开始不同程度使用共识算法来达到上述高可用与副本数据一致。

为了提供这样的数据一致与高可用，同时保证应用开发的便捷，共识算法使用一种称为复制状态机的系统模型。用户的应用被抽象为状态机，对状态机状态的更新由用户提交的称为Proposal的提议来实现。共识算法确保当有多个用户同时提交提议时，那些被多数副本成功收到且持久保存的提议会被采纳（committed），在不同的副本上，各提议将被赋予相同的index序号并根据该序号顺序记录在称为Raft Log的一个有序数据结构中。这些被采纳的提议，将以上述既定的顺序被逐一应用到用户应用的状态机中以更新状态机的状态。当状态机的多个副本的初始状态相同，且在各个副本上均严格按照Log内的内容逐一更新，那么Log中同一被采纳的提议被执行后，各个状态副本的状态也将是相同的。

如下图所示，该示例Raft组有三个分布的副本。状态机的状态为一个整形数变量X，各副本通过Raft协议维护一个Raft Log的副本，所有已采纳的提议在各Raft Log副本中的各值与顺序均严格相同，它们将按照index值升序次序逐一应用至状态机以更新状态机状态。对于各状态机副本，在应用了同一index值所对应的Raft Log那的提议记录后，各副本的状态将是相同的。

![rsm](rsm.png)

读操作的一致需要满足以下两点：
* 当一个写请求以提议的形式被采纳以后，此时通过读操作，必须可以读到这个写请求的结果，或者比它更新的结果。
* 当一个读请求返回了结果后开始下一个新的读请求，该新的读请求必须返回上一次读的结果或比它更新的结果。

共识库最主要功能即实现诸如Raft这样的共识算法并提供编程接口供使用者方便地实现他们的应用状态机，进而方便用户使用共识库的状态机读写功能来操作应用状态机及其数据，以实现有高可用和数据一致保障的各私有应用逻辑。

下面各节将展开讨论如何使用dragonboat提供的各项便利，方便地构建基于Raft分布式共识算法的应用。

## 术语 ##

Raft组：Raft协议控制下的一个独立的具有多个副本的实体，组内各个副本提供上一节中描述的一致性保证。一个应用可以使用管理一个或者多个Raft组。每个Raft组由一个系统内全局唯一的用户设定的64位整形数ShardID来指代。
集群Shard：Raft组的别称。
节点Node：Raft组中的一个成员副本。每个节点由一个Raft组内唯一的用户设定的64位整形数ReplicaID来指代。
初始成员节点Initial Member：一个Raft组在最初出现的时候所设定的原始成员。
Leader：Raft协议中定义的扮演Leader角色的节点。每个Raft组应有一个Leader节点，只有当Leader节点确定时才能对该Raft组进行读写。
快照Snapshot：把状态机在某具体时间点上的状态完全保存所得到的数据，可用于快速恢复状态机状态。

## 状态机 ##

状态机是用户应用的核心，它实现用户的业务逻辑，比如当您想构建类似Redis的基于内存的Key-Value数据库，那您的状态机就是这样一个KV数据库。状态机同时也是用户应用与Dragonboat一大交互接口，Dragonboat库通过状态机所实现的IStateMachine或IOnDiskStateMachine接口与之交互，完成状态机状态更新与查询等操作。

### 普通状态机IStateMachine ###

普通状态机通常将数据存放于内存内，这决定了其总的数据量相对较小。普通状态机的状态会在每次节点重启后被完全重置，其管理下的存储于磁盘上的数据也需要清理并忽略，然后通过已持久化保存的Log和快照Snapshot给予恢复。它的特点是数据量受限于内存大小、实现简单、吞吐可达千万次每秒，但定期的保存快照Snapshot以及每次重启后重建状态所带来额外的CPU、IO以及磁盘空间损耗较大。较常见的此类状态机的例子是类似Redis的基于内存的Key-Value数据库。此类型状态机是Raft论文中提及的通常类型的状态机。

用户需要实现statemachine包中的IStateMachine接口以实现这类普通状态机。此类状态机状态在每次重启后应确保其初始状态为空，Dragonboat负责通过状态机的Update与RecoverFromSnapshot两个方法来恢复状态机的状态。请注意，普通状态机的状态在重启后被重置，但没有任何数据会被丢失，已持久保存的快照Snapshot和Raft Log包含所有已被采纳的用户数据。

### 基于磁盘的状态机IOnDiskStateMachine ###

基于磁盘的状态机的主要数据保存于磁盘上，因此其总数据量相对较大，状态机的状态始终在磁盘上持久保存，每次节点重启后状态机状态不受影响。Dragonboat依旧需要定期保存快照Snapshot，但此类快照仅包含少量元数据，创建的开销极小。它的特点是较前述普通状态机而言，它保存快照的额外IO开销极小，且重启后无需状态重建的过程。较常见的此类状态机的例子是基于RocksDB或LevelDB的带多副本与强一致保障的分布式KV数据库。此类型状态机可认为是Raft论文5.2节提及的特殊类型状态机。

用户需要实现statemachine包中的IOnDiskStateMachine接口以实现基于磁盘的状态机。此类基于磁盘的状态机由用户负责其状态机状态的持久化保存以及并发读写的支持，同时需要保存所最后应用的Log的index值，Dragonboat仅负责在每次重启后打开并启用已保存的状态机。

### 两类状态机的选择 ###

上述两类状态机的选择的最重要指标是状态机所管理的总数据大小。在所有状态机数据可以被存放于内存内的时候，比如几十G字节以内，建议使用基于内存的状态机，基于磁盘的状态机可以视为是状态机管理数据量较大情况下的一种针对避免额外开销的优化。

## 节点启动 ##

使用一个节点前首先需要启动该节点，使得其被NodeHost装载并管理。NodeHost的StartReplica, StartConcurrentReplica与StartOnDiskReplica方法用于启动相应节点。

当一个Raft shard的各初始成员首次启动时，用户需要提供该Raft shard的所有初始成员信息(initial members)，且各副本必须以完全相同的初始成员信息启动。该初始成员信息用于确保各副本从一个一致的成员列表开始演进后续用户要求的成员变更。当一个副本并非该Raft shard的初始成员，而是后续通过成员变更（如SyncRequestAddReplica）所新增的节点，其第一次启动时无需提供初始成员信息，只需要将join参数设置为true。

当一个节点重启时，不论该节点是一个初始节点还是后续通过成员变更添加的节点，均无需再次提供初始成员信息，也不再需要设置join参数为true。

## 节点停止 ##

用户可以通过NodeHost的StopShard方法来停止所指定的Raft shard在该NodeHost管理下的副本。停止后的节点不再响应读写请求，但可以通过上述节点启动方式再次重新启动。

在一个副本被StopShard要求停止后，如果它正在执行快照的创建或恢复，该节点可能不会立刻停止而需等待至快照的创建或恢复完成。为避免这种长期等待，由用户实现的快照创建与恢复方法提供了一个<-chan struct{}的参数，当节点被要求停止后，该<-chan struct{}会被关闭，用户的快照创建与恢复方法可据此选择是否放弃当前的快照创建与恢复，从而快速响应节点停止的请求。

## 写操作 ##

对状态机的写操作称为提议Propose。用户可以通过使用NodeHost的Propose与SyncPropose方法发起异步或者同步的提议。

前述的共识算法常识可知，一个Raft组的过半数成员在线且可以互相正常交换网络消息，此时用户的提议才可以成为被采纳状态（committed）最终被送至各状态机的副本执行。Dragonboat通过向状态机接口的Update方法提供已采纳的提议，供状态机依次执行。

NodeHost的Propose方法开始一个异步的提议，它会立刻返回。如果发起成功它会提供一个RequestState对象，用户可以通过它等待提议的操作结果并获取结果状态。一个被成功开始的提议的可能结果有成功、超时和失败。成功表示提议被采纳且已经被应用当当前的节点Node中，此后所有开始的读操作均将可以读到该提议的执行结果或者更新的结果。超时表示在用户指定的时间内无法完成提议的整个流程，提议状态未明，这是典型的分布式系统中的三态情况。失败是指向Propose提供的参数不合理或节点在提议流程完成前已经被关闭。

NodeHost的SyncPropose方法开始一个同步的提议，调用者的Goroutine会被挂起直到SyncPropose返回了明确成功、超时或者错误结果才会返回。它目前是对Propose方法的一个封装，可查看源代码比较两者实现的区别。

因为写操作超时以后导致上述三态情况，当用户重试一个超时的写请求的时候需要充分考虑这一情况，确保之前超时的操作如果已经成功执行了写操作，那么再次重试的写操作的再次执行不会对系统带来负面干扰，这样的特性称为幂等。显然，用户可以选择在状态机的设计上实现自己的幂等处理的方法。用户同时也可以选择使用Dragonboat自带的针对普通状态机的幂等方案，该方案基于Raft论文。

简单来说，Propose与SyncPropose方法均需要一个称为Session的输入参数，它是一个客户端访问状态进行写的Session。当用户选择不使用Dragonboat内建的幂等功能的时候，可以使用通过调用NodeHost的GetNoOPSession方法获得一个NOOP Session，它仅用来指明写操作针对的Raft组的ShardID标示。如果选择使用内建的幂等支持，那可以使用GetNewSession以获取一个有效的具体Session对象，并在当前的client每次调用Propose或者SyncPropose方法时使用这个Session对象。当Propose或SyncPropose成功后，需要调用Session的ProposalCompleted方法，而Propose或SyncPropose超时后则不调用ProposalCompleted方法而直接再次调用Propose或SyncPropose来重试已超时的提议。

基于磁盘的状态机不支持该内置的幂等功能，必须使用NOOP Session，如有需求，用户需自行在状态机内实现其幂等的支持。

写操作的用户输入数据应该是状态机执行更新操作的唯一输入数据来源，状态机在Update方法被调用以更新状态机状态时，不应该使用诸如系统时间、随机数、当前进程号等等在各个副本上不确定的数据源。

默认设置下，一个Proposal只有在被确认已采纳且已成功应用于状态机时才会通知客户端，对部分有特殊需求的应用，可以通过设置NodeHostConfig的NotifyCommit参数使Dragonboat在Proposal被采纳后另行通知客户端。

## 读操作 ##

对状态机的读操作通常用以查询状态机内容与状态，且读操作不改变状态机的状态。读操作必须采用确保一致性的协议，绝不可直接读取状态机内容。Dragonboat使用Raft论文中描述的称为ReadIndex的协议来实现高效的确保一致性的读。用户可以使用NodeHost的SyncRead以或者ReadIndex与ReadLocalNode的组合方式，完成对状态机的读，前者为同步操作，后者为异步。

与写操作类似，读操作所依赖的ReadIndex协议的顺利执行需要一个Raft组的过半数成员在线且可以互相正常交换网络消息。Dragonboat通过向状态机接口的Lookup方法提供用户查询内容，执行后返回查询结果。

NodeHost的ReadIndex方法开始一次异步的ReadIndex协议的执行，它立刻返回一个RequestState对象，用户可以通过它等待ReadIndex协议执行的成功。一旦成功，用户可以立即使用ReadLocalNode方法对同一节点开始查询操作。与前述的Propose方法类似，一个被成功开始的ReadIndex协议可能有三种结果，分别为成功、超时和失败。超时和失败的意义与Propose的超时或失败类似。ReadLocalNode方法只有在每次ReadIndex成功后才可以被调用，直接调用ReadLocalNode将破坏所读取得到的结果的一致性保证。

NodeHost的SyncRead方法进行一次同步的读操作，该方法只有当SyncRead返回了明确的查询结果或者确认了超时或者失败以后才会返回。它是对ReadIndex与ReadLocalNode方法的一个封装，可查看源代码参考具体实现的方法。

因为读操作始终不改变状态机状态，因此读操作超时以后不存在写操作超时以后的重复写入更新问题，也因此不涉及幂等操作的考虑。

上述读操作使用interface{}为输入输出数据，它会引起额外的堆内存分配heap allocation，对于密集读应用，用户可在状态机那实现可选的IExtended接口以后，使用NAReadLocalNode来进行读操作，它以[]byte为输入输出数据，可避免额外的堆内存分配。

NodeHost同时提供名为StaleRead的函数，如它的方法名称所表述的，它不保证任何一致性，仅用来读取状态机当前状态。

## 成员变更 ##

对于一个Raft组，成员变更可以改变其组成员节点Node的组成，比如可以增加一个新节点或者删除一个已经失效的节点。成员变更对于用户状态机透明，无需用户在状态机内做任何处理，用户只需要通过调用NodeHost的RequestAddReplica与RequestDeleteReplica方法来增删节点完成成员变更。

成员变更的注意事项如下：
* 成员变更的操作本身也是通过Raft协议的一种提议，与普通的用户提议一样，被采纳的成员变更操作会被持久保存并复制到各副本节点。
* 已经被删除的节点不允许被再次添加回Raft组内。
* 成员变更改变系统对于Raft组成员信息的记录，所增删的节点需由用户负责实际启停。
* 对于每个Raft组，成员变更需要逐个依次进行，一次只能有一个等待完成的成员变更请求。
* 仅由同一个线程那逐一执行的成员变更操作是幂等的，比如增加一个节点的请求超时以后如果重试该操作，并不会引起意外结果。

通过成员变更将某节点删除以后，可使用NodeHost的RemoveData方法删除该节点的所有数据以释放磁盘空间。该操作需谨慎，对尚未通过成员变更删除的节点使用RemoveData清理数据将不可修复的损坏该Raft组。

绝大多数情况下，用户应用只在一个线程内逐一的执行成员变更操作，此时成员变更操作是幂等的。如无法满足这一条件，所有的成员变更请求前（含重试）可通过GetShardMembership方法首先获得Raft组当前的Membership成员记录，用户软件根据当前成员情况做出成员变更决定后，在调用成员变更的API时提供上一步所返回的Membership记录的ConfigChangeID值，从而确保多线程并发执行成员变更的幂等。

## 快照Snapshot ##

快照是对某一特定时间点时候的节点状态的一次保存，快照含有如下信息：
* 当前已执行的提议的序号
* 当前Raft组成员信息
* 当前活动的用户Session信息
* 用户状态机当前状态

基于磁盘的状态机不支持用户Session、它的状态机当前状态始终在磁盘上，因此上述两项均不会在定期产生的或者用户通过RequestSnapshot接口请求的快照中包括。基于
磁盘的状态机仅在向进度落后的节点实时传输一个快照时或者根据用户请求导出一个快照时才会包含完整的对状态机状态。

快照可由下列方式产生：
* 通过Config对象设置定期产生快照的频率，此处定期是指状态机每执行N个更新以后
* 通过调用NodeHost的RequestSnapshot方法请求产生一个快照

快照Snapshot有如下作用：
* 普通状态机重启后内存那数据丢失状态被重置，使用快照可以快速恢复状态机状态，无需逐一执行大量的历史Raft Log记录以恢复状态机状态，通常可节省执行时间与磁盘读带宽的消耗。
* Raft组中某节点显著落后于Leader节点时，Leader可以通过向该落后节点网络发送快照而非逐一复制各提议，通常可节省传输时间与带宽。
* 普通状态机状态被保存至快照后，该节点之前的历史Log均可清理以释放磁盘存储空间。
* 导出后的快照可被用于修复已永久丢失多数节点的Raft组。

状态机接口的SaveSnapshot方法用于将状态机状态保存至一个io.Writer对象中，RecoverFromSnapshot方法负责将以io.Reader形式提供的快照数据恢复至状态机内完成快照的应用。快照内其余信息，如当前成员与Session信息，均由Dragonboat负责维护与应用。

由前述关于状态机一致性的描述可知，当状态机执行至某特定提议，那么所有副本的状态应该是相同。快照的产生也必须考虑这点，状态机执行至某特定提议后开始产生快照，状态机多个副本在执行了同一特定提议后所产生的快照应该是严格相同的。

用户可以通过启动各个节点时提供的Config实例中的SnapshotEntries字段设置每执行多少个提议定期进行一次快照的保存，通常：
* 对于普通状态机，建议一天保存1-2次快照
* 对于基于磁盘的状态机，建议每小时保存一次快照

用户也可以使用NodeHost的RequestSnapshot方法对指定的Raft组请求创建快照。使用DefaultSnapshotOption所创建的快照就是一个普通快照，它由系统管理。如果讲SnapshotOption的Exported值设为true，那么所创建的快照会被导出到ExportPath所指向的目录，导出后的快照可备份后未来被用于修复已永久丢失多数节点的Raft组，导出至上述指定目录的快照由用户完全负责保存、转移和释放清理，系统不再干预。对于所请求的非导出的快照，用户同时可以通过SnapshotOption的OverrideCompactionOverhead和CompactionOverhead值来控制每次请求的快照产生以后多少已在快照中包含的Log需要被清理以释放磁盘空间。

上述导出的快照，可以在多数节点均永久失效以后通过tools包提供的ImportSnapshot方法被用来修复已无法使用的Raft组。此时因为Raft组的多数节点已经永久失效，数据已有丢失，该操作为数据有损操作。用户程序应该通过设置合理的副本数以及加强服务器监控维护，通过避免发生多数节点永久失效来规避数据丢失问题。请注意，多数节点发生可恢复的失效，比如多数节点发生重启或短暂网络故障，并不会引起已保存数据的丢失。多数节点永久失效是指多数服务器上的磁盘损坏或服务器永久不再可用等故障。ImportSnapshot具体使用请参考其godoc文档。

## Gossip ##

默认下，每个Raft组的每个副本在被加入系统时都由用户明确指定它所在的节点的RaftAddress位置，系统以此确保各类Raft消息可以被正确发送给该副本。该方案简单直接，但缺点是RaftAddress必须是固定不变的，这要求使用固定的IP或者由用户维护一个DNS Name。在这一要求无法满足时，可以使用gossip功能来规避这一问题。

从v3.3版本开始，每个NodeHost节点都会被随机分配一个永久固定不变的NodeHostID值，它的值如nhid-1234567890形式，该值可由NodeHost的ID方法返回。在NodeHostConfig的DefaultNodeRegistryEnabled项被设置为真后，所有新创建的副本都需要被指定其对应的NodeHostID值。此后，每次启动NodeHost实例时用于NodeHost间通讯的RaftAddress值可随意变化，每个NodeHost实例的RaftAddress与NodeHostID的对应关系将自动由后台的一个gossip服务来动态的维护，当Raft消息需要在两个副本间传递时，首先发生ReplicaID到NodeHostID的转换，接着由NodeHostID通过gossip服务查询得到对应的RaftAddress地址并完成消息副本间的传输。

Gossip服务本身是一个全分布的网络服务，用户仅需要通过NodeHostConfig.Gossip项简单设置其相关地址参数即可。

## 其它功能 ##

Dragonboat通过NodeHost提供下列其它常用功能：

* Non-Voting节点。观察者节点不参与Leader的选举，不参与一个提议是否可以被采纳，它仅仅用来接受并执行Raft组各个已采纳的提议。观察者节点的状态机与普通节点一样，正常情况下将具备完整且相同的状态机状态，它可以被用来做为一个额外的只读节点，供用户读取有一致性保证的状态机状态。观察者节点的另一大作用是允许一个新加入的节点以观察者身份加入Raft组，在其逐渐获取所有状态机状态后再提升其为正常节点。在观察者节点所在的NodeHost上发起一次SyncRead或者一次GetShardMembership，如果成功返回则表示ReadIndex协议被完整执行了一轮，这表示观察者节点已经拥有基本所有Log Entry，具备了将其升级为正常节点的条件。
* Leader迁移。正常情况下，Leader以选举方式由用户程序透明的方式选举产生。用户可以使用NodeHost提供的RequestLeaderTransfer方法尝试将Leader迁移至指定节点。
* NodeHost同时提供GetNodeHostInfo与GetShardMembership方法供查询当前各NodeHost管理下的各Raft组信息。
````

## File: docs/storage.CHS.md
````markdown
# 存储 #

Dragonboat使用[Pebble](https://github.com/cockroachdb/pebble)来存储Raft协议的日志数据。

## 兼容性 ##

Pebble是一个全新的以Go实现的Key-Value store，它提供与RocksDB双向数据文件格式兼容。

## Pebble ##

系统默认使用Pebble，无需设置额外设置。

## RocksDB ##

曾经的RocksDB支持已经在v3.4版本中被移除，请默认使用pebble

## 使用自定义的存储方案 ##

您可以扩展Dragonboat以使用您所选择的其它存储方案来保存Raft协议的日志数据。您需要实现在github.com/lni/dragonboat/v4/raftio中定义的ILogDB接口，并将其实现以一个factory function的方式提供给NodeHostConfig的LogDBFactory成员。

## Tan

Tan是新一代Raft日志数据存储实现，未来Dragonboat将默认使用Tan，该功能切换将会确保老用户不受影响，Pebble支持将长期继续维护。
````

## File: docs/storage.md
````markdown
# Raft Log Storage #

Dragonboat uses [Pebble](https://github.com/cockroachdb/pebble) to store Raft logs, Pebble is a RocksDB compatible Key-Value store.

## Compatibility ##

Pebble is a new Key-Value store implemented in Go with bidirectional compatibility with RocksDB.

## Pebble ##

Pebble is used by default, no configuration is required.

## RocksDB ##

RocksDB support was removed in the v3.4 release. 

## Use custom storage solution ##

You can extend Dragonboat to use your preferred storage solution to store Raft logs -

* implement the ILogDB interface defined in the github.com/lni/dragonboat/v4/raftio package
* pass a factory function that creates such a custom Log DB instance to the LogDBFactory field of your NodeHostConfig.Expert instance

## Tan

Tan is Dragonboat's new Raft Log storage solution, it will be made the default in future releases. Impacts to existing users will be minimized in such planned transition and pebble based log storage will continue to be support.
````

## File: docs/test.md
````markdown
# Testing #

## Approaches ##
* Test driven development. Relevant [etcd raft](https://github.com/coreos/etcd/tree/master/raft) tests have been ported to dragonboat to ensure all corner cases identified by the etcd project have been handled.
* High test coverage. Extensively tested by unit testing and [monkey testing](https://en.wikipedia.org/wiki/Monkey_testing).
* Linearizability checkers. [Jepsen's](https://github.com/jepsen-io/jepsen) [Knossos](https://github.com/jepsen-io/knossos) and [porcupine](https://github.com/anishathalye/porcupine) are utilized to check whether IOs are linearizable.
* Fuzz testing using [go-fuzz](https://github.com/dvyukov/go-fuzz).
* I/O error injection tests. [charybdefs](https://github.com/scylladb/charybdefs) from [scylladb](http://www.scylladb.com/) is employed to inject I/O errors to the underlying file-system to ensure that Dragonboat handle them correctly.
* Power loss tests. We test the system to see what actually happens after power loss.

## Monkey Testing ##
### Setup ###
* 5 NodeNosts and 3 Drummer servers per process
* hundreds of Raft shards per process
* randomly kill and restart NodeHosts and Drummer servers, each NodeHost usually stay online for a few minutes
* randomly delete all data owned by a certain NodeHost to emulate permanent disk failure
* randomly drop and re-order messages exchanged between NodeHosts
* randomly partition NodeHosts from rest of the network
* for selected instances, snapshotting and log compaction happen all the time in the background
* committed entries are applied with random delays
* snapshots are captured and applied with random delays
* a list of background workers keep writing to/reading from random Raft shards with stale read checks
* client activity history files are verified by linearizability checkers such as Jepsen's Knossos
* run hundreds of above described processes concurrently on each test server, 30 minutes each iteration, many iterations every night
* run concurrently on many servers every night

### Checks ###
* no linearizability violation
* no shard is permanently stuck
* state machines must be in sync
* shard membership must be consistent
* raft log saved in LogDB must be consistent
* no zombie shard node

### Results ###
Some history files in Jepsen's [Knossos](https://github.com/jepsen-io/knossos) edn format have been made publicly [available](https://github.com/lni/knossos-data).

# Benchmark #

## Setup ##
* Three servers each with a single 22-core Intel XEON E5-2696v4 processor, all cores can boost to 2.8Ghz
* 40GE Mellanox NIC
* Intel 900P for storing the RocksDB's WAL and Intel P3700 1.6T for storing all other data
* Ubuntu 16.04 with Spectre and Meltdown patches, ext4 file-system

## Benchmark method ##
* 48 Raft shards on three NodeHost instances across three servers
* Each Raft node is backed by a in-memory Key-Value data store as RSM
* Mostly update operations in the Key-Value store
* All I/O requests are launched from local processes
* Each request is handled in its own goroutine, simple threading model & easy for application debugging
* fsync is strictly honored
* MutualTLS is disabled

## Intel Optane SSD ##
Compared with enterprise NVME SSDs such as Intel P3700, Optane based SSD doesn't increase throughput when payload is 16/128 bytes. It does slightly increase the throughput when the payload size is 1024 byte each. It also improves write latency when the payload size is 1024.
````

## File: examples/README.md
````markdown
## Examples ##

Examples are available at [https://github.com/lni/dragonboat-example](https://github.com/lni/dragonboat-example).
````

## File: internal/fileutil/go114.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +build !go1.16

package fileutil

import (
	"io"
	"io/ioutil"
	"os"
)

// TODO:
// io/iotuil has been deprecated in go1.16
// ioutil.Discard, ioutil.TempFile and other functions have been moved to the
// other stdlib packages (io and os) in go1.16.
// remove this file when we require go1.16 for dragonboat

// Discard ...
var Discard = ioutil.Discard

// CreateTemp ...
func CreateTemp(dir string, pattern string) (*os.File, error) {
	f, err := ioutil.TempFile(dir, pattern)
	return f, ws(err)
}

// ReadAll ...
func ReadAll(r io.Reader) ([]byte, error) {
	result, err := ioutil.ReadAll(r)
	return result, ws(err)
}

// MkdirTemp ...
func MkdirTemp(dir string, pattern string) (string, error) {
	path, err := ioutil.TempDir(dir, pattern)
	return path, ws(err)
}

// ReadFile ...
func ReadFile(name string) ([]byte, error) {
	result, err := ioutil.ReadFile(name)
	return result, ws(err)
}
````

## File: internal/fileutil/go116.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//go:build go1.16
// +build go1.16

package fileutil

import (
	"io"
	"os"
)

// Discard ...
var Discard = io.Discard

// CreateTemp ...
func CreateTemp(dir string, pattern string) (*os.File, error) {
	f, err := os.CreateTemp(dir, pattern)
	return f, ws(err)
}

// ReadAll ...
func ReadAll(r io.Reader) ([]byte, error) {
	result, err := io.ReadAll(r)
	return result, ws(err)
}

// MkdirTemp ...
func MkdirTemp(dir string, pattern string) (string, error) {
	path, err := os.MkdirTemp(dir, pattern)
	return path, ws(err)
}

// ReadFile ...
func ReadFile(name string) ([]byte, error) {
	result, err := os.ReadFile(name)
	return result, ws(err)
}
````

## File: internal/fileutil/utils_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package fileutil

import (
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/internal/vfs"
)

func TestTempDir(t *testing.T) {
	dir1, err := TempDir("", "test-dir", vfs.DefaultFS)
	require.NoError(t, err)
	dir2, err := TempDir("", "test-dir", vfs.DefaultFS)
	require.NoError(t, err)
	require.NotEqual(t, dir1, dir2)
}
````

## File: internal/fileutil/utils.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package fileutil

import (
	"archive/tar"
	"bytes"
	"compress/bzip2"
	"crypto/md5"
	"fmt"
	"io"
	"os"
	"runtime"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/cockroachdb/errors"
	"github.com/cockroachdb/errors/oserror"

	"github.com/lni/dragonboat/v4/internal/utils"
	"github.com/lni/dragonboat/v4/internal/vfs"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

const (
	// DefaultFileMode is the default file mode for files generated by
	// Dragonboat.
	DefaultFileMode = 0640
	// SnapshotFlagFilename defines the filename of the snapshot flag file.
	SnapshotFlagFilename = "dragonboat.snapshot.message"
	defaultDirFileMode   = 0750
	deleteFilename       = "DELETED.dragonboat"
)

var firstError = utils.FirstError

var ws = errors.WithStack

// MustWrite writes the specified data to the input writer. It will panic if
// there is any error.
func MustWrite(w io.Writer, data []byte) {
	if _, err := w.Write(data); err != nil {
		panic(err)
	}
}

// DirExist returns whether the specified filesystem entry exists.
func DirExist(name string, fs vfs.IFS) (result bool, err error) {
	if name == "." || name == "/" {
		return true, nil
	}
	f, err := fs.OpenDir(name)
	if err != nil && vfs.IsNotExist(err) {
		return false, nil
	}
	if err != nil {
		return false, err
	}
	defer func() {
		err = firstError(err, ws(f.Close()))
	}()
	s, err := f.Stat()
	if err != nil {
		return false, ws(err)
	}
	if !s.IsDir() {
		panic("not a dir")
	}
	return true, nil
}

// Exist returns whether the specified filesystem entry exists.
func Exist(name string, fs vfs.IFS) (bool, error) {
	if name == "." || name == "/" {
		return true, nil
	}
	_, err := fs.Stat(name)
	if err != nil && vfs.IsNotExist(err) {
		return false, nil
	}
	if err != nil {
		return false, err
	}
	return true, nil
}

// MkdirAll creates the specified dir along with any necessary parents.
func MkdirAll(dir string, fs vfs.IFS) error {
	exist, err := DirExist(dir, fs)
	if err != nil {
		return err
	}
	if exist {
		return nil
	}
	parent := fs.PathDir(dir)
	exist, err = DirExist(parent, fs)
	if err != nil {
		return err
	}
	if !exist {
		if err := MkdirAll(parent, fs); err != nil {
			return err
		}
	}
	return Mkdir(dir, fs)
}

// Mkdir creates the specified dir.
func Mkdir(dir string, fs vfs.IFS) error {
	parent := fs.PathDir(dir)
	exist, err := DirExist(parent, fs)
	if err != nil {
		return err
	}
	if !exist {
		panic(fmt.Sprintf("%s doesn't exist when creating %s", parent, dir))
	}
	if err := fs.MkdirAll(dir, defaultDirFileMode); err != nil {
		return err
	}
	return SyncDir(parent, fs)
}

// SyncDir calls fsync on the specified directory.
func SyncDir(dir string, fs vfs.IFS) (err error) {
	if runtime.GOOS == "windows" {
		return nil
	}
	if dir == "." {
		return nil
	}
	f, err := fs.OpenDir(dir)
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, ws(f.Close()))
	}()
	fileInfo, err := f.Stat()
	if err != nil {
		return ws(err)
	}
	if !fileInfo.IsDir() {
		panic("not a dir")
	}
	df, err := fs.OpenDir(vfs.Clean(dir))
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, ws(df.Close()))
	}()
	return ws(df.Sync())
}

// MarkDirAsDeleted marks the specified directory as deleted.
func MarkDirAsDeleted(dir string, msg pb.Marshaler, fs vfs.IFS) error {
	return CreateFlagFile(dir, deleteFilename, msg, fs)
}

// IsDirMarkedAsDeleted returns a boolean flag indicating whether the specified
// directory has been marked as deleted.
func IsDirMarkedAsDeleted(dir string, fs vfs.IFS) (bool, error) {
	return Exist(fs.PathJoin(dir, deleteFilename), fs)
}

func getHash(data []byte) []byte {
	h := md5.New()
	MustWrite(h, data)
	s := h.Sum(nil)
	return s[8:]
}

// CreateFlagFile creates a flag file in the specific location. The flag file
// contains the marshaled data of the specified protobuf message.
//
// CreateFlagFile is not atomic meaning you can end up having a file at
// fs.PathJoin(dir, filename) with partial or corrupted content when the machine
// crashes in the middle of this function call. Special care must be taken to
// handle such situation, see how CreateFlagFile is used by snapshot images as
// an example.
func CreateFlagFile(dir string,
	filename string, msg pb.Marshaler, fs vfs.IFS) (err error) {
	fp := fs.PathJoin(dir, filename)
	f, err := fs.Create(fp)
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, ws(f.Close()))
		err = firstError(err, SyncDir(dir, fs))
	}()
	data := pb.MustMarshal(msg)
	h := getHash(data)
	n, err := f.Write(h)
	if err != nil {
		return ws(err)
	}
	if n != len(h) {
		return ws(io.ErrShortWrite)
	}
	n, err = f.Write(data)
	if err != nil {
		return ws(err)
	}
	if n != len(data) {
		return ws(io.ErrShortWrite)
	}
	return ws(f.Sync())
}

// GetFlagFileContent gets the content of the flag file found in the specified
// location. The data of the flag file will be unmarshaled into the specified
// protobuf message.
func GetFlagFileContent(dir string,
	filename string, msg pb.Unmarshaler, fs vfs.IFS) (err error) {
	fp := fs.PathJoin(dir, filename)
	f, err := fs.Open(vfs.Clean(fp))
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, ws(f.Close()))
	}()
	data, err := ReadAll(f)
	if err != nil {
		return ws(err)
	}
	if len(data) < 8 {
		panic("corrupted flag file")
	}
	h := data[:8]
	buf := data[8:]
	expectedHash := getHash(buf)
	if !bytes.Equal(h, expectedHash) {
		panic("corrupted flag file content")
	}
	pb.MustUnmarshal(msg, buf)
	return nil
}

// HasFlagFile returns a boolean value indicating whether flag file can be
// found in the specified location.
func HasFlagFile(dir string, filename string, fs vfs.IFS) bool {
	fp := fs.PathJoin(dir, filename)
	fi, err := fs.Stat(fp)
	if err != nil {
		return false
	}
	if fi.IsDir() {
		return false
	}
	return true
}

// RemoveFlagFile removes the specified flag file.
func RemoveFlagFile(dir string, filename string, fs vfs.IFS) error {
	return fs.Remove(fs.PathJoin(dir, filename))
}

// ExtractTarBz2 extracts files and directories from the specified tar.bz2 file
// to the specified target directory.
func ExtractTarBz2(bz2fn string, toDir string, fs vfs.IFS) (err error) {
	f, err := fs.Open(bz2fn)
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, f.Close())
	}()
	ts := bzip2.NewReader(f)
	tarReader := tar.NewReader(ts)
	for {
		header, err := tarReader.Next()
		if err == io.EOF {
			return nil
		}
		if err != nil {
			return err
		}
		switch header.Typeflag {
		case tar.TypeDir:
			target := fs.PathJoin(toDir, header.Name)
			if err := fs.MkdirAll(target, defaultDirFileMode); err != nil {
				return err
			}
		case tar.TypeReg:
			if err := func() error {
				fp := fs.PathJoin(toDir, header.Name)
				nf, err := fs.Create(fp)
				if err != nil {
					return err
				}
				defer func() {
					err = firstError(err, nf.Close())
				}()
				_, err = io.Copy(nf, tarReader)
				return err
			}(); err != nil {
				return err
			}
		default:
			panic("unknown type")
		}
	}
}

// TempFile and the following rand functions are derived from the golang source
// https://golang.org/src/io/ioutil/tempfile.go
//
// Copyright 2010 The Go Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.
//
var rand uint32
var randmu sync.Mutex

func reseed() uint32 {
	return uint32(time.Now().UnixNano())
}

func nextRandom() string {
	randmu.Lock()
	r := rand
	if r == 0 {
		r = reseed()
	}
	r = r*1664525 + 1013904223 // constants from Numerical Recipes
	rand = r
	randmu.Unlock()
	return strconv.Itoa(int(1e9 + r%1e9))[1:]
}

// TempFile and TempDir functions below are modified from golang's
// TempFile and TempDir functions.
//
// Copyright 2010 The Go Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

// TempFile returns a temp file.
func TempFile(dir,
	pattern string, fs vfs.IFS) (f vfs.File, name string, err error) {
	if dir == "" {
		dir = vfs.TempDir()
		if fs != vfs.DefaultFS {
			if err := fs.MkdirAll(dir, defaultDirFileMode); err != nil {
				return nil, "", err
			}
		}
	}
	var prefix, suffix string
	if pos := strings.LastIndex(pattern, "*"); pos != -1 {
		prefix, suffix = pattern[:pos], pattern[pos+1:]
	} else {
		prefix = pattern
	}
	nconflict := 0
	for i := 0; i < 10000; i++ {
		name = fs.PathJoin(dir, prefix+nextRandom()+suffix)
		f, err = fs.Create(name)
		if vfs.IsExist(err) {
			if nconflict++; nconflict > 10 {
				randmu.Lock()
				rand = reseed()
				randmu.Unlock()
			}
			continue
		}
		break
	}
	return
}

var errPatternHasSeparator = errors.New("pattern contains path separator")

// prefixAndSuffix splits pattern by the last wildcard "*", if applicable,
// returning prefix as the part before "*" and suffix as the part after "*".
func prefixAndSuffix(pattern string) (prefix, suffix string, err error) {
	if strings.ContainsRune(pattern, os.PathSeparator) {
		err = errPatternHasSeparator
		return
	}
	if pos := strings.LastIndex(pattern, "*"); pos != -1 {
		prefix, suffix = pattern[:pos], pattern[pos+1:]
	} else {
		prefix = pattern
	}
	return
}

// TempDir creates a new temporary directory in the directory dir.
// The directory name is generated by taking pattern and applying a
// random string to the end. If pattern includes a "*", the random string
// replaces the last "*". TempDir returns the name of the new directory.
// If dir is the empty string, TempDir uses the
// default directory for temporary files (see os.TempDir).
// Multiple programs calling TempDir simultaneously
// will not choose the same directory. It is the caller's responsibility
// to remove the directory when no longer needed.
func TempDir(dir, pattern string, fs vfs.IFS) (name string, err error) {
	if dir == "" {
		dir = os.TempDir()
	}

	prefix, suffix, err := prefixAndSuffix(pattern)
	if err != nil {
		return
	}

	nconflict := 0
	for i := 0; i < 10000; i++ {
		try := fs.PathJoin(dir, prefix+nextRandom()+suffix)
		err = fs.MkdirAll(try, 0700)
		if oserror.IsExist(err) {
			if nconflict++; nconflict > 10 {
				randmu.Lock()
				rand = reseed()
				randmu.Unlock()
			}
			continue
		}
		if oserror.IsNotExist(err) {
			if _, err := fs.Stat(dir); oserror.IsNotExist(err) {
				return "", err
			}
		}
		if err == nil {
			name = try
		}
		break
	}
	return
}
````

## File: internal/fileutil/vfs_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package fileutil

import (
	"testing"

	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/stretchr/testify/require"
)

func TestVFSSync(t *testing.T) {
	fs := vfs.GetTestFS()
	if fs == vfs.DefaultFS {
		t.Skip("not using memfs, skipped")
	}
	err := MkdirAll("/dragonboat-test-data/data", fs)
	require.NoError(t, err, "failed to mkdir")

	ffs, ok := fs.(*vfs.MemFS)
	require.True(t, ok, "not a memfs")

	ffs.ResetToSyncedState()

	ok, err = DirExist("/dragonboat-test-data", fs)
	require.NoError(t, err, "failed to check exist")
	require.True(t, ok, "test dir disappeared")
}
````

## File: internal/id/nhid_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package id

import (
	"testing"

	"github.com/stretchr/testify/require"
)

func TestIsNodeHostID(t *testing.T) {
	v := New()
	require.True(t, IsNodeHostID(v.String()))
	require.False(t, IsNodeHostID("this is not a uuid"))
}

func TestNew(t *testing.T) {
	values := make(map[string]struct{})
	for i := 0; i < 1000; i++ {
		u := New()
		values[u.String()] = struct{}{}
	}
	require.Equal(t, 1000, len(values))
}

func TestNewUUID(t *testing.T) {
	u := New()
	v, err := NewUUID(u.String())
	require.NoError(t, err)
	require.Equal(t, u.String(), v.String())
}

func TestMarshalUnMarshal(t *testing.T) {
	v := New()
	data, err := v.Marshal()
	require.NoError(t, err)
	v2 := New()
	require.NoError(t, v2.Unmarshal(data))
	require.Equal(t, v.String(), v2.String())

	v3 := New()
	data, err = v3.Marshal()
	require.NoError(t, err)
	data2 := make([]byte, len(data))
	l, err := v3.MarshalTo(data2)
	require.NoError(t, err)
	require.Equal(t, len(data), l)
	require.Equal(t, data, data2)
}
````

## File: internal/id/nhid.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package id

import (
	"github.com/cockroachdb/errors"
	"github.com/google/uuid"

	pb "github.com/lni/dragonboat/v4/raftpb"
)

// IsNodeHostID returns a boolean value indicating whether the specified value
// is a valid string representation of NodeHostID.
func IsNodeHostID(v string) bool {
	_, err := uuid.Parse(v)
	return err == nil
}

func NewUUID(v string) (*UUID, error) {
	u, err := uuid.Parse(v)
	if err != nil {
		return nil, err
	}
	return &UUID{v: u}, nil
}

func New() *UUID {
	return &UUID{v: uuid.New()}
}

type UUID struct {
	v uuid.UUID
}

var _ pb.Marshaler = (*UUID)(nil)
var _ pb.Unmarshaler = (*UUID)(nil)

func (u UUID) String() string {
	return u.v.String()
}

func (u *UUID) Marshal() ([]byte, error) {
	return u.v.MarshalBinary()
}

func (u *UUID) MarshalTo(data []byte) (int, error) {
	v, err := u.v.MarshalBinary()
	if err != nil {
		return 0, err
	}
	if len(data) < len(v) {
		return 0, errors.New("input slice too short")
	}
	copy(data, v)
	return len(v), nil
}

func (u *UUID) Unmarshal(data []byte) error {
	return u.v.UnmarshalBinary(data)
}
````

## File: internal/invariants/arch.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package invariants

import (
	"runtime"
)

// Is32BitArch returns a boolean value indicating whether running on a 32bit
// architecture.
func Is32BitArch() bool {
	return is32BitArch(runtime.GOARCH)
}

// IsSupportedArch returns a boolean value indicating whether running on a
// supported architecture.
func IsSupportedArch() bool {
	supported := []string{
		"arm64",
		"amd64",
	}
	for _, v := range supported {
		if runtime.GOARCH == v {
			return true
		}
	}
	return false
}

// IsSupportedOS returns a boolean value indicating whether running on a
// supported OS.
func IsSupportedOS() bool {
	return runtime.GOOS == "linux" || runtime.GOOS == "darwin"
}

func is32BitArch(arch string) bool {
	known := []string{
		"386",
		"amd64p32",
		"arm",
		"armbe",
		"mips",
		"mipsle",
		"mips64p32",
		"mips64p32le",
		"ppc",
		"riscv",
		"s390",
		"sparc",
	}
	for _, v := range known {
		if arch == v {
			return true
		}
	}
	return false
}
````

## File: internal/invariants/memfs.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +build dragonboat_memfs_test

package invariants

// MemfsTest is a boolean flag indicating whether dragonboat is running memfs
// test mode.
const MemfsTest = true
````

## File: internal/invariants/monkey.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +build dragonboat_monkeytest

package invariants

// MonkeyTest is a boolean flag indicating whether dragonboat is running in
// monkey test mode.
const MonkeyTest = true
````

## File: internal/invariants/nomemfs.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//go:build !dragonboat_memfs_test
// +build !dragonboat_memfs_test

package invariants

// MemfsTest is a boolean flag indicating whether dragonboat is running memfs
// test mode.
const MemfsTest = false
````

## File: internal/invariants/nomonkey.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//go:build !dragonboat_monkeytest
// +build !dragonboat_monkeytest

package invariants

// MonkeyTest is a boolean flag indicating whether dragonboat is running in
// monkey test mode.
const MonkeyTest = false
````

## File: internal/invariants/norace.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//go:build !race
// +build !race

package invariants

// Race is a boolean flag indicating whether the process is running in race
// mode.
const Race = false
````

## File: internal/invariants/race.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +build race

package invariants

// Race is a boolean flag indicating whether the process is running in race
// mode.
const Race = true
````

## File: internal/logdb/kv/pebble/kv_pebble.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package pebble

// WARNING: pebble support is expermental, DO NOT USE IT IN PRODUCTION.

import (
	"bytes"
	"fmt"
	"sync"

	"github.com/cockroachdb/pebble"
	"github.com/lni/goutils/syncutil"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/logdb/kv"
	"github.com/lni/dragonboat/v4/internal/utils"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/logger"
)

var (
	plog = logger.GetLogger("pebblekv")
)

const (
	maxLogFileSize = 1024 * 1024 * 128
)

var firstError = utils.FirstError

type eventListener struct {
	kv      *KV
	stopper *syncutil.Stopper
}

func (l *eventListener) close() {
	l.stopper.Stop()
}

func (l *eventListener) notify() {
	l.stopper.RunWorker(func() {
		select {
		case <-l.kv.dbSet:
			if l.kv.callback != nil {
				memSizeThreshold := l.kv.config.KVWriteBufferSize *
					l.kv.config.KVMaxWriteBufferNumber * 19 / 20
				l0FileNumThreshold := l.kv.config.KVLevel0StopWritesTrigger - 1
				m := l.kv.db.Metrics()
				busy := m.MemTable.Size >= memSizeThreshold ||
					uint64(m.Levels[0].Sublevels) >= l0FileNumThreshold
				l.kv.callback(busy)
			}
		default:
		}
	})
}

func (l *eventListener) onCompactionEnd(pebble.CompactionInfo) {
	l.notify()
}

func (l *eventListener) onFlushEnd(pebble.FlushInfo) {
	l.notify()
}

func (l *eventListener) onWALCreated(pebble.WALCreateInfo) {
	l.notify()
}

type pebbleWriteBatch struct {
	wb *pebble.Batch
	db *pebble.DB
	wo *pebble.WriteOptions
}

func (w *pebbleWriteBatch) Destroy() {
	if err := w.wb.Close(); err != nil {
		panic(err)
	}
}

func (w *pebbleWriteBatch) Put(key []byte, val []byte) {
	if err := w.wb.Set(key, val, w.wo); err != nil {
		panic(err)
	}
}

func (w *pebbleWriteBatch) Delete(key []byte) {
	if err := w.wb.Delete(key, w.wo); err != nil {
		panic(err)
	}
}

func (w *pebbleWriteBatch) Clear() {
	if err := w.wb.Close(); err != nil {
		panic(err)
	}
	w.wb = w.db.NewBatch()
}

func (w *pebbleWriteBatch) Count() int {
	return int(w.wb.Count())
}

type pebbleLogger struct{}

var _ pebble.Logger = (*pebbleLogger)(nil)

// PebbleLogger is the logger used by pebble
var PebbleLogger pebbleLogger

func (pebbleLogger) Infof(format string, args ...interface{}) {
	pebble.DefaultLogger.Infof(format, args...)
}

func (pebbleLogger) Fatalf(format string, args ...interface{}) {
	pebble.DefaultLogger.Infof(format, args...)
	panic(fmt.Errorf(format, args...))
}

// NewKVStore returns a pebble based IKVStore instance.
func NewKVStore(config config.LogDBConfig, callback kv.LogDBCallback,
	dir string, wal string, fs vfs.IFS) (kv.IKVStore, error) {
	return openPebbleDB(config, callback, dir, wal, fs)
}

// KV is a pebble based IKVStore type.
type KV struct {
	db       *pebble.DB
	dbSet    chan struct{}
	opts     *pebble.Options
	ro       *pebble.IterOptions
	wo       *pebble.WriteOptions
	event    *eventListener
	callback kv.LogDBCallback
	config   config.LogDBConfig
}

var _ kv.IKVStore = (*KV)(nil)

var pebbleWarning sync.Once

func openPebbleDB(config config.LogDBConfig, callback kv.LogDBCallback,
	dir string, walDir string, fs vfs.IFS) (kv.IKVStore, error) {
	if config.IsEmpty() {
		panic("invalid LogDBConfig")
	}
	pebbleWarning.Do(func() {
		if fs == vfs.MemStrictFS {
			plog.Warningf("running in pebble memfs test mode")
		}
	})
	blockSize := int(config.KVBlockSize)
	writeBufferSize := int(config.KVWriteBufferSize)
	maxWriteBufferNumber := int(config.KVMaxWriteBufferNumber)
	l0FileNumCompactionTrigger := int(config.KVLevel0FileNumCompactionTrigger)
	l0StopWritesTrigger := int(config.KVLevel0StopWritesTrigger)
	maxBytesForLevelBase := int64(config.KVMaxBytesForLevelBase)
	targetFileSizeBase := int64(config.KVTargetFileSizeBase)
	cacheSize := int64(config.KVLRUCacheSize)
	levelSizeMultiplier := int64(config.KVTargetFileSizeMultiplier)
	numOfLevels := int64(config.KVNumOfLevels)
	lopts := make([]pebble.LevelOptions, 0)
	sz := targetFileSizeBase
	for l := int64(0); l < numOfLevels; l++ {
		opt := pebble.LevelOptions{
			Compression:    pebble.NoCompression,
			BlockSize:      blockSize,
			TargetFileSize: sz,
		}
		sz = sz * levelSizeMultiplier
		lopts = append(lopts, opt)
	}
	if inMonkeyTesting {
		writeBufferSize = 1024 * 1024 * 4
	}
	cache := pebble.NewCache(cacheSize)
	ro := &pebble.IterOptions{}
	wo := &pebble.WriteOptions{Sync: true}
	opts := &pebble.Options{
		Levels:                      lopts,
		MaxManifestFileSize:         maxLogFileSize,
		MemTableSize:                writeBufferSize,
		MemTableStopWritesThreshold: maxWriteBufferNumber,
		LBaseMaxBytes:               maxBytesForLevelBase,
		L0CompactionFileThreshold:   l0FileNumCompactionTrigger,
		L0StopWritesThreshold:       l0StopWritesTrigger,
		Cache:                       cache,
		Logger:                      PebbleLogger,
	}
	if fs != vfs.DefaultFS {
		opts.FS = vfs.NewPebbleFS(fs)
	}
	kv := &KV{
		ro:       ro,
		wo:       wo,
		opts:     opts,
		config:   config,
		callback: callback,
		dbSet:    make(chan struct{}),
	}
	event := &eventListener{
		kv:      kv,
		stopper: syncutil.NewStopper(),
	}
	opts.EventListener = pebble.EventListener{
		WALCreated:    event.onWALCreated,
		FlushEnd:      event.onFlushEnd,
		CompactionEnd: event.onCompactionEnd,
	}
	if len(walDir) > 0 {
		if err := fileutil.MkdirAll(walDir, fs); err != nil {
			return nil, err
		}
		opts.WALDir = walDir
	}
	if err := fileutil.MkdirAll(dir, fs); err != nil {
		return nil, err
	}
	pdb, err := pebble.Open(dir, opts)
	if err != nil {
		return nil, err
	}
	cache.Unref()
	kv.db = pdb
	kv.setEventListener(event)
	return kv, nil
}

func (r *KV) setEventListener(event *eventListener) {
	if r.db == nil || r.event != nil {
		panic("unexpected kv state")
	}
	r.event = event
	close(r.dbSet)
	// force a WALCreated event as the one issued when opening the DB didn't get
	// handled
	event.onWALCreated(pebble.WALCreateInfo{})
}

// Name returns the IKVStore type name.
func (r *KV) Name() string {
	return "pebble"
}

// Close closes the RDB object.
func (r *KV) Close() error {
	if err := r.db.Close(); err != nil {
		return err
	}
	r.event.close()
	return nil
}

func iteratorIsValid(iter *pebble.Iterator) bool {
	v := iter.Valid()
	if err := iter.Error(); err != nil {
		plog.Panicf("%+v", err)
	}
	return v
}

// IterateValue ...
func (r *KV) IterateValue(fk []byte, lk []byte, inc bool,
	op func(key []byte, data []byte) (bool, error)) (err error) {
	iter := r.db.NewIter(r.ro)
	defer func() {
		err = firstError(err, iter.Close())
	}()
	for iter.SeekGE(fk); iteratorIsValid(iter); iter.Next() {
		key := iter.Key()
		val := iter.Value()
		if inc {
			if bytes.Compare(key, lk) > 0 {
				return nil
			}
		} else {
			if bytes.Compare(key, lk) >= 0 {
				return nil
			}
		}
		cont, err := op(key, val)
		if err != nil {
			return err
		}
		if !cont {
			break
		}
	}
	return nil
}

// GetValue ...
func (r *KV) GetValue(key []byte, op func([]byte) error) (err error) {
	val, closer, err := r.db.Get(key)
	if err != nil && err != pebble.ErrNotFound {
		return err
	}
	defer func() {
		if closer != nil {
			err = firstError(err, closer.Close())
		}
	}()
	return op(val)
}

// SaveValue ...
func (r *KV) SaveValue(key []byte, value []byte) error {
	return r.db.Set(key, value, r.wo)
}

// DeleteValue ...
func (r *KV) DeleteValue(key []byte) error {
	return r.db.Delete(key, r.wo)
}

// GetWriteBatch ...
func (r *KV) GetWriteBatch() kv.IWriteBatch {
	return &pebbleWriteBatch{
		wb: r.db.NewBatch(),
		db: r.db,
		wo: r.wo,
	}
}

// CommitWriteBatch ...
func (r *KV) CommitWriteBatch(wb kv.IWriteBatch) error {
	pwb, ok := wb.(*pebbleWriteBatch)
	if !ok {
		panic("unknown type")
	}
	if pwb.db != r.db {
		panic("pwb.db != r.db")
	}
	return r.db.Apply(pwb.wb, r.wo)
}

// BulkRemoveEntries ...
func (r *KV) BulkRemoveEntries(fk []byte, lk []byte) (err error) {
	wb := r.db.NewBatch()
	defer func() {
		err = firstError(err, wb.Close())
	}()
	if err := wb.DeleteRange(fk, lk, r.wo); err != nil {
		return err
	}
	return r.db.Apply(wb, r.wo)
}

// CompactEntries ...
func (r *KV) CompactEntries(fk []byte, lk []byte) error {
	return r.db.Compact(fk, lk, false)
}

// FullCompaction ...
func (r *KV) FullCompaction() error {
	fk := make([]byte, kv.MaxKeyLength)
	lk := make([]byte, kv.MaxKeyLength)
	for i := uint64(0); i < kv.MaxKeyLength; i++ {
		fk[i] = 0
		lk[i] = 0xFF
	}
	return r.db.Compact(fk, lk, false)
}
````

## File: internal/logdb/kv/pebble/monkey.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +build dragonboat_monkeytest

package pebble

const (
	inMonkeyTesting = true
)
````

## File: internal/logdb/kv/pebble/monkeynoop.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//go:build !dragonboat_monkeytest
// +build !dragonboat_monkeytest

package pebble

const (
	inMonkeyTesting = false
)
````

## File: internal/logdb/kv/kv.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package kv

import (
	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/vfs"
)

const (
	// MaxKeyLength is the max length of keys allowed
	MaxKeyLength uint64 = 1024
)

// Factory is the factory function type used for creating IKVStore instances.
type Factory func(config.LogDBConfig,
	LogDBCallback, string, string, vfs.IFS) (IKVStore, error)

// LogDBCallback is a callback function called by the LogDB
type LogDBCallback func(busy bool)

// IWriteBatch is the interface representing a write batch capable of
// atomically writing many key-value pairs to the key-value store.
type IWriteBatch interface {
	Destroy()
	Put([]byte, []byte)
	Delete([]byte)
	Clear()
	Count() int
}

// IKVStore is the interface used by the RDB struct to access the underlying
// Key-Value store.
type IKVStore interface {
	// Name is the IKVStore name.
	Name() string
	// Close closes the underlying Key-Value store.
	Close() error
	// IterateValue iterates the key range specified by the first key fk and
	// last key lk. The inc boolean flag indicates whether it is inclusive for
	// the last key lk. For each iterated entry, the specified op will be invoked
	// on that key-value pair, the specified op func returns a boolean flag to
	// indicate whether IterateValue should continue to iterate entries.
	IterateValue(fk []byte,
		lk []byte, inc bool, op func(key []byte, data []byte) (bool, error)) error
	// GetValue queries the value specified the input key, the returned value
	// byte slice is passed to the specified op func.
	GetValue(key []byte, op func([]byte) error) error
	// Save value saves the specified key value pair to the underlying key-value
	// pair.
	SaveValue(key []byte, value []byte) error
	// DeleteValue deletes the key-value pair specified by the input key.
	DeleteValue(key []byte) error
	// GetWriteBatch returns an IWriteBatch object to be used by RDB.
	GetWriteBatch() IWriteBatch
	// CommitWriteBatch atomically writes everything included in the write batch
	// to the underlying key-value store.
	CommitWriteBatch(wb IWriteBatch) error
	// BulkRemoveEntries removes entries specified by the range [firstKey,
	// lastKey). BulkRemoveEntries is called in the main execution thread of raft,
	// it is supposed to immediately return without significant delay.
	// BulkRemoveEntries is usually implemented in KV store's range delete feature.
	BulkRemoveEntries(firstKey []byte, lastKey []byte) error
	// CompactEntries reclaims the underlying disk space used for storing entries
	// specified the input range.
	CompactEntries(firstKey []byte, lastKey []byte) error
	// FullCompaction compact the entire key space.
	FullCompaction() error
}
````

## File: internal/logdb/tee/tee.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tee

import (
	"path/filepath"
	"reflect"
	"sort"
	"sync"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/logutil"
	"github.com/lni/goutils/syncutil"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/logdb"
	"github.com/lni/dragonboat/v4/internal/logdb/kv"
	"github.com/lni/dragonboat/v4/internal/logdb/kv/pebble"
	"github.com/lni/dragonboat/v4/internal/tan"
	"github.com/lni/dragonboat/v4/logger"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

var (
	plog = logger.GetLogger("LogDB")
)

var dn = logutil.DescribeNode

func assertSameError(shardID uint64, replicaID uint64, e1 error, e2 error) {
	if errors.Is(e1, e2) || errors.Is(e2, e1) {
		return
	}
	plog.Panicf("conflict errors, %s, e1 %v, e2 %v",
		dn(shardID, replicaID), e1, e2)
}

// LogDB is a special LogDB module used for testing purposes.
type LogDB struct {
	mu      sync.Mutex
	stopper *syncutil.Stopper
	odb     raftio.ILogDB
	ndb     raftio.ILogDB
}

// NewTanLogDB creates a new RocksDB based LogDB instance
func NewTanLogDB(nhConfig config.NodeHostConfig,
	cb config.LogDBCallback,
	dirs []string, wals []string) (raftio.ILogDB, error) {
	return tan.Factory.Create(nhConfig, cb, dirs, wals)
}

// NewPebbleLogDB creates a new LogDB instance.
func NewPebbleLogDB(nhConfig config.NodeHostConfig,
	cb config.LogDBCallback,
	dirs []string, wals []string) (raftio.ILogDB, error) {
	return newKVLogDB(nhConfig, cb, dirs, wals, "tee-pebble", pebble.NewKVStore)
}

func newKVLogDB(nhConfig config.NodeHostConfig,
	cb config.LogDBCallback, dirs []string, wals []string,
	subdir string, f kv.Factory) (raftio.ILogDB, error) {
	ndirs := make([]string, 0)
	nwals := make([]string, 0)
	for _, v := range dirs {
		ndirs = append(ndirs, filepath.Join(v, subdir))
	}
	for _, v := range wals {
		nwals = append(nwals, filepath.Join(v, subdir))
	}
	return logdb.NewLogDB(nhConfig, cb, ndirs, nwals, false, false, f)
}

// NewTeeLogDB creates a new LogDB instance backed by a pebble and a tan
// based ILogDB.
func NewTeeLogDB(nhConfig config.NodeHostConfig,
	cb config.LogDBCallback,
	dirs []string, wals []string) (raftio.ILogDB, error) {
	odb, err := NewTanLogDB(nhConfig, cb, dirs, wals)
	if err != nil {
		return nil, err
	}
	ndb, err := NewPebbleLogDB(nhConfig, cb, dirs, wals)
	if err != nil {
		return nil, err
	}
	return MakeTeeLogDB(odb, ndb), nil
}

// MakeTeeLogDB returns a LogDB instance combined from the specified odb and
// ndb instances.
func MakeTeeLogDB(odb raftio.ILogDB, ndb raftio.ILogDB) raftio.ILogDB {
	return &LogDB{
		stopper: syncutil.NewStopper(),
		odb:     odb,
		ndb:     ndb,
	}
}

// Name ...
func (t *LogDB) Name() string {
	return "Tee"
}

// Close ...
func (t *LogDB) Close() error {
	t.stopper.Stop()
	if err := t.odb.Close(); err != nil {
		return nil
	}
	return t.ndb.Close()
}

// BinaryFormat ...
func (t *LogDB) BinaryFormat() uint32 {
	o := t.odb.BinaryFormat()
	n := t.ndb.BinaryFormat()
	if o != n {
		plog.Panicf("binary format changed, odb %d, ndb %d", o, n)
	}
	return o
}

// ListNodeInfo ...
func (t *LogDB) ListNodeInfo() ([]raftio.NodeInfo, error) {
	t.mu.Lock()
	defer t.mu.Unlock()
	o, oe := t.odb.ListNodeInfo()
	n, ne := t.ndb.ListNodeInfo()
	assertSameError(0, 0, oe, ne)
	if oe != nil {
		return nil, oe
	}
	sort.Slice(o, func(i, j int) bool {
		if o[i].ShardID == o[j].ShardID {
			return o[i].ReplicaID < o[j].ReplicaID
		}
		return o[i].ShardID < o[j].ShardID
	})
	sort.Slice(n, func(i, j int) bool {
		if n[i].ShardID == n[j].ShardID {
			return n[i].ReplicaID < n[j].ReplicaID
		}
		return n[i].ShardID < n[j].ShardID
	})
	if !reflect.DeepEqual(o, n) {
		plog.Panicf("conflict NodeInfo list, %+v, %+v", o, n)
	}
	return o, nil
}

// SaveBootstrapInfo ...
func (t *LogDB) SaveBootstrapInfo(shardID uint64,
	replicaID uint64, bootstrap pb.Bootstrap) error {
	t.mu.Lock()
	defer t.mu.Unlock()
	oe := t.odb.SaveBootstrapInfo(shardID, replicaID, bootstrap)
	ne := t.ndb.SaveBootstrapInfo(shardID, replicaID, bootstrap)
	assertSameError(shardID, replicaID, oe, ne)
	return oe
}

// GetBootstrapInfo ...
func (t *LogDB) GetBootstrapInfo(shardID uint64,
	replicaID uint64) (pb.Bootstrap, error) {
	t.mu.Lock()
	defer t.mu.Unlock()
	ob, oe := t.odb.GetBootstrapInfo(shardID, replicaID)
	nb, ne := t.ndb.GetBootstrapInfo(shardID, replicaID)
	assertSameError(shardID, replicaID, oe, ne)
	if oe != nil {
		return pb.Bootstrap{}, oe
	}
	if !reflect.DeepEqual(ob, nb) {
		plog.Panicf("%s conflict GetBootstrapInfo values, %+v, %+v",
			dn(shardID, replicaID), ob, nb)
	}
	return ob, nil
}

// SaveRaftState ...
func (t *LogDB) SaveRaftState(updates []pb.Update, shardID uint64) error {
	t.mu.Lock()
	defer t.mu.Unlock()
	oe := t.odb.SaveRaftState(updates, shardID)
	ne := t.ndb.SaveRaftState(updates, shardID)
	assertSameError(0, 0, oe, ne)
	return oe
}

// IterateEntries ...
func (t *LogDB) IterateEntries(ents []pb.Entry,
	size uint64, shardID uint64, replicaID uint64, low uint64,
	high uint64, maxSize uint64) ([]pb.Entry, uint64, error) {
	t.mu.Lock()
	defer t.mu.Unlock()
	ce := make([]pb.Entry, len(ents))
	copy(ce, ents)
	ov, os, oe := t.odb.IterateEntries(ents,
		size, shardID, replicaID, low, high, maxSize)
	nv, ns, ne := t.ndb.IterateEntries(ce,
		size, shardID, replicaID, low, high, maxSize)
	assertSameError(0, 0, oe, ne)
	if oe != nil {
		return nil, 0, oe
	}
	if os != ns {
		plog.Infof("")
		plog.Panicf("%s conflict sizes, %d, %d, %+v, %+v",
			dn(shardID, replicaID), os, ns, ov, nv)
	}
	if len(ov) != 0 || len(nv) != 0 {
		if !reflect.DeepEqual(ov, nv) {
			plog.Panicf("%s conflict entry lists, len: %d, %+v \n\n len: %d, %+v",
				dn(shardID, replicaID), len(ov), ov, len(nv), nv)
		}
	}
	return ov, os, nil
}

// ReadRaftState ...
func (t *LogDB) ReadRaftState(shardID uint64,
	replicaID uint64, lastIndex uint64) (raftio.RaftState, error) {
	t.mu.Lock()
	defer t.mu.Unlock()
	os, oe := t.odb.ReadRaftState(shardID, replicaID, lastIndex)
	ns, ne := t.ndb.ReadRaftState(shardID, replicaID, lastIndex)
	assertSameError(shardID, replicaID, oe, ne)
	if oe != nil {
		return raftio.RaftState{}, oe
	}
	if !reflect.DeepEqual(os, ns) {
		plog.Panicf("%s conflict ReadRaftState values, %+v, %+v",
			dn(shardID, replicaID), os, ns)
	}
	return os, nil
}

// RemoveEntriesTo ...
func (t *LogDB) RemoveEntriesTo(shardID uint64,
	replicaID uint64, index uint64) error {
	t.mu.Lock()
	defer t.mu.Unlock()
	oe := t.odb.RemoveEntriesTo(shardID, replicaID, index)
	ne := t.ndb.RemoveEntriesTo(shardID, replicaID, index)
	assertSameError(shardID, replicaID, oe, ne)
	return oe
}

// SaveSnapshots ...
func (t *LogDB) SaveSnapshots(updates []pb.Update) error {
	t.mu.Lock()
	defer t.mu.Unlock()
	oe := t.odb.SaveSnapshots(updates)
	ne := t.ndb.SaveSnapshots(updates)
	assertSameError(0, 0, oe, ne)
	return oe
}

// GetSnapshot ...
func (t *LogDB) GetSnapshot(shardID uint64,
	replicaID uint64) (pb.Snapshot, error) {
	t.mu.Lock()
	defer t.mu.Unlock()
	ov, oe := t.odb.GetSnapshot(shardID, replicaID)
	nv, ne := t.ndb.GetSnapshot(shardID, replicaID)
	assertSameError(shardID, replicaID, oe, ne)
	if oe != nil {
		return pb.Snapshot{}, oe
	}
	if !reflect.DeepEqual(ov, nv) {
		plog.Panicf("%s conflict snapshot lists, \n%+v \n\n %+v",
			dn(shardID, replicaID), ov, nv)
	}
	return ov, nil
}

// RemoveNodeData ...
func (t *LogDB) RemoveNodeData(shardID uint64, replicaID uint64) error {
	t.mu.Lock()
	defer t.mu.Unlock()
	oe := t.odb.RemoveNodeData(shardID, replicaID)
	ne := t.ndb.RemoveNodeData(shardID, replicaID)
	assertSameError(shardID, replicaID, oe, ne)
	return oe
}

// ImportSnapshot ...
func (t *LogDB) ImportSnapshot(ss pb.Snapshot, replicaID uint64) error {
	t.mu.Lock()
	defer t.mu.Unlock()
	oe := t.odb.ImportSnapshot(ss, replicaID)
	ne := t.ndb.ImportSnapshot(ss, replicaID)
	assertSameError(ss.ShardID, replicaID, oe, ne)
	return oe
}

// CompactEntriesTo ...
func (t *LogDB) CompactEntriesTo(shardID uint64,
	replicaID uint64, index uint64) (<-chan struct{}, error) {
	t.mu.Lock()
	defer t.mu.Unlock()
	done := make(chan struct{}, 1)
	oc, oe := t.odb.CompactEntriesTo(shardID, replicaID, index)
	nc, ne := t.ndb.CompactEntriesTo(shardID, replicaID, index)
	assertSameError(shardID, replicaID, oe, ne)
	if oe != nil {
		return nil, oe
	}
	t.stopper.RunWorker(func() {
		count := 0
		for {
			select {
			case <-oc:
				count++
			case <-nc:
				count++
			case <-t.stopper.ShouldStop():
				done <- struct{}{}
				return
			}
			if count == 2 {
				done <- struct{}{}
				return
			}
		}
	})
	return done, nil
}
````

## File: internal/logdb/batch_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"math"
	"reflect"
	"testing"

	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

func TestGetBatchIDRange(t *testing.T) {
	tests := []struct {
		low       uint64
		high      uint64
		batchLow  uint64
		batchHigh uint64
	}{
		{2, 3, 0, 1},
		{1, batchSize, 0, 1},
		{batchSize, 2 * batchSize, 1, 2},
		{1, batchSize + 1, 0, 2},
		{batchSize, 2*batchSize + 1, 1, 3},
		{batchSize + 1, 2 * batchSize, 1, 2},
		{batchSize + 1, 2*batchSize + 1, 1, 3},
	}

	for idx, tt := range tests {
		low, high := getBatchIDRange(tt.low, tt.high)
		assert.Equal(t, tt.batchLow, low,
			"%d, low %d, want %d", idx, low, tt.batchLow)
		assert.Equal(t, tt.batchHigh, high,
			"%d, high %d, want %d", idx, high, tt.batchHigh)
	}
}

func TestEntryBatchFieldsNotCompactedWhenIndexHasGap(t *testing.T) {
	fn := func() pb.EntryBatch {
		ents := make([]pb.Entry, 0)
		for i := uint64(1); i < batchSize; i++ {
			if i == batchSize/2 {
				continue
			}
			e := pb.Entry{
				Index: i,
				Term:  2,
			}
			ents = append(ents, e)
		}
		return pb.EntryBatch{Entries: ents}
	}
	eb1 := fn()
	eb2 := fn()
	assert.True(t, reflect.DeepEqual(&eb1, &eb2), "initial value not equal")
	eb1 = compactBatchFields(eb1)
	assert.True(t, reflect.DeepEqual(&eb1, &eb2), "unexpectedly compacted")
}

func TestEntryBatchFieldsNotCompactedWhenMultipleTerms(t *testing.T) {
	fn := func() pb.EntryBatch {
		ents := make([]pb.Entry, 0)
		for i := uint64(1); i < batchSize; i++ {
			term := uint64(1)
			if i == batchSize-1 || i == batchSize-2 {
				term = uint64(2)
			}
			e := pb.Entry{
				Index: i,
				Term:  term,
			}
			ents = append(ents, e)
		}
		return pb.EntryBatch{Entries: ents}
	}
	eb1 := fn()
	eb2 := fn()
	assert.True(t, reflect.DeepEqual(&eb1, &eb2), "initial value not equal")
	eb1 = compactBatchFields(eb1)
	assert.True(t, reflect.DeepEqual(&eb1, &eb2), "unexpectedly compacted")
}

func TestEntryBatchFieldsCanBeCompacted(t *testing.T) {
	fn := func() pb.EntryBatch {
		ents := make([]pb.Entry, 0)
		for i := uint64(1); i < batchSize; i++ {
			e := pb.Entry{
				Index: i,
				Term:  1,
			}
			ents = append(ents, e)
		}
		return pb.EntryBatch{Entries: ents}
	}
	eb1 := fn()
	eb2 := fn()
	assert.True(t, reflect.DeepEqual(&eb1, &eb2), "input not equal")
	eb1 = compactBatchFields(eb1)
	assert.False(t, reflect.DeepEqual(&eb1, &eb2), "eb not changed")
	assert.Less(t, eb1.Size(), eb2.Size(), "size didn't reduce")
	for i := 0; i < len(eb1.Entries); i++ {
		if i == 0 {
			assert.NotZero(t, eb1.Entries[i].Index, "first index is 0, %+v",
				eb1.Entries)
			assert.NotZero(t, eb1.Entries[i].Term, "first term is 0, %+v",
				eb1.Entries)
		} else {
			assert.Zero(t, eb1.Entries[i].Index, "first index/term is not 0")
			assert.Zero(t, eb1.Entries[i].Term, "first index/term is not 0")
		}
	}
	eb1 = restoreBatchFields(eb1)
	assert.True(t, reflect.DeepEqual(&eb1, &eb2), "not restored")
}

func TestNotCompactedEntryBatchIsNotRestored(t *testing.T) {
	fn := func() pb.EntryBatch {
		ents := make([]pb.Entry, 0)
		for i := uint64(1); i < batchSize; i++ {
			e := pb.Entry{
				Index: i,
				Term:  1,
			}
			ents = append(ents, e)
		}
		return pb.EntryBatch{Entries: ents}
	}
	eb1 := fn()
	eb2 := fn()
	assert.True(t, reflect.DeepEqual(&eb1, &eb2), "input not equal")
	eb1 = restoreBatchFields(eb1)
	assert.True(t, reflect.DeepEqual(&eb1, &eb2), "not restored")
}

func TestCompactBatchFieldsPanicWhenBatchIsTooSmall(t *testing.T) {
	require.Panics(t, func() {
		compactBatchFields(pb.EntryBatch{})
	})
	require.Panics(t, func() {
		compactBatchFields(pb.EntryBatch{Entries: []pb.Entry{{}}})
	})
}

func TestRestoreBatchFieldsPanicWhenBatchIsTooSmall(t *testing.T) {
	require.Panics(t, func() {
		restoreBatchFields(pb.EntryBatch{})
	})
	require.Panics(t, func() {
		restoreBatchFields(pb.EntryBatch{Entries: []pb.Entry{{}}})
	})
}

func TestMergeFirstBatchPanicWhenInputBatchIsEmpty(t *testing.T) {
	empty := pb.EntryBatch{}
	nonEmpty := pb.EntryBatch{Entries: []pb.Entry{{Index: 1, Term: 1}}}
	require.Panics(t, func() {
		getMergedFirstBatch(empty, nonEmpty)
	})
	require.Panics(t, func() {
		getMergedFirstBatch(nonEmpty, empty)
	})
}

func TestMergeFirstBatchPanicWhenIncomingBatchIsNotMoreRecent(t *testing.T) {
	eb := pb.EntryBatch{Entries: []pb.Entry{{Index: batchSize, Term: 1}}}
	lb := pb.EntryBatch{Entries: []pb.Entry{{Index: 2 * batchSize, Term: 1}}}
	require.Panics(t, func() {
		getMergedFirstBatch(eb, lb)
	})
}

func TestIncomingBatchIsTheMergedBatchWhenMoreRecentThanLastBatch(t *testing.T) {
	eb := pb.EntryBatch{Entries: []pb.Entry{{Index: 2 * batchSize, Term: 1}}}
	lb := pb.EntryBatch{Entries: []pb.Entry{{Index: batchSize, Term: 1}}}
	result := getMergedFirstBatch(eb, lb)
	assert.True(t, reflect.DeepEqual(&result, &eb), "unexpected result")
}

func TestGetMergedFirstBatch(t *testing.T) {
	tests := []struct {
		ebfirst  uint64
		eblast   uint64
		lbfirst  uint64
		lblast   uint64
		mfirst   uint64
		mlast    uint64
		newindex uint64
	}{
		{1, 10, 2, 10, 1, 10, 1},
		{1, 10, 2, 11, 1, 10, 1},
		{1, 10, 2, 9, 1, 10, 1},
		{2, 10, 2, 10, 2, 10, 2},
		{2, 10, 2, 9, 2, 10, 2},
		{2, 10, 2, 11, 2, 10, 2},
		{2, 10, 1, 10, 1, 10, 2},
		{3, 10, 1, 3, 1, 10, 3},
		{3, 10, 1, 2, 1, 10, 3},
		{3, 10, 1, 4, 1, 10, 3},
	}

	for idx, tt := range tests {
		eb := pb.EntryBatch{}
		lb := pb.EntryBatch{}
		for i := tt.ebfirst; i <= tt.eblast; i++ {
			entry := pb.Entry{
				Index: i,
				Term:  2,
			}
			eb.Entries = append(eb.Entries, entry)
		}
		for i := tt.lbfirst; i <= tt.lblast; i++ {
			entry := pb.Entry{
				Index: i,
				Term:  1,
			}
			lb.Entries = append(lb.Entries, entry)
		}
		result := getMergedFirstBatch(eb, lb)
		assert.Equal(t, tt.mfirst, result.Entries[0].Index,
			"%d, first index %d, want %d", idx, result.Entries[0].Index,
			tt.mfirst)
		assert.Equal(t, tt.mlast, result.Entries[len(result.Entries)-1].Index,
			"%d, last index %d, want %d", idx,
			result.Entries[len(result.Entries)-1].Index, tt.mlast)
		for i := 0; i < len(result.Entries); i++ {
			if result.Entries[i].Index >= tt.newindex {
				assert.Equal(t, uint64(2), result.Entries[i].Term, "unexpected term")
			}
		}
	}
}

func TestEntryBatchWillNotBeMergedToPreviousBatch(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		shardID := uint64(0)
		replicaID := uint64(4)
		e1 := pb.Entry{
			Term:  1,
			Index: 1,
			Type:  pb.ApplicationEntry,
		}
		ud := pb.Update{
			EntriesToSave: []pb.Entry{e1},
			ShardID:       shardID,
			ReplicaID:     replicaID,
		}
		err := db.SaveRaftState([]pb.Update{ud}, 1)
		assert.NoError(t, err, "failed to save recs")
		nextIndex := 1 + batchSize
		e2 := pb.Entry{
			Term:  1,
			Index: nextIndex,
			Type:  pb.ApplicationEntry,
		}
		ud = pb.Update{
			EntriesToSave: []pb.Entry{e2},
			ShardID:       shardID,
			ReplicaID:     replicaID,
		}
		err = db.SaveRaftState([]pb.Update{ud}, 1)
		assert.NoError(t, err, "failed to save recs")
		maxIndex, err := db.(*ShardedDB).shards[0].getMaxIndex(shardID,
			replicaID)
		assert.NoError(t, err, "failed to get max index")
		assert.Equal(t, nextIndex, maxIndex, "unexpected max index")
		eb, ok := db.(*ShardedDB).shards[0].entries.(*batchedEntries).
			getBatchFromDB(shardID, replicaID, 1)
		assert.True(t, ok, "failed to get the eb")
		assert.Equal(t, 1, len(eb.Entries), "unexpected len %d, want 1",
			len(eb.Entries))
		assert.Equal(t, nextIndex, eb.Entries[0].Index,
			"unexpected index %d, want 10", eb.Entries[0].Index)
	}
	fs := vfs.GetTestFS()
	runBatchedLogDBTest(t, tf, fs)
}

func TestEntryBatchMergedNotLastBatch(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		shardID := uint64(0)
		replicaID := uint64(4)
		ud := pb.Update{
			EntriesToSave: make([]pb.Entry, 0),
			ShardID:       shardID,
			ReplicaID:     replicaID,
		}
		for i := uint64(1); i < batchSize+4; i++ {
			e := pb.Entry{Index: i, Term: 1}
			ud.EntriesToSave = append(ud.EntriesToSave, e)
		}
		err := db.SaveRaftState([]pb.Update{ud}, 1)
		assert.NoError(t, err, "failed to save recs")
		ud = pb.Update{
			EntriesToSave: make([]pb.Entry, 0),
			ShardID:       shardID,
			ReplicaID:     replicaID,
		}
		for i := batchSize - 4; i <= batchSize+2; i++ {
			e := pb.Entry{Index: i, Term: 2}
			ud.EntriesToSave = append(ud.EntriesToSave, e)
		}
		err = db.SaveRaftState([]pb.Update{ud}, 1)
		assert.NoError(t, err, "failed to save recs")
		maxIndex, err := db.(*ShardedDB).shards[0].getMaxIndex(shardID,
			replicaID)
		assert.NoError(t, err, "failed to get max index")
		assert.Equal(t, batchSize+2, maxIndex, "unexpected max index")
		eb, ok := db.(*ShardedDB).shards[0].entries.(*batchedEntries).
			getBatchFromDB(shardID, replicaID, 0)
		assert.True(t, ok, "failed to get the eb")
		assert.Equal(t, batchSize-1, uint64(len(eb.Entries)),
			"unexpected len %d, want %d", len(eb.Entries), batchSize-1)
		for i := uint64(0); i < batchSize-1; i++ {
			e := eb.Entries[i]
			assert.Equal(t, i+1, e.Index, "unexpected index %d, want %d",
				e.Index, i+1)
			if e.Index < batchSize-4 {
				assert.Equal(t, uint64(1), e.Term, "unexpected term %d", e.Term)
			} else {
				assert.Equal(t, uint64(2), e.Term, "unexpected term %d", e.Term)
			}
		}
	}
	fs := vfs.GetTestFS()
	runBatchedLogDBTest(t, tf, fs)
}

func TestSaveEntriesAcrossMultipleBatches(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		shardID := uint64(0)
		replicaID := uint64(4)
		e1 := pb.Entry{
			Term:  1,
			Index: 1,
			Type:  pb.ApplicationEntry,
		}
		ud := pb.Update{
			EntriesToSave: []pb.Entry{e1},
			ShardID:       shardID,
			ReplicaID:     replicaID,
		}
		err := db.SaveRaftState([]pb.Update{ud}, 1)
		assert.NoError(t, err, "failed to save recs")
		e2 := pb.Entry{
			Term:  1,
			Index: 2,
			Type:  pb.ApplicationEntry,
		}
		ud = pb.Update{
			EntriesToSave: []pb.Entry{e2},
			ShardID:       shardID,
			ReplicaID:     replicaID,
		}
		err = db.SaveRaftState([]pb.Update{ud}, 1)
		assert.NoError(t, err, "failed to save recs")
		ud = pb.Update{
			EntriesToSave: make([]pb.Entry, 0),
			ShardID:       shardID,
			ReplicaID:     replicaID,
		}
		for idx := uint64(3); idx <= batchSize+1; idx++ {
			e := pb.Entry{
				Term:  1,
				Index: idx,
			}
			ud.EntriesToSave = append(ud.EntriesToSave, e)
		}
		err = db.SaveRaftState([]pb.Update{ud}, 1)
		assert.NoError(t, err, "failed to save recs")
		ents, _, err := db.IterateEntries([]pb.Entry{}, 0,
			shardID, replicaID, 1, batchSize+2, math.MaxUint64)
		assert.NoError(t, err, "iterate entries failed %v", err)
		assert.Equal(t, batchSize+1, uint64(len(ents)),
			"ents sz %d, want %d", len(ents), batchSize+1)
		eb, ok := db.(*ShardedDB).shards[0].entries.(*batchedEntries).
			getBatchFromDB(shardID, replicaID, 1)
		assert.True(t, ok, "failed to get first batch")
		for _, e := range eb.Entries {
			plog.Infof("idx %d", e.Index)
		}
	}
	fs := vfs.GetTestFS()
	runBatchedLogDBTest(t, tf, fs)
}
````

## File: internal/logdb/batch.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"math"

	"github.com/cockroachdb/errors"

	"github.com/lni/dragonboat/v4/internal/logdb/kv"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

//
// Rather than storing each raft entry as an individual record, dragonboat can
// batches entries first and store multiple consecutive entries as a batch
// record. This idea is based on the observations that:
//  * consecutive entries are usually saved together
//  * consecutive entries are usually read and consumed together
//  * it is quite CPU expensive to individually insert entries to the
//    underlying memtables used by the Key-Value stores.
//
// Maintaining and reusing some kind of insert hint to avoid repeatedly locating
// the memtable positions for new entries is not enough as profiling shows that
// only saves a relatively small percentage of CPU cycles.
//
// We also compact redundant index/term values whenever possible. In such
// approach, rather than repeatedly storing the consecutive increamental
// index values and the identifical term values, we represent them in the
// way implemented in the compactBatchFields function below.
//
// These optimizations helped to achieve better latency and throughput
// performance.
//
// The obvious disadvantages of doing these are -
//  * slightly increased complexity
//  * last few entries may need to be stored in RAM waiting to be used to
//    form the next batch, this increases the memory footprint and
//   	potentially be a problem when there are large number of raft groups
//    or some raft groups have huge batches
//
// To the maximum of our knowledge, dragonboat is the original inventor
// of the optimizations above, they were publicly disclosed on github.com
// when dragnoboat made its first public release.
//
// To use/implement the above optimizations in your software, please include
// the above copyright notice in your source file, dragonboat's Apache2 license
// also explicitly requires to include dragonboat's NOTICE file.
//

func getBatchID(index uint64) uint64 {
	return index / batchSize
}

func entriesSize(ents []pb.Entry) uint64 {
	sz := uint64(0)
	for _, e := range ents {
		sz += uint64(e.SizeUpperLimit())
	}
	return sz
}

func getBatchIDRange(low uint64, high uint64) (uint64, uint64) {
	lowBatchID := getBatchID(low)
	highBatchID := getBatchID(high)
	if high%batchSize == 0 {
		return lowBatchID, highBatchID
	}
	return lowBatchID, highBatchID + 1
}

func restoreBatchFields(eb pb.EntryBatch) pb.EntryBatch {
	if len(eb.Entries) <= 1 {
		panic("restore called on small batch")
	}
	if eb.Entries[len(eb.Entries)-1].Term == 0 {
		term := eb.Entries[0].Term
		idx := eb.Entries[0].Index
		for i := 1; i < len(eb.Entries); i++ {
			eb.Entries[i].Term = term
			eb.Entries[i].Index = idx + uint64(i)
		}
	}
	return eb
}

func compactBatchFields(eb pb.EntryBatch) pb.EntryBatch {
	if len(eb.Entries) <= 1 {
		panic("compact called on small batch")
	}
	expLastIdx := eb.Entries[0].Index + uint64(len(eb.Entries)-1)
	if eb.Entries[0].Term == eb.Entries[len(eb.Entries)-1].Term &&
		expLastIdx == eb.Entries[len(eb.Entries)-1].Index {
		for i := 1; i < len(eb.Entries); i++ {
			eb.Entries[i].Term = 0
			eb.Entries[i].Index = 0
		}
	}
	return eb
}

func getMergedFirstBatch(eb pb.EntryBatch, lb pb.EntryBatch) pb.EntryBatch {
	if len(eb.Entries) == 0 || len(lb.Entries) == 0 {
		panic("getMergedFirstBatch called on empty batch")
	}
	batchID := getBatchID(eb.Entries[0].Index)
	if batchID < getBatchID(lb.Entries[0].Index) {
		panic("eb batch < lb batch")
	}
	if batchID > getBatchID(lb.Entries[0].Index) {
		return eb
	}
	if eb.Entries[0].Index > lb.Entries[0].Index {
		if eb.Entries[0].Index <= lb.Entries[len(lb.Entries)-1].Index {
			firstIndex := eb.Entries[0].Index
			i := 0
			for ; i < len(lb.Entries); i++ {
				if lb.Entries[i].Index >= firstIndex {
					break
				}
			}
			lb.Entries = append(lb.Entries[:i], eb.Entries...)
		} else {
			lb.Entries = append(lb.Entries, eb.Entries...)
		}
		return lb
	}
	return eb
}

type batchedEntries struct {
	cs   *cache
	keys *keyPool
	kvs  kv.IKVStore
}

var _ entryManager = (*batchedEntries)(nil)

func newBatchedEntries(cs *cache,
	keys *keyPool, kvs kv.IKVStore) entryManager {
	return &batchedEntries{
		cs:   cs,
		keys: keys,
		kvs:  kvs,
	}
}

func (be *batchedEntries) iterate(ents []pb.Entry, maxIndex uint64,
	size uint64, shardID uint64, replicaID uint64, low uint64, high uint64,
	maxSize uint64) ([]pb.Entry, uint64, error) {
	if high > maxIndex+1 {
		high = maxIndex + 1
	}
	lowID, highID := getBatchIDRange(low, high)
	ebs, err := be.iterateBatches(shardID, replicaID, lowID, highID)
	if err != nil {
		return nil, 0, err
	}
	if len(ebs) == 0 {
		return ents, size, nil
	}
	exp := low
	for _, eb := range ebs {
		if len(eb.Entries) > 1 {
			eb = restoreBatchFields(eb)
		}
		for _, e := range eb.Entries {
			if e.Index >= low && e.Index < high {
				if e.Index != exp {
					return ents, size, nil
				}
				exp = e.Index + 1
				size += uint64(e.SizeUpperLimit())
				ents = append(ents, e)
				if size > maxSize {
					return ents, size, nil
				}
			}
		}
	}
	return ents, entriesSize(ents), nil
}

func (be *batchedEntries) iterateBatches(shardID uint64,
	replicaID uint64, low uint64, high uint64) ([]pb.EntryBatch, error) {
	ents := make([]pb.EntryBatch, 0)
	if low+1 == high {
		e, ok := be.getBatchFromDB(shardID, replicaID, low)
		if !ok {
			return []pb.EntryBatch{}, nil
		}
		ents = append(ents, e)
		return ents, nil
	}
	fk := be.keys.get()
	lk := be.keys.get()
	defer fk.Release()
	defer lk.Release()
	fk.SetEntryBatchKey(shardID, replicaID, low)
	lk.SetEntryBatchKey(shardID, replicaID, high)
	expectedID := low
	op := func(key []byte, data []byte) (bool, error) {
		var eb pb.EntryBatch
		pb.MustUnmarshal(&eb, data)
		if getBatchID(eb.Entries[0].Index) != expectedID {
			return false, nil
		}
		ents = append(ents, eb)
		expectedID++
		return true, nil
	}
	if err := be.kvs.IterateValue(fk.Key(), lk.Key(), false, op); err != nil {
		return nil, err
	}
	return ents, nil
}

func (be *batchedEntries) getRange(shardID uint64,
	replicaID uint64, snapshotIndex uint64, maxIndex uint64) (uint64, uint64, error) {
	fk := be.keys.get()
	lk := be.keys.get()
	defer fk.Release()
	defer lk.Release()
	low, high := getBatchIDRange(snapshotIndex, maxIndex+1)
	fk.SetEntryBatchKey(shardID, replicaID, low)
	lk.SetEntryBatchKey(shardID, replicaID, high)
	firstIndex := uint64(0)
	length := uint64(0)
	op := func(key []byte, data []byte) (bool, error) {
		var eb pb.EntryBatch
		pb.MustUnmarshal(&eb, data)
		if len(eb.Entries) == 0 {
			panic("empty batch found")
		}
		if len(eb.Entries) > 1 {
			eb = restoreBatchFields(eb)
		}
		for _, e := range eb.Entries {
			if e.Index >= snapshotIndex && e.Index <= maxIndex {
				if firstIndex == uint64(0) {
					firstIndex = e.Index
					return false, nil
				}
			}
		}
		return true, nil
	}
	if err := be.kvs.IterateValue(fk.Key(), lk.Key(), false, op); err != nil {
		return 0, 0, err
	}
	if firstIndex == 0 && maxIndex != 0 {
		panic("firstIndex == 0 && maxIndex != 0")
	}
	if firstIndex > 0 {
		length = maxIndex - firstIndex + 1
	}
	return firstIndex, length, nil
}

func (be *batchedEntries) rangedOp(shardID uint64,
	replicaID uint64, index uint64, op func(fk *Key, lk *Key) error) error {
	fk := be.keys.get()
	lk := be.keys.get()
	defer fk.Release()
	defer lk.Release()
	batchID := getBatchID(index)
	if batchID == 0 || batchID == 1 {
		return nil
	}
	fk.SetEntryBatchKey(shardID, replicaID, 0)
	lk.SetEntryBatchKey(shardID, replicaID, batchID-1)
	return op(fk, lk)
}

func (be *batchedEntries) recordBatch(wb kv.IWriteBatch,
	shardID uint64, replicaID uint64, eb pb.EntryBatch,
	firstBatchID uint64, lastBatchID uint64, ctx IContext) {
	if len(eb.Entries) == 0 {
		return
	}
	batchID := getBatchID(eb.Entries[0].Index)
	var meb pb.EntryBatch
	if firstBatchID == batchID {
		lb := ctx.GetLastEntryBatch()
		meb = be.getMergedFirstBatch(shardID, replicaID, eb, lb)
	} else {
		meb = eb
	}
	if lastBatchID == batchID {
		be.cs.setLastBatch(shardID, replicaID, meb)
	}
	if len(meb.Entries) > 1 {
		meb = compactBatchFields(meb)
	}
	szul := meb.SizeUpperLimit()
	data := ctx.GetValueBuffer(uint64(szul))
	data = pb.MustMarshalTo(&meb, data)
	k := ctx.GetKey()
	k.SetEntryBatchKey(shardID, replicaID, batchID)
	wb.Put(k.Key(), data)
}

func (be *batchedEntries) record(wb kv.IWriteBatch,
	shardID uint64, replicaID uint64, ctx IContext, entries []pb.Entry) uint64 {
	if len(entries) == 0 {
		panic("empty entries")
	}
	eb := ctx.GetEntryBatch()
	eb.Entries = eb.Entries[:0]
	currentBatchIdx := uint64(math.MaxUint64)
	idx := 0
	maxIndex := uint64(0)
	firstBatchID := getBatchID(entries[0].Index)
	lastBatchID := getBatchID(entries[len(entries)-1].Index)
	for idx < len(entries) {
		ent := entries[idx]
		if ent.Index > maxIndex {
			maxIndex = ent.Index
		}
		batchID := getBatchID(ent.Index)
		if batchID != currentBatchIdx {
			be.recordBatch(wb, shardID, replicaID, eb, firstBatchID, lastBatchID, ctx)
			eb.Entries = eb.Entries[:0]
			currentBatchIdx = batchID
		}
		eb.Entries = append(eb.Entries, ent)
		idx++
	}
	if len(eb.Entries) > 0 {
		be.recordBatch(wb, shardID, replicaID, eb, firstBatchID, lastBatchID, ctx)
	}
	return maxIndex
}

func (be *batchedEntries) getBatchFromDB(shardID uint64,
	replicaID uint64, batchID uint64) (pb.EntryBatch, bool) {
	var e pb.EntryBatch
	k := be.keys.get()
	defer k.Release()
	k.SetEntryBatchKey(shardID, replicaID, batchID)
	if err := be.kvs.GetValue(k.Key(), func(data []byte) error {
		if len(data) == 0 {
			return errors.New("no such entry")
		}
		pb.MustUnmarshal(&e, data)
		return nil
	}); err != nil {
		return e, false
	}
	if len(e.Entries) > 1 {
		return restoreBatchFields(e), true
	}
	return e, true
}

func (be *batchedEntries) getLastBatch(shardID uint64,
	replicaID uint64, firstIndex uint64, lb pb.EntryBatch) (pb.EntryBatch, bool) {
	batchID := getBatchID(firstIndex)
	lb, ok := be.cs.getLastBatch(shardID, replicaID, lb)
	if !ok || batchID < getBatchID(lb.Entries[0].Index) {
		lb, ok = be.getBatchFromDB(shardID, replicaID, batchID)
		if !ok {
			return pb.EntryBatch{}, false
		}
	}
	return lb, true
}

func (be *batchedEntries) getMergedFirstBatch(shardID uint64,
	replicaID uint64, eb pb.EntryBatch, lb pb.EntryBatch) pb.EntryBatch {
	// batch aligned
	if eb.Entries[0].Index%batchSize == 0 {
		return eb
	}
	lb, ok := be.getLastBatch(shardID, replicaID, eb.Entries[0].Index, lb)
	if !ok {
		return eb
	}
	return getMergedFirstBatch(eb, lb)
}

func (be *batchedEntries) binaryFormat() uint32 {
	return raftio.LogDBBinVersion
}
````

## File: internal/logdb/cache_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"reflect"
	"testing"

	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/stretchr/testify/require"
)

func TestTrySaveSnapshot(t *testing.T) {
	c := newCache()
	require.True(t, c.trySaveSnapshot(1, 1, 10))
	require.False(t, c.trySaveSnapshot(1, 1, 10))
	require.False(t, c.trySaveSnapshot(1, 1, 9))
	require.True(t, c.trySaveSnapshot(1, 1, 11))
	require.True(t, c.trySaveSnapshot(1, 2, 10))
}

func TestCachedNodeInfoCanBeSet(t *testing.T) {
	c := newCache()
	require.Equal(t, 0, len(c.nodeInfo), "unexpected map len")

	toSet := c.setNodeInfo(100, 2)
	require.True(t, toSet, "unexpected return value %t", toSet)

	toSet = c.setNodeInfo(100, 2)
	require.False(t, toSet, "unexpected return value %t", toSet)

	require.Equal(t, 1, len(c.nodeInfo), "unexpected map len")

	ni := raftio.NodeInfo{
		ShardID:   100,
		ReplicaID: 2,
	}
	_, ok := c.nodeInfo[ni]
	require.True(t, ok, "node info not set")
}

func TestCachedStateCanBeSet(t *testing.T) {
	c := newCache()
	require.Equal(t, 0, len(c.ps), "unexpected savedState len %d", len(c.ps))

	st := pb.State{
		Term:   1,
		Vote:   2,
		Commit: 3,
	}

	toSet := c.setState(100, 2, st)
	require.True(t, toSet, "unexpected return value")

	toSet = c.setState(100, 2, st)
	require.False(t, toSet, "unexpected return value")

	st.Term = 3
	toSet = c.setState(100, 2, st)
	require.True(t, toSet, "unexpected return value")

	require.Equal(t, 1, len(c.ps), "unexpected savedState len %d", len(c.ps))

	ni := raftio.NodeInfo{
		ShardID:   100,
		ReplicaID: 2,
	}
	v, ok := c.ps[ni]
	require.True(t, ok, "unexpected savedState map value")
	require.True(t, reflect.DeepEqual(&v, &st),
		"unexpected persistent state values")
}

func TestLastEntryBatchCanBeSetAndGet(t *testing.T) {
	c := newCache()
	eb := pb.EntryBatch{Entries: make([]pb.Entry, 0)}
	for i := uint64(1); i < uint64(16); i++ {
		eb.Entries = append(eb.Entries, pb.Entry{Index: i, Term: i})
	}

	c.setLastBatch(10, 2, eb)
	lb := pb.EntryBatch{}
	reb, ok := c.getLastBatch(10, 2, lb)
	require.True(t, ok, "last batch not returned")
	require.Equal(t, 15, len(reb.Entries), "unexpected entry length")
}

func TestLastEntryBatchCanBeUpdated(t *testing.T) {
	c := newCache()
	eb := pb.EntryBatch{Entries: make([]pb.Entry, 0)}
	for i := uint64(1); i < uint64(16); i++ {
		eb.Entries = append(eb.Entries, pb.Entry{Index: i, Term: i})
	}

	eb2 := pb.EntryBatch{Entries: make([]pb.Entry, 0)}
	for i := uint64(100); i < uint64(116); i++ {
		eb2.Entries = append(eb2.Entries, pb.Entry{Index: i, Term: i})
	}

	c.setLastBatch(10, 2, eb)
	c.setLastBatch(10, 2, eb2)
}

func TestChangeReturnedLastBatchWillNotAffectTheCache(t *testing.T) {
	c := newCache()
	eb := pb.EntryBatch{Entries: make([]pb.Entry, 0)}
	for i := uint64(1); i < uint64(16); i++ {
		eb.Entries = append(eb.Entries, pb.Entry{Index: i, Term: 1})
	}

	c.setLastBatch(10, 2, eb)
	v, _ := c.getLastBatch(10, 2, pb.EntryBatch{})
	require.Equal(t, 15, len(v.Entries), "unexpected entry count")

	for i := uint64(0); i < uint64(15); i++ {
		v.Entries[i].Term = 2
	}

	v2, _ := c.getLastBatch(10, 2, pb.EntryBatch{})
	for i := uint64(0); i < uint64(15); i++ {
		require.NotEqual(t, uint64(2), v2.Entries[i].Term,
			"cache content changed")
	}
}

func TestMaxIndexCanBeSetAndGet(t *testing.T) {
	c := newCache()
	c.setMaxIndex(10, 10, 100)
	v, ok := c.getMaxIndex(10, 10)
	require.True(t, ok, "failed to get max index")
	require.Equal(t, uint64(100), v, "unexpected max index, got %d, want 100", v)
}
````

## File: internal/logdb/cache.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"sync"

	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

type cache struct {
	nodeInfo       map[raftio.NodeInfo]struct{}
	ps             map[raftio.NodeInfo]pb.State
	lastEntryBatch map[raftio.NodeInfo]pb.EntryBatch
	maxIndex       map[raftio.NodeInfo]uint64
	snapshotIndex  map[raftio.NodeInfo]uint64
	mu             sync.Mutex
}

func newCache() *cache {
	return &cache{
		nodeInfo:       make(map[raftio.NodeInfo]struct{}),
		ps:             make(map[raftio.NodeInfo]pb.State),
		lastEntryBatch: make(map[raftio.NodeInfo]pb.EntryBatch),
		maxIndex:       make(map[raftio.NodeInfo]uint64),
		snapshotIndex:  make(map[raftio.NodeInfo]uint64),
	}
}

func (r *cache) setNodeInfo(shardID uint64, replicaID uint64) bool {
	key := raftio.NodeInfo{ShardID: shardID, ReplicaID: replicaID}
	r.mu.Lock()
	defer r.mu.Unlock()
	_, ok := r.nodeInfo[key]
	if !ok {
		r.nodeInfo[key] = struct{}{}
	}
	return !ok
}

func (r *cache) setState(shardID uint64, replicaID uint64, st pb.State) bool {
	key := raftio.NodeInfo{ShardID: shardID, ReplicaID: replicaID}
	r.mu.Lock()
	defer r.mu.Unlock()
	v, ok := r.ps[key]
	if !ok {
		r.ps[key] = st
		return true
	}
	if pb.IsStateEqual(v, st) {
		return false
	}
	r.ps[key] = st
	return true
}

func (r *cache) setSnapshotIndex(shardID uint64, replicaID uint64, index uint64) {
	r.mu.Lock()
	defer r.mu.Unlock()
	key := raftio.NodeInfo{ShardID: shardID, ReplicaID: replicaID}
	r.snapshotIndex[key] = index
}

func (r *cache) trySaveSnapshot(shardID uint64,
	replicaID uint64, index uint64) bool {
	r.mu.Lock()
	defer r.mu.Unlock()
	key := raftio.NodeInfo{ShardID: shardID, ReplicaID: replicaID}
	v, ok := r.snapshotIndex[key]
	if !ok {
		r.snapshotIndex[key] = index
		return true
	}
	return index > v
}

func (r *cache) setMaxIndex(shardID uint64, replicaID uint64, maxIndex uint64) {
	r.mu.Lock()
	defer r.mu.Unlock()
	key := raftio.NodeInfo{ShardID: shardID, ReplicaID: replicaID}
	r.maxIndex[key] = maxIndex
}

func (r *cache) getMaxIndex(shardID uint64, replicaID uint64) (uint64, bool) {
	r.mu.Lock()
	defer r.mu.Unlock()
	key := raftio.NodeInfo{ShardID: shardID, ReplicaID: replicaID}
	v, ok := r.maxIndex[key]
	if !ok {
		return 0, false
	}
	return v, true
}

func (r *cache) setLastBatch(shardID uint64,
	replicaID uint64, eb pb.EntryBatch) {
	r.mu.Lock()
	defer r.mu.Unlock()
	key := raftio.NodeInfo{ShardID: shardID, ReplicaID: replicaID}
	oeb, ok := r.lastEntryBatch[key]
	if !ok {
		oeb = pb.EntryBatch{Entries: make([]pb.Entry, 0, len(eb.Entries))}
	} else {
		oeb.Entries = oeb.Entries[:0]
	}
	oeb.Entries = append(oeb.Entries, eb.Entries...)
	r.lastEntryBatch[key] = oeb
}

func (r *cache) getLastBatch(shardID uint64,
	replicaID uint64, lb pb.EntryBatch) (pb.EntryBatch, bool) {
	r.mu.Lock()
	defer r.mu.Unlock()
	key := raftio.NodeInfo{ShardID: shardID, ReplicaID: replicaID}
	v, ok := r.lastEntryBatch[key]
	if !ok {
		return v, false
	}
	lb.Entries = lb.Entries[:0]
	lb.Entries = append(lb.Entries, v.Entries...)
	return lb, true
}
````

## File: internal/logdb/compaction_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"reflect"
	"testing"

	"github.com/lni/dragonboat/v4/raftio"
	"github.com/lni/goutils/leaktest"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

func TestCompactionTaskCanBeCreated(t *testing.T) {
	defer leaktest.AfterTest(t)()
	p := newCompactions()
	assert.Equal(t, 0, p.len(), "size is not 0")
}

func TestCompactionTaskCanBeAdded(t *testing.T) {
	defer leaktest.AfterTest(t)()
	p := newCompactions()
	p.addTask(task{shardID: 1, replicaID: 2, index: 3})
	assert.Equal(t, 1, p.len(), "len unexpectedly reported %d", p.len())
	assert.Equal(t, 1, len(p.pendings), "p.pending len is not 1")
	v, ok := p.pendings[raftio.NodeInfo{ShardID: 1, ReplicaID: 2}]
	assert.True(t, ok, "not added")
	assert.Equal(t, uint64(3), v.index, "unexpected index %d", v)
}

func TestCompactionTaskCanBeUpdated(t *testing.T) {
	defer leaktest.AfterTest(t)()
	p := newCompactions()
	p.addTask(task{shardID: 1, replicaID: 2, index: 3})
	p.addTask(task{shardID: 1, replicaID: 2, index: 10})
	assert.Equal(t, 1, len(p.pendings), "p.pending len is not 1")
	v, ok := p.pendings[raftio.NodeInfo{ShardID: 1, ReplicaID: 2}]
	assert.True(t, ok, "not added")
	assert.Equal(t, uint64(10), v.index, "unexpected index %d", v)
}

func TestCompactionDoneChanIsRetained(t *testing.T) {
	defer leaktest.AfterTest(t)()
	p := newCompactions()
	p.addTask(task{shardID: 1, replicaID: 2, index: 3})
	done := p.pendings[raftio.NodeInfo{ShardID: 1, ReplicaID: 2}].done
	p.addTask(task{shardID: 1, replicaID: 2, index: 10})
	v, ok := p.pendings[raftio.NodeInfo{ShardID: 1, ReplicaID: 2}]
	assert.True(t, ok, "not added")
	assert.Equal(t, done, v.done, "chan not retained")
}

func TestCompactionTaskGetReturnTheExpectedValue(t *testing.T) {
	defer leaktest.AfterTest(t)()
	p := newCompactions()
	tt := task{shardID: 1, replicaID: 2, index: 3}
	p.addTask(tt)
	assert.Equal(t, 1, p.len(), "len unexpectedly reported %d", p.len())
	task, ok := p.getTask()
	assert.True(t, ok, "ok flag unexpected")
	require.NotNil(t, task.done, "task.done == nil")
	task.done = nil
	assert.True(t, reflect.DeepEqual(&tt, &task), "%v vs %v", tt, task)
}

func TestCompactionTaskGetReturnAllExpectedValues(t *testing.T) {
	defer leaktest.AfterTest(t)()
	p := newCompactions()
	p.addTask(task{shardID: 1, replicaID: 2, index: 3})
	p.addTask(task{shardID: 2, replicaID: 2, index: 10})
	assert.Equal(t, 2, p.len(), "len unexpectedly reported %d", p.len())
	task1, ok := p.getTask()
	assert.True(t, ok, "ok flag unexpected")
	task2, ok := p.getTask()
	assert.True(t, ok, "ok flag unexpected")
	assert.True(t, task1.index == 3 || task1.index == 10, "unexpected task obj")
	assert.True(t, task2.index == 3 || task2.index == 10, "unexpected task obj")
	_, ok = p.getTask()
	assert.False(t, ok, "unexpected ok flag value")
}

func TestMovingCompactionIndexBackWillCausePanic(t *testing.T) {
	defer leaktest.AfterTest(t)()
	p := newCompactions()
	p.addTask(task{shardID: 1, replicaID: 2, index: 3})
	require.Panics(t, func() {
		p.addTask(task{shardID: 1, replicaID: 2, index: 2})
	})
}
````

## File: internal/logdb/compaction.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"sync"

	"github.com/lni/dragonboat/v4/raftio"
)

type task struct {
	done      chan struct{}
	shardID   uint64
	replicaID uint64
	index     uint64
}

type compactionInfo struct {
	done  chan struct{}
	index uint64
}

type compactions struct {
	pendings map[raftio.NodeInfo]compactionInfo
	mu       sync.Mutex
}

func newCompactions() *compactions {
	return &compactions{
		pendings: make(map[raftio.NodeInfo]compactionInfo),
	}
}

func (p *compactions) len() int {
	p.mu.Lock()
	defer p.mu.Unlock()
	return len(p.pendings)
}

func (p *compactions) addTask(task task) chan struct{} {
	p.mu.Lock()
	defer p.mu.Unlock()
	key := raftio.NodeInfo{
		ShardID:   task.shardID,
		ReplicaID: task.replicaID,
	}
	ci := compactionInfo{index: task.index}
	v, ok := p.pendings[key]
	if ok && v.index > task.index {
		panic("existing index > task.index")
	}
	if ok {
		ci.done = v.done
	} else {
		ci.done = make(chan struct{})
	}
	p.pendings[key] = ci
	return ci.done
}

func (p *compactions) getTask() (task, bool) {
	p.mu.Lock()
	defer p.mu.Unlock()
	for k, v := range p.pendings {
		task := task{
			shardID:   k.ShardID,
			replicaID: k.ReplicaID,
			index:     v.index,
			done:      v.done,
		}
		delete(p.pendings, k)
		return task, true
	}
	return task{}, false
}
````

## File: internal/logdb/context_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"testing"

	"github.com/stretchr/testify/require"
)

type kvpair struct {
	delete bool
	key    []byte
	val    []byte
}

type testWriteBatch struct {
	vals []kvpair
}

func newtestWriteBatch() *testWriteBatch {
	return &testWriteBatch{vals: make([]kvpair, 0)}
}

func (wb *testWriteBatch) Destroy() {
	wb.vals = nil
}

func (wb *testWriteBatch) Put(key []byte, val []byte) {
	k := make([]byte, len(key))
	v := make([]byte, len(val))
	copy(k, key)
	copy(v, val)
	wb.vals = append(wb.vals, kvpair{key: k, val: v})
}

func (wb *testWriteBatch) Delete(key []byte) {
	k := make([]byte, len(key))
	copy(k, key)
	wb.vals = append(wb.vals, kvpair{key: k, val: nil, delete: true})
}

func (wb *testWriteBatch) Clear() {
	wb.vals = make([]kvpair, 0)
}

func (wb *testWriteBatch) Count() int {
	return len(wb.vals)
}

func TestRDBContextCanBeCreated(t *testing.T) {
	ctx := newContext(128, 128)
	require.NotNil(t, ctx.key, "key should not be nil")
	require.Equal(t, 128, len(ctx.val), "val should have length 128")
	require.Nil(t, ctx.wb, "wb should be nil")
}

func TestRDBContextCaBeDestroyed(t *testing.T) {
	ctx := newContext(128, 128)
	ctx.Destroy()
}

func TestRDBContextCaBeReset(t *testing.T) {
	ctx := newContext(128, 128)
	ctx.SetWriteBatch(newtestWriteBatch())
	ctx.wb.Put([]byte("key"), []byte("val"))
	require.Equal(t, 1, ctx.wb.Count(), "unexpected count")
	ctx.Reset()
	require.Equal(t, 0, ctx.wb.Count(), "wb not cleared")
}

func TestGetValueBuffer(t *testing.T) {
	ctx := newContext(128, 128)
	buf := ctx.GetValueBuffer(100)
	require.Equal(t, 128, cap(buf), "didn't return the default buffer")
	buf = ctx.GetValueBuffer(1024)
	require.Equal(t, 1024, cap(buf), "didn't return a new buffer")
}
````

## File: internal/logdb/context.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"github.com/lni/dragonboat/v4/internal/logdb/kv"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

// context is an IContext implementation suppose to be owned and used
// by a single thread throughout its life time.
type context struct {
	wb      kv.IWriteBatch
	key     *Key
	eb      pb.EntryBatch
	lb      pb.EntryBatch
	val     []byte
	maxSize uint64
	size    uint64
}

// newContext creates a new RDB context instance.
func newContext(size uint64, maxSize uint64) *context {
	ctx := &context{
		size:    size,
		maxSize: maxSize,
		key:     newKey(maxKeySize, nil),
		val:     make([]byte, size),
	}
	ctx.lb.Entries = make([]pb.Entry, 0, batchSize)
	ctx.eb.Entries = make([]pb.Entry, 0, batchSize)
	return ctx
}

func (c *context) Destroy() {
	if c.wb != nil {
		c.wb.Destroy()
	}
	c.val = nil
	c.lb.Entries = nil
	c.eb.Entries = nil
}

func (c *context) Reset() {
	if c.wb != nil {
		c.wb.Clear()
	}
}

func (c *context) GetKey() IReusableKey {
	return c.key
}

func (c *context) GetValueBuffer(sz uint64) []byte {
	if sz <= c.size {
		return c.val
	}
	val := make([]byte, sz)
	if sz < c.maxSize {
		c.size = sz
		c.val = val
	}
	return val
}

func (c *context) GetEntryBatch() pb.EntryBatch {
	return c.eb
}

func (c *context) GetLastEntryBatch() pb.EntryBatch {
	return c.lb
}

func (c *context) GetWriteBatch() interface{} {
	return c.wb
}

func (c *context) SetWriteBatch(wb interface{}) {
	if c.wb != nil {
		panic("c.wb is not nil")
	}
	c.wb = wb.(kv.IWriteBatch)
}
````

## File: internal/logdb/db_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"math"
	"reflect"
	"strings"
	"sync/atomic"
	"testing"
	"time"

	"github.com/lni/goutils/leaktest"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

const (
	RDBTestDirectory = "db_test_dir_safe_to_delete"
)

func getDirSize(path string,
	includeLogSize bool, fs vfs.IFS) (int64, error) {
	var size int64
	results, err := fs.List(path)
	if err != nil {
		return 0, err
	}
	for _, v := range results {
		info, err := fs.Stat(fs.PathJoin(path, v))
		if err != nil {
			return 0, err
		}
		if !info.IsDir() {
			if !includeLogSize && strings.HasSuffix(info.Name(), ".log") {
				continue
			}
			size += info.Size()
		}
	}
	return size, err
}

func getNewTestDB(dir string,
	lldir string, batched bool, fs vfs.IFS) raftio.ILogDB {
	d := fs.PathJoin(RDBTestDirectory, dir)
	lld := fs.PathJoin(RDBTestDirectory, lldir)
	if err := fileutil.MkdirAll(d, fs); err != nil {
		panic(err)
	}
	if err := fileutil.MkdirAll(lld, fs); err != nil {
		panic(err)
	}
	expert := config.GetDefaultExpertConfig()
	expert.LogDB.Shards = 4
	expert.FS = fs
	cfg := config.NodeHostConfig{
		Expert: expert,
	}

	db, err := NewLogDB(cfg, nil,
		[]string{d}, []string{lld}, batched, false, newDefaultKVStore)
	if err != nil {
		panic(err)
	}
	return db
}

func deleteTestDB(fs vfs.IFS) {
	if err := fs.RemoveAll(RDBTestDirectory); err != nil {
		panic(err)
	}
}

func runLogDBTestAs(t *testing.T,
	batched bool, tf func(t *testing.T, db raftio.ILogDB), fs vfs.IFS) {
	defer leaktest.AfterTest(t)()
	dir := "db-dir"
	lldir := "wal-db-dir"
	d := fs.PathJoin(RDBTestDirectory, dir)
	lld := fs.PathJoin(RDBTestDirectory, lldir)
	require.NoError(t, fs.RemoveAll(d))
	require.NoError(t, fs.RemoveAll(lld))
	db := getNewTestDB(dir, lldir, batched, fs)
	defer deleteTestDB(fs)
	defer func() {
		require.NoError(t, db.Close())
	}()
	tf(t, db)
}

func runLogDBTest(t *testing.T,
	tf func(t *testing.T, db raftio.ILogDB), fs vfs.IFS) {
	runLogDBTestAs(t, false, tf, fs)
	runLogDBTestAs(t, true, tf, fs)
}

func runBatchedLogDBTest(t *testing.T,
	tf func(t *testing.T, db raftio.ILogDB), fs vfs.IFS) {
	runLogDBTestAs(t, true, tf, fs)
}

func TestRDBReturnErrNoBootstrapInfoWhenNoBootstrap(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(t *testing.T, db raftio.ILogDB) {
		_, err := db.GetBootstrapInfo(1, 2)
		require.ErrorIs(t, err, raftio.ErrNoBootstrapInfo)
	}
	runLogDBTest(t, tf, fs)
}

func TestBootstrapInfoCanBeSavedAndChecked(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		nodes := make(map[uint64]string)
		nodes[100] = "address1"
		nodes[200] = "address2"
		nodes[300] = "address3"
		bs := pb.Bootstrap{
			Join:      false,
			Addresses: nodes,
		}
		require.NoError(t, db.SaveBootstrapInfo(1, 2, bs))
		bootstrap, err := db.GetBootstrapInfo(1, 2)
		require.NoError(t, err)
		require.False(t, bootstrap.Join)
		require.Len(t, bootstrap.Addresses, 3)
		ni, err := db.ListNodeInfo()
		require.NoError(t, err)
		require.Len(t, ni, 1)
		require.Equal(t, uint64(1), ni[0].ShardID)
		require.Equal(t, uint64(2), ni[0].ReplicaID)
		require.NoError(t, db.SaveBootstrapInfo(2, 3, bs))
		require.NoError(t, db.SaveBootstrapInfo(3, 4, bs))
		ni, err = db.ListNodeInfo()
		require.NoError(t, err)
		require.Len(t, ni, 3)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestSnapshotHasMaxIndexSet(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		ud1 := pb.Update{
			EntriesToSave: []pb.Entry{{Index: 2}, {Index: 3}, {Index: 4}},
			ShardID:       3,
			ReplicaID:     4,
		}
		err := db.SaveRaftState([]pb.Update{ud1}, 1)
		require.NoError(t, err)
		p := db.(*ShardedDB).shards
		maxIndex, err := p[3].getMaxIndex(3, 4)
		require.NoError(t, err)
		require.Equal(t, uint64(4), maxIndex)
		ud2 := pb.Update{
			ShardID:   3,
			ReplicaID: 4,
			Snapshot:  pb.Snapshot{Index: 3},
		}
		err = db.SaveRaftState([]pb.Update{ud2}, 1)
		require.NoError(t, err)
		maxIndex, err = p[3].getMaxIndex(3, 4)
		require.NoError(t, err)
		require.Equal(t, uint64(3), maxIndex)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestSaveSnapshotTogetherWithUnexpectedEntriesWillPanic(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		ud1 := pb.Update{
			EntriesToSave: []pb.Entry{{Index: 2}, {Index: 3}, {Index: 4}},
			ShardID:       3,
			ReplicaID:     4,
			Snapshot:      pb.Snapshot{Index: 5},
		}
		require.Panics(t, func() {
			err := db.SaveRaftState([]pb.Update{ud1}, 1)
			require.NoError(t, err)
		})
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestSnapshotsSavedInSaveRaftState(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		hs1 := pb.State{
			Term:   2,
			Vote:   3,
			Commit: 100,
		}
		e1 := pb.Entry{
			Term:  1,
			Index: 10,
			Type:  pb.ApplicationEntry,
			Cmd:   []byte("test data"),
		}
		snapshot1 := pb.Snapshot{
			Filepath: "p1",
			FileSize: 100,
			Index:    5,
			Term:     1,
		}
		ud1 := pb.Update{
			EntriesToSave: []pb.Entry{e1},
			State:         hs1,
			ShardID:       3,
			ReplicaID:     4,
			Snapshot:      snapshot1,
		}
		hs2 := pb.State{
			Term:   2,
			Vote:   3,
			Commit: 100,
		}
		e2 := pb.Entry{
			Term:  1,
			Index: 20,
			Type:  pb.ApplicationEntry,
			Cmd:   []byte("test data"),
		}
		snapshot2 := pb.Snapshot{
			Filepath: "p2",
			FileSize: 200,
			Index:    12,
			Term:     1,
		}
		ud2 := pb.Update{
			EntriesToSave: []pb.Entry{e2},
			State:         hs2,
			ShardID:       3,
			ReplicaID:     3,
			Snapshot:      snapshot2,
		}
		uds := []pb.Update{ud1, ud2}
		err := db.SaveRaftState(uds, 1)
		require.NoError(t, err)
		v, err := db.GetSnapshot(3, 4)
		require.NoError(t, err)
		require.Equal(t, snapshot1.Index, v.Index)
		v, err = db.GetSnapshot(3, 3)
		require.NoError(t, err)
		require.Equal(t, snapshot2.Index, v.Index)

		p := db.(*ShardedDB).shards
		maxIndex, err := p[3].getMaxIndex(3, 3)
		require.NoError(t, err)
		require.Equal(t, uint64(20), maxIndex)
		maxIndex, err = p[3].getMaxIndex(3, 4)
		require.NoError(t, err)
		require.Equal(t, uint64(10), maxIndex)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestSnapshotOnlyNodeIsHandledByReadRaftState(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		ss := pb.Snapshot{
			Index: 100,
			Term:  2,
		}
		hs := pb.State{
			Term:   2,
			Vote:   3,
			Commit: 100,
		}
		ud := pb.Update{
			State:     hs,
			ShardID:   3,
			ReplicaID: 4,
			Snapshot:  ss,
		}
		require.NoError(t, db.SaveRaftState([]pb.Update{ud}, 1))
		rs, err := db.ReadRaftState(3, 4, ss.Index)
		require.NoError(t, err)
		require.Equal(t, uint64(0), rs.EntryCount)
		require.Equal(t, ss.Index, rs.FirstIndex)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestReadRaftStateReturnsNoSavedLogErrorWhenStateIsNeverSaved(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		ss := pb.Snapshot{
			Index: 100,
			Term:  2,
		}
		ud := pb.Update{
			ShardID:   3,
			ReplicaID: 4,
			Snapshot:  ss,
		}
		require.NoError(t, db.SaveRaftState([]pb.Update{ud}, 1))
		_, err := db.ReadRaftState(3, 4, ss.Index)
		require.ErrorIs(t, err, raftio.ErrNoSavedLog)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestMaxIndexRuleIsEnforced(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		hs := pb.State{
			Term:   2,
			Vote:   3,
			Commit: 100,
		}
		e1 := pb.Entry{
			Term:  1,
			Index: 10,
			Type:  pb.ApplicationEntry,
			Cmd:   []byte("test data"),
		}
		e2 := pb.Entry{
			Term:  2,
			Index: 3,
			Type:  pb.ApplicationEntry,
			Cmd:   []byte("test data 2"),
		}
		ud := pb.Update{
			EntriesToSave: []pb.Entry{e1, e2},
			State:         hs,
			ShardID:       3,
			ReplicaID:     4,
		}
		err := db.SaveRaftState([]pb.Update{ud}, 1)
		require.NoError(t, err)
		ud = pb.Update{
			EntriesToSave: []pb.Entry{e2},
			State:         hs,
			ShardID:       3,
			ReplicaID:     4,
		}
		err = db.SaveRaftState([]pb.Update{ud}, 1)
		require.NoError(t, err)
		rs, err := db.ReadRaftState(3, 4, 0)
		require.NoError(t, err)
		require.Equal(t, uint64(1), rs.EntryCount)
		require.Equal(t, uint64(3), rs.FirstIndex)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestSavedEntrieseAreOrderedByTheKey(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		hs := pb.State{
			Term:   2,
			Vote:   3,
			Commit: 100,
		}
		ents := make([]pb.Entry, 0)
		for i := uint64(1); i < 1025; i++ {
			e := pb.Entry{
				Term:  2,
				Index: i,
				Type:  pb.ApplicationEntry,
				Cmd:   []byte("test-data"),
			}
			ents = append(ents, e)
		}
		ud := pb.Update{
			EntriesToSave: ents,
			State:         hs,
			ShardID:       3,
			ReplicaID:     4,
		}
		err := db.SaveRaftState([]pb.Update{ud}, 1)
		require.NoError(t, err)
		rs, err := db.ReadRaftState(3, 4, 0)
		require.NoError(t, err)
		require.Equal(t, uint64(1024), rs.EntryCount)
		re, _, err := db.IterateEntries([]pb.Entry{},
			0, 3, 4, 1, math.MaxUint64, math.MaxUint64)
		require.NoError(t, err)
		require.Len(t, re, 1024)
		lastIndex := re[0].Index
		for _, e := range re[1:] {
			require.Equal(t, lastIndex+1, e.Index)
			lastIndex = e.Index
		}
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func testSaveRaftState(t *testing.T, db raftio.ILogDB) {
	hs := pb.State{
		Term:   2,
		Vote:   3,
		Commit: 100,
	}
	ud := pb.Update{
		State:     hs,
		ShardID:   3,
		ReplicaID: 4,
	}
	for i := uint64(1); i <= 10; i++ {
		term := uint64(1)
		if i > 5 {
			term = 2
		}
		e := pb.Entry{
			Term:  term,
			Index: i,
			Type:  pb.ApplicationEntry,
			Cmd:   []byte("test data"),
		}
		ud.EntriesToSave = append(ud.EntriesToSave, e)
	}
	err := db.SaveRaftState([]pb.Update{ud}, 1)
	require.NoError(t, err)
	rs, err := db.ReadRaftState(3, 4, 0)
	require.NoError(t, err)
	require.False(t, reflect.DeepEqual(rs.State, raftio.RaftState{}))
	require.Equal(t, uint64(2), rs.State.Term)
	require.Equal(t, uint64(3), rs.State.Vote)
	require.Equal(t, uint64(100), rs.State.Commit)
	require.Equal(t, uint64(10), rs.EntryCount)
}

func TestSaveRaftState(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		testSaveRaftState(t, db)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestStateIsUpdated(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		hs := pb.State{
			Term:   2,
			Vote:   3,
			Commit: 100,
		}
		ud := pb.Update{
			EntriesToSave: []pb.Entry{},
			State:         hs,
			ShardID:       3,
			ReplicaID:     4,
		}
		err := db.SaveRaftState([]pb.Update{ud}, 1)
		require.NoError(t, err)
		rs, err := db.ReadRaftState(3, 4, 0)
		require.NoError(t, err)
		require.Equal(t, hs.Term, rs.State.Term)
		require.Equal(t, hs.Vote, rs.State.Vote)
		require.Equal(t, hs.Commit, rs.State.Commit)
		hs2 := pb.State{
			Term:   3,
			Vote:   3,
			Commit: 100,
		}
		ud2 := pb.Update{
			EntriesToSave: []pb.Entry{},
			State:         hs2,
			ShardID:       3,
			ReplicaID:     4,
		}
		err = db.SaveRaftState([]pb.Update{ud2}, 1)
		require.NoError(t, err)
		rs, err = db.ReadRaftState(3, 4, 0)
		require.NoError(t, err)
		require.Equal(t, hs2.Term, rs.State.Term)
		require.Equal(t, hs2.Vote, rs.State.Vote)
		require.Equal(t, hs2.Commit, rs.State.Commit)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestMaxIndexIsUpdated(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		hs := pb.State{
			Term:   2,
			Vote:   3,
			Commit: 100,
		}
		e1 := pb.Entry{
			Term:  1,
			Index: 10,
			Type:  pb.ApplicationEntry,
			Cmd:   []byte("test data"),
		}
		ud := pb.Update{
			EntriesToSave: []pb.Entry{e1},
			State:         hs,
			ShardID:       3,
			ReplicaID:     4,
		}
		err := db.SaveRaftState([]pb.Update{ud}, 1)
		require.NoError(t, err)
		p := db.(*ShardedDB).shards
		maxIndex, err := p[3].getMaxIndex(3, 4)
		require.NoError(t, err)
		require.Equal(t, uint64(10), maxIndex)
		e1 = pb.Entry{
			Term:  1,
			Index: 11,
			Type:  pb.ApplicationEntry,
			Cmd:   []byte("test data"),
		}
		ud = pb.Update{
			EntriesToSave: []pb.Entry{e1},
			State:         hs,
			ShardID:       3,
			ReplicaID:     4,
		}
		err = db.SaveRaftState([]pb.Update{ud}, 1)
		require.NoError(t, err)
		maxIndex, err = p[3].getMaxIndex(3, 4)
		require.NoError(t, err)
		require.Equal(t, uint64(11), maxIndex)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestReadAllEntriesOnlyReturnEntriesFromTheSpecifiedNode(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		hs := pb.State{
			Term:   2,
			Vote:   3,
			Commit: 100,
		}
		e1 := pb.Entry{
			Term:  1,
			Index: 10,
			Type:  pb.ApplicationEntry,
			Cmd:   []byte("test data"),
		}
		e2 := pb.Entry{
			Term:  2,
			Index: 11,
			Type:  pb.ApplicationEntry,
			Cmd:   []byte("test data 2"),
		}
		ud := pb.Update{
			EntriesToSave: []pb.Entry{e1, e2},
			State:         hs,
			ShardID:       3,
			ReplicaID:     4,
		}
		err := db.SaveRaftState([]pb.Update{ud}, 1)
		require.NoError(t, err)
		rs, err := db.ReadRaftState(3, 4, 0)
		require.NoError(t, err)
		require.Equal(t, uint64(2), rs.EntryCount)
		// save the same data but with different node id
		ud.ReplicaID = 5
		err = db.SaveRaftState([]pb.Update{ud}, 2)
		require.NoError(t, err)
		rs, err = db.ReadRaftState(3, 4, 0)
		require.NoError(t, err)
		require.Equal(t, uint64(2), rs.EntryCount)
		// save the same data but with different shard id
		ud.ReplicaID = 4
		ud.ShardID = 4
		err = db.SaveRaftState([]pb.Update{ud}, 3)
		require.NoError(t, err)
		rs, err = db.ReadRaftState(3, 4, 0)
		require.NoError(t, err)
		require.Equal(t, uint64(2), rs.EntryCount)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestIterateEntriesOnlyReturnCurrentNodeEntries(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		ents, _, err := db.IterateEntries([]pb.Entry{}, 0, 3, 4, 10, 13, math.MaxUint64)
		require.NoError(t, err)
		require.Empty(t, ents)
		hs := pb.State{
			Term:   2,
			Vote:   3,
			Commit: 100,
		}
		e1 := pb.Entry{
			Term:  1,
			Index: 10,
			Type:  pb.ApplicationEntry,
			Cmd:   []byte("test data 1"),
		}
		e2 := pb.Entry{
			Term:  2,
			Index: 11,
			Type:  pb.ApplicationEntry,
			Cmd:   []byte("test data 2"),
		}
		e3 := pb.Entry{
			Term:  2,
			Index: 12,
			Type:  pb.ApplicationEntry,
			Cmd:   []byte("test data 3"),
		}
		ud := pb.Update{
			EntriesToSave: []pb.Entry{e1, e2, e3},
			State:         hs,
			ShardID:       3,
			ReplicaID:     4,
		}
		err = db.SaveRaftState([]pb.Update{ud}, 1)
		require.NoError(t, err)
		// save the same data again but under a different node id
		ud.ReplicaID = 5
		err = db.SaveRaftState([]pb.Update{ud}, 2)
		require.NoError(t, err)
		ents, _, err = db.IterateEntries([]pb.Entry{},
			0, 3, 4, 10, 13, math.MaxUint64)
		require.NoError(t, err)
		require.Len(t, ents, 3)
		// save the same data again but under a different shard id
		ud.ReplicaID = 4
		ud.ShardID = 4
		err = db.SaveRaftState([]pb.Update{ud}, 3)
		require.NoError(t, err)
		ents, _, err = db.IterateEntries([]pb.Entry{},
			0, 3, 4, 10, 13, math.MaxUint64)
		require.NoError(t, err)
		require.Len(t, ents, 3)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestIterateEntries(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		ents, _, err := db.IterateEntries([]pb.Entry{}, 0, 3, 4, 10, 13, math.MaxUint64)
		require.NoError(t, err)
		require.Empty(t, ents)
		hs := pb.State{
			Term:   2,
			Vote:   3,
			Commit: 100,
		}
		e1 := pb.Entry{
			Term:  1,
			Index: 10,
			Type:  pb.ApplicationEntry,
			Cmd:   []byte("test data 1"),
		}
		e2 := pb.Entry{
			Term:  2,
			Index: 11,
			Type:  pb.ApplicationEntry,
			Cmd:   []byte("test data 2"),
		}
		e3 := pb.Entry{
			Term:  2,
			Index: 12,
			Type:  pb.ApplicationEntry,
			Cmd:   []byte("test data 3"),
		}
		ud := pb.Update{
			EntriesToSave: []pb.Entry{e1, e2, e3},
			State:         hs,
			ShardID:       3,
			ReplicaID:     4,
		}
		err = db.SaveRaftState([]pb.Update{ud}, 1)
		require.NoError(t, err)
		ents, _, err = db.IterateEntries([]pb.Entry{},
			0, 3, 4, 10, 11, math.MaxUint64)
		require.NoError(t, err)
		require.Len(t, ents, 1)
		require.Equal(t, uint64(10), ents[0].Index)
		ents, _, err = db.IterateEntries([]pb.Entry{},
			0, 3, 4, 10, 13, math.MaxUint64)
		require.NoError(t, err)
		require.Len(t, ents, 3)
		ents, _, err = db.IterateEntries([]pb.Entry{}, 0, 3, 4, 10, 13, 0)
		require.NoError(t, err)
		require.Len(t, ents, 1)
		ents, _, err = db.IterateEntries([]pb.Entry{},
			0, 3, 4, 10, 12, math.MaxUint64)
		require.NoError(t, err)
		require.Len(t, ents, 2)
		ents, _, err = db.IterateEntries([]pb.Entry{},
			0, 3, 4, 10, 13, uint64(e1.Size()-1))
		require.NoError(t, err)
		require.Len(t, ents, 1)
		// write an entry with index 11
		ud = pb.Update{
			EntriesToSave: []pb.Entry{e2},
			State:         hs,
			ShardID:       3,
			ReplicaID:     4,
		}
		err = db.SaveRaftState([]pb.Update{ud}, 1)
		require.NoError(t, err)
		ents, _, err = db.IterateEntries([]pb.Entry{},
			0, 3, 4, 10, 13, math.MaxUint64)
		require.NoError(t, err)
		require.Len(t, ents, 2)
		for _, ent := range ents {
			require.NotEqual(t, uint64(12), ent.Index)
		}
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestSaveSnapshot(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		_, err := db.GetSnapshot(1, 2)
		require.NoError(t, err)
		s1 := pb.Snapshot{
			Index: 1,
			Term:  2,
		}
		s2 := pb.Snapshot{
			Index: 2,
			Term:  2,
		}
		rec1 := pb.Update{
			ShardID:   1,
			ReplicaID: 2,
			Snapshot:  s1,
		}
		rec2 := pb.Update{
			ShardID:   1,
			ReplicaID: 2,
			Snapshot:  s2,
		}
		require.NoError(t, db.SaveSnapshots([]pb.Update{rec1, rec2}))
		snapshot, err := db.GetSnapshot(1, 2)
		require.NoError(t, err)
		require.Equal(t, uint64(2), snapshot.Index)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestOldSnapshotIsIgnored(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		rec0 := pb.Update{
			ShardID:   1,
			ReplicaID: 2,
			Snapshot:  pb.Snapshot{Index: 5},
		}
		rec1 := pb.Update{
			ShardID:   1,
			ReplicaID: 2,
			Snapshot:  pb.Snapshot{Index: 20},
		}
		rec2 := pb.Update{
			ShardID:   1,
			ReplicaID: 2,
			Snapshot:  pb.Snapshot{Index: 10},
		}
		require.NoError(t, db.SaveSnapshots([]pb.Update{rec0}))
		require.NoError(t, db.SaveSnapshots([]pb.Update{rec1}))
		require.NoError(t, db.SaveSnapshots([]pb.Update{rec2}))
		snapshot, err := db.GetSnapshot(1, 2)
		require.NoError(t, err)
		require.Equal(t, uint64(20), snapshot.Index)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestParseNodeInfoKeyPanicOnUnexpectedKeySize(t *testing.T) {
	require.Panics(t, func() {
		parseNodeInfoKey(make([]byte, 21))
	})
}

func TestSaveEntriesWithIndexGap(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		shardID := uint64(0)
		replicaID := uint64(4)
		e1 := pb.Entry{
			Term:  1,
			Index: 1,
			Type:  pb.ApplicationEntry,
		}
		e2 := pb.Entry{
			Term:  1,
			Index: 2,
			Type:  pb.ApplicationEntry,
		}
		ud := pb.Update{
			EntriesToSave: []pb.Entry{e1, e2},
			ShardID:       shardID,
			ReplicaID:     replicaID,
		}
		err := db.SaveRaftState([]pb.Update{ud}, 1)
		require.NoError(t, err)
		e1 = pb.Entry{
			Term:  1,
			Index: 4,
			Type:  pb.ApplicationEntry,
		}
		e2 = pb.Entry{
			Term:  1,
			Index: 5,
			Type:  pb.ApplicationEntry,
		}
		ud = pb.Update{
			EntriesToSave: []pb.Entry{e1, e2},
			ShardID:       shardID,
			ReplicaID:     replicaID,
		}
		err = db.SaveRaftState([]pb.Update{ud}, 1)
		require.NoError(t, err)
		ents, _, err := db.IterateEntries([]pb.Entry{}, 0,
			shardID, replicaID, 1, 6, math.MaxUint64)
		require.NoError(t, err)
		require.Len(t, ents, 2)
		require.Equal(t, uint64(1), ents[0].Index)
		require.Equal(t, uint64(2), ents[1].Index)
		ents, _, err = db.IterateEntries([]pb.Entry{}, 0,
			shardID, replicaID, 3, 6, math.MaxUint64)
		require.NoError(t, err)
		require.Empty(t, ents)
		ents, _, err = db.IterateEntries([]pb.Entry{}, 0,
			shardID, replicaID, 4, 6, math.MaxUint64)
		require.NoError(t, err)
		require.Len(t, ents, 2)
		require.Equal(t, uint64(4), ents[0].Index)
		require.Equal(t, uint64(5), ents[1].Index)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func testAllWantedEntriesAreAccessible(t *testing.T,
	first uint64, last uint64) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		shardID := uint64(0)
		replicaID := uint64(4)
		ents := make([]pb.Entry, 0)
		for i := first; i <= last; i++ {
			e := pb.Entry{
				Term:  1,
				Index: i,
				Type:  pb.ApplicationEntry,
			}
			ents = append(ents, e)
		}
		ud := pb.Update{
			EntriesToSave: ents,
			State:         pb.State{Commit: 1},
			ShardID:       shardID,
			ReplicaID:     replicaID,
		}
		err := db.SaveRaftState([]pb.Update{ud}, 1)
		require.NoError(t, err)
		results, _, err := db.IterateEntries(nil,
			0, shardID, replicaID, first, last+1, math.MaxUint64)
		require.NoError(t, err)
		require.Equal(t, last-first+1, uint64(len(results)))
		require.Equal(t, last, results[len(results)-1].Index)
		require.Equal(t, first, results[0].Index)
		rs, err := db.ReadRaftState(shardID, replicaID, first-1)
		require.NoError(t, err)
		firstIndex := rs.FirstIndex
		length := rs.EntryCount
		require.Equal(t, first, firstIndex)
		require.Equal(t, last-first+1, length)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestRemoveEntriesTo(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	dir := "db-dir"
	lldir := "wal-db-dir"
	d := fs.PathJoin(RDBTestDirectory, dir)
	lld := fs.PathJoin(RDBTestDirectory, lldir)
	require.NoError(t, fs.RemoveAll(d))
	require.NoError(t, fs.RemoveAll(lld))
	defer func() {
		require.NoError(t, fs.RemoveAll(RDBTestDirectory))
	}()
	shardID := uint64(0)
	replicaID := uint64(4)
	ents := make([]pb.Entry, 0)
	maxIndex := uint64(1024)
	skipSizeCheck := false
	func() {
		db := getNewTestDB(dir, lldir, false, fs)
		sdb, ok := db.(*ShardedDB)
		require.True(t, ok)
		name := sdb.Name()
		plog.Infof("name: %s", name)
		skipSizeCheck = strings.Contains(name, "leveldb")
		failed, err := sdb.SelfCheckFailed()
		require.False(t, failed)
		require.NoError(t, err)
		defer func() {
			require.NoError(t, db.Close())
		}()
		for i := uint64(0); i < maxIndex; i++ {
			e := pb.Entry{
				Term:  1,
				Index: i,
				Type:  pb.ApplicationEntry,
				Cmd:   make([]byte, 1024*4),
			}
			ents = append(ents, e)
		}
		ud := pb.Update{
			EntriesToSave: ents,
			State:         pb.State{Commit: 1},
			ShardID:       shardID,
			ReplicaID:     replicaID,
		}
		err = db.SaveRaftState([]pb.Update{ud}, 1)
		require.NoError(t, err)
		require.NoError(t, db.RemoveEntriesTo(shardID, replicaID, maxIndex))
		done, err := db.CompactEntriesTo(shardID, replicaID, maxIndex)
		require.NoError(t, err)
		for i := 0; i < 1000; i++ {
			count := atomic.LoadUint64(&(sdb.completedCompactions))
			if count == 0 {
				time.Sleep(10 * time.Millisecond)
			} else {
				plog.Infof("count: %d, done", count)
				break
			}
			if i == 999 {
				require.Fail(t, "failed to trigger compaction")
			}
		}
		select {
		case <-done:
		default:
			require.Fail(t, "done chan not closed")
		}
		results, _, err := db.IterateEntries(nil,
			0, shardID, replicaID, 1, 100, math.MaxUint64)
		require.NoError(t, err)
		require.Empty(t, results)
	}()
	// leveldb has the leftover ldb file
	// https://github.com/google/leveldb/issues/573
	// https://github.com/google/leveldb/issues/593
	if !skipSizeCheck {
		sz, err := getDirSize(RDBTestDirectory, false, fs)
		require.NoError(t, err)
		plog.Infof("sz: %d", sz)
		require.LessOrEqual(t, sz, int64(1024*1024))
	}
}

func TestAllWantedEntriesAreAccessible(t *testing.T) {
	testAllWantedEntriesAreAccessible(t, 1, 2)
	testAllWantedEntriesAreAccessible(t, 3, batchSize/2)
	testAllWantedEntriesAreAccessible(t, 1, batchSize-1)
	testAllWantedEntriesAreAccessible(t, 1, batchSize)
	testAllWantedEntriesAreAccessible(t, 1, batchSize+1)
	testAllWantedEntriesAreAccessible(t, 1, batchSize*3-1)
	testAllWantedEntriesAreAccessible(t, 1, batchSize*3)
	testAllWantedEntriesAreAccessible(t, 1, batchSize*3+1)
	testAllWantedEntriesAreAccessible(t, batchSize-1, batchSize*3-1)
	testAllWantedEntriesAreAccessible(t, batchSize, batchSize*3-1)
	testAllWantedEntriesAreAccessible(t, batchSize+1, batchSize*3-1)
	testAllWantedEntriesAreAccessible(t, batchSize-1, batchSize*3)
	testAllWantedEntriesAreAccessible(t, batchSize, batchSize*3)
	testAllWantedEntriesAreAccessible(t, batchSize+1, batchSize*3)
	testAllWantedEntriesAreAccessible(t, batchSize-1, batchSize*3+1)
	testAllWantedEntriesAreAccessible(t, batchSize, batchSize*3+1)
	testAllWantedEntriesAreAccessible(t, batchSize+1, batchSize*3+1)
}

type noopCompactor struct{}

func (noopCompactor) Compact(uint64) error { return nil }

var testCompactor = &noopCompactor{}

func TestReadRaftStateWithSnapshot(t *testing.T) {
	tests := []struct {
		snapshotIndex uint64
		entryCount    uint64
		firstIndex    uint64
		lastIndex     uint64
	}{
		{16, 85, 17, 100},
		{100, 0, 101, 100},
	}
	for _, tt := range tests {
		snapshotIndex := tt.snapshotIndex
		entryCount := tt.entryCount
		firstIndex := tt.firstIndex
		lastIndex := tt.lastIndex
		tf := func(t *testing.T, db raftio.ILogDB) {
			shardID := uint64(0)
			replicaID := uint64(4)
			ents := make([]pb.Entry, 0)
			hs := pb.State{
				Term:   1,
				Vote:   3,
				Commit: 100,
			}
			ss := pb.Snapshot{
				Index: snapshotIndex,
				Term:  1,
			}
			for i := uint64(1); i <= 100; i++ {
				e := pb.Entry{
					Term:  1,
					Index: i,
					Type:  pb.ApplicationEntry,
				}
				ents = append(ents, e)
			}
			ud := pb.Update{
				EntriesToSave: ents,
				State:         hs,
				Snapshot:      ss,
				ShardID:       shardID,
				ReplicaID:     replicaID,
			}
			err := db.SaveRaftState([]pb.Update{ud}, 1)
			require.NoError(t, err)
			state, err := db.ReadRaftState(shardID, replicaID, ss.Index)
			require.NoError(t, err)
			require.Equal(t, ss.Index, state.FirstIndex)
			require.Equal(t, entryCount, state.EntryCount)
			logReader := NewLogReader(shardID, replicaID, db)
			logReader.SetCompactor(testCompactor)
			require.NoError(t, logReader.ApplySnapshot(ss))
			logReader.SetRange(state.FirstIndex, state.EntryCount)
			fi, li := logReader.GetRange()
			require.Equal(t, firstIndex, fi)
			require.Equal(t, lastIndex, li)
		}
		fs := vfs.GetTestFS()
		runLogDBTest(t, tf, fs)
	}
}

func TestReadRaftStateWithEntriesOnly(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		shardID := uint64(0)
		replicaID := uint64(4)
		ents := make([]pb.Entry, 0)
		hs := pb.State{
			Term:   1,
			Vote:   3,
			Commit: 100,
		}
		for i := uint64(1); i <= batchSize*3+1; i++ {
			e := pb.Entry{
				Term:  1,
				Index: i,
				Type:  pb.ApplicationEntry,
			}
			ents = append(ents, e)
		}
		ud := pb.Update{
			EntriesToSave: ents,
			State:         hs,
			ShardID:       shardID,
			ReplicaID:     replicaID,
		}
		err := db.SaveRaftState([]pb.Update{ud}, 1)
		require.NoError(t, err)
		state, err := db.ReadRaftState(shardID, replicaID, 1)
		require.NoError(t, err)
		require.Equal(t, uint64(1), state.FirstIndex)
		require.Equal(t, batchSize*3+1, state.EntryCount)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestRemoveNodeData(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		shardID := uint64(0)
		replicaID := uint64(4)
		ents := make([]pb.Entry, 0)
		hs := pb.State{
			Term:   1,
			Vote:   3,
			Commit: 100,
		}
		ss := pb.Snapshot{
			FileSize: 1234,
			Filepath: "f2",
			Index:    1,
			Term:     2,
		}
		for i := uint64(1); i <= batchSize*3+1; i++ {
			e := pb.Entry{
				Term:  1,
				Index: i,
				Type:  pb.ApplicationEntry,
			}
			ents = append(ents, e)
		}
		ud := pb.Update{
			EntriesToSave: ents,
			State:         hs,
			ShardID:       shardID,
			ReplicaID:     replicaID,
			Snapshot:      ss,
		}
		require.NoError(t, db.SaveRaftState([]pb.Update{ud}, 1))
		state, err := db.ReadRaftState(shardID, replicaID, 1)
		require.NoError(t, err)
		require.Equal(t, uint64(1), state.FirstIndex)
		require.Equal(t, batchSize*3+1, state.EntryCount)
		require.NoError(t, db.RemoveNodeData(shardID, replicaID))
		_, err = db.ReadRaftState(shardID, replicaID, 1)
		require.ErrorIs(t, err, raftio.ErrNoSavedLog)

		snapshot, err := db.GetSnapshot(shardID, replicaID)
		require.NoError(t, err)
		require.True(t, pb.IsEmptySnapshot(snapshot))

		_, err = db.GetBootstrapInfo(shardID, replicaID)
		require.ErrorIs(t, err, raftio.ErrNoBootstrapInfo)

		iteratedEnts, sz, err := db.IterateEntries(nil, 0, shardID, replicaID, 0,
			math.MaxUint64, math.MaxUint64)
		require.NoError(t, err)
		require.Empty(t, iteratedEnts)
		require.Zero(t, sz)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}

func TestImportSnapshot(t *testing.T) {
	tf := func(t *testing.T, db raftio.ILogDB) {
		shardID := uint64(2)
		replicaID := uint64(4)
		ents := make([]pb.Entry, 0)
		hs := pb.State{
			Term:   1,
			Vote:   3,
			Commit: 100,
		}
		ss := pb.Snapshot{
			FileSize: 1234,
			Filepath: "f2",
			Index:    120,
			Term:     2,
		}
		for i := uint64(0); i <= batchSize*3+1; i++ {
			e := pb.Entry{
				Term:  1,
				Index: i,
				Type:  pb.ApplicationEntry,
			}
			ents = append(ents, e)
		}
		ud := pb.Update{
			EntriesToSave: ents,
			State:         hs,
			ShardID:       shardID,
			ReplicaID:     replicaID,
			Snapshot:      ss,
		}
		require.NoError(t, db.SaveRaftState([]pb.Update{ud}, 1))
		ssimport := pb.Snapshot{
			Type:    pb.OnDiskStateMachine,
			ShardID: shardID,
			Index:   110,
			Term:    2,
		}
		require.NoError(t, db.ImportSnapshot(ssimport, replicaID))
		snapshot, err := db.GetSnapshot(shardID, replicaID)
		require.NoError(t, err)
		require.Equal(t, ssimport.Index, snapshot.Index)

		bs, err := db.GetBootstrapInfo(shardID, replicaID)
		require.NoError(t, err)
		require.Equal(t, pb.OnDiskStateMachine, bs.Type)

		state, err := db.ReadRaftState(shardID, replicaID, 1)
		require.NoError(t, err)
		require.NotEqual(t, raftio.RaftState{}, state)
		require.Equal(t, snapshot.Index, state.State.Commit)

		sdb := db.(*ShardedDB).shards[2]
		sdb.cs.maxIndex = make(map[raftio.NodeInfo]uint64)
		maxIndex, err := sdb.getMaxIndex(shardID, replicaID)
		require.NoError(t, err)
		require.Equal(t, ssimport.Index, maxIndex)

		state, err = db.ReadRaftState(shardID, replicaID, snapshot.Index)
		require.NoError(t, err)
		require.NotEqual(t, raftio.RaftState{}, state)
		require.Equal(t, snapshot.Index, state.FirstIndex)
		require.NotEqual(t, 0, state.EntryCount)
	}
	fs := vfs.GetTestFS()
	runLogDBTest(t, tf, fs)
}
````

## File: internal/logdb/db.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"encoding/binary"
	"math"

	"github.com/cockroachdb/errors"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/logdb/kv"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

var (
	batchSize = settings.Hard.LogDBEntryBatchSize
)

type entryManager interface {
	binaryFormat() uint32
	record(wb kv.IWriteBatch,
		shardID uint64, replicaID uint64, ctx IContext, entries []pb.Entry) uint64
	iterate(ents []pb.Entry, maxIndex uint64,
		size uint64, shardID uint64, replicaID uint64,
		low uint64, high uint64, maxSize uint64) ([]pb.Entry, uint64, error)
	getRange(shardID uint64,
		replicaID uint64, snapshotIndex uint64,
		maxIndex uint64) (uint64, uint64, error)
	rangedOp(shardID uint64,
		replicaID uint64, index uint64, op func(*Key, *Key) error) error
}

// db is the struct used to manage log DB.
type db struct {
	cs      *cache
	keys    *keyPool
	kvs     kv.IKVStore
	entries entryManager
}

func hasEntryRecord(kvs kv.IKVStore, batched bool) (bool, error) {
	fk := newKey(entryKeySize, nil)
	lk := newKey(entryKeySize, nil)
	if !batched {
		fk.SetEntryKey(0, 0, 0)
		lk.SetEntryKey(math.MaxUint64, math.MaxUint64, math.MaxUint64)
	} else {
		fk.SetEntryBatchKey(0, 0, 0)
		lk.SetEntryBatchKey(math.MaxUint64, math.MaxUint64, math.MaxUint64)
	}
	located := false
	op := func(key []byte, data []byte) (bool, error) {
		located = true
		return false, nil
	}
	if err := kvs.IterateValue(fk.Key(), lk.Key(), true, op); err != nil {
		return false, err
	}
	return located, nil
}

func openRDB(config config.LogDBConfig,
	callback kv.LogDBCallback, dir string, wal string, batched bool,
	fs vfs.IFS, kvf kv.Factory) (*db, error) {
	kvs, err := kvf(config, callback, dir, wal, fs)
	if err != nil {
		return nil, err
	}
	cs := newCache()
	pool := newLogDBKeyPool()
	var em entryManager
	if batched {
		em = newBatchedEntries(cs, pool, kvs)
	} else {
		em = newPlainEntries(cs, pool, kvs)
	}
	return &db{
		cs:      cs,
		keys:    pool,
		kvs:     kvs,
		entries: em,
	}, nil
}

func (r *db) name() string {
	return r.kvs.Name()
}

func (r *db) selfCheckFailed() (bool, error) {
	_, batched := r.entries.(*batchedEntries)
	return hasEntryRecord(r.kvs, !batched)
}

func (r *db) binaryFormat() uint32 {
	return r.entries.binaryFormat()
}

func (r *db) close() error {
	return r.kvs.Close()
}

func (r *db) getWriteBatch(ctx IContext) kv.IWriteBatch {
	if ctx != nil {
		wb := ctx.GetWriteBatch()
		if wb == nil {
			wb = r.kvs.GetWriteBatch()
			ctx.SetWriteBatch(wb)
		}
		return wb.(kv.IWriteBatch)
	}
	return r.kvs.GetWriteBatch()
}

func (r *db) listNodeInfo() ([]raftio.NodeInfo, error) {
	fk := newKey(bootstrapKeySize, nil)
	lk := newKey(bootstrapKeySize, nil)
	fk.setBootstrapKey(0, 0)
	lk.setBootstrapKey(math.MaxUint64, math.MaxUint64)
	ni := make([]raftio.NodeInfo, 0)
	op := func(key []byte, data []byte) (bool, error) {
		cid, nid := parseNodeInfoKey(key)
		ni = append(ni, raftio.GetNodeInfo(cid, nid))
		return true, nil
	}
	if err := r.kvs.IterateValue(fk.Key(), lk.Key(), true, op); err != nil {
		return []raftio.NodeInfo{}, err
	}
	return ni, nil
}

func (r *db) readRaftState(shardID uint64,
	replicaID uint64, snapshotIndex uint64) (raftio.RaftState, error) {
	firstIndex, length, err := r.getRange(shardID, replicaID, snapshotIndex)
	if err != nil {
		return raftio.RaftState{}, err
	}
	state, err := r.getState(shardID, replicaID)
	if err != nil {
		return raftio.RaftState{}, err
	}
	return raftio.RaftState{
		State:      state,
		FirstIndex: firstIndex,
		EntryCount: length,
	}, nil
}

func (r *db) getRange(shardID uint64,
	replicaID uint64, snapshotIndex uint64) (uint64, uint64, error) {
	maxIndex, err := r.getMaxIndex(shardID, replicaID)
	if err == raftio.ErrNoSavedLog {
		return snapshotIndex, 0, nil
	}
	if err != nil {
		return 0, 0, err
	}
	if snapshotIndex == maxIndex {
		return snapshotIndex, 0, nil
	}
	return r.entries.getRange(shardID, replicaID, snapshotIndex, maxIndex)
}

func (r *db) saveRaftState(updates []pb.Update, ctx IContext) error {
	wb := r.getWriteBatch(ctx)
	for _, ud := range updates {
		r.saveState(ud.ShardID, ud.ReplicaID, ud.State, wb, ctx)
		if !pb.IsEmptySnapshot(ud.Snapshot) &&
			r.cs.trySaveSnapshot(ud.ShardID, ud.ReplicaID, ud.Snapshot.Index) {
			if len(ud.EntriesToSave) > 0 {
				// raft/inMemory makes sure such entries no longer need to be saved
				lastIndex := ud.EntriesToSave[len(ud.EntriesToSave)-1].Index
				if ud.Snapshot.Index > lastIndex {
					plog.Panicf("max index not handled, %d, %d",
						ud.Snapshot.Index, lastIndex)
				}
			}
			if err := r.saveSnapshot(wb, ud); err != nil {
				return nil
			}
			r.setMaxIndex(wb, ud, ud.Snapshot.Index, ctx)
		}
	}
	r.saveEntries(updates, wb, ctx)
	if wb.Count() > 0 {
		return r.kvs.CommitWriteBatch(wb)
	}
	return nil
}

func (r *db) importSnapshot(ss pb.Snapshot, replicaID uint64) error {
	if ss.Type == pb.UnknownStateMachine {
		panic("Unknown state machine type")
	}
	snapshots, err := r.listSnapshots(ss.ShardID, replicaID, math.MaxUint64)
	if err != nil {
		return err
	}
	selectedss := make([]pb.Snapshot, 0)
	for _, curss := range snapshots {
		if curss.Index >= ss.Index {
			selectedss = append(selectedss, curss)
		}
	}
	wb := r.getWriteBatch(nil)
	bsrec := pb.Bootstrap{
		Join: true,
		Type: ss.Type,
	}
	state := pb.State{
		Term:   ss.Term,
		Commit: ss.Index,
	}
	r.saveRemoveNodeData(wb, selectedss, ss.ShardID, replicaID)
	r.saveBootstrap(wb, ss.ShardID, replicaID, bsrec)
	r.saveStateAllocs(wb, ss.ShardID, replicaID, state)
	if err := r.saveSnapshot(wb, pb.Update{
		ShardID:   ss.ShardID,
		ReplicaID: replicaID,
		Snapshot:  ss,
	}); err != nil {
		return err
	}
	r.saveMaxIndex(wb, ss.ShardID, replicaID, ss.Index, nil)
	return r.kvs.CommitWriteBatch(wb)
}

func (r *db) setMaxIndex(wb kv.IWriteBatch,
	ud pb.Update, maxIndex uint64, ctx IContext) {
	r.cs.setMaxIndex(ud.ShardID, ud.ReplicaID, maxIndex)
	r.saveMaxIndex(wb, ud.ShardID, ud.ReplicaID, maxIndex, ctx)
}

func (r *db) saveBootstrap(wb kv.IWriteBatch,
	shardID uint64, replicaID uint64, bs pb.Bootstrap) {
	k := newKey(maxKeySize, nil)
	k.setBootstrapKey(shardID, replicaID)
	data := pb.MustMarshal(&bs)
	wb.Put(k.Key(), data)
}

func (r *db) saveSnapshot(wb kv.IWriteBatch, ud pb.Update) error {
	if pb.IsEmptySnapshot(ud.Snapshot) {
		return nil
	}
	snapshots, err := r.listSnapshots(ud.ShardID, ud.ReplicaID, math.MaxUint64)
	if err != nil {
		return err
	}
	for _, ss := range snapshots {
		if ud.Snapshot.Index > ss.Index {
			k := newKey(maxKeySize, nil)
			k.setSnapshotKey(ud.ShardID, ud.ReplicaID, ss.Index)
			wb.Delete(k.Key())
		}
	}
	k := newKey(snapshotKeySize, nil)
	k.setSnapshotKey(ud.ShardID, ud.ReplicaID, ud.Snapshot.Index)
	data := pb.MustMarshal(&ud.Snapshot)
	wb.Put(k.Key(), data)
	return nil
}

func (r *db) saveMaxIndex(wb kv.IWriteBatch,
	shardID uint64, replicaID uint64, index uint64, ctx IContext) {
	var data []byte
	var k IReusableKey
	if ctx != nil {
		data = ctx.GetValueBuffer(8)
	} else {
		data = make([]byte, 8)
	}
	binary.BigEndian.PutUint64(data, index)
	data = data[:8]
	if ctx != nil {
		k = ctx.GetKey()
	} else {
		k = newKey(maxKeySize, nil)
	}
	k.SetMaxIndexKey(shardID, replicaID)
	wb.Put(k.Key(), data)
}

func (r *db) saveStateAllocs(wb kv.IWriteBatch,
	shardID uint64, replicaID uint64, st pb.State) {
	data := pb.MustMarshal(&st)
	k := newKey(snapshotKeySize, nil)
	k.SetStateKey(shardID, replicaID)
	wb.Put(k.Key(), data)
}

func (r *db) saveState(shardID uint64,
	replicaID uint64, st pb.State, wb kv.IWriteBatch, ctx IContext) {
	if pb.IsEmptyState(st) {
		return
	}
	if !r.cs.setState(shardID, replicaID, st) {
		return
	}
	data := ctx.GetValueBuffer(uint64(st.Size()))
	result := pb.MustMarshalTo(&st, data)
	k := ctx.GetKey()
	k.SetStateKey(shardID, replicaID)
	wb.Put(k.Key(), result)
}

func (r *db) saveBootstrapInfo(shardID uint64,
	replicaID uint64, bs pb.Bootstrap) error {
	wb := r.getWriteBatch(nil)
	r.saveBootstrap(wb, shardID, replicaID, bs)
	return r.kvs.CommitWriteBatch(wb)
}

func (r *db) getBootstrapInfo(shardID uint64,
	replicaID uint64) (pb.Bootstrap, error) {
	k := newKey(maxKeySize, nil)
	k.setBootstrapKey(shardID, replicaID)
	bootstrap := pb.Bootstrap{}
	if err := r.kvs.GetValue(k.Key(), func(data []byte) error {
		if len(data) == 0 {
			return raftio.ErrNoBootstrapInfo
		}
		pb.MustUnmarshal(&bootstrap, data)
		return nil
	}); err != nil {
		return pb.Bootstrap{}, err
	}
	return bootstrap, nil
}

func (r *db) saveSnapshots(updates []pb.Update) error {
	wb := r.getWriteBatch(nil)
	defer wb.Destroy()
	toSave := false
	for _, ud := range updates {
		if !pb.IsEmptySnapshot(ud.Snapshot) &&
			r.cs.trySaveSnapshot(ud.ShardID, ud.ReplicaID, ud.Snapshot.Index) {
			if err := r.saveSnapshot(wb, ud); err != nil {
				return nil
			}
			toSave = true
		}
	}
	if toSave {
		return r.kvs.CommitWriteBatch(wb)
	}
	return nil
}

func (r *db) getSnapshot(shardID uint64, replicaID uint64) (pb.Snapshot, error) {
	snapshots, err := r.listSnapshots(shardID, replicaID, math.MaxUint64)
	if err != nil {
		return pb.Snapshot{}, err
	}
	if len(snapshots) > 0 {
		ss := snapshots[len(snapshots)-1]
		r.cs.setSnapshotIndex(shardID, replicaID, ss.Index)
		return ss, nil
	}
	return pb.Snapshot{}, nil
}

// previously, snapshots are stored with its index value as the least
// significant part of the key. from v3.4, we only store the latest snapshot in
// LogDB and the least significant part of the key is set to math.MaxUint64.
func (r *db) listSnapshots(shardID uint64,
	replicaID uint64, index uint64) ([]pb.Snapshot, error) {
	fk := r.keys.get()
	lk := r.keys.get()
	defer fk.Release()
	defer lk.Release()
	fk.setSnapshotKey(shardID, replicaID, 0)
	lk.setSnapshotKey(shardID, replicaID, index)
	snapshots := make([]pb.Snapshot, 0)
	op := func(key []byte, data []byte) (bool, error) {
		var ss pb.Snapshot
		pb.MustUnmarshal(&ss, data)
		snapshots = append(snapshots, ss)
		return true, nil
	}
	if err := r.kvs.IterateValue(fk.Key(), lk.Key(), true, op); err != nil {
		return []pb.Snapshot{}, err
	}
	return snapshots, nil
}

func (r *db) getMaxIndex(shardID uint64, replicaID uint64) (uint64, error) {
	if v, ok := r.cs.getMaxIndex(shardID, replicaID); ok {
		return v, nil
	}
	k := r.keys.get()
	defer k.Release()
	k.SetMaxIndexKey(shardID, replicaID)
	maxIndex := uint64(0)
	if err := r.kvs.GetValue(k.Key(), func(data []byte) error {
		if len(data) == 0 {
			return raftio.ErrNoSavedLog
		}
		maxIndex = binary.BigEndian.Uint64(data)
		return nil
	}); err != nil {
		return 0, err
	}
	return maxIndex, nil
}

func (r *db) getState(shardID uint64, replicaID uint64) (pb.State, error) {
	k := r.keys.get()
	defer k.Release()
	k.SetStateKey(shardID, replicaID)
	hs := pb.State{}
	if err := r.kvs.GetValue(k.Key(), func(data []byte) error {
		if len(data) == 0 {
			return raftio.ErrNoSavedLog
		}
		pb.MustUnmarshal(&hs, data)
		return nil
	}); err != nil {
		return pb.State{}, err
	}
	return hs, nil
}

func (r *db) removeEntriesTo(shardID uint64,
	replicaID uint64, index uint64) error {
	op := func(fk *Key, lk *Key) error {
		return r.kvs.BulkRemoveEntries(fk.Key(), lk.Key())
	}
	return r.entries.rangedOp(shardID, replicaID, index, op)
}

func (r *db) removeNodeData(shardID uint64, replicaID uint64) error {
	wb := r.getWriteBatch(nil)
	defer wb.Clear()
	snapshots, err := r.listSnapshots(shardID, replicaID, math.MaxUint64)
	if err != nil {
		return err
	}
	r.saveRemoveNodeData(wb, snapshots, shardID, replicaID)
	if err := r.kvs.CommitWriteBatch(wb); err != nil {
		return err
	}
	r.cs.setMaxIndex(shardID, replicaID, 0)
	return r.removeEntriesTo(shardID, replicaID, math.MaxUint64)
}

func (r *db) saveRemoveNodeData(wb kv.IWriteBatch,
	snapshots []pb.Snapshot, shardID uint64, replicaID uint64) {
	stateKey := newKey(maxKeySize, nil)
	stateKey.SetStateKey(shardID, replicaID)
	wb.Delete(stateKey.Key())
	bsKey := newKey(maxKeySize, nil)
	bsKey.setBootstrapKey(shardID, replicaID)
	wb.Delete(bsKey.Key())
	miKey := newKey(maxKeySize, nil)
	miKey.SetMaxIndexKey(shardID, replicaID)
	wb.Delete(miKey.Key())
	for _, ss := range snapshots {
		k := newKey(maxKeySize, nil)
		k.setSnapshotKey(shardID, replicaID, ss.Index)
		wb.Delete(k.Key())
	}
}

func (r *db) compact(shardID uint64, replicaID uint64, index uint64) error {
	op := func(fk *Key, lk *Key) error {
		return r.kvs.CompactEntries(fk.Key(), lk.Key())
	}
	return r.entries.rangedOp(shardID, replicaID, index, op)
}

func (r *db) saveEntries(updates []pb.Update, wb kv.IWriteBatch, ctx IContext) {
	for _, ud := range updates {
		if len(ud.EntriesToSave) > 0 {
			mi := r.entries.record(wb, ud.ShardID, ud.ReplicaID, ctx, ud.EntriesToSave)
			if mi > 0 {
				r.setMaxIndex(wb, ud, mi, ctx)
			}
		}
	}
}

func (r *db) iterateEntries(ents []pb.Entry,
	size uint64, shardID uint64, replicaID uint64, low uint64, high uint64,
	maxSize uint64) ([]pb.Entry, uint64, error) {
	maxIndex, err := r.getMaxIndex(shardID, replicaID)
	if err == raftio.ErrNoSavedLog {
		return ents, size, nil
	}
	if err != nil {
		err = errors.Wrapf(err, "%s failed to get max index", dn(shardID, replicaID))
		return nil, 0, err
	}
	entries, sz, err := r.entries.iterate(ents, maxIndex, size,
		shardID, replicaID, low, high, maxSize)
	err = errors.Wrapf(err, "%s failed to iterate entries, %d, %d, %d, %d",
		dn(shardID, replicaID), low, high, maxSize, maxIndex)
	return entries, sz, err
}
````

## File: internal/logdb/key_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"bytes"
	"math"
	"math/rand"
	"testing"

	"github.com/stretchr/testify/require"
)

func TestEntryKeysOrdered(t *testing.T) {
	p := newLogDBKeyPool()
	for i := uint64(0); i < 65536+10; i++ {
		k1 := p.get()
		k1.SetEntryKey(100, 100, i)
		k2 := p.get()
		k2.SetEntryKey(100, 100, i+1)
		require.True(t, bytes.Compare(k1.Key(), k2.Key()) < 0)
	}
	k1 := p.get()
	k1.SetEntryKey(100, 100, 0)
	k2 := p.get()
	k2.SetEntryKey(100, 100, 1)
	k3 := p.get()
	k3.SetEntryKey(100, 100, math.MaxUint64)
	require.True(t, bytes.Compare(k1.Key(), k2.Key()) < 0)
	require.True(t, bytes.Compare(k2.Key(), k3.Key()) < 0)
}

func TestSnapshotKeysOrdered(t *testing.T) {
	p := newLogDBKeyPool()
	k1 := p.get()
	k1.setSnapshotKey(100, 100, 0)
	k2 := p.get()
	k2.setSnapshotKey(100, 100, 1)
	require.True(t, bytes.Compare(k1.Key(), k2.Key()) < 0)
}

func TestNodeInfoKeyCanBeParsed(t *testing.T) {
	p := newLogDBKeyPool()
	for i := 0; i < 1024; i++ {
		k1 := p.get()
		cid := rand.Uint64()
		nid := rand.Uint64()
		k1.setNodeInfoKey(cid, nid)
		v1, v2 := parseNodeInfoKey(k1.Key())
		require.Equal(t, cid, v1)
		require.Equal(t, nid, v2)
	}
}
````

## File: internal/logdb/key.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"encoding/binary"
	"sync"
)

const (
	maxKeySize             uint64 = 28
	entryKeySize           uint64 = 28
	persistentStateKeySize uint64 = 20
	maxIndexKeySize        uint64 = 20
	nodeInfoKeySize        uint64 = 20
	bootstrapKeySize       uint64 = 20
	snapshotKeySize        uint64 = 28
	dataSize               uint64 = entryKeySize
)

var (
	entryKeyHeader           = [2]byte{0x1, 0x1}
	persistentStateKeyHeader = [2]byte{0x2, 0x2}
	maxIndexKeyHeader        = [2]byte{0x3, 0x3}
	nodeInfoKeyHeader        = [2]byte{0x4, 0x4}
	snapshotKeyHeader        = [2]byte{0x5, 0x5}
	bootstrapKeyHeader       = [2]byte{0x6, 0x6}
	entryBatchKeyHeader      = [2]byte{0x7, 0x7}
)

// Key represents keys that are managed by a sync.Pool to be reused.
type Key struct {
	pool *sync.Pool
	data []byte
	key  []byte
}

// NewKey creates and returns a new Key instance.
func NewKey(sz uint64, pool *sync.Pool) *Key {
	return newKey(sz, pool)
}

func newKey(sz uint64, pool *sync.Pool) *Key {
	return &Key{
		data: make([]byte, sz),
		pool: pool,
	}
}

// Release puts the key back to the pool.
func (k *Key) Release() {
	k.key = nil
	if k.pool != nil {
		k.pool.Put(k)
	}
}

// Key returns the []byte of the key.
func (k *Key) Key() []byte {
	return k.key
}

// SetMinimumKey sets the key to the minimum possible value.
func (k *Key) SetMinimumKey() {
	k.key = k.data
	for i := 0; i < len(k.key); i++ {
		k.key[i] = byte(0)
	}
}

// SetMaximumKey sets the key to the maximum possible value.
func (k *Key) SetMaximumKey() {
	k.key = k.data
	for i := 0; i < len(k.key); i++ {
		k.key[i] = byte(0xFF)
	}
}

// SetEntryBatchKey sets the key value opf the entry batch.
func (k *Key) SetEntryBatchKey(shardID uint64,
	replicaID uint64, batchID uint64) {
	k.useAsEntryKey()
	k.key[0] = entryBatchKeyHeader[0]
	k.key[1] = entryBatchKeyHeader[1]
	k.key[2] = 0
	k.key[3] = 0
	binary.BigEndian.PutUint64(k.key[4:], shardID)
	binary.BigEndian.PutUint64(k.key[12:], replicaID)
	binary.BigEndian.PutUint64(k.key[20:], batchID)
}

// SetEntryKey sets the key value to the specified entry key.
func (k *Key) SetEntryKey(shardID uint64, replicaID uint64, index uint64) {
	k.useAsEntryKey()
	k.key[0] = entryKeyHeader[0]
	k.key[1] = entryKeyHeader[1]
	k.key[2] = 0
	k.key[3] = 0
	binary.BigEndian.PutUint64(k.key[4:], shardID)
	// the 8 bytes node ID is actually not required in the key. it is stored as
	// an extra safenet - we don't know what we don't know, it is used as extra
	// protection between different node instances when things get ugly.
	// the wasted 8 bytes per entry is not a big deal - storing the index is
	// wasteful as well.
	binary.BigEndian.PutUint64(k.key[12:], replicaID)
	binary.BigEndian.PutUint64(k.key[20:], index)
}

// SetStateKey sets the key value to the specified State.
func (k *Key) SetStateKey(shardID uint64, replicaID uint64) {
	k.useAsStateKey()
	k.key[0] = persistentStateKeyHeader[0]
	k.key[1] = persistentStateKeyHeader[1]
	k.key[2] = 0
	k.key[3] = 0
	binary.BigEndian.PutUint64(k.key[4:], shardID)
	binary.BigEndian.PutUint64(k.key[12:], replicaID)
}

// SetMaxIndexKey sets the key value to the max index record key.
func (k *Key) SetMaxIndexKey(shardID uint64, replicaID uint64) {
	k.useAsMaxIndexKey()
	k.key[0] = maxIndexKeyHeader[0]
	k.key[1] = maxIndexKeyHeader[1]
	k.key[2] = 0
	k.key[3] = 0
	binary.BigEndian.PutUint64(k.key[4:], shardID)
	binary.BigEndian.PutUint64(k.key[12:], replicaID)
}

func (k *Key) useAsEntryKey() {
	k.key = k.data
}

func (k *Key) useAsSnapshotKey() {
	k.key = k.data
}

func (k *Key) useAsStateKey() {
	k.key = k.data[:persistentStateKeySize]
}

func (k *Key) useAsMaxIndexKey() {
	k.key = k.data[:maxIndexKeySize]
}

func (k *Key) useAsNodeInfoKey() {
	k.key = k.data[:nodeInfoKeySize]
}

func (k *Key) useAsBootstrapKey() {
	k.key = k.data[:bootstrapKeySize]
}

func parseNodeInfoKey(data []byte) (uint64, uint64) {
	if len(data) != 20 {
		panic("invalid node info data")
	}
	cid := binary.BigEndian.Uint64(data[4:])
	nid := binary.BigEndian.Uint64(data[12:])
	return cid, nid
}

func (k *Key) setNodeInfoKey(shardID uint64, replicaID uint64) {
	k.useAsNodeInfoKey()
	k.key[0] = nodeInfoKeyHeader[0]
	k.key[1] = nodeInfoKeyHeader[1]
	k.key[2] = 0
	k.key[3] = 0
	binary.BigEndian.PutUint64(k.key[4:], shardID)
	binary.BigEndian.PutUint64(k.key[12:], replicaID)
}

func (k *Key) setBootstrapKey(shardID uint64, replicaID uint64) {
	k.useAsBootstrapKey()
	k.key[0] = bootstrapKeyHeader[0]
	k.key[1] = bootstrapKeyHeader[1]
	k.key[2] = 0
	k.key[3] = 0
	binary.BigEndian.PutUint64(k.key[4:], shardID)
	binary.BigEndian.PutUint64(k.key[12:], replicaID)
}

func (k *Key) setSnapshotKey(shardID uint64, replicaID uint64, index uint64) {
	k.useAsSnapshotKey()
	k.key[0] = snapshotKeyHeader[0]
	k.key[1] = snapshotKeyHeader[1]
	k.key[2] = 0
	k.key[3] = 0
	binary.BigEndian.PutUint64(k.key[4:], shardID)
	binary.BigEndian.PutUint64(k.key[12:], replicaID)
	binary.BigEndian.PutUint64(k.key[20:], index)
}

type keyPool struct {
	pool *sync.Pool
}

func newLogDBKeyPool() *keyPool {
	p := &sync.Pool{}
	p.New = func() interface{} {
		return newKey(dataSize, p)
	}
	return &keyPool{pool: p}
}

func (p *keyPool) get() *Key {
	return p.pool.Get().(*Key)
}
````

## File: internal/logdb/kv_default.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//go:build !dragonboat_memfs_test
// +build !dragonboat_memfs_test

package logdb

import (
	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/logdb/kv"
	"github.com/lni/dragonboat/v4/internal/logdb/kv/pebble"
	"github.com/lni/dragonboat/v4/internal/vfs"
)

const (
	// DefaultKVStoreTypeName is the type name of the default kv store
	DefaultKVStoreTypeName = "pebble"
)

func newDefaultKVStore(config config.LogDBConfig,
	callback kv.LogDBCallback,
	dir string, wal string, fs vfs.IFS) (kv.IKVStore, error) {
	if fs != vfs.DefaultFS {
		_, isErrorFS := fs.(*vfs.ErrorFS)
		_, isMemFS := fs.(*vfs.MemFS)
		if !isErrorFS && !isMemFS {
			panic("invalid fs")
		}
	}
	return pebble.NewKVStore(config, callback, dir, wal, fs)
}
````

## File: internal/logdb/kv_pebble_memfs.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +build dragonboat_memfs_test

package logdb

import (
	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/logdb/kv"
	"github.com/lni/dragonboat/v4/internal/logdb/kv/pebble"
	"github.com/lni/dragonboat/v4/internal/vfs"
)

const (
	// DefaultKVStoreTypeName is the type name of the default kv store
	DefaultKVStoreTypeName = "pebble"
)

func newDefaultKVStore(config config.LogDBConfig,
	callback kv.LogDBCallback,
	dir string, wal string, fs vfs.IFS) (kv.IKVStore, error) {
	if fs == nil {
		panic("nil fs")
	}
	if _, ok := fs.(*vfs.MemFS); !ok {
		if _, ok := fs.(*vfs.ErrorFS); !ok {
			panic("invalid fs")
		}
	}
	return pebble.NewKVStore(config, callback, dir, wal, fs)
}
````

## File: internal/logdb/kv_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"bytes"
	"crypto/rand"
	"fmt"
	"io"
	"strings"
	"testing"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/leaktest"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/logdb/kv"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/vfs"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

func TestKVCanBeCreatedAndClosed(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	cfg := config.GetDefaultLogDBConfig()
	kvs, err := newDefaultKVStore(cfg, nil, RDBTestDirectory,
		RDBTestDirectory, fs)
	require.NoError(t, err, "failed to open kv store")
	defer deleteTestDB(fs)
	require.NoError(t, kvs.Close(), "failed to close kv store")
}

func runKVTest(t *testing.T, tf func(t *testing.T, kvs kv.IKVStore),
	fs vfs.IFS) {
	defer leaktest.AfterTest(t)()
	defer deleteTestDB(fs)
	cfg := config.GetDefaultLogDBConfig()
	kvs, err := newDefaultKVStore(cfg, nil, RDBTestDirectory,
		RDBTestDirectory, fs)
	require.NoError(t, err, "failed to open kv store")
	defer func() {
		require.NoError(t, kvs.Close(), "failed to close kvs")
	}()
	tf(t, kvs)
}

func TestKVGetAndSet(t *testing.T) {
	tf := func(t *testing.T, kvs kv.IKVStore) {
		err := kvs.SaveValue([]byte("test-key"), []byte("test-value"))
		require.NoError(t, err, "failed to save the value")
		found := false
		opcalled := false
		op := func(val []byte) error {
			opcalled = true
			if string(val) == "test-value" {
				found = true
			}
			return nil
		}
		err = kvs.GetValue([]byte("test-key"), op)
		require.NoError(t, err, "get value failed")
		require.True(t, opcalled, "op func not called")
		require.True(t, found, "failed to get value")
	}
	fs := vfs.GetTestFS()
	runKVTest(t, tf, fs)
}

func TestKVValueCanBeDeleted(t *testing.T) {
	tf := func(t *testing.T, kvs kv.IKVStore) {
		err := kvs.SaveValue([]byte("test-key"), []byte("test-value"))
		require.NoError(t, err, "failed to save the value")
		err = kvs.DeleteValue([]byte("test-key"))
		require.NoError(t, err, "failed to delete")
		found := false
		opcalled := false
		op := func(val []byte) error {
			opcalled = true
			if string(val) == "test-value" {
				found = true
			}
			return nil
		}
		err = kvs.GetValue([]byte("test-key"), op)
		require.NoError(t, err, "get value failed")
		require.True(t, opcalled, "op func not called")
		require.False(t, found, "failed to delete result")
	}
	fs := vfs.GetTestFS()
	runKVTest(t, tf, fs)
}

func TestKVWriteBatch(t *testing.T) {
	tf := func(t *testing.T, kvs kv.IKVStore) {
		wb := kvs.GetWriteBatch()
		defer wb.Destroy()
		wb.Put([]byte("test-key"), []byte("test-value"))
		require.Equal(t, 1, wb.Count(), "incorrect count")
		err := kvs.CommitWriteBatch(wb)
		require.NoError(t, err, "failed to commit the write batch")
		found := false
		opcalled := false
		op := func(val []byte) error {
			opcalled = true
			if string(val) == "test-value" {
				found = true
			}
			return nil
		}
		err = kvs.GetValue([]byte("test-key"), op)
		require.NoError(t, err, "get value failed")
		require.True(t, opcalled, "op func not called")
		require.True(t, found, "failed to get the result")
	}
	fs := vfs.GetTestFS()
	runKVTest(t, tf, fs)
}

func testKVIterateValue(t *testing.T,
	fk []byte, lk []byte, inc bool, count uint64) {
	tf := func(t *testing.T, kvs kv.IKVStore) {
		for i := 0; i < 10; i++ {
			key := fmt.Sprintf("key%d", i)
			val := fmt.Sprintf("val%d", i)
			err := kvs.SaveValue([]byte(key), []byte(val))
			require.NoError(t, err, "failed to save the value")
		}
		opcalled := uint64(0)
		op := func(k []byte, v []byte) (bool, error) {
			opcalled++
			return true, nil
		}
		err := kvs.IterateValue(fk, lk, inc, op)
		require.NoError(t, err, "iterate value failed")
		require.Equal(t, count, opcalled, "op called wrong number of times")
	}
	fs := vfs.GetTestFS()
	runKVTest(t, tf, fs)
}

func TestKVIterateValue(t *testing.T) {
	testKVIterateValue(t, []byte("key0"), []byte("key5"), true, 6)
	testKVIterateValue(t, []byte("key0"), []byte("key5"), false, 5)
}

func TestWriteBatchCanBeCleared(t *testing.T) {
	tf := func(t *testing.T, kvs kv.IKVStore) {
		wb := kvs.GetWriteBatch()
		wb.Put([]byte("key-1"), []byte("val-1"))
		wb.Put([]byte("key-2"), []byte("val-2"))
		require.Equal(t, 2, wb.Count(), "unexpected count")
		wb.Clear()
		err := kvs.CommitWriteBatch(wb)
		require.NoError(t, err, "failed to commit write batch")
		err = kvs.GetValue([]byte("key-1"),
			func(data []byte) error {
				require.Empty(t, data, "unexpected value")
				return nil
			})
		require.NoError(t, err, "get value failed")
	}
	fs := vfs.GetTestFS()
	runKVTest(t, tf, fs)
}

func TestHasEntryRecord(t *testing.T) {
	tf := func(t *testing.T, kvs kv.IKVStore) {
		has, err := hasEntryRecord(kvs, true)
		require.NoError(t, err, "hasEntryRecord failed")
		require.False(t, has, "unexpected result")
		has, err = hasEntryRecord(kvs, false)
		require.NoError(t, err, "hasEntryRecord failed")
		require.False(t, has, "unexpected result")
		eb := pb.EntryBatch{}
		data, err := eb.Marshal()
		require.NoError(t, err)
		k := newKey(entryKeySize, nil)
		k.SetEntryBatchKey(1, 1, 1)
		err = kvs.SaveValue(k.Key(), data)
		require.NoError(t, err, "failed to save entry batch")
		has, err = hasEntryRecord(kvs, true)
		require.NoError(t, err, "hasEntryRecord failed")
		require.True(t, has, "unexpected result")
		has, err = hasEntryRecord(kvs, false)
		require.NoError(t, err, "hasEntryRecord failed")
		require.False(t, has, "unexpected result")
		ent := pb.Entry{}
		data, err = ent.Marshal()
		require.NoError(t, err)
		k = newKey(entryKeySize, nil)
		k.SetEntryKey(1, 1, 1)
		err = kvs.SaveValue(k.Key(), data)
		require.NoError(t, err, "failed to save entry batch")
		has, err = hasEntryRecord(kvs, true)
		require.NoError(t, err, "hasEntryRecord failed")
		require.True(t, has, "unexpected result")
		has, err = hasEntryRecord(kvs, false)
		require.NoError(t, err, "hasEntryRecord failed")
		require.True(t, has, "unexpected result")
	}
	fs := vfs.GetTestFS()
	runKVTest(t, tf, fs)
}

func TestEntriesCanBeRemovedFromKVStore(t *testing.T) {
	tf := func(t *testing.T, kvs kv.IKVStore) {
		wb := kvs.GetWriteBatch()
		defer wb.Destroy()
		for i := uint64(1); i <= 100; i++ {
			key := newKey(entryKeySize, nil)
			key.SetEntryKey(100, 1, i)
			data := make([]byte, 16)
			wb.Put(key.Key(), data)
		}
		err := kvs.CommitWriteBatch(wb)
		require.NoError(t, err, "failed to commit wb")
		fk := newKey(entryKeySize, nil)
		lk := newKey(entryKeySize, nil)
		fk.SetEntryKey(100, 1, 1)
		lk.SetEntryKey(100, 1, 100)
		count := 0
		op := func(key []byte, data []byte) (bool, error) {
			count++
			return true, nil
		}
		err = kvs.IterateValue(fk.Key(), lk.Key(), true, op)
		require.NoError(t, err, "iterate value failed")
		require.Equal(t, 100, count, "failed to get all key value pairs")
		lk.SetEntryKey(100, 1, 21)
		err = kvs.BulkRemoveEntries(fk.Key(), lk.Key())
		require.NoError(t, err, "remove entry failed")
		err = kvs.CompactEntries(fk.Key(), lk.Key())
		require.NoError(t, err, "compaction failed")
		count = 0
		lk.SetEntryKey(100, 1, 100)
		err = kvs.IterateValue(fk.Key(), lk.Key(), true, op)
		require.NoError(t, err, "iterate value failed")
		require.Equal(t, 80, count, "failed to get all key value pairs")
	}
	fs := vfs.GetTestFS()
	runKVTest(t, tf, fs)
}

func TestCompactionReleaseStorageSpace(t *testing.T) {
	fs := vfs.GetTestFS()
	deleteTestDB(fs)
	defer deleteTestDB(fs)
	maxIndex := uint64(1024 * 128)
	fk := newKey(entryKeySize, nil)
	lk := newKey(entryKeySize, nil)
	fk.SetEntryKey(100, 1, 1)
	lk.SetEntryKey(100, 1, maxIndex+1)
	cfg := config.GetDefaultLogDBConfig()
	func() {
		kvs, err := newDefaultKVStore(cfg, nil, RDBTestDirectory,
			RDBTestDirectory, fs)
		require.NoError(t, err, "failed to open kv store")
		defer func() {
			require.NoError(t, kvs.Close())
		}()
		wb := kvs.GetWriteBatch()
		defer wb.Destroy()
		for i := uint64(1); i <= maxIndex; i++ {
			key := newKey(entryKeySize, nil)
			key.SetEntryKey(100, 1, i)
			data := make([]byte, 64)
			_, err = rand.Read(data)
			require.NoError(t, err)
			wb.Put(key.Key(), data)
		}
		err = kvs.CommitWriteBatch(wb)
		require.NoError(t, err, "failed to commit wb")
	}()
	sz, err := getDirSize(RDBTestDirectory, true, fs)
	require.NoError(t, err, "failed to get sz")
	require.GreaterOrEqual(t, sz, int64(1024*1024*8), "unexpected size")
	kvs, err := newDefaultKVStore(cfg, nil, RDBTestDirectory,
		RDBTestDirectory, fs)
	require.NoError(t, err, "failed to open kv store")
	defer func() {
		require.NoError(t, kvs.Close())
	}()
	err = kvs.BulkRemoveEntries(fk.Key(), lk.Key())
	require.NoError(t, err, "remove entry failed")
	err = kvs.CompactEntries(fk.Key(), lk.Key())
	require.NoError(t, err, "compaction failed")
	sz, err = getDirSize(RDBTestDirectory, false, fs)
	require.NoError(t, err, "failed to get sz")
	require.LessOrEqual(t, sz, int64(1024*1024), "unexpected size")
}

var flagContent = "YYYY"
var corruptedContent = "XXXX"

func getDataFilePathList(dir string, wal bool, fs vfs.IFS) ([]string, error) {
	elms, err := fs.List(dir)
	if err != nil {
		return nil, err
	}
	result := make([]string, 0)
	for _, path := range elms {
		v, err := fs.Stat(fs.PathJoin(dir, path))
		if err != nil {
			return nil, err
		}
		if !wal {
			if strings.HasSuffix(v.Name(), ".ldb") ||
				strings.HasSuffix(v.Name(), ".sst") {
				result = append(result, fs.PathJoin(dir, v.Name()))
			}
		} else {
			if strings.HasSuffix(v.Name(), ".log") {
				result = append(result, fs.PathJoin(dir, v.Name()))
			}
		}
	}
	return result, nil
}

func cutDataFile(fp string, fs vfs.IFS) (bool, error) {
	buf := bytes.NewBuffer(nil)
	f, err := fs.Open(fp)
	if err != nil {
		return false, err
	}
	_, err = io.Copy(buf, f)
	if err != nil {
		return false, err
	}
	if err := f.Close(); err != nil {
		return false, err
	}
	data := buf.Bytes()
	f, err = fs.Create(fp)
	if err != nil {
		return false, err
	}
	defer func() {
		if err := f.Close(); err != nil {
			panic(err)
		}
	}()
	_, err = f.Write(data[:len(data)-1])
	if err != nil {
		return false, err
	}
	return true, nil
}

func modifyDataFile(fp string, fs vfs.IFS) (bool, error) {
	tmpFp := fs.PathJoin(fs.PathDir(fp), "tmp")
	if err := func() error {
		idx := int64(0)
		f, err := fs.ReuseForWrite(fp, tmpFp)
		if err != nil {
			return err
		}
		defer func() {
			if err := f.Close(); err != nil {
				panic(err)
			}
		}()
		located := false
		data := make([]byte, 4)
		for {
			if _, err := f.Read(data); err != nil {
				if err == io.EOF {
					break
				} else {
					return err
				}
			}
			if string(data) == flagContent {
				located = true
				break
			}
			idx += 4
		}
		if !located {
			return errors.New("failed to locaate the data")
		}
		if _, err = f.WriteAt([]byte(corruptedContent), idx); err != nil {
			plog.Infof("failed to write")
			return err
		}
		return nil
	}(); err != nil {
		return false, err
	}
	if err := fs.Rename(tmpFp, fp); err != nil {
		return false, err
	}
	return true, nil
}

func testDiskCorruptionIsHandled(t *testing.T, wal bool, cut bool, fs vfs.IFS) {
	deleteTestDB(fs)
	defer deleteTestDB(fs)
	cfg := config.GetDefaultLogDBConfig()
	func() {
		kvs, err := newDefaultKVStore(cfg, nil, RDBTestDirectory,
			RDBTestDirectory, fs)
		require.NoError(t, err, "failed to open kv store")
		defer func() {
			require.NoError(t, kvs.Close())
		}()
		if cut && !wal {
			require.Fail(t, "cut && !wal")
		}
		if wal && kvs.Name() != "rocksdb" {
			t.Skip("test skipped, WAL hardware corruption is not handled")
		}
		if wal && kvs.Name() == "rocksdb" &&
			!settings.Soft.KVTolerateCorruptedTailRecords {
			t.Skip("test skipped, RocksDBTolerateCorruptedTailRecords disabled")
		}
		wb := kvs.GetWriteBatch()
		defer wb.Destroy()
		data := make([]byte, 0)
		for i := 0; i < 16; i++ {
			data = append(data, []byte(flagContent)...)
		}
		for i := uint64(1); i <= 1; i++ {
			key := newKey(entryKeySize, nil)
			key.SetEntryKey(100, 1, i)
			wb.Put(key.Key(), data)
		}
		err = kvs.CommitWriteBatch(wb)
		require.NoError(t, err, "failed to commit wb")
		if !wal {
			err := kvs.FullCompaction()
			require.NoError(t, err, "full compaction failed")
		}
	}()
	files, err := getDataFilePathList(RDBTestDirectory, wal, fs)
	require.NoError(t, err, "failed to get data files")
	corrupted := false
	for _, fp := range files {
		var done bool
		var err error
		if cut {
			done, err = cutDataFile(fp, fs)
		} else {
			done, err = modifyDataFile(fp, fs)
		}
		require.NoError(t, err, "failed to modify data file")
		if done {
			corrupted = true
			break
		}
	}
	require.True(t, corrupted, "failed to corrupt data files")
	kvs, err := newDefaultKVStore(cfg, nil, RDBTestDirectory,
		RDBTestDirectory, fs)
	if err == nil {
		defer func() {
			require.NoError(t, kvs.Close())
		}()
	}
	if !cut {
		if wal && err == nil {
			require.Fail(t, "corrupted WAL not reported")
		} else {
			return
		}
	}
	require.NoError(t, err, "failed to open kv store")
	fk := newKey(entryKeySize, nil)
	lk := newKey(entryKeySize, nil)
	fk.SetEntryKey(100, 1, 1)
	lk.SetEntryKey(100, 1, 1024)
	count := 0
	op := func(key []byte, data []byte) (bool, error) {
		count++
		return true, nil
	}
	err = kvs.IterateValue(fk.Key(), lk.Key(), true, op)
	if !cut {
		require.Error(t, err, "no checksum error returned")
	} else {
		require.NoError(t, err, "failed to iterate the db")
		require.Equal(t, 0, count, "unexpected count")
	}
}

func TestTailCorruptionIsIgnored(t *testing.T) {
	fs := vfs.GetTestFS()
	testDiskCorruptionIsHandled(t, true, true, fs)
}

func TestSSTCorruptionIsHandled(t *testing.T) {
	fs := vfs.GetTestFS()
	testDiskCorruptionIsHandled(t, false, false, fs)
}

// see testDiskCorruptionIsHandled's comments for more details
func TestWALCorruptionIsHandled(t *testing.T) {
	fs := vfs.GetTestFS()
	testDiskCorruptionIsHandled(t, true, false, fs)
}
````

## File: internal/logdb/log_logdb_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
// Copyright 2015 The etcd Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

// This file contains tests ported from etcd raft

import (
	"math"
	"testing"

	"github.com/lni/dragonboat/v4/internal/raft"
	"github.com/lni/dragonboat/v4/internal/vfs"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/stretchr/testify/require"
)

func removeTestLogdbDir(fs vfs.IFS) {
	if err := fs.RemoveAll(RDBTestDirectory); err != nil {
		panic(err)
	}
}

func getTestLogReaderWithoutCache(entries []pb.Entry) *LogReader {
	logdb := getNewLogReaderTestDB(entries, vfs.GetTestFS())
	ls := NewLogReader(LogReaderTestShardID, LogReaderTestReplicaID, logdb)
	ls.SetCompactor(testCompactor)
	if len(entries) > 0 {
		if err := ls.Append(entries); err != nil {
			panic(err)
		}
	}
	return ls
}

func TestRLLTFindConflict(t *testing.T) {
	previousEnts := []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2},
		{Index: 3, Term: 3}}
	tests := []struct {
		ents      []pb.Entry
		wconflict uint64
	}{
		// no conflict, empty ent
		{[]pb.Entry{}, 0},
		// no conflict
		{[]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2},
			{Index: 3, Term: 3}}, 0},
		{[]pb.Entry{{Index: 2, Term: 2}, {Index: 3, Term: 3}}, 0},
		{[]pb.Entry{{Index: 3, Term: 3}}, 0},
		// no conflict, but has new entries
		{[]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2},
			{Index: 3, Term: 3}, {Index: 4, Term: 4},
			{Index: 5, Term: 4}}, 4},
		{[]pb.Entry{{Index: 2, Term: 2}, {Index: 3, Term: 3},
			{Index: 4, Term: 4}, {Index: 5, Term: 4}}, 4},
		{[]pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 4},
			{Index: 5, Term: 4}}, 4},
		{[]pb.Entry{{Index: 4, Term: 4}, {Index: 5, Term: 4}}, 4},
		// conflicts with existing entries
		{[]pb.Entry{{Index: 1, Term: 4}, {Index: 2, Term: 4}}, 1},
		{[]pb.Entry{{Index: 2, Term: 1}, {Index: 3, Term: 4},
			{Index: 4, Term: 4}}, 2},
		{[]pb.Entry{{Index: 3, Term: 1}, {Index: 4, Term: 2},
			{Index: 5, Term: 4}, {Index: 6, Term: 4}}, 3},
	}
	stable := getTestLogReaderWithoutCache(previousEnts)
	defer removeTestLogdbDir(vfs.GetTestFS())
	defer func() {
		require.NoError(t, stable.logdb.(*ShardedDB).Close())
	}()
	for i, tt := range tests {
		raftLog := raft.NewLog(stable)
		gconflict, err := raftLog.GetConflictIndex(tt.ents)
		require.NoError(t, err)
		require.Equal(t, tt.wconflict, gconflict,
			"#%d: conflict = %d, want %d", i, gconflict, tt.wconflict)
	}
}

func TestRLLTIsUpToDate(t *testing.T) {
	previousEnts := []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2},
		{Index: 3, Term: 3}}
	stable := getTestLogReaderWithoutCache(previousEnts)
	defer removeTestLogdbDir(vfs.GetTestFS())
	defer func() {
		require.NoError(t, stable.logdb.(*ShardedDB).Close())
	}()
	raftLog := raft.NewLog(stable)
	tests := []struct {
		lastIndex uint64
		term      uint64
		wUpToDate bool
	}{
		// greater term, ignore lastIndex
		{raftLog.LastIndex() - 1, 4, true},
		{raftLog.LastIndex(), 4, true},
		{raftLog.LastIndex() + 1, 4, true},
		// smaller term, ignore lastIndex
		{raftLog.LastIndex() - 1, 2, false},
		{raftLog.LastIndex(), 2, false},
		{raftLog.LastIndex() + 1, 2, false},
		// equal term, equal or lager lastIndex wins
		{raftLog.LastIndex() - 1, 3, false},
		{raftLog.LastIndex(), 3, true},
		{raftLog.LastIndex() + 1, 3, true},
	}
	for i, tt := range tests {
		gUpToDate, err := raftLog.UpToDate(tt.lastIndex, tt.term)
		require.NoError(t, err)
		require.Equal(t, tt.wUpToDate, gUpToDate,
			"#%d: uptodate = %v, want %v", i, gUpToDate, tt.wUpToDate)
	}
}

func TestRLLTAppend(t *testing.T) {
	previousEnts := []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}}
	tests := []struct {
		ents      []pb.Entry
		windex    uint64
		wents     []pb.Entry
		wunstable uint64
	}{
		{
			[]pb.Entry{},
			2,
			[]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}},
			3,
		},
		{
			[]pb.Entry{{Index: 3, Term: 2}},
			3,
			[]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2},
				{Index: 3, Term: 2}},
			3,
		},
		// conflicts with index 1
		{
			[]pb.Entry{{Index: 1, Term: 2}},
			1,
			[]pb.Entry{{Index: 1, Term: 2}},
			1,
		},
		// conflicts with index 2
		{
			[]pb.Entry{{Index: 2, Term: 3}, {Index: 3, Term: 3}},
			3,
			[]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 3},
				{Index: 3, Term: 3}},
			2,
		},
	}
	for i, tt := range tests {
		stable := getTestLogReaderWithoutCache(previousEnts)
		raftLog := raft.NewLog(stable)

		fi, _ := stable.GetRange()
		plog.Infof("stable first index: %d", fi)
		require.NoError(t, raftLog.Append(tt.ents))
		index := raftLog.LastIndex()
		require.Equal(t, tt.windex, index,
			"#%d: lastIndex = %d, want %d", i, index, tt.windex)
		g, err := raftLog.Entries(1, math.MaxUint64)
		require.NoError(t, err, "#%d: unexpected error %v", i, err)
		require.Equal(t, tt.wents, g,
			"#%d: logEnts = %+v, want %+v", i, g, tt.wents)
		if goff := raftLog.UnstableOffset(); goff != tt.wunstable {
			t.Errorf("#%d: unstable = %d, want %d", i, goff, tt.wunstable)
		}
		require.NoError(t, stable.logdb.(*ShardedDB).Close())
		removeTestLogdbDir(vfs.GetTestFS())
	}
}

// TestLogMaybeAppend ensures:
// If the given (index, term) matches with the existing log:
//  1. If an existing entry conflicts with a new one (same index
//     but different terms), delete the existing entry and all that
//     follow it
//     2.Append any new entries not already in the log
//
// If the given (index, term) does not match with the existing log:
//
//	return false
func TestRLLTLogMaybeAppend(t *testing.T) {
	previousEnts := []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2},
		{Index: 3, Term: 3}}
	lastindex := uint64(3)
	lastterm := uint64(3)
	commit := uint64(1)

	tests := []struct {
		logTerm   uint64
		index     uint64
		committed uint64
		ents      []pb.Entry

		wlasti  uint64
		wappend bool
		wcommit uint64
		wpanic  bool
	}{
		// not match: term is different
		{
			lastterm - 1, lastindex, lastindex,
			[]pb.Entry{{Index: lastindex + 1, Term: 4}},
			0, false, commit, false,
		},
		// not match: index out of bound
		{
			lastterm, lastindex + 1, lastindex,
			[]pb.Entry{{Index: lastindex + 2, Term: 4}},
			0, false, commit, false,
		},
		// match with the last existing entry
		{
			lastterm, lastindex, lastindex, nil,
			lastindex, true, lastindex, false,
		},
		{
			lastterm, lastindex, lastindex + 1, nil,
			lastindex, true, lastindex, false, // do not increase commit higher than lastnewi
		},
		{
			lastterm, lastindex, lastindex - 1, nil,
			lastindex, true, lastindex - 1, false, // commit up to the commit in the message
		},
		{
			lastterm, lastindex, 0, nil,
			lastindex, true, commit, false, // commit do not decrease
		},
		{
			0, 0, lastindex, nil,
			0, true, commit, false, // commit do not decrease
		},
		{
			lastterm, lastindex, lastindex,
			[]pb.Entry{{Index: lastindex + 1, Term: 4}},
			lastindex + 1, true, lastindex, false,
		},
		{
			lastterm, lastindex, lastindex + 1,
			[]pb.Entry{{Index: lastindex + 1, Term: 4}},
			lastindex + 1, true, lastindex + 1, false,
		},
		{
			lastterm, lastindex, lastindex + 2,
			[]pb.Entry{{Index: lastindex + 1, Term: 4}},
			lastindex + 1, true, lastindex + 1, false, // do not increase commit higher than lastnewi
		},
		{
			lastterm, lastindex, lastindex + 2,
			[]pb.Entry{{Index: lastindex + 1, Term: 4},
				{Index: lastindex + 2, Term: 4}},
			lastindex + 2, true, lastindex + 2, false,
		},
		// match with the the entry in the middle
		{
			lastterm - 1, lastindex - 1, lastindex,
			[]pb.Entry{{Index: lastindex, Term: 4}},
			lastindex, true, lastindex, false,
		},
		{
			lastterm - 2, lastindex - 2, lastindex,
			[]pb.Entry{{Index: lastindex - 1, Term: 4}},
			lastindex - 1, true, lastindex - 1, false,
		},
		{
			lastterm - 3, lastindex - 3, lastindex,
			[]pb.Entry{{Index: lastindex - 2, Term: 4}},
			lastindex - 2, true, lastindex - 2, true, // conflict with existing committed entry
		},
		{
			lastterm - 2, lastindex - 2, lastindex,
			[]pb.Entry{{Index: lastindex - 1, Term: 4},
				{Index: lastindex, Term: 4}},
			lastindex, true, lastindex, false,
		},
	}

	for i, tt := range tests {
		stable := getTestLogReaderWithoutCache(previousEnts)
		raftLog := raft.NewLog(stable)
		raftLog.SetCommitted(commit)

		if tt.wpanic {
			require.Panics(t, func() {
				glasti, gappend, err := raftLog.TryAppend(tt.index, tt.logTerm,
					tt.committed, tt.ents)
				require.NoError(t, err)
				gcommit := raftLog.GetCommitted()

				require.Equal(t, tt.wlasti, glasti,
					"#%d: lastindex = %d, want %d", i, glasti, tt.wlasti)
				require.Equal(t, tt.wappend, gappend,
					"#%d: append = %v, want %v", i, gappend, tt.wappend)
				require.Equal(t, tt.wcommit, gcommit,
					"#%d: committed = %d, want %d", i, gcommit, tt.wcommit)
				if gappend && len(tt.ents) != 0 {
					gents, err := raftLog.GetEntries(
						raftLog.LastIndex()-uint64(len(tt.ents))+1,
						raftLog.LastIndex()+1, math.MaxUint64)
					require.NoError(t, err)
					require.Equal(t, tt.ents, gents,
						"%d: appended entries = %v, want %v", i, gents, tt.ents)
				}
			})
		} else {
			require.NotPanics(t, func() {
				glasti, gappend, err := raftLog.TryAppend(tt.index, tt.logTerm,
					tt.committed, tt.ents)
				require.NoError(t, err)
				gcommit := raftLog.GetCommitted()

				require.Equal(t, tt.wlasti, glasti,
					"#%d: lastindex = %d, want %d", i, glasti, tt.wlasti)
				require.Equal(t, tt.wappend, gappend,
					"#%d: append = %v, want %v", i, gappend, tt.wappend)
				require.Equal(t, tt.wcommit, gcommit,
					"#%d: committed = %d, want %d", i, gcommit, tt.wcommit)
				if gappend && len(tt.ents) != 0 {
					gents, err := raftLog.GetEntries(
						raftLog.LastIndex()-uint64(len(tt.ents))+1,
						raftLog.LastIndex()+1, math.MaxUint64)
					require.NoError(t, err)
					require.Equal(t, tt.ents, gents,
						"%d: appended entries = %v, want %v", i, gents, tt.ents)
				}
			})
		}

		require.NoError(t, stable.logdb.(*ShardedDB).Close())
		removeTestLogdbDir(vfs.GetTestFS())
	}
}

/*
// TestCompactionSideEffects ensures that all the log related functionality works correctly after
// a compaction.
func TestRLLTCompactionSideEffects(t *testing.T) {
	var i uint64
	// Populate the log with 1000 entries; 750 in stable storage and 250 in unstable.
	lastIndex := uint64(1000)
	unstableIndex := uint64(750)
	lastTerm := lastIndex
	noLimit := uint64(math.MaxUint64)

	entries := make([]pb.Entry, 0)
	for i = 1; i <= unstableIndex; i++ {
		entries = append(entries, pb.Entry{Term: uint64(i), Index: uint64(i)})
	}

	stable := getTestLogReaderWithoutCache(entries)
	raftLog := raft.NewLog(stable)
	for i = unstableIndex; i < lastIndex; i++ {
		raftLog.Append([]pb.Entry{pb.Entry{Term: uint64(i + 1), Index: uint64(i + 1)}})
	}

	ok := raftLog.TryCommit(lastIndex, lastTerm)
	if !ok {
		t.Fatalf("maybeCommit returned false")
	}
	raftLog.AppliedTo(raftLog.GetCommitted())

	offset := uint64(500)
	stable.Compact(offset)

	if raftLog.LastIndex() != lastIndex {
		t.Errorf("lastIndex = %d, want %d", raftLog.LastIndex(), lastIndex)
	}

	for j := offset; j <= raftLog.LastIndex(); j++ {
		if mustTerm(raftLog.Term(j)) != j {
			t.Errorf("term(%d) = %d, want %d", j, mustTerm(raftLog.Term(j)), j)
		}
	}

	for j := offset; j <= raftLog.LastIndex(); j++ {
		if !raftLog.MatchTerm(j, j) {
			t.Errorf("matchTerm(%d) = false, want true", j)
		}
	}

	unstableEnts := raftLog.EntriesToSave()
	if g := len(unstableEnts); g != 250 {
		t.Errorf("len(unstableEntries) = %d, want = %d", g, 250)
	}
	if unstableEnts[0].Index != 751 {
		t.Errorf("Index = %d, want = %d", unstableEnts[0].Index, 751)
	}

	prev := raftLog.LastIndex()
	raftLog.Append([]pb.Entry{pb.Entry{Index: raftLog.LastIndex() + 1, Term: raftLog.LastIndex() + 1}})
	if raftLog.LastIndex() != prev+1 {
		t.Errorf("lastIndex = %d, want = %d", raftLog.LastIndex(), prev+1)
	}

	ents, err := raftLog.Entries(raftLog.LastIndex(), noLimit)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if len(ents) != 1 {
		t.Errorf("len(entries) = %d, want = %d", len(ents), 1)
	}

	stable.logdb.(*ShardedDB).Close()
	removeTestLogdbDir()
}
*/

func TestRLLTHasNextEnts(t *testing.T) {
	snap := pb.Snapshot{
		Term: 1, Index: 3,
	}
	ents := []pb.Entry{
		{Term: 1, Index: 4},
		{Term: 1, Index: 5},
		{Term: 1, Index: 6},
	}
	tests := []struct {
		applied uint64
		hasNext bool
	}{
		{0, true},
		{3, true},
		{4, true},
		{5, false},
	}
	for i, tt := range tests {
		stable := getTestLogReaderWithoutCache(nil)
		require.NoError(t, stable.ApplySnapshot(snap))
		raftLog := raft.NewLog(stable)
		require.NoError(t, raftLog.Append(ents))
		_, err := raftLog.TryCommit(5, 1)
		require.NoError(t, err)
		raftLog.AppliedTo(tt.applied)

		hasNext := raftLog.HasEntriesToApply()
		require.Equal(t, tt.hasNext, hasNext,
			"#%d: hasNext = %v, want %v", i, hasNext, tt.hasNext)

		require.NoError(t, stable.logdb.(*ShardedDB).Close())
		removeTestLogdbDir(vfs.GetTestFS())
	}
}

func TestRLLTNextEnts(t *testing.T) {
	snap := pb.Snapshot{
		Term: 1, Index: 3,
	}
	ents := []pb.Entry{
		{Term: 1, Index: 4},
		{Term: 1, Index: 5},
		{Term: 1, Index: 6},
	}
	tests := []struct {
		applied uint64
		wents   []pb.Entry
	}{
		{0, ents[:2]},
		{3, ents[:2]},
		{4, ents[1:2]},
		{5, nil},
	}
	for i, tt := range tests {
		stable := getTestLogReaderWithoutCache(nil)
		require.NoError(t, stable.ApplySnapshot(snap))
		raftLog := raft.NewLog(stable)
		require.NoError(t, raftLog.Append(ents))
		_, err := raftLog.TryCommit(5, 1)
		require.NoError(t, err)
		raftLog.AppliedTo(tt.applied)

		nents, err := raftLog.EntriesToApply()
		require.NoError(t, err)
		require.Equal(t, tt.wents, nents,
			"#%d: nents = %+v, want %+v", i, nents, tt.wents)

		require.NoError(t, stable.logdb.(*ShardedDB).Close())
		removeTestLogdbDir(vfs.GetTestFS())
	}
}

// TestCompaction ensures that the number of log entries is correct after compactions.
func TestRLLTCompaction(t *testing.T) {
	tests := []struct {
		lastIndex uint64
		compact   []uint64
		wleft     []int
		wallow    bool
	}{
		// out of upper bound
		{1000, []uint64{1001}, []int{-1}, false},
		{1000, []uint64{300, 500, 800, 900}, []int{700, 500, 200, 100}, true},
		// out of lower bound
		{1000, []uint64{300, 299}, []int{700, -1}, false},
	}

	for i, tt := range tests {
		entries := make([]pb.Entry, 0)
		for i := uint64(1); i <= tt.lastIndex; i++ {
			entries = append(entries, pb.Entry{Index: i})
		}
		stable := getTestLogReaderWithoutCache(entries)
		raftLog := raft.NewLog(stable)

		func() {
			defer func() {
				if r := recover(); r != nil {
					if tt.wallow {
						t.Errorf("%d: allow = %v, want %v: %v", i, false, true, r)
					}
				}
			}()

			if _, err := raftLog.TryCommit(tt.lastIndex, 0); err != nil {
				t.Fatalf("unexpected error %v", err)
			}
			raftLog.AppliedTo(raftLog.GetCommitted())

			for j := 0; j < len(tt.compact); j++ {
				err := stable.Compact(tt.compact[j])
				if err != nil {
					if tt.wallow {
						t.Errorf("#%d.%d allow = %t, want %t", i, j, false, tt.wallow)
					}
					continue
				}
				if len(raftLog.AllEntries()) != tt.wleft[j] {
					t.Errorf("#%d.%d len = %d, want %d", i, j, len(raftLog.AllEntries()), tt.wleft[j])
				}
			}
		}()
		require.NoError(t, stable.logdb.(*ShardedDB).Close())
		removeTestLogdbDir(vfs.GetTestFS())
	}
}

func TestRLLTLogRestore(t *testing.T) {
	index := uint64(1000)
	term := uint64(1000)
	stable := getTestLogReaderWithoutCache(nil)
	require.NoError(t, stable.ApplySnapshot(pb.Snapshot{Index: index, Term: term}))
	raftLog := raft.NewLog(stable)

	require.Equal(t, 0, len(raftLog.AllEntries()),
		"len = %d, want 0", len(raftLog.AllEntries()))
	require.Equal(t, index+1, raftLog.FirstIndex(),
		"firstIndex = %d, want %d", raftLog.FirstIndex(), index+1)
	require.Equal(t, index, raftLog.GetCommitted(),
		"committed = %d, want %d", raftLog.GetCommitted(), index)
	require.Equal(t, index+1, raftLog.UnstableOffset(),
		"unstable = %d, want %d", raftLog.UnstableOffset(), index+1)
	require.Equal(t, term, mustTerm(raftLog.Term(index)),
		"term = %d, want %d", mustTerm(raftLog.Term(index)), term)

	require.NoError(t, stable.logdb.(*ShardedDB).Close())
	removeTestLogdbDir(vfs.GetTestFS())
}

func TestRLLTIsOutOfBounds(t *testing.T) {
	offset := uint64(100)
	num := uint64(100)

	stable := getTestLogReaderWithoutCache(nil)
	require.NoError(t, stable.ApplySnapshot(pb.Snapshot{Index: offset}))
	l := raft.NewLog(stable)

	for i := uint64(1); i <= num; i++ {
		require.NoError(t, l.Append([]pb.Entry{{Index: i + offset}}))
	}

	first := offset + 1
	tests := []struct {
		lo, hi        uint64
		wpanic        bool
		wErrCompacted bool
	}{
		{
			first - 2, first + 1,
			false,
			true,
		},
		{
			first - 1, first + 1,
			false,
			true,
		},
		{
			first, first,
			false,
			false,
		},
		{
			first + num/2, first + num/2,
			false,
			false,
		},
		{
			first + num - 1, first + num - 1,
			false,
			false,
		},
		{
			first + num, first + num,
			false,
			false,
		},
		{
			first + num, first + num + 1,
			true,
			false,
		},
		{
			first + num + 1, first + num + 1,
			true,
			false,
		},
	}

	for i, tt := range tests {
		if tt.wpanic {
			require.Panics(t, func() {
				err := l.CheckBound(tt.lo, tt.hi)
				require.NoError(t, err)
			})
		} else {
			require.NotPanics(t, func() {
				err := l.CheckBound(tt.lo, tt.hi)
				if tt.wErrCompacted {
					require.Equal(t, raft.ErrCompacted, err,
						"%d: err = %v, want %v", i, err, raft.ErrCompacted)
				} else {
					require.NoError(t, err, "%d: unexpected err %v", i, err)
				}
			})
		}
	}

	require.NoError(t, stable.logdb.(*ShardedDB).Close())
	removeTestLogdbDir(vfs.GetTestFS())
}

func TestRLLTTerm(t *testing.T) {
	var i uint64
	offset := uint64(100)
	num := uint64(100)

	stable := getTestLogReaderWithoutCache(nil)
	require.NoError(t, stable.ApplySnapshot(pb.Snapshot{Index: offset, Term: 1}))
	l := raft.NewLog(stable)
	for i = 1; i < num; i++ {
		require.NoError(t, l.Append([]pb.Entry{{Index: offset + i, Term: i}}))
	}

	tests := []struct {
		index uint64
		w     uint64
	}{
		{offset - 1, 0},
		{offset, 1},
		{offset + num/2, num / 2},
		{offset + num - 1, num - 1},
		{offset + num, 0},
	}

	for j, tt := range tests {
		term := mustTerm(l.Term(tt.index))
		require.Equal(t, tt.w, term, "#%d: at = %d, want %d", j, term, tt.w)
	}

	require.NoError(t, stable.logdb.(*ShardedDB).Close())
	removeTestLogdbDir(vfs.GetTestFS())
}

// not related to logstable
/*
func TestTermWithUnstableSnapshot(t *testing.T) {
}*/

func TestRLLTSlice(t *testing.T) {
	var i uint64
	offset := uint64(100)
	num := uint64(100)
	last := offset + num
	half := offset + num/2
	halfe := pb.Entry{Index: half, Term: half}
	noLimit := uint64(math.MaxUint64)

	stable := getTestLogReaderWithoutCache(nil)
	require.NoError(t, stable.ApplySnapshot(pb.Snapshot{Index: offset}))
	for i = 1; i < num/2; i++ {
		require.NoError(t, stable.Append([]pb.Entry{{Index: offset + i, Term: offset + i}}))
		ud := pb.Update{
			ShardID:       LogReaderTestShardID,
			ReplicaID:     LogReaderTestReplicaID,
			EntriesToSave: []pb.Entry{{Index: offset + i, Term: offset + i}},
		}
		require.NoError(t, stable.logdb.SaveRaftState([]pb.Update{ud}, 1))
	}
	l := raft.NewLog(stable)
	for i = num / 2; i < num; i++ {
		require.NoError(t, l.Append([]pb.Entry{{Index: offset + i, Term: offset + i}}))
	}

	tests := []struct {
		from  uint64
		to    uint64
		limit uint64

		w      []pb.Entry
		wpanic bool
	}{
		// test no limit
		{offset - 1, offset + 1, noLimit, nil, false},
		{offset, offset + 1, noLimit, nil, false},
		{half - 1, half + 1, noLimit,
			[]pb.Entry{{Index: half - 1, Term: half - 1}, {Index: half, Term: half}}, false},
		{half, half + 1, noLimit, []pb.Entry{{Index: half, Term: half}}, false},
		{last - 1, last, noLimit, []pb.Entry{{Index: last - 1, Term: last - 1}}, false},
		{last, last + 1, noLimit, nil, true},

		// test limit
		{half - 1, half + 1, 0,
			[]pb.Entry{{Index: half - 1, Term: half - 1}}, false},
		{half - 1, half + 1, uint64(halfe.SizeUpperLimit() + 1),
			[]pb.Entry{{Index: half - 1, Term: half - 1}}, false},
		{half - 2, half + 1, uint64(halfe.SizeUpperLimit() + 1),
			[]pb.Entry{{Index: half - 2, Term: half - 2}}, false},
		{half - 1, half + 1, uint64(halfe.SizeUpperLimit() * 2),
			[]pb.Entry{{Index: half - 1, Term: half - 1}, {Index: half, Term: half}}, false},
		{half - 1, half + 2, uint64(halfe.SizeUpperLimit() * 3),
			[]pb.Entry{{Index: half - 1, Term: half - 1}, {Index: half, Term: half},
				{Index: half + 1, Term: half + 1}}, false},
		{half, half + 2, uint64(halfe.SizeUpperLimit()),
			[]pb.Entry{{Index: half, Term: half}}, false},
		{half, half + 2, uint64(halfe.SizeUpperLimit() * 2),
			[]pb.Entry{{Index: half, Term: half}, {Index: half + 1, Term: half + 1}}, false},
	}

	for j, tt := range tests {
		if tt.wpanic {
			require.Panics(t, func() {
				g, err := l.GetEntries(tt.from, tt.to, tt.limit)
				require.NoError(t, err)
				require.Equal(t, tt.w, g,
					"#%d: from %d to %d = %v, want %v", j, tt.from, tt.to, g, tt.w)
			})
		} else {
			require.NotPanics(t, func() {
				g, err := l.GetEntries(tt.from, tt.to, tt.limit)
				if tt.from <= offset {
					require.Equal(t, raft.ErrCompacted, err,
						"#%d: err = %v, want %v", j, err, raft.ErrCompacted)
				} else {
					require.NoError(t, err, "#%d: unexpected error %v", j, err)
				}
				require.Equal(t, tt.w, g,
					"#%d: from %d to %d = %v, want %v", j, tt.from, tt.to, g, tt.w)
			})
		}
	}

	require.NoError(t, stable.logdb.(*ShardedDB).Close())
	removeTestLogdbDir(vfs.GetTestFS())
}

func mustTerm(term uint64, err error) uint64 {
	if err != nil {
		panic(err)
	}
	return term
}
````

## File: internal/logdb/logdb.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

/*
Package logdb implements the persistent log storage used by Dragonboat.

This package is internally used by Dragonboat, applications are not expected
to import this package.
*/
package logdb

import (
	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/logdb/kv"
	"github.com/lni/dragonboat/v4/logger"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

var (
	plog = logger.GetLogger("logdb")
)

// IReusableKey is the interface for keys that can be reused. A reusable key is
// usually obtained by calling the GetKey() function of the IContext
// instance.
type IReusableKey interface {
	SetEntryBatchKey(shardID uint64, replicaID uint64, index uint64)
	// SetEntryKey sets the key to be an entry key for the specified Raft node
	// with the specified entry index.
	SetEntryKey(shardID uint64, replicaID uint64, index uint64)
	// SetStateKey sets the key to be an persistent state key suitable
	// for the specified Raft shard node.
	SetStateKey(shardID uint64, replicaID uint64)
	// SetMaxIndexKey sets the key to be the max possible index key for the
	// specified Raft shard node.
	SetMaxIndexKey(shardID uint64, replicaID uint64)
	// Key returns the underlying byte slice of the key.
	Key() []byte
	// Release releases the key instance so it can be reused in the future.
	Release()
}

// IContext is the per thread context used in the logdb module.
// IContext is expected to contain a list of reusable keys and byte
// slices that are owned per thread so they can be safely reused by the same
// thread when accessing ILogDB.
type IContext interface {
	// Destroy destroys the IContext instance.
	Destroy()
	// Reset resets the IContext instance, all previous returned keys and
	// buffers will be put back to the IContext instance and be ready to
	// be used for the next iteration.
	Reset()
	// GetKey returns a reusable key.
	GetKey() IReusableKey
	// GetValueBuffer returns a byte buffer with at least sz bytes in length.
	GetValueBuffer(sz uint64) []byte
	// GetWriteBatch returns a write batch or transaction instance.
	GetWriteBatch() interface{}
	// SetWriteBatch adds the write batch to the IContext instance.
	SetWriteBatch(wb interface{})
	// GetEntryBatch returns an entry batch instance.
	GetEntryBatch() pb.EntryBatch
	// GetLastEntryBatch returns an entry batch instance.
	GetLastEntryBatch() pb.EntryBatch
}

// DefaultFactory is the default factory for creating LogDB instance.
type DefaultFactory struct {
}

// NewDefaultFactory creates a new DefaultFactory instance.
func NewDefaultFactory() *DefaultFactory {
	return &DefaultFactory{}
}

// Create creates the LogDB instance.
func (f *DefaultFactory) Create(cfg config.NodeHostConfig,
	cb config.LogDBCallback,
	dirs []string, lldirs []string) (raftio.ILogDB, error) {
	return NewDefaultLogDB(cfg, cb, dirs, lldirs)
}

// Name returns the name of the default LogDB instance.
func (f *DefaultFactory) Name() string {
	return "sharded-pebble"
}

// NewDefaultLogDB creates a Log DB instance using the default KV store
// implementation. The created Log DB tries to store entry records in
// plain format but it switches to the batched mode if there is already
// batched entries saved in the existing DB.
func NewDefaultLogDB(config config.NodeHostConfig,
	callback config.LogDBCallback,
	dirs []string, lldirs []string) (raftio.ILogDB, error) {
	return NewLogDB(config,
		callback, dirs, lldirs, false, true, newDefaultKVStore)
}

// NewDefaultBatchedLogDB creates a Log DB instance using the default KV store
// implementation with batched entry support.
func NewDefaultBatchedLogDB(config config.NodeHostConfig,
	callback config.LogDBCallback,
	dirs []string, lldirs []string) (raftio.ILogDB, error) {
	return NewLogDB(config,
		callback, dirs, lldirs, true, false, newDefaultKVStore)
}

// NewLogDB creates a Log DB instance based on provided configuration
// parameters. The underlying KV store used by the Log DB instance is created
// by the provided factory function.
func NewLogDB(config config.NodeHostConfig,
	callback config.LogDBCallback, dirs []string, lldirs []string,
	batched bool, check bool, f kv.Factory) (raftio.ILogDB, error) {
	checkDirs(config.Expert.LogDB.Shards, dirs, lldirs)
	llDirRequired := len(lldirs) == 1
	if len(dirs) == 1 {
		for i := uint64(1); i < config.Expert.LogDB.Shards; i++ {
			dirs = append(dirs, dirs[0])
			if llDirRequired {
				lldirs = append(lldirs, lldirs[0])
			}
		}
	}
	return OpenShardedDB(config, callback, dirs, lldirs, batched, check, f)
}

func checkDirs(numOfShards uint64, dirs []string, lldirs []string) {
	if len(dirs) == 1 {
		if len(lldirs) != 0 && len(lldirs) != 1 {
			plog.Panicf("only 1 regular dir but %d low latency dirs", len(lldirs))
		}
	} else if len(dirs) > 1 {
		if uint64(len(dirs)) != numOfShards {
			plog.Panicf("%d regular dirs, but expect to have %d rdb instances",
				len(dirs), numOfShards)
		}
		if len(lldirs) > 0 {
			if len(dirs) != len(lldirs) {
				plog.Panicf("%v regular dirs, but %v low latency dirs", dirs, lldirs)
			}
		}
	} else {
		panic("no regular dir")
	}
}
````

## File: internal/logdb/logreader_etcd_test.go
````go
// Copyright 2015 The etcd Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"math"
	"reflect"
	"testing"

	"github.com/lni/dragonboat/v4/internal/raft"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/lni/goutils/leaktest"
	"github.com/stretchr/testify/require"
)

// most tests below are ported from etcd rafts

const (
	LogReaderTestShardID   uint64 = 2
	LogReaderTestReplicaID uint64 = 12345
)

func getNewLogReaderTestDB(entries []pb.Entry, fs vfs.IFS) raftio.ILogDB {
	logdb := getNewTestDB("db-dir", "wal-db-dir", false, fs)
	ud := pb.Update{
		EntriesToSave: entries,
		ShardID:       LogReaderTestShardID,
		ReplicaID:     LogReaderTestReplicaID,
	}
	if err := logdb.SaveRaftState([]pb.Update{ud}, 1); err != nil {
		panic(err)
	}
	return logdb
}

func getTestLogReader(entries []pb.Entry, fs vfs.IFS) *LogReader {
	logdb := getNewLogReaderTestDB(entries, fs)
	ls := NewLogReader(LogReaderTestShardID, LogReaderTestReplicaID, logdb)
	ls.SetCompactor(testCompactor)
	ls.markerIndex = entries[0].Index
	ls.markerTerm = entries[0].Term
	ls.length = uint64(len(entries))
	return ls
}

func TestLogReaderEntries(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	testLogReaderEntries(t, fs)
}

func testLogReaderEntries(t *testing.T, fs vfs.IFS) {
	ents := []pb.Entry{
		{Index: 3, Term: 3}, {Index: 4, Term: 4},
		{Index: 5, Term: 5}, {Index: 6, Term: 6},
	}
	tests := []struct {
		lo, hi, maxsize uint64
		werr            error
		wentries        []pb.Entry
	}{
		{2, 6, math.MaxUint64, raft.ErrCompacted, nil},
		{3, 4, math.MaxUint64, raft.ErrCompacted, nil},
		{4, 5, math.MaxUint64, nil, []pb.Entry{{Index: 4, Term: 4}}},
		{4, 6, math.MaxUint64, nil, []pb.Entry{
			{Index: 4, Term: 4}, {Index: 5, Term: 5},
		}},
		{4, 7, math.MaxUint64, nil, []pb.Entry{
			{Index: 4, Term: 4}, {Index: 5, Term: 5}, {Index: 6, Term: 6},
		}},
		// even if maxsize is zero, the first entry should be returned
		{4, 7, 0, nil, []pb.Entry{{Index: 4, Term: 4}}},
		// limit to 2
		{4, 7, uint64(ents[1].SizeUpperLimit() + ents[2].SizeUpperLimit()),
			nil, []pb.Entry{{Index: 4, Term: 4}, {Index: 5, Term: 5}}},
		// limit to 2
		{4, 7, uint64(ents[1].SizeUpperLimit() + ents[2].SizeUpperLimit() +
			ents[3].SizeUpperLimit()/2), nil,
			[]pb.Entry{{Index: 4, Term: 4}, {Index: 5, Term: 5}}},
		{4, 7, uint64(ents[1].SizeUpperLimit() + ents[2].SizeUpperLimit() +
			ents[3].SizeUpperLimit() - 1), nil,
			[]pb.Entry{{Index: 4, Term: 4}, {Index: 5, Term: 5}}},
		// all
		{4, 7, uint64(ents[1].SizeUpperLimit() + ents[2].SizeUpperLimit() +
			ents[3].SizeUpperLimit()), nil,
			[]pb.Entry{{Index: 4, Term: 4}, {Index: 5, Term: 5}, {Index: 6, Term: 6}}},
	}

	for i, tt := range tests {
		s := getTestLogReader(ents, fs)
		entries, err := s.Entries(tt.lo, tt.hi, tt.maxsize)
		require.Equal(t, tt.werr, err, "#%d: err mismatch", i)
		require.Equal(t, tt.wentries, entries, "#%d: entries mismatch", i)
		require.NoError(t, s.logdb.Close())
		deleteTestDB(fs)
	}
}

func TestLogReaderTerm(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	testLogReaderTerm(t, fs)
}

func testLogReaderTerm(t *testing.T, fs vfs.IFS) {
	ents := []pb.Entry{
		{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5},
	}
	tests := []struct {
		i      uint64
		werr   error
		wterm  uint64
		wpanic bool
	}{
		{2, raft.ErrCompacted, 0, false},
		{3, nil, 3, false},
		{4, nil, 4, false},
		{5, nil, 5, false},
		{6, raft.ErrUnavailable, 0, false},
	}
	for i, tt := range tests {
		s := getTestLogReader(ents, fs)
		if tt.wpanic {
			require.Panics(t, func() {
				_, err := s.Term(tt.i)
				require.NoError(t, err)
			}, "#%d: expected panic", i)
		} else {
			require.NotPanics(t, func() {
				term, err := s.Term(tt.i)
				require.Equal(t, tt.werr, err, "#%d: err mismatch", i)
				require.Equal(t, tt.wterm, term, "#%d: term mismatch", i)
			}, "#%d: unexpected panic", i)
		}
		require.NoError(t, s.logdb.Close())
		deleteTestDB(fs)
	}
}

func TestLogReaderLastIndex(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	testLogReaderLastIndex(t, fs)
}

func testLogReaderLastIndex(t *testing.T, fs vfs.IFS) {
	ents := []pb.Entry{
		{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5},
	}
	s := getTestLogReader(ents, fs)
	_, last := s.GetRange()
	require.Equal(t, uint64(5), last, "last index mismatch")
	require.NoError(t, s.Append([]pb.Entry{{Index: 6, Term: 5}}))
	_, last = s.GetRange()
	require.Equal(t, uint64(6), last, "last index after append mismatch")
	require.NoError(t, s.logdb.Close())
	deleteTestDB(fs)
}

func TestLogReaderFirstIndex(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	testLogReaderFirstIndex(t, fs)
}

func testLogReaderFirstIndex(t *testing.T, fs vfs.IFS) {
	ents := []pb.Entry{
		{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5},
	}
	s := getTestLogReader(ents, fs)
	first, _ := s.GetRange()
	require.Equal(t, uint64(4), first, "first index mismatch")
	_, li := s.GetRange()
	require.Equal(t, uint64(5), li, "last index mismatch")
	require.NoError(t, s.Compact(4))
	first, _ = s.GetRange()
	require.Equal(t, uint64(5), first, "first index after compact mismatch")
	_, li = s.GetRange()
	require.Equal(t, uint64(5), li, "last index after compact mismatch")
	require.NoError(t, s.logdb.Close())
	deleteTestDB(fs)
}

func TestLogReaderAppend(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	testLogReaderAppend(t, fs)
}

func testLogReaderAppend(t *testing.T, fs vfs.IFS) {
	ents := []pb.Entry{
		{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5},
	}
	tests := []struct {
		entries  []pb.Entry
		werr     error
		wentries []pb.Entry
	}{
		{
			[]pb.Entry{
				{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5},
			},
			nil,
			[]pb.Entry{
				{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5},
			},
		},
		{
			[]pb.Entry{
				{Index: 3, Term: 3}, {Index: 4, Term: 6}, {Index: 5, Term: 6},
			},
			nil,
			[]pb.Entry{
				{Index: 3, Term: 3}, {Index: 4, Term: 6}, {Index: 5, Term: 6},
			},
		},
		{
			[]pb.Entry{
				{Index: 3, Term: 3}, {Index: 4, Term: 4},
				{Index: 5, Term: 5}, {Index: 6, Term: 5},
			},
			nil,
			[]pb.Entry{
				{Index: 3, Term: 3}, {Index: 4, Term: 4},
				{Index: 5, Term: 5}, {Index: 6, Term: 5},
			},
		},
		// truncate incoming entries, truncate the existing entries and append
		{
			[]pb.Entry{
				{Index: 2, Term: 3}, {Index: 3, Term: 3}, {Index: 4, Term: 5},
			},
			nil,
			[]pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 5}},
		},
		// truncate the existing entries and append
		{
			[]pb.Entry{{Index: 4, Term: 5}},
			nil,
			[]pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 5}},
		},
		// direct append
		{
			[]pb.Entry{{Index: 6, Term: 5}},
			nil,
			[]pb.Entry{
				{Index: 3, Term: 3}, {Index: 4, Term: 4},
				{Index: 5, Term: 5}, {Index: 6, Term: 5},
			},
		},
	}
	for i, tt := range tests {
		s := getTestLogReader(ents, fs)
		require.NoError(t, s.Append(tt.entries))
		// put tt.entries to logdb
		ud := pb.Update{
			EntriesToSave: tt.entries,
			ShardID:       LogReaderTestShardID,
			ReplicaID:     LogReaderTestReplicaID,
		}
		require.NoError(t, s.logdb.SaveRaftState([]pb.Update{ud}, 1))
		bfi := tt.wentries[0].Index - 1
		_, err := s.Term(bfi)
		require.Error(t, err, "expected error for index %d", bfi)
		ali := tt.wentries[len(tt.wentries)-1].Index + 1
		_, err = s.Term(ali)
		require.Error(t, err, "expected error for index %d", ali)
		for ii, e := range tt.wentries {
			if e.Index == 6 {
				plog.Infof("going to check term for index 6")
			}
			term, err := s.Term(e.Index)
			if e.Index == 6 {
				plog.Infof("Term returned")
			}
			require.NoError(t, err, "idx %d, ii %d Term() failed", i, ii)
			require.Equal(t, e.Term, term, "term mismatch for index %d", e.Index)
		}
		require.NoError(t, s.logdb.Close())
		deleteTestDB(fs)
	}
}

func TestLogReaderApplySnapshot(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	ents := []pb.Entry{{Index: 0, Term: 0}}
	cs := &pb.Membership{
		Addresses: map[uint64]string{1: "", 2: "", 3: ""},
	}
	tests := []pb.Snapshot{
		{Index: 4, Term: 4, Membership: *cs},
		{Index: 3, Term: 3, Membership: *cs},
	}
	s := getTestLogReader(ents, fs)
	//Apply Snapshot successful
	i := 0
	tt := tests[i]
	err := s.ApplySnapshot(tt)
	require.NoError(t, err, "#%d: unexpected error", i)
	fi, _ := s.GetRange()
	require.Equal(t, uint64(5), fi, "first index mismatch")
	_, li := s.GetRange()
	require.Equal(t, uint64(4), li, "last index mismatch")
	//Apply Snapshot fails due to ErrSnapOutOfDate
	i = 1
	tt = tests[i]
	err = s.ApplySnapshot(tt)
	require.Equal(t, raft.ErrSnapshotOutOfDate, err, "#%d: error mismatch", i)
	require.NoError(t, s.logdb.Close())
	deleteTestDB(fs)
}

func TestLogReaderCreateSnapshot(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	ents := []pb.Entry{
		{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5},
	}
	cs := &pb.Membership{
		Addresses: map[uint64]string{1: "", 2: "", 3: ""},
	}
	tests := []struct {
		i     uint64
		term  uint64
		werr  error
		wsnap pb.Snapshot
	}{
		{4, 4, nil, pb.Snapshot{Index: 4, Term: 4, Membership: *cs}},
		{5, 5, nil, pb.Snapshot{Index: 5, Term: 5, Membership: *cs}},
	}
	for i, tt := range tests {
		s := getTestLogReader(ents, fs)
		err := s.CreateSnapshot(tt.wsnap)
		require.Equal(t, tt.werr, err, "#%d: error mismatch", i)
		require.Equal(t, tt.wsnap.Index, s.snapshot.Index,
			"#%d: snapshot index mismatch", i)
		require.NoError(t, s.logdb.Close())
		deleteTestDB(fs)
	}
}

func TestLogReaderSetRange(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	ents := []pb.Entry{
		{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5},
	}
	tests := []struct {
		firstIndex     uint64
		length         uint64
		expLength      uint64
		expMarkerIndex uint64
	}{
		{2, 2, 3, 3},
		{2, 5, 4, 3},
		{3, 5, 5, 3},
		{6, 6, 9, 3},
	}
	for idx, tt := range tests {
		s := getTestLogReader(ents, fs)
		s.SetRange(tt.firstIndex, tt.length)
		require.Equal(t, tt.expMarkerIndex, s.markerIndex,
			"%d: marker index mismatch", idx)
		require.Equal(t, tt.expLength, s.length,
			"%d: length mismatch", idx)
		require.NoError(t, s.logdb.Close())
		deleteTestDB(fs)
	}
}

func TestLogReaderGetSnapshot(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	ents := []pb.Entry{
		{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5},
	}
	cs := &pb.Membership{
		Addresses: map[uint64]string{1: "", 2: "", 3: ""},
	}
	ss := pb.Snapshot{Index: 4, Term: 4, Membership: *cs}
	s := getTestLogReader(ents, fs)
	defer deleteTestDB(fs)
	defer func() {
		require.NoError(t, s.logdb.Close())
	}()
	require.NoError(t, s.ApplySnapshot(ss), "create snapshot failed")
	rs := s.Snapshot()
	require.Equal(t, ss.Index, rs.Index, "unexpected snapshot record")
}

func TestLogReaderInitialState(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	ents := []pb.Entry{
		{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5},
	}
	cs := &pb.Membership{
		Addresses: map[uint64]string{1: "", 2: "", 3: ""},
	}
	ss := pb.Snapshot{Index: 4, Term: 4, Membership: *cs}
	s := getTestLogReader(ents, fs)
	defer deleteTestDB(fs)
	defer func() {
		require.NoError(t, s.logdb.Close())
	}()
	require.NoError(t, s.ApplySnapshot(ss), "create snapshot failed")
	ps := pb.State{
		Term:   2,
		Vote:   3,
		Commit: 5,
	}
	s.SetState(ps)
	rps, ms := s.NodeState()
	require.True(t, reflect.DeepEqual(&ms, cs), "membership mismatch")
	require.True(t, reflect.DeepEqual(&rps, &ps), "state mismatch")
}
````

## File: internal/logdb/logreader_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"testing"

	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/stretchr/testify/require"
)

func TestLogReaderNewLogReader(t *testing.T) {
	lr := NewLogReader(1, 1, nil)
	require.Equal(t, uint64(1), lr.length)
}

func TestInitialState(t *testing.T) {
	lr := NewLogReader(1, 1, nil)
	lr.SetCompactor(testCompactor)
	ps := pb.State{
		Term:   100,
		Vote:   112,
		Commit: 123,
	}
	lr.SetState(ps)
	ss := pb.Snapshot{
		Index: 123,
		Term:  124,
		Membership: pb.Membership{
			ConfigChangeId: 1234,
			Addresses:      make(map[uint64]string),
		},
	}
	ss.Membership.Addresses[123] = "address123"
	ss.Membership.Addresses[234] = "address234"
	err := lr.CreateSnapshot(ss)
	require.NoError(t, err)
	rps, m := lr.NodeState()
	require.Equal(t, &ss.Membership, &m)
	require.Equal(t, &rps, &ps)
}

func TestApplySnapshotUpdateMarkerIndexTerm(t *testing.T) {
	lr := NewLogReader(1, 1, nil)
	lr.SetCompactor(testCompactor)
	ss := pb.Snapshot{
		Index: 123,
		Term:  124,
		Membership: pb.Membership{
			ConfigChangeId: 1234,
			Addresses:      make(map[uint64]string),
		},
	}
	ss.Membership.Addresses[123] = "address123"
	ss.Membership.Addresses[234] = "address234"
	err := lr.ApplySnapshot(ss)
	require.NoError(t, err)
	require.Equal(t, uint64(123), lr.markerIndex)
	require.Equal(t, uint64(124), lr.markerTerm)
	require.Equal(t, uint64(1), lr.length)
	require.Equal(t, ss.Index, lr.snapshot.Index)
	rs := lr.Snapshot()
	require.Equal(t, ss.Index, rs.Index)
}

func TestLogReaderIndexRange(t *testing.T) {
	lr := NewLogReader(1, 1, nil)
	lr.SetCompactor(testCompactor)
	ss := pb.Snapshot{
		Index: 123,
		Term:  124,
		Membership: pb.Membership{
			ConfigChangeId: 1234,
			Addresses:      make(map[uint64]string),
		},
	}
	ss.Membership.Addresses[123] = "address123"
	ss.Membership.Addresses[234] = "address234"
	err := lr.ApplySnapshot(ss)
	require.NoError(t, err)
	first := lr.firstIndex()
	require.Equal(t, uint64(124), first)
	last := lr.lastIndex()
	require.Equal(t, uint64(123), last)
	// last < first here, see internal/raft/log.go for details
	fi, li := lr.GetRange()
	require.Equal(t, first, fi)
	require.Equal(t, last, li)
}

func TestSetRange(t *testing.T) {
	tests := []struct {
		marker    uint64
		length    uint64
		index     uint64
		idxLength uint64
		expLength uint64
	}{
		{1, 10, 1, 1, 10},
		{1, 10, 1, 0, 10},
		{10, 10, 8, 10, 8},
		{10, 10, 20, 10, 20},
	}
	for idx, tt := range tests {
		lr := LogReader{
			markerIndex: tt.marker,
			length:      tt.length,
		}
		lr.SetRange(tt.index, tt.idxLength)
		require.Equal(t, tt.expLength, lr.length,
			"test case %d: unexpected length", idx)
	}
}

func TestSetRangePanicWhenThereIsIndexHole(t *testing.T) {
	lr := LogReader{
		markerIndex: 10,
		length:      10,
	}
	require.Panics(t, func() {
		lr.SetRange(100, 100)
	})
}
````

## File: internal/logdb/logreader.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
//
// The implementation of the LogReader struct below is influenced by
// CockroachDB's replicaRaftStorage.
//
// Copyright 2015 The Cockroach Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
// implied. See the License for the specific language governing
// permissions and limitations under the License.

package logdb

import (
	"fmt"
	"sync"
	"unsafe"

	"github.com/lni/goutils/logutil"

	"github.com/lni/dragonboat/v4/internal/raft"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

const (
	maxEntrySliceSize uint64 = 4 * 1024 * 1024
)

var dn = logutil.DescribeNode

// LogReader is the struct used to manage logs that have already been persisted
// into LogDB. This implementation is influenced by CockroachDB's
// replicaRaftStorage.
type LogReader struct {
	sync.Mutex
	logdb       raftio.ILogDB
	compactor   pb.ICompactor
	snapshot    pb.Snapshot
	state       pb.State
	markerIndex uint64
	shardID     uint64
	replicaID   uint64
	markerTerm  uint64
	length      uint64
}

var _ raft.ILogDB = (*LogReader)(nil)

// NewLogReader creates and returns a new LogReader instance.
func NewLogReader(shardID uint64,
	replicaID uint64, logdb raftio.ILogDB) *LogReader {
	l := &LogReader{
		logdb:     logdb,
		shardID:   shardID,
		replicaID: replicaID,
		length:    1,
	}
	return l
}

// SetCompactor sets the compactor or the LogReader instance.
func (lr *LogReader) SetCompactor(c pb.ICompactor) {
	if lr.compactor != nil {
		panic("compactor already set")
	}
	lr.compactor = c
}

func (lr *LogReader) id() string {
	return fmt.Sprintf("logreader %s index %d term %d length %d",
		dn(lr.shardID, lr.replicaID), lr.markerIndex, lr.markerTerm, lr.length)
}

// NodeState returns the initial state.
func (lr *LogReader) NodeState() (pb.State, pb.Membership) {
	lr.Lock()
	defer lr.Unlock()
	return lr.state, lr.snapshot.Membership
}

// Entries returns persisted entries between [low, high) with a total limit of
// up to maxSize bytes.
func (lr *LogReader) Entries(low uint64,
	high uint64, maxSize uint64) ([]pb.Entry, error) {
	ents, size, err := lr.entries(low, high, maxSize)
	if err != nil {
		return nil, err
	}
	if maxSize > 0 && size > maxSize && len(ents) > 1 {
		return ents[:len(ents)-1], nil
	} else if maxSize == 0 && size > maxSize && len(ents) > 1 {
		return ents[:1], nil
	}
	return ents, nil
}

func (lr *LogReader) entries(low uint64,
	high uint64, maxSize uint64) ([]pb.Entry, uint64, error) {
	lr.Lock()
	defer lr.Unlock()
	return lr.entriesLocked(low, high, maxSize)
}

func (lr *LogReader) entriesLocked(low uint64,
	high uint64, maxSize uint64) ([]pb.Entry, uint64, error) {
	if low > high {
		return nil, 0, fmt.Errorf("high (%d) < low (%d)", high, low)
	}
	if low <= lr.markerIndex {
		return nil, 0, raft.ErrCompacted
	}
	if high > lr.lastIndex()+1 {
		plog.Errorf("%s, low %d high %d, lastIndex %d",
			lr.id(), low, high, lr.lastIndex())
		return nil, 0, raft.ErrUnavailable
	}
	// limit the size the ents slice to handle the extreme situation in which
	// high-low can be tens of millions, slice cap is > 50,000 when
	// maxEntrySliceSize is 4MBytes
	maxEntries := maxEntrySliceSize / uint64(unsafe.Sizeof(pb.Entry{}))
	if high-low > maxEntries {
		high = low + maxEntries
		plog.Warningf("%s limited high to %d in logReader.entriesLocked", lr.id(), high)
	}
	ents := make([]pb.Entry, 0, high-low)
	size := uint64(0)
	hitIndex := low
	ents, size, err := lr.logdb.IterateEntries(ents, size, lr.shardID,
		lr.replicaID, hitIndex, high, maxSize)
	if err != nil {
		return nil, 0, err
	}
	if uint64(len(ents)) == high-low || size > maxSize {
		return ents, size, nil
	}
	if len(ents) > 0 {
		if ents[0].Index > low {
			return nil, 0, raft.ErrCompacted
		}
		expected := ents[len(ents)-1].Index + 1
		if lr.lastIndex() <= expected {
			plog.Errorf("%s, %v, low %d high %d, expected %d, lastIndex %d",
				lr.id(), raft.ErrUnavailable, low, high, expected, lr.lastIndex())
			return nil, 0, raft.ErrUnavailable
		}
		return nil, 0, fmt.Errorf("gap found between [%d:%d) at %d",
			low, high, expected)
	}
	plog.Warningf("%s failed to get anything from logreader", lr.id())
	return nil, 0, raft.ErrUnavailable
}

// Term returns the term of the entry specified by the entry index.
func (lr *LogReader) Term(index uint64) (uint64, error) {
	lr.Lock()
	defer lr.Unlock()
	return lr.termLocked(index)
}

func (lr *LogReader) termLocked(index uint64) (uint64, error) {
	if index == lr.markerIndex {
		t := lr.markerTerm
		return t, nil
	}
	ents, _, err := lr.entriesLocked(index, index+1, 0)
	if err != nil {
		return 0, err
	}
	if len(ents) == 0 {
		return 0, nil
	}
	return ents[0].Term, nil
}

// GetRange returns the index range of all logs managed by the LogReader
// instance.
func (lr *LogReader) GetRange() (uint64, uint64) {
	lr.Lock()
	defer lr.Unlock()
	return lr.firstIndex(), lr.lastIndex()
}

func (lr *LogReader) firstIndex() uint64 {
	return lr.markerIndex + 1
}

func (lr *LogReader) lastIndex() uint64 {
	return lr.markerIndex + lr.length - 1
}

// TODO: check where this method is called, double check whether
// Unref() got called as expected

// Snapshot returns the metadata of the lastest snapshot.
func (lr *LogReader) Snapshot() pb.Snapshot {
	lr.Lock()
	defer lr.Unlock()
	ss := lr.snapshot
	if !pb.IsEmptySnapshot(ss) {
		ss.Ref()
	}
	return ss
}

// ApplySnapshot applies the specified snapshot.
func (lr *LogReader) ApplySnapshot(snapshot pb.Snapshot) error {
	lr.Lock()
	defer lr.Unlock()
	if err := lr.setSnapshot(snapshot); err != nil {
		return err
	}
	lr.markerIndex = snapshot.Index
	lr.markerTerm = snapshot.Term
	lr.length = 1
	return nil
}

// CreateSnapshot keeps the metadata of the specified snapshot.
func (lr *LogReader) CreateSnapshot(snapshot pb.Snapshot) error {
	lr.Lock()
	defer lr.Unlock()
	return lr.setSnapshot(snapshot)
}

func (lr *LogReader) setSnapshot(snapshot pb.Snapshot) error {
	if lr.snapshot.Index >= snapshot.Index {
		plog.Debugf("%s called setSnapshot, existing %d, new %d",
			lr.id(), lr.snapshot.Index, snapshot.Index)
		return raft.ErrSnapshotOutOfDate
	}
	snapshot.Load(lr.compactor)
	if !pb.IsEmptySnapshot(lr.snapshot) {
		plog.Debugf("%s unref snapshot %d", lr.id(), lr.snapshot.Index)
		if err := lr.snapshot.Unref(); err != nil {
			return err
		}
	}
	plog.Debugf("%s set snapshot %d", lr.id(), snapshot.Index)
	lr.snapshot = snapshot
	return nil
}

// Append marks the specified entries as persisted and make them available from
// logreader.
func (lr *LogReader) Append(entries []pb.Entry) error {
	if len(entries) == 0 {
		return nil
	}
	if len(entries) > 0 {
		if entries[0].Index+uint64(len(entries))-1 != entries[len(entries)-1].Index {
			panic("gap in entries")
		}
	}
	lr.SetRange(entries[0].Index, uint64(len(entries)))
	return nil
}

// SetRange updates the LogReader to reflect what is available in it.
func (lr *LogReader) SetRange(firstIndex uint64, length uint64) {
	if length == 0 {
		return
	}
	lr.Lock()
	defer lr.Unlock()
	first := lr.firstIndex()
	last := firstIndex + length - 1
	if last < first {
		return
	}
	if first > firstIndex {
		cut := first - firstIndex
		firstIndex = first
		length -= cut
	}
	offset := firstIndex - lr.markerIndex
	switch {
	case lr.length > offset:
		lr.length = offset + length
	case lr.length == offset:
		lr.length += length
	default:
		plog.Panicf("%s gap in log entries, marker %d, len %d, first %d, len %d",
			lr.id(), lr.markerIndex, lr.length, firstIndex, length)
	}
}

// SetState sets the persistent state.
func (lr *LogReader) SetState(s pb.State) {
	lr.Lock()
	defer lr.Unlock()
	lr.state = s
}

// Compact compacts raft log entries up to index.
func (lr *LogReader) Compact(index uint64) error {
	lr.Lock()
	defer lr.Unlock()
	if index < lr.markerIndex {
		return raft.ErrCompacted
	}
	if index > lr.lastIndex() {
		return raft.ErrUnavailable
	}
	term, err := lr.termLocked(index)
	if err != nil {
		return err
	}
	i := index - lr.markerIndex
	lr.length -= i
	lr.markerIndex = index
	lr.markerTerm = term
	return nil
}
````

## File: internal/logdb/plain.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"github.com/lni/dragonboat/v4/internal/logdb/kv"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

type plainEntries struct {
	cs   *cache
	keys *keyPool
	kvs  kv.IKVStore
}

var _ entryManager = (*plainEntries)(nil)

func newPlainEntries(cs *cache, keys *keyPool, kvs kv.IKVStore) entryManager {
	return &plainEntries{
		cs:   cs,
		keys: keys,
		kvs:  kvs,
	}
}

func (pe *plainEntries) record(wb kv.IWriteBatch,
	shardID uint64, replicaID uint64, ctx IContext, entries []pb.Entry) uint64 {
	idx := 0
	maxIndex := uint64(0)
	for idx < len(entries) {
		ent := entries[idx]
		esz := uint64(ent.SizeUpperLimit())
		data := ctx.GetValueBuffer(esz)
		if uint64(len(data)) < esz {
			panic("got a small buffer")
		}
		data = pb.MustMarshalTo(&ent, data)
		k := ctx.GetKey()
		k.SetEntryKey(shardID, replicaID, ent.Index)
		wb.Put(k.Key(), data)
		if ent.Index > maxIndex {
			maxIndex = ent.Index
		}
		idx++
	}
	return maxIndex
}

func (pe *plainEntries) iterate(ents []pb.Entry, maxIndex uint64,
	size uint64, shardID uint64, replicaID uint64,
	low uint64, high uint64, maxSize uint64) ([]pb.Entry, uint64, error) {
	if low+1 == high && low <= maxIndex {
		e, err := pe.getEntry(shardID, replicaID, low)
		if err != nil {
			return nil, 0, err
		}
		ents = append(ents, e)
		size += uint64(e.SizeUpperLimit())
		return ents, size, nil
	}
	if high > maxIndex+1 {
		high = maxIndex + 1
	}
	fk := pe.keys.get()
	lk := pe.keys.get()
	defer fk.Release()
	defer lk.Release()
	fk.SetEntryKey(shardID, replicaID, low)
	lk.SetEntryKey(shardID, replicaID, high)
	expectedIndex := low
	op := func(key []byte, data []byte) (bool, error) {
		var e pb.Entry
		pb.MustUnmarshal(&e, data)
		if e.Index != expectedIndex {
			return false, nil
		}
		size += uint64(e.SizeUpperLimit())
		ents = append(ents, e)
		expectedIndex++
		if size > maxSize {
			return false, nil
		}
		return true, nil
	}
	if err := pe.kvs.IterateValue(fk.Key(), lk.Key(), false, op); err != nil {
		return nil, 0, err
	}
	return ents, size, nil
}

func (pe *plainEntries) getEntry(shardID uint64,
	replicaID uint64, index uint64) (pb.Entry, error) {
	k := pe.keys.get()
	defer k.Release()
	k.SetEntryKey(shardID, replicaID, index)
	var e pb.Entry
	op := func(data []byte) error {
		pb.MustUnmarshal(&e, data)
		return nil
	}
	if err := pe.kvs.GetValue(k.Key(), op); err != nil {
		return pb.Entry{}, err
	}
	return e, nil
}

func (pe *plainEntries) getRange(shardID uint64,
	replicaID uint64, snapshotIndex uint64, maxIndex uint64) (uint64, uint64, error) {
	fk := pe.keys.get()
	lk := pe.keys.get()
	defer fk.Release()
	defer lk.Release()
	fk.SetEntryKey(shardID, replicaID, snapshotIndex)
	lk.SetEntryKey(shardID, replicaID, maxIndex)
	firstIndex := uint64(0)
	length := uint64(0)
	op := func(key []byte, data []byte) (bool, error) {
		if firstIndex == 0 {
			var e pb.Entry
			pb.MustUnmarshal(&e, data)
			firstIndex = e.Index
			return false, nil
		}
		return true, nil
	}
	if err := pe.kvs.IterateValue(fk.Key(), lk.Key(), true, op); err != nil {
		return 0, 0, err
	}
	if firstIndex == 0 && maxIndex != 0 {
		plog.Panicf("first index %d, max index %d", firstIndex, maxIndex)
	}
	if firstIndex > 0 {
		length = maxIndex - firstIndex + 1
	}
	return firstIndex, length, nil
}

func (pe *plainEntries) rangedOp(shardID uint64,
	replicaID uint64, index uint64, op func(fk *Key, lk *Key) error) error {
	fk := pe.keys.get()
	lk := pe.keys.get()
	defer fk.Release()
	defer lk.Release()
	fk.SetEntryKey(shardID, replicaID, 0)
	lk.SetEntryKey(shardID, replicaID, index)
	return op(fk, lk)
}

func (pe *plainEntries) binaryFormat() uint32 {
	return raftio.PlainLogDBBinVersion
}
````

## File: internal/logdb/sharded.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logdb

import (
	"fmt"
	"math"
	"sync/atomic"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/syncutil"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/logdb/kv"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/utils"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

// ShardedDB is a LogDB implementation using sharded ILogDB instances.
type ShardedDB struct {
	partitioner          server.IPartitioner
	compactions          *compactions
	stopper              *syncutil.Stopper
	compactionCh         chan struct{}
	ctxs                 []IContext
	shards               []*db
	config               config.LogDBConfig
	completedCompactions uint64
}

var _ raftio.ILogDB = (*ShardedDB)(nil)

var firstError = utils.FirstError

type shardCallback struct {
	f     config.LogDBCallback
	shard uint64
}

func (sc *shardCallback) callback(busy bool) {
	if sc.f != nil {
		sc.f(config.LogDBInfo{Shard: sc.shard, Busy: busy})
	}
}

// OpenShardedDB creates a ShardedDB instance.
func OpenShardedDB(config config.NodeHostConfig, cb config.LogDBCallback,
	dirs []string, lldirs []string, batched bool, check bool,
	kvf kv.Factory) (*ShardedDB, error) {
	fs := config.Expert.FS
	if config.Expert.LogDB.IsEmpty() {
		panic("config.Expert.LogDB.IsEmpty()")
	}
	if check && batched {
		plog.Panicf("check and batched both set")
	}
	shards := make([]*db, 0)
	closeAll := func(all []*db) {
		var err error
		for _, s := range all {
			err = firstError(err, s.close())
		}
		if err != nil {
			plog.Panicf("%+v", err)
			panic("not suppose to reach here")
		}
	}
	for i := uint64(0); i < config.Expert.LogDB.Shards; i++ {
		dir := fs.PathJoin(dirs[i], fmt.Sprintf("logdb-%d", i))
		lldir := ""
		if len(lldirs) > 0 {
			lldir = fs.PathJoin(lldirs[i], fmt.Sprintf("logdb-%d", i))
		}
		sc := shardCallback{shard: i, f: cb}
		db, err := openRDB(config.Expert.LogDB,
			sc.callback, dir, lldir, batched, fs, kvf)
		if err != nil {
			closeAll(shards)
			return nil, errors.WithStack(err)
		}
		shards = append(shards, db)
	}
	if check && !batched {
		for _, s := range shards {
			located, err := hasEntryRecord(s.kvs, true)
			if err != nil {
				closeAll(shards)
				return nil, errors.WithStack(err)
			}
			if located {
				closeAll(shards)
				return OpenShardedDB(config, cb, dirs, lldirs, true, false, kvf)
			}
		}
	}
	if batched {
		plog.Infof("using batched logdb")
	} else {
		plog.Infof("using plain logdb")
	}
	partitioner := server.NewDoubleFixedPartitioner(config.Expert.Engine.ExecShards,
		config.Expert.LogDB.Shards)
	mw := &ShardedDB{
		config:       config.Expert.LogDB,
		shards:       shards,
		ctxs:         make([]IContext, config.Expert.Engine.ExecShards),
		partitioner:  partitioner,
		compactions:  newCompactions(),
		compactionCh: make(chan struct{}, 1),
		stopper:      syncutil.NewStopper(),
	}
	for i := uint64(0); i < config.Expert.Engine.ExecShards; i++ {
		mw.ctxs[i] = newContext(mw.config.SaveBufferSize, mw.config.MaxSaveBufferSize)
	}
	mw.stopper.RunWorker(func() {
		mw.compactionWorkerMain()
	})
	return mw, nil
}

// Name returns the type name of the instance.
func (s *ShardedDB) Name() string {
	return fmt.Sprintf("sharded-%s", s.shards[0].name())
}

// BinaryFormat is the binary format supported by the sharded DB.
func (s *ShardedDB) BinaryFormat() uint32 {
	return s.shards[0].binaryFormat()
}

// SelfCheckFailed runs a self check on all db shards and report whether any
// failure is observed.
func (s *ShardedDB) SelfCheckFailed() (bool, error) {
	for _, shard := range s.shards {
		failed, err := shard.selfCheckFailed()
		if err != nil {
			return false, errors.WithStack(err)
		}
		if failed {
			return true, nil
		}
	}
	return false, nil
}

// SaveRaftState saves the raft state and logs found in the raft.Update list
// to the log db.
func (s *ShardedDB) SaveRaftState(updates []pb.Update, shardID uint64) error {
	if shardID-1 >= uint64(len(s.ctxs)) {
		plog.Panicf("invalid shardID %d, len(s.ctxs): %d", shardID, len(s.ctxs))
	}
	ctx := s.ctxs[shardID-1]
	ctx.Reset()
	return errors.WithStack(s.SaveRaftStateCtx(updates, ctx))
}

// GetLogDBThreadContext return an IContext instance. This method is expected
// to be used in benchmarks and tests only.
func (s *ShardedDB) GetLogDBThreadContext() IContext {
	return newContext(s.config.SaveBufferSize, s.config.MaxSaveBufferSize)
}

// SaveRaftStateCtx saves the raft state and logs found in the raft.Update list
// to the log db.
func (s *ShardedDB) SaveRaftStateCtx(updates []pb.Update, ctx IContext) error {
	if len(updates) == 0 {
		return nil
	}
	p := s.getParititionID(updates)
	return errors.WithStack(s.shards[p].saveRaftState(updates, ctx))
}

// ReadRaftState returns the persistent state of the specified raft node.
func (s *ShardedDB) ReadRaftState(shardID uint64,
	replicaID uint64, lastIndex uint64) (raftio.RaftState, error) {
	p := s.partitioner.GetPartitionID(shardID)
	rs, err := s.shards[p].readRaftState(shardID, replicaID, lastIndex)
	return rs, errors.WithStack(err)
}

// ListNodeInfo lists all available NodeInfo found in the log db.
func (s *ShardedDB) ListNodeInfo() ([]raftio.NodeInfo, error) {
	r := make([]raftio.NodeInfo, 0)
	for _, v := range s.shards {
		n, err := v.listNodeInfo()
		if err != nil {
			return nil, errors.WithStack(err)
		}
		r = append(r, n...)
	}
	return r, nil
}

// SaveSnapshots saves all snapshot metadata found in the raft.Update list.
func (s *ShardedDB) SaveSnapshots(updates []pb.Update) error {
	if len(updates) == 0 {
		return nil
	}
	p := s.getParititionID(updates)
	return errors.WithStack(s.shards[p].saveSnapshots(updates))
}

// GetSnapshot returns the most recent snapshot associated with the specified
// shard.
func (s *ShardedDB) GetSnapshot(shardID uint64,
	replicaID uint64) (pb.Snapshot, error) {
	p := s.partitioner.GetPartitionID(shardID)
	ss, err := s.shards[p].getSnapshot(shardID, replicaID)
	return ss, errors.WithStack(err)
}

// SaveBootstrapInfo saves the specified bootstrap info for the given node.
func (s *ShardedDB) SaveBootstrapInfo(shardID uint64,
	replicaID uint64, bootstrap pb.Bootstrap) error {
	p := s.partitioner.GetPartitionID(shardID)
	err := s.shards[p].saveBootstrapInfo(shardID, replicaID, bootstrap)
	return errors.WithStack(err)
}

// GetBootstrapInfo returns the saved bootstrap info for the given node.
func (s *ShardedDB) GetBootstrapInfo(shardID uint64,
	replicaID uint64) (pb.Bootstrap, error) {
	p := s.partitioner.GetPartitionID(shardID)
	bs, err := s.shards[p].getBootstrapInfo(shardID, replicaID)
	return bs, errors.WithStack(err)
}

// IterateEntries returns a list of saved entries starting with index low up to
// index high with a max size of maxSize.
func (s *ShardedDB) IterateEntries(ents []pb.Entry,
	size uint64, shardID uint64, replicaID uint64, low uint64, high uint64,
	maxSize uint64) ([]pb.Entry, uint64, error) {
	p := s.partitioner.GetPartitionID(shardID)
	entries, sz, err := s.shards[p].iterateEntries(ents,
		size, shardID, replicaID, low, high, maxSize)
	return entries, sz, errors.WithStack(err)
}

// RemoveEntriesTo removes entries associated with the specified raft node up
// to the specified index.
func (s *ShardedDB) RemoveEntriesTo(shardID uint64,
	replicaID uint64, index uint64) error {
	p := s.partitioner.GetPartitionID(shardID)
	if err := s.shards[p].removeEntriesTo(shardID, replicaID, index); err != nil {
		return errors.WithStack(err)
	}
	return nil
}

// CompactEntriesTo reclaims underlying storage space used for storing
// entries up to the specified index.
func (s *ShardedDB) CompactEntriesTo(shardID uint64,
	replicaID uint64, index uint64) (<-chan struct{}, error) {
	done := s.addCompaction(shardID, replicaID, index)
	return done, nil
}

// RemoveNodeData deletes all node data that belongs to the specified node.
func (s *ShardedDB) RemoveNodeData(shardID uint64, replicaID uint64) error {
	p := s.partitioner.GetPartitionID(shardID)
	return errors.WithStack(s.shards[p].removeNodeData(shardID, replicaID))
}

// ImportSnapshot imports the snapshot record and other metadata records to the
// system.
func (s *ShardedDB) ImportSnapshot(ss pb.Snapshot, replicaID uint64) error {
	p := s.partitioner.GetPartitionID(ss.ShardID)
	return errors.WithStack(s.shards[p].importSnapshot(ss, replicaID))
}

// Close closes the ShardedDB instance.
func (s *ShardedDB) Close() (err error) {
	s.stopper.Stop()
	for _, v := range s.shards {
		err = firstError(err, v.close())
	}
	for _, v := range s.ctxs {
		v.Destroy()
	}
	return err
}

func (s *ShardedDB) getParititionID(updates []pb.Update) uint64 {
	pid := uint64(math.MaxUint64)
	for _, ud := range updates {
		id := s.partitioner.GetPartitionID(ud.ShardID)
		if pid == math.MaxUint64 {
			pid = id
		} else if pid != id {
			plog.Panicf("multiple pid value found")
		}
	}
	if pid == uint64(math.MaxUint64) {
		plog.Panicf("invalid partition id")
	}
	return pid
}

func (s *ShardedDB) compactionWorkerMain() {
	for {
		select {
		case <-s.stopper.ShouldStop():
			return
		case <-s.compactionCh:
			if err := s.compact(); err != nil {
				panicNow(err)
			}
		}
		select {
		case <-s.stopper.ShouldStop():
			return
		default:
		}
	}
}

func (s *ShardedDB) addCompaction(shardID uint64,
	replicaID uint64, index uint64) chan struct{} {
	task := task{
		shardID:   shardID,
		replicaID: replicaID,
		index:     index,
	}
	done := s.compactions.addTask(task)
	select {
	case s.compactionCh <- struct{}{}:
	default:
	}
	return done
}

func (s *ShardedDB) compact() error {
	for {
		if t, hasTask := s.compactions.getTask(); hasTask {
			idx := s.partitioner.GetPartitionID(t.shardID)
			shard := s.shards[idx]
			if err := shard.compact(t.shardID, t.replicaID, t.index); err != nil {
				return err
			}
			atomic.AddUint64(&s.completedCompactions, 1)
			close(t.done)
			plog.Infof("%s completed LogDB compaction up to index %d",
				dn(t.shardID, t.replicaID), t.index)
			select {
			case <-s.stopper.ShouldStop():
				return nil
			default:
			}
		} else {
			return nil
		}
	}
}

func panicNow(err error) {
	plog.Panicf("%+v", err)
	panic(err)
}
````

## File: internal/raft/entryutils_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raft

import (
	"testing"

	pb "github.com/lni/dragonboat/v4/raftpb"
)

func TestLimitSizeOnEmptyEntryList(t *testing.T) {
	ents := []pb.Entry{}
	limitSize(ents, 0)
}

func TestCheckEntriesToAppend(t *testing.T) {
	checkEntriesToAppend(nil, nil)
	checkEntriesToAppend([]pb.Entry{}, []pb.Entry{})
	checkEntriesToAppend(nil, []pb.Entry{{Index: 101}})
	checkEntriesToAppend([]pb.Entry{{Index: 100}},
		[]pb.Entry{{Index: 101}})
	checkEntriesToAppend([]pb.Entry{{Index: 100, Term: 90}},
		[]pb.Entry{{Index: 101, Term: 100}})
}

func TestCheckEntriesToAppendWillPanicWhenIndexHasHole(t *testing.T) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("not panic")
	}()
	checkEntriesToAppend([]pb.Entry{{Index: 100}}, []pb.Entry{{Index: 102}})
}

func TestCheckEntriesToAppendWillPanicWhenTermMovesBack(t *testing.T) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("not panic")
	}()
	checkEntriesToAppend([]pb.Entry{{Index: 100, Term: 100}},
		[]pb.Entry{{Index: 101, Term: 99}})
}
````

## File: internal/raft/entryutils.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raft

import (
	pb "github.com/lni/dragonboat/v4/raftpb"
)

func countConfigChange(entries []pb.Entry) int {
	c := 0
	for i := range entries {
		if entries[i].Type == pb.ConfigChangeEntry {
			c++
		}
	}
	return c
}

func newEntrySlice(ents []pb.Entry) []pb.Entry {
	var n []pb.Entry
	return append(n, ents...)
}

func checkEntriesToAppend(ents []pb.Entry, toAppend []pb.Entry) {
	if len(ents) == 0 || len(toAppend) == 0 {
		return
	}
	if ents[len(ents)-1].Index+1 != toAppend[0].Index {
		plog.Panicf("found a hole, last %d, first to append %d",
			ents[len(ents)-1].Index, toAppend[0].Index)
	}
	if ents[len(ents)-1].Term > toAppend[0].Term {
		plog.Panicf("term value not expected, %d vs %d",
			ents[len(ents)-1].Term, toAppend[0].Term)
	}
}

func limitSize(ents []pb.Entry, limit uint64) []pb.Entry {
	if len(ents) == 0 {
		return ents
	}
	total := ents[0].SizeUpperLimit()
	var inc int
	for inc = 1; inc < len(ents); inc++ {
		total += ents[inc].SizeUpperLimit()
		if uint64(total) > limit {
			break
		}
	}
	return ents[:inc]
}

func min(x uint64, y uint64) uint64 {
	if x > y {
		return y
	}
	return x
}

func max(x uint64, y uint64) uint64 {
	if x > y {
		return x
	}
	return y
}

func getEntrySliceInMemSize(ents []pb.Entry) uint64 {
	return pb.GetEntrySliceInMemSize(ents)
}

func getEntrySliceSize(ents []pb.Entry) uint64 {
	return pb.GetEntrySliceSize(ents)
}

// IsLocalMessageType returns a boolean value indicating whether the specified
// message type is a local message type.
func IsLocalMessageType(t pb.MessageType) bool {
	return isLocalMessageType(t)
}

func isLocalMessageType(t pb.MessageType) bool {
	return t == pb.Election ||
		t == pb.LeaderHeartbeat ||
		t == pb.Unreachable ||
		t == pb.SnapshotStatus ||
		t == pb.CheckQuorum ||
		t == pb.LocalTick ||
		t == pb.BatchedReadIndex
}

func isResponseMessageType(t pb.MessageType) bool {
	return t == pb.ReplicateResp ||
		t == pb.RequestVoteResp ||
		t == pb.HeartbeatResp ||
		t == pb.ReadIndexResp ||
		t == pb.Unreachable ||
		t == pb.SnapshotStatus ||
		t == pb.LeaderTransfer
}
````

## File: internal/raft/inmemory_etcd_test.go
````go
// Copyright 2015 The etcd Authors
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//
// inmemory_etcd_test.go is ported from etcd raft for testing purposes.
// some new tests are added by dragonboat authors
//
// tests in inmemory_etcd_test.go have been updated to reflect that inmemory.go
// is used to manage log entries that are likely to be used in immediate future,
// not just those have not been persisted yet.
//

package raft

import (
	"reflect"
	"testing"

	pb "github.com/lni/dragonboat/v4/raftpb"
)

func TestUnstableMaybeFirstIndex(t *testing.T) {
	tests := []struct {
		entries []pb.Entry
		offset  uint64
		snap    *pb.Snapshot

		wok    bool
		windex uint64
	}{
		// no snapshot
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, nil,
			false, 0,
		},
		{
			[]pb.Entry{}, 0, nil,
			false, 0,
		},
		// has snapshot
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, &pb.Snapshot{Index: 4, Term: 1},
			true, 5,
		},
		{
			[]pb.Entry{}, 5, &pb.Snapshot{Index: 4, Term: 1},
			true, 5,
		},
	}

	for i, tt := range tests {
		u := inMemory{
			entries:     tt.entries,
			markerIndex: tt.offset,
			snapshot:    tt.snap,
		}
		index, ok := u.getSnapshotIndex()
		if ok != tt.wok {
			t.Errorf("#%d: ok = %t, want %t", i, ok, tt.wok)
		}
		if ok {
			if index+1 != tt.windex {
				t.Errorf("#%d: index = %d, want %d", i, index, tt.windex)
			}
		}
	}
}

func TestMaybeLastIndex(t *testing.T) {
	tests := []struct {
		entries []pb.Entry
		offset  uint64
		snap    *pb.Snapshot

		wok    bool
		windex uint64
	}{
		// last in entries
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, nil,
			true, 5,
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, &pb.Snapshot{Index: 4, Term: 1},
			true, 5,
		},
		// last in snapshot
		{
			[]pb.Entry{}, 5, &pb.Snapshot{Index: 4, Term: 1},
			true, 4,
		},
		// empty inMemory
		{
			[]pb.Entry{}, 0, nil,
			false, 0,
		},
	}

	for i, tt := range tests {
		u := inMemory{
			entries:     tt.entries,
			markerIndex: tt.offset,
			snapshot:    tt.snap,
		}
		index, ok := u.getLastIndex()
		if ok != tt.wok {
			t.Errorf("#%d: ok = %t, want %t", i, ok, tt.wok)
		}
		if index != tt.windex {
			t.Errorf("#%d: index = %d, want %d", i, index, tt.windex)
		}
	}
}

func TestUnstableMaybeTerm(t *testing.T) {
	tests := []struct {
		entries []pb.Entry
		offset  uint64
		snap    *pb.Snapshot
		index   uint64

		wok   bool
		wterm uint64
	}{
		// term from entries
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, nil,
			5,
			true, 1,
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, nil,
			6,
			false, 0,
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, nil,
			4,
			false, 0,
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, &pb.Snapshot{Index: 4, Term: 1},
			5,
			true, 1,
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, &pb.Snapshot{Index: 4, Term: 1},
			6,
			false, 0,
		},
		// term from snapshot
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, &pb.Snapshot{Index: 4, Term: 1},
			4,
			true, 1,
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, &pb.Snapshot{Index: 4, Term: 1},
			3,
			false, 0,
		},
		{
			[]pb.Entry{}, 5, &pb.Snapshot{Index: 4, Term: 1},
			5,
			false, 0,
		},
		{
			[]pb.Entry{}, 5, &pb.Snapshot{Index: 4, Term: 1},
			4,
			true, 1,
		},
		{
			[]pb.Entry{}, 0, nil,
			5,
			false, 0,
		},
	}

	for i, tt := range tests {
		u := inMemory{
			entries:     tt.entries,
			markerIndex: tt.offset,
			snapshot:    tt.snap,
		}
		term, ok := u.getTerm(tt.index)
		if ok != tt.wok {
			t.Errorf("#%d: ok = %t, want %t", i, ok, tt.wok)
		}
		if term != tt.wterm {
			t.Errorf("#%d: term = %d, want %d", i, term, tt.wterm)
		}
	}
}

func TestUnstableRestore(t *testing.T) {
	u := inMemory{
		entries:     []pb.Entry{{Index: 5, Term: 1}},
		markerIndex: 5,
		snapshot:    &pb.Snapshot{Index: 4, Term: 1},
	}
	s := pb.Snapshot{Index: 6, Term: 2}
	u.restore(s)

	if u.markerIndex != s.Index+1 {
		t.Errorf("offset = %d, want %d", u.markerIndex, s.Index+1)
	}
	if len(u.entries) != 0 {
		t.Errorf("len = %d, want 0", len(u.entries))
	}
	if !reflect.DeepEqual(u.snapshot, &s) {
		t.Errorf("snap = %v, want %v", u.snapshot, &s)
	}
}

func TestUnstableTruncateAndAppend(t *testing.T) {
	tests := []struct {
		entries  []pb.Entry
		offset   uint64
		snap     *pb.Snapshot
		toappend []pb.Entry

		woffset  uint64
		wentries []pb.Entry
	}{
		// append to the end
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, nil,
			[]pb.Entry{{Index: 6, Term: 1}, {Index: 7, Term: 1}},
			5, []pb.Entry{{Index: 5, Term: 1}, {Index: 6, Term: 1}, {Index: 7, Term: 1}},
		},
		// replace the inMemory entries
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, nil,
			[]pb.Entry{{Index: 5, Term: 2}, {Index: 6, Term: 2}},
			5, []pb.Entry{{Index: 5, Term: 2}, {Index: 6, Term: 2}},
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, nil,
			[]pb.Entry{{Index: 4, Term: 2}, {Index: 5, Term: 2}, {Index: 6, Term: 2}},
			4, []pb.Entry{{Index: 4, Term: 2}, {Index: 5, Term: 2}, {Index: 6, Term: 2}},
		},
		// truncate the existing entries and append
		{
			[]pb.Entry{{Index: 5, Term: 1}, {Index: 6, Term: 1}, {Index: 7, Term: 1}}, 5, nil,
			[]pb.Entry{{Index: 6, Term: 2}},
			5, []pb.Entry{{Index: 5, Term: 1}, {Index: 6, Term: 2}},
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}, {Index: 6, Term: 1}, {Index: 7, Term: 1}}, 5, nil,
			[]pb.Entry{{Index: 7, Term: 2}, {Index: 8, Term: 2}},
			5, []pb.Entry{{Index: 5, Term: 1}, {Index: 6, Term: 1}, {Index: 7, Term: 2}, {Index: 8, Term: 2}},
		},
	}

	for i, tt := range tests {
		u := inMemory{
			entries:     tt.entries,
			markerIndex: tt.offset,
			snapshot:    tt.snap,
		}
		u.merge(tt.toappend)
		if u.markerIndex != tt.woffset {
			t.Errorf("#%d: offset = %d, want %d", i, u.markerIndex, tt.woffset)
		}
		if !reflect.DeepEqual(u.entries, tt.wentries) {
			t.Errorf("#%d: entries = %v, want %v", i, u.entries, tt.wentries)
		}
	}
}

// added by dragonboat authors
func TestEntryMergeThreadSafety(t *testing.T) {
	tests := []struct {
		entries  []pb.Entry
		marker   uint64
		merge    []pb.Entry
		expIndex uint64
		expTerm  uint64
	}{
		{
			[]pb.Entry{{Index: 5, Term: 1}, {Index: 6, Term: 1}, {Index: 7, Term: 1}},
			5,
			[]pb.Entry{{Index: 7, Term: 2}, {Index: 7, Term: 2}},
			7, 1,
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}, {Index: 6, Term: 1}, {Index: 7, Term: 1}},
			5,
			[]pb.Entry{{Index: 4, Term: 2}, {Index: 5, Term: 2}},
			5, 1,
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}, {Index: 6, Term: 1}, {Index: 7, Term: 1}},
			5,
			[]pb.Entry{{Index: 5, Term: 2}, {Index: 6, Term: 2}},
			5, 1,
		},
	}

	for idx, tt := range tests {
		im := inMemory{
			entries:     tt.entries,
			markerIndex: tt.marker,
		}
		old := im.entries[0:]
		im.merge(tt.merge)
		for _, e := range old {
			if e.Index == tt.expIndex {
				if e.Term != tt.expTerm {
					t.Errorf("%d, entry term unexpectedly changed", idx)
				}
			}
		}
	}
}

func TestUnstableStableTo(t *testing.T) {
	tests := []struct {
		entries     []pb.Entry
		offset      uint64
		snap        *pb.Snapshot
		index, term uint64
		savedTo     uint64
		woffset     uint64
		wlen        int
	}{
		{
			[]pb.Entry{}, 0, nil,
			5, 1,
			0,
			0, 0,
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, nil,
			5, 1, // stable to the first entry
			5,
			6, 0,
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}, {Index: 6, Term: 1}}, 5, nil,
			5, 1, // stable to the first entry
			5,
			6, 1,
		},
		{
			[]pb.Entry{{Index: 6, Term: 2}}, 6, nil,
			6, 1, // stable to the first entry and term mismatch
			0,
			7, 0,
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, nil,
			4, 1, // stable to old entry
			0,
			5, 1,
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, nil,
			4, 2, // stable to old entry
			0,
			5, 1,
		},
		// with snapshot
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, &pb.Snapshot{Index: 4, Term: 1},
			5, 1, // stable to the first entry
			5,
			6, 0,
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}, {Index: 6, Term: 1}}, 5, &pb.Snapshot{Index: 4, Term: 1},
			5, 1, // stable to the first entry
			5,
			6, 1,
		},
		{
			[]pb.Entry{{Index: 6, Term: 2}}, 6, &pb.Snapshot{Index: 5, Term: 1},
			6, 1, // stable to the first entry and term mismatch
			0,
			7, 0,
		},
		{
			[]pb.Entry{{Index: 5, Term: 1}}, 5, &pb.Snapshot{Index: 4, Term: 1},
			4, 1, // stable to snapshot
			0,
			5, 1,
		},
		{
			[]pb.Entry{{Index: 5, Term: 2}}, 5, &pb.Snapshot{Index: 4, Term: 2},
			4, 1, // stable to old entry
			0,
			5, 1,
		},
	}

	for i, tt := range tests {
		u := inMemory{
			entries:     tt.entries,
			markerIndex: tt.offset,
			snapshot:    tt.snap,
		}
		u.savedLogTo(tt.index, tt.term)
		u.appliedLogTo(tt.index)
		if u.savedTo != tt.savedTo {
			t.Errorf("#%d: savedTo = %d, want %d", i, u.savedTo, tt.savedTo)
		}
		if u.markerIndex != tt.woffset {
			t.Errorf("#%d: offset = %d, want %d", i, u.markerIndex, tt.woffset)
		}
		if len(u.entries) != tt.wlen {
			t.Errorf("#%d: len = %d, want %d", i, len(u.entries), tt.wlen)
		}
	}
}
````

## File: internal/raft/inmemory_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raft

import (
	"math"
	"testing"

	"github.com/lni/dragonboat/v4/internal/server"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

func TestInMemCheckMarkerIndex(t *testing.T) {
	im := inMemory{markerIndex: 10}
	im.checkMarkerIndex()
	im = inMemory{
		entries: []pb.Entry{
			{Index: 1, Term: 2},
			{Index: 2, Term: 2},
		},
		markerIndex: 1,
	}
	im.checkMarkerIndex()
}

func TestInMemCheckMarkerIndexPanicOnInvalidInMem(t *testing.T) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not triggered")
	}()
	im := inMemory{
		entries: []pb.Entry{
			{Index: 1, Term: 2},
			{Index: 2, Term: 2},
		},
		markerIndex: 2,
	}
	im.checkMarkerIndex()
}

func TestInMemGetSnapshotIndex(t *testing.T) {
	im := inMemory{}
	if idx, ok := im.getSnapshotIndex(); ok || idx != 0 {
		t.Errorf("invalid result %t, %d", ok, idx)
	}
	im = inMemory{
		snapshot: &pb.Snapshot{
			Index: 100,
		},
	}
	if idx, ok := im.getSnapshotIndex(); !ok || idx != 100 {
		t.Errorf("invalid result %t, %d", ok, idx)
	}
}

func testGetEntriesPanicWithInvalidInput(low uint64,
	high uint64, marker uint64, firstIndex uint64, length uint64, t *testing.T) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not triggered")
	}()
	im := &inMemory{
		markerIndex: marker,
		entries:     make([]pb.Entry, 0),
	}
	for i := firstIndex; i < firstIndex+length; i++ {
		im.entries = append(im.entries, pb.Entry{Index: i, Term: 1})
	}
	im.getEntries(low, high)
}

func TestInMemGetEntriesPanicWithInvalidInput(t *testing.T) {
	tests := []struct {
		low        uint64
		high       uint64
		marker     uint64
		firstIndex uint64
		length     uint64
	}{
		{10, 9, 10, 10, 10},  // low > high
		{10, 11, 11, 10, 10}, // low < markerIndex
		{10, 11, 5, 5, 5},    // high > upperBound
	}
	for _, tt := range tests {
		testGetEntriesPanicWithInvalidInput(tt.low,
			tt.high, tt.marker, tt.firstIndex, tt.length, t)
	}
}

func TestInMemGetEntries(t *testing.T) {
	im := &inMemory{
		markerIndex: 2,
		entries: []pb.Entry{
			{Index: 2, Term: 2},
			{Index: 3, Term: 2},
		},
	}
	ents := im.getEntries(2, 3)
	if len(ents) != 1 {
		t.Errorf("ents len %d", len(ents))
	}
	ents = im.getEntries(2, 4)
	if len(ents) != 2 {
		t.Errorf("ents len %d", len(ents))
	}
}

func TestInMemGetLastIndexReturnSnapshotIndexWithEmptyEntries(t *testing.T) {
	for _, idx := range []uint64{100, 200, 205} {
		im := inMemory{
			snapshot: &pb.Snapshot{
				Index: idx,
			},
		}
		if index, ok := im.getLastIndex(); !ok || index != idx {
			t.Errorf("index %d, want %d, ok %t", index, idx, ok)
		}
	}
	in := inMemory{}
	index, ok := in.getLastIndex()
	if ok || index != 0 {
		t.Errorf("unexpected last index")
	}
}

func TestInMemGetLastIndex(t *testing.T) {
	tests := []struct {
		first  uint64
		length uint64
	}{
		{100, 5},
		{1, 100},
	}
	for idx, tt := range tests {
		im := inMemory{
			entries: make([]pb.Entry, 0),
		}
		for i := tt.first; i < tt.first+tt.length; i++ {
			im.entries = append(im.entries, pb.Entry{Index: i, Term: 1})
		}
		index, ok := im.getLastIndex()
		if !ok || index != tt.first+tt.length-1 {
			t.Errorf("%d, ok %t, index %d, want %d", idx, ok, index, tt.first+tt.length-1)
		}
	}
}

func TestInMemGetTermReturnSnapshotTerm(t *testing.T) {
	tests := []struct {
		markerIndex uint64
		ssIndex     uint64
		ssTerm      uint64
		index       uint64
		term        uint64
		ok          bool
	}{
		{10, 0, 0, 5, 0, false},
		{10, 5, 2, 5, 2, true},
		{10, 5, 2, 4, 0, false},
		{10, 5, 2, 10, 0, false},
	}
	for idx, tt := range tests {
		im := inMemory{
			markerIndex: tt.markerIndex,
			snapshot: &pb.Snapshot{
				Index: tt.ssIndex,
				Term:  tt.ssTerm,
			},
		}
		if im.snapshot.Index == 0 {
			im.snapshot = nil
		}
		r, ok := im.getTerm(tt.index)
		if r != tt.term {
			t.Errorf("%d, term %d, want %d", idx, r, tt.term)
		}
		if ok != tt.ok {
			t.Errorf("unexpected result")
		}
	}
}

func TestInMemGetTerm(t *testing.T) {
	tests := []struct {
		first  uint64
		length uint64
		index  uint64
		term   uint64
		ok     bool
	}{
		{100, 5, 103, 103, true},
		{100, 5, 104, 104, true},
		{100, 5, 105, 0, false},
	}
	for idx, tt := range tests {
		im := inMemory{
			markerIndex: tt.first,
			entries:     make([]pb.Entry, 0),
		}
		for i := tt.first; i < tt.first+tt.length; i++ {
			im.entries = append(im.entries, pb.Entry{Index: i, Term: i})
		}
		r, ok := im.getTerm(tt.index)
		if r != tt.term || ok != tt.ok {
			t.Errorf("%d, term %d, want %d, ok %t, want %t", idx, r, tt.term, ok, tt.ok)
		}
	}
}

func TestInMemRestore(t *testing.T) {
	im := inMemory{
		markerIndex: 10,
		entries: []pb.Entry{
			{Index: 10, Term: 1},
			{Index: 11, Term: 1},
		},
	}
	ss := pb.Snapshot{Index: 100}
	im.shrunk = true
	im.restore(ss)
	if im.shrunk {
		t.Errorf("shrunk flag not cleared")
	}
	if len(im.entries) != 0 || im.markerIndex != 101 || im.snapshot == nil {
		t.Errorf("unexpected im state")
	}
}

func TestInMemSaveSnapshotTo(t *testing.T) {
	im := inMemory{}
	im.savedSnapshotTo(10)
	im = inMemory{
		snapshot: &pb.Snapshot{Index: 100},
	}
	im.savedSnapshotTo(10)
	if im.snapshot == nil {
		t.Errorf("snapshot unexpected unset")
	}
	im.savedSnapshotTo(100)
	if im.snapshot != nil {
		t.Errorf("snapshot not unset")
	}
}

func testInMemMergeFullAppend(t *testing.T, shrunk bool) {
	im := inMemory{
		markerIndex: 5,
	}
	im.resize()
	im.entries = append(im.entries, []pb.Entry{
		{Index: 5, Term: 5},
		{Index: 6, Term: 6},
		{Index: 7, Term: 7},
	}...)
	im.shrunk = shrunk
	ents := []pb.Entry{
		{Index: 8, Term: 8},
		{Index: 9, Term: 9},
	}
	im.merge(ents)
	if im.shrunk != shrunk {
		t.Errorf("shrunk flag unexpectedly changed, %t:%t", im.shrunk, shrunk)
	}
	if len(im.entries) != 5 || im.markerIndex != 5 {
		t.Errorf("not fully appended")
	}
	if idx, ok := im.getLastIndex(); !ok || idx != 9 {
		t.Errorf("last index %d, want 9", idx)
	}
}

func TestInMemMergeFullAppend(t *testing.T) {
	testInMemMergeFullAppend(t, false)
	testInMemMergeFullAppend(t, true)
}

func TestInMemMergeReplace(t *testing.T) {
	im := inMemory{
		markerIndex: 5,
	}
	im.resize()
	im.entries = append(im.entries, []pb.Entry{
		{Index: 5, Term: 5},
		{Index: 6, Term: 6},
		{Index: 7, Term: 7},
	}...)
	im.shrunk = true
	ents := []pb.Entry{
		{Index: 2, Term: 2},
		{Index: 3, Term: 3},
	}
	im.merge(ents)
	if im.shrunk {
		t.Errorf("shrunk flag unexpectedly not cleared")
	}
	if len(im.entries) != 2 || im.markerIndex != 2 {
		t.Errorf("not fully appended")
	}
	if idx, ok := im.getLastIndex(); !ok || idx != 3 {
		t.Errorf("last index %d, want 3", idx)
	}
}

func TestInMemMergeWithHoleCausePanic(t *testing.T) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not triggered")
	}()
	im := inMemory{
		markerIndex: 5,
		entries: []pb.Entry{
			{Index: 5, Term: 5},
			{Index: 6, Term: 6},
			{Index: 7, Term: 7},
		},
	}
	ents := []pb.Entry{
		{Index: 9, Term: 9},
		{Index: 10, Term: 10},
	}
	im.merge(ents)
}

func TestInMemMerge(t *testing.T) {
	im := inMemory{
		markerIndex: 5,
	}
	im.resize()
	im.entries = append(im.entries, []pb.Entry{
		{Index: 5, Term: 5},
		{Index: 6, Term: 6},
		{Index: 7, Term: 7},
	}...)
	im.shrunk = true
	ents := []pb.Entry{
		{Index: 6, Term: 7},
		{Index: 7, Term: 10},
	}
	im.merge(ents)
	if im.shrunk {
		t.Errorf("shrunk flag unexpectedly not cleared")
	}
	if len(im.entries) != 3 || im.markerIndex != 5 {
		t.Errorf("not fully appended")
	}
	if idx, ok := im.getLastIndex(); !ok || idx != 7 {
		t.Errorf("last index %d, want 3", idx)
	}
	if term, ok := im.getTerm(6); !ok || term != 7 {
		t.Errorf("unexpected term %d, want 7", term)
	}
	if term, ok := im.getTerm(7); !ok || term != 10 {
		t.Errorf("unexpected term %d, want 10", term)
	}
}

func TestInMemEntriesToSaveReturnNotSavedEntries(t *testing.T) {
	im := inMemory{
		markerIndex: 5,
		entries: []pb.Entry{
			{Index: 5, Term: 5},
			{Index: 6, Term: 6},
			{Index: 7, Term: 7},
		},
	}
	im.savedTo = 4
	ents := im.entriesToSave()
	if len(ents) != 3 {
		t.Errorf("didn't return all entries")
	}
	if ents[0].Index != 5 {
		t.Errorf("unexpected first entry")
	}
	im.savedTo = 5
	ents = im.entriesToSave()
	if len(ents) != 2 {
		t.Errorf("didn't return all entries")
	}
	im.savedTo = 7
	ents = im.entriesToSave()
	if len(ents) != 0 {
		t.Errorf("unexpected entries returned")
	}
	im.savedTo = 8
	ents = im.entriesToSave()
	if len(ents) != 0 {
		t.Errorf("unexpected entries returned")
	}
}

func TestInMemSaveLogToUpdatesSaveTo(t *testing.T) {
	tests := []struct {
		index   uint64
		term    uint64
		savedTo uint64
	}{
		{4, 1, 4},
		{8, 1, 4},
		{6, 7, 4},
		{6, 6, 6},
	}
	for idx, tt := range tests {
		im := inMemory{
			markerIndex: 5,
			entries: []pb.Entry{
				{Index: 5, Term: 5},
				{Index: 6, Term: 6},
				{Index: 7, Term: 7},
			},
			savedTo: 4,
		}
		im.savedLogTo(tt.index, tt.term)
		if im.savedTo != tt.savedTo {
			t.Errorf("%d, savedTo %d, want %d", idx, im.savedTo, tt.savedTo)
		}
	}
}

func TestInMemSetSaveToWhenRestoringSnapshot(t *testing.T) {
	ss := pb.Snapshot{Index: 100, Term: 10}
	im := inMemory{
		markerIndex: 5,
		entries: []pb.Entry{
			{Index: 5, Term: 5},
		},
		savedTo: 4,
	}
	im.restore(ss)
	if im.savedTo != ss.Index {
		t.Errorf("savedTo %d, want %d", im.savedTo, ss.Index)
	}
}

func TestInMemMergeSetSaveTo(t *testing.T) {
	im := inMemory{
		markerIndex: 6,
		entries: []pb.Entry{
			{Index: 6, Term: 6},
			{Index: 7, Term: 7},
		},
		savedTo: 5,
	}
	ents := []pb.Entry{{Index: 6, Term: 6}, {Index: 7, Term: 8}}
	im.merge(ents)
	if im.savedTo != 5 {
		t.Errorf("savedTo %d, want 5", im.savedTo)
	}
	im = inMemory{
		markerIndex: 5,
		entries: []pb.Entry{
			{Index: 5, Term: 5},
			{Index: 6, Term: 6},
			{Index: 7, Term: 7},
			{Index: 8, Term: 8},
			{Index: 9, Term: 9},
			{Index: 10, Term: 10},
		},
		savedTo: 4,
	}
	im.merge(ents)
	if im.savedTo != 4 {
		t.Errorf("savedTo %d, want 4", im.savedTo)
	}
	im = inMemory{
		markerIndex: 5,
		entries: []pb.Entry{
			{Index: 5, Term: 5},
			{Index: 6, Term: 6},
			{Index: 7, Term: 7},
			{Index: 8, Term: 8},
			{Index: 9, Term: 9},
			{Index: 10, Term: 10},
		},
		savedTo: 6,
	}
	im.merge(ents)
	if im.savedTo != 5 {
		t.Errorf("savedTo %d, want 5", im.savedTo)
	}
	im = inMemory{
		markerIndex: 6,
		entries: []pb.Entry{
			{Index: 6, Term: 6},
			{Index: 7, Term: 7},
		},
		savedTo: 5,
	}
	ents = []pb.Entry{{Index: 8, Term: 8}, {Index: 9, Term: 9}}
	im.merge(ents)
	if im.savedTo != 5 {
		t.Errorf("savedTo %d, want 5", im.savedTo)
	}
}

func TestAppliedLogTo(t *testing.T) {
	tests := []struct {
		appliedTo  uint64
		length     int
		firstIndex uint64
	}{
		{4, 6, 5},
		{5, 5, 6},
		{11, 6, 5},
		{6, 4, 7},
		{10, 0, 11},
	}
	for idx, tt := range tests {
		im := inMemory{
			markerIndex: 5,
			entries: []pb.Entry{
				{Index: 5, Term: 5},
				{Index: 6, Term: 6},
				{Index: 7, Term: 7},
				{Index: 8, Term: 8},
				{Index: 9, Term: 9},
				{Index: 10, Term: 10},
			},
			savedTo: 4,
		}
		im.appliedLogTo(tt.appliedTo)
		if len(im.entries) != tt.length {
			t.Errorf("%d, unexpected entry slice len %d, want %d",
				idx, len(im.entries), tt.length)
		}
		if len(im.entries) > 0 && im.entries[0].Index != tt.firstIndex {
			t.Errorf("%d, unexpected first index %d, want %d",
				idx, im.entries[0].Index, tt.firstIndex)
		}
	}
}

func TestRateLimited(t *testing.T) {
	tests := []struct {
		rl      *server.InMemRateLimiter
		limited bool
	}{
		{nil, false},
		{server.NewInMemRateLimiter(0), false},
		{server.NewInMemRateLimiter(math.MaxUint64), false},
		{server.NewInMemRateLimiter(1), true},
		{server.NewInMemRateLimiter(math.MaxUint64 - 1), true},
	}
	for idx, tt := range tests {
		im := newInMemory(0, tt.rl)
		if im.rateLimited() != tt.limited {
			t.Errorf("%d, rate limited %t, want %t", idx, im.rateLimited(), tt.limited)
		}
	}
}

func TestRateLimitClearedAfterRestoringSnapshot(t *testing.T) {
	im := newInMemory(0, server.NewInMemRateLimiter(10000))
	im.merge([]pb.Entry{{Cmd: make([]byte, 1024)}})
	if im.rl.Get() == 0 {
		t.Errorf("log size not updated")
	}
	im.restore(pb.Snapshot{})
	if im.rl.Get() != 0 {
		t.Errorf("log size not cleared")
	}
}

func TestRateLimitIsUpdatedAfterMergingEntries(t *testing.T) {
	im := newInMemory(0, server.NewInMemRateLimiter(10000))
	im.merge([]pb.Entry{{Index: 1, Cmd: make([]byte, 1024)}})
	logsz := im.rl.Get()
	ents := []pb.Entry{
		{Index: 2, Cmd: make([]byte, 16)},
		{Index: 3, Cmd: make([]byte, 64)},
	}
	addSz := getEntrySliceInMemSize(ents)
	im.merge(ents)
	if logsz+addSz != im.rl.Get() {
		t.Errorf("log size %d, want %d", im.rl.Get(), logsz+addSz)
	}
}

func TestRateLimitIsDecreasedAfterEntriesAreApplied(t *testing.T) {
	ents := []pb.Entry{
		{Index: 2, Cmd: make([]byte, 16)},
		{Index: 3, Cmd: make([]byte, 64)},
		{Index: 4, Cmd: make([]byte, 128)},
	}
	im := newInMemory(2, server.NewInMemRateLimiter(10000))
	im.merge(ents)
	if im.rl.Get() != getEntrySliceInMemSize(ents) {
		t.Errorf("unexpected log size")
	}
	for idx := uint64(2); idx < uint64(5); idx++ {
		im.appliedLogTo(idx)
		if len(im.entries) > 0 {
			if im.entries[0].Index != idx+1 {
				t.Errorf("alignment error")
			}
		}
		if im.rl.Get() != getEntrySliceInMemSize(im.entries) {
			t.Errorf("log size not updated")
		}
	}
}

func TestRateLimitCanBeResetWhenMergingEntries(t *testing.T) {
	ents := []pb.Entry{
		{Index: 2, Cmd: make([]byte, 16)},
		{Index: 3, Cmd: make([]byte, 64)},
		{Index: 4, Cmd: make([]byte, 128)},
	}
	im := newInMemory(2, server.NewInMemRateLimiter(10000))
	im.merge(ents)
	ents = []pb.Entry{
		{Index: 1, Cmd: make([]byte, 16)},
	}
	im.merge(ents)
	expSz := getEntrySliceInMemSize(ents)
	if im.rl.Get() != expSz {
		t.Errorf("log size %d, want %d", im.rl.Get(), expSz)
	}
}

func TestRateLimitCanBeUpdatedAfterCutAndMergingEntries(t *testing.T) {
	ents := []pb.Entry{
		{Index: 2, Cmd: make([]byte, 16)},
		{Index: 3, Cmd: make([]byte, 64)},
		{Index: 4, Cmd: make([]byte, 128)},
	}
	im := newInMemory(2, server.NewInMemRateLimiter(10000))
	im.merge(ents)
	ents = []pb.Entry{
		{Index: 3, Cmd: make([]byte, 1024)},
		{Index: 4, Cmd: make([]byte, 1024)},
	}
	im.merge(ents)
	expSz := getEntrySliceInMemSize(im.entries)
	if im.rl.Get() != expSz {
		t.Errorf("log size %d, want %d", im.rl.Get(), expSz)
	}
}

func TestResize(t *testing.T) {
	im := inMemory{
		markerIndex: 10,
		entries: []pb.Entry{
			{Index: 10, Term: 1},
			{Index: 11, Term: 1},
		},
		shrunk: true,
	}
	im.resize()
	if uint64(cap(im.entries)) != entrySliceSize {
		t.Errorf("not resized")
	}
	if len(im.entries) != 2 {
		t.Errorf("unexpected len %d", len(im.entries))
	}
	if im.shrunk {
		t.Errorf("shrunk flag not clearaed")
	}
}

func TestTryResize(t *testing.T) {
	im := inMemory{
		markerIndex: 10,
		entries: []pb.Entry{
			{Index: 10, Term: 1},
			{Index: 11, Term: 1},
		},
	}
	initcap := cap(im.entries)
	initlen := len(im.entries)
	im.tryResize()
	if cap(im.entries) != initcap || len(im.entries) != initlen {
		t.Errorf("cap/len unexpectedly changed")
	}
	im.shrunk = true
	im.tryResize()
	if cap(im.entries) == initcap {
		t.Errorf("cap/len unexpectedly not changed")
	}
}

func TestNewEntrySlice(t *testing.T) {
	tests := []struct {
		input uint64
		oCap  uint64
		oLen  uint64
	}{
		{entrySliceSize, entrySliceSize, entrySliceSize},
		{entrySliceSize - 1, entrySliceSize, entrySliceSize - 1},
		{entrySliceSize + 1, entrySliceSize + 1, entrySliceSize + 1},
	}
	for idx, tt := range tests {
		ents := make([]pb.Entry, tt.input)
		im := inMemory{}
		output := im.newEntrySlice(ents)
		if uint64(cap(output)) != tt.oCap {
			t.Errorf("%d, unexpected cap %d, want %d", idx, cap(output), tt.oCap)
		}
		if uint64(len(output)) != tt.oLen {
			t.Errorf("%d, unexpected len %d, want %d", idx, len(output), tt.oLen)
		}
	}
}
````

## File: internal/raft/inmemory.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raft

import (
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/settings"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

var (
	entrySliceSize    = settings.Soft.InMemEntrySliceSize
	minEntrySliceSize = settings.Soft.MinEntrySliceFreeSize
)

// inMemory is a two stage in memory log storage struct to keep log entries
// that will be used by the raft protocol in immediate future.
type inMemory struct {
	snapshot       *pb.Snapshot
	rl             *server.InMemRateLimiter
	entries        []pb.Entry
	savedTo        uint64
	markerIndex    uint64
	appliedToIndex uint64
	appliedToTerm  uint64
	shrunk         bool
}

func newInMemory(lastIndex uint64, rl *server.InMemRateLimiter) inMemory {
	if minEntrySliceSize >= entrySliceSize {
		panic("minEntrySliceSize >= entrySliceSize")
	}
	return inMemory{
		markerIndex: lastIndex + 1,
		savedTo:     lastIndex,
		rl:          rl,
	}
}

func (im *inMemory) checkMarkerIndex() {
	if len(im.entries) > 0 {
		if im.entries[0].Index != im.markerIndex {
			plog.Panicf("marker index %d, first index %d",
				im.markerIndex, im.entries[0].Index)
		}
	}
}

func (im *inMemory) getEntries(low uint64, high uint64) []pb.Entry {
	upperBound := im.markerIndex + uint64(len(im.entries))
	if low > high || low < im.markerIndex {
		plog.Panicf("invalid low value %d, high %d, marker index %d",
			low, high, im.markerIndex)
	}
	if high > upperBound {
		plog.Panicf("invalid high value %d, upperBound %d", high, upperBound)
	}
	return im.entries[low-im.markerIndex : high-im.markerIndex]
}

func (im *inMemory) getSnapshotIndex() (uint64, bool) {
	if im.snapshot != nil {
		return im.snapshot.Index, true
	}
	return 0, false
}

func (im *inMemory) getLastIndex() (uint64, bool) {
	if len(im.entries) > 0 {
		return im.entries[len(im.entries)-1].Index, true
	}
	return im.getSnapshotIndex()
}

func (im *inMemory) getTerm(index uint64) (uint64, bool) {
	if index > 0 && index == im.appliedToIndex {
		if im.appliedToTerm == 0 {
			plog.Panicf("im.appliedToTerm == 0, index %d", index)
		}
		return im.appliedToTerm, true
	}
	if index < im.markerIndex {
		if idx, ok := im.getSnapshotIndex(); ok && idx == index {
			return im.snapshot.Term, true
		}
		return 0, false
	}
	lastIndex, ok := im.getLastIndex()
	if ok && index <= lastIndex {
		return im.entries[index-im.markerIndex].Term, true
	}
	return 0, false
}

func (im *inMemory) commitUpdate(cu pb.UpdateCommit) {
	if cu.StableLogTo > 0 {
		im.savedLogTo(cu.StableLogTo, cu.StableLogTerm)
	}
	if cu.StableSnapshotTo > 0 {
		im.savedSnapshotTo(cu.StableSnapshotTo)
	}
}

func (im *inMemory) entriesToSave() []pb.Entry {
	idx := im.savedTo + 1
	if idx-im.markerIndex > uint64(len(im.entries)) {
		return []pb.Entry{}
	}
	return im.entries[idx-im.markerIndex:]
}

func (im *inMemory) savedLogTo(index uint64, term uint64) {
	if index < im.markerIndex {
		return
	}
	if len(im.entries) == 0 {
		return
	}
	if index > im.entries[len(im.entries)-1].Index ||
		term != im.entries[index-im.markerIndex].Term {
		return
	}
	im.savedTo = index
}

func (im *inMemory) appliedLogTo(index uint64) {
	if index < im.markerIndex {
		return
	}
	if len(im.entries) == 0 {
		return
	}
	if index > im.entries[len(im.entries)-1].Index {
		return
	}
	lastEntry := im.entries[index-im.markerIndex]
	if lastEntry.Index != index {
		panic("lastEntry.Index != index")
	}
	im.appliedToIndex = lastEntry.Index
	im.appliedToTerm = lastEntry.Term
	newMarkerIndex := index + 1
	applied := im.entries[:newMarkerIndex-im.markerIndex]
	im.shrunk = true
	im.entries = im.entries[newMarkerIndex-im.markerIndex:]
	im.markerIndex = newMarkerIndex
	im.resizeEntrySlice()
	im.checkMarkerIndex()
	if im.rateLimited() {
		im.rl.Decrease(getEntrySliceInMemSize(applied))
	}
}

func (im *inMemory) savedSnapshotTo(index uint64) {
	if idx, ok := im.getSnapshotIndex(); ok && idx == index {
		im.snapshot = nil
	} else if ok && idx != index {
		plog.Warningf("snapshot index does not match")
	}
}

func (im *inMemory) resize() {
	im.shrunk = false
	im.entries = im.newEntrySlice(im.entries)
}

func (im *inMemory) tryResize() {
	if im.shrunk {
		im.resize()
	}
}

func (im *inMemory) resizeEntrySlice() {
	toResize := cap(im.entries)-len(im.entries) < int(minEntrySliceSize)
	if im.shrunk && (len(im.entries) <= 1 || toResize) {
		im.resize()
	}
}

func (im *inMemory) newEntrySlice(ents []pb.Entry) []pb.Entry {
	sz := max(entrySliceSize, uint64(len(ents)))
	newEntries := make([]pb.Entry, 0, sz)
	newEntries = append(newEntries, ents...)
	return newEntries
}

func (im *inMemory) merge(ents []pb.Entry) {
	firstNewIndex := ents[0].Index
	im.resizeEntrySlice()
	if firstNewIndex == im.markerIndex+uint64(len(im.entries)) {
		checkEntriesToAppend(im.entries, ents)
		im.entries = append(im.entries, ents...)
		if im.rateLimited() {
			im.rl.Increase(getEntrySliceInMemSize(ents))
		}
	} else if firstNewIndex <= im.markerIndex {
		im.markerIndex = firstNewIndex
		// ents might come from entryQueue, copy it to its own storage
		im.shrunk = false
		im.entries = im.newEntrySlice(ents)
		im.savedTo = firstNewIndex - 1
		if im.rateLimited() {
			im.rl.Set(getEntrySliceInMemSize(ents))
		}
	} else {
		existing := im.getEntries(im.markerIndex, firstNewIndex)
		checkEntriesToAppend(existing, ents)
		im.shrunk = false
		im.entries = im.newEntrySlice(existing)
		im.entries = append(im.entries, ents...)
		im.savedTo = min(im.savedTo, firstNewIndex-1)
		if im.rateLimited() {
			sz := getEntrySliceInMemSize(ents) + getEntrySliceInMemSize(existing)
			im.rl.Set(sz)
		}
	}
	im.checkMarkerIndex()
}

func (im *inMemory) restore(ss pb.Snapshot) {
	im.snapshot = &ss
	im.markerIndex = ss.Index + 1
	im.appliedToIndex = ss.Index
	im.appliedToTerm = ss.Term
	im.shrunk = false
	im.entries = nil
	im.savedTo = ss.Index
	if im.rateLimited() {
		im.rl.Set(0)
	}
}

func (im *inMemory) rateLimited() bool {
	return im.rl != nil && im.rl.Enabled()
}
````

## File: internal/raft/logdb_etcd_test.go
````go
// Copyright 2015 The etcd Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//
// logdb_etcd_test.go is ported from etcd raft, it is used to test the
// TestLogDB struct in logdb_test.go - testing your tests is important!
// updates have been made to reflect the interface & implementation differences
//

package raft

import (
	"reflect"
	"testing"

	pb "github.com/lni/dragonboat/v4/raftpb"
)

func TestLogDBTerm(t *testing.T) {
	ents := []pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5}}
	tests := []struct {
		i      uint64
		werr   error
		wterm  uint64
		wpanic bool
	}{
		{2, ErrCompacted, 0, false},
		{3, nil, 3, false},
		{4, nil, 4, false},
		{5, nil, 5, false},
		{6, ErrUnavailable, 0, false},
	}

	for i, tt := range tests {
		v := make([]pb.Entry, len(ents))
		copy(v, ents)
		s := &TestLogDB{
			markerIndex: v[0].Index,
			markerTerm:  v[0].Term,
			entries:     v[1:],
		}
		func() {
			defer func() {
				if r := recover(); r != nil {
					if !tt.wpanic {
						t.Errorf("#%d: panic = %v, want %v", i, true, tt.wpanic)
					}
				}
			}()
			plog.Infof("checking term for index %d", tt.i)
			term, err := s.Term(tt.i)
			if err != tt.werr {
				t.Errorf("#%d: err = %v, want %v", i, err, tt.werr)
			}
			if term != tt.wterm {
				t.Errorf("#%d: term = %d, want %d", i, term, tt.wterm)
			}
		}()
	}
}

func TestLogDBLastIndex(t *testing.T) {
	ents := []pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5}}
	s := &TestLogDB{
		markerIndex: ents[0].Index,
		markerTerm:  ents[0].Term,
		entries:     ents[1:],
	}

	_, last := s.GetRange()
	if last != 5 {
		t.Errorf("term = %d, want %d", last, 5)
	}

	plog.Infof("going to append")
	if err := s.Append([]pb.Entry{{Index: 6, Term: 5}}); err != nil {
		t.Fatalf("%v", err)
	}
	_, last = s.GetRange()
	if last != 6 {
		t.Errorf("last = %d, want %d", last, 6)
	}
}

func TestLogDBFirstIndex(t *testing.T) {
	ents := []pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5}}
	s := &TestLogDB{
		markerIndex: ents[0].Index,
		markerTerm:  ents[0].Term,
		entries:     ents[1:],
	}

	first, _ := s.GetRange()
	if first != 4 {
		t.Errorf("first = %d, want %d", first, 4)
	}

	if err := s.Compact(4); err != nil {
		t.Fatalf("%v", err)
	}
	first, _ = s.GetRange()
	if first != 5 {
		t.Errorf("first = %d, want %d", first, 5)
	}
}

func TestLogDBCompact(t *testing.T) {
	ents := []pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5}}
	tests := []struct {
		i uint64

		werr   error
		windex uint64
		wterm  uint64
		wlen   int
	}{
		{2, ErrCompacted, 3, 3, 3},
		{3, ErrCompacted, 3, 3, 3},
		{4, nil, 4, 4, 2},
		{5, nil, 5, 5, 1},
	}
	for i, tt := range tests {
		v := make([]pb.Entry, len(ents))
		copy(v, ents)
		s := &TestLogDB{
			markerIndex: v[0].Index,
			markerTerm:  v[0].Term,
			entries:     v[1:],
		}
		err := s.Compact(tt.i)
		if err != tt.werr {
			t.Errorf("#%d: err = %v, want %v", i, err, tt.werr)
		}
		if s.markerIndex != tt.windex {
			t.Errorf("#%d: index = %d, want %d", i, s.markerIndex, tt.windex)
		}
		if s.markerTerm != tt.wterm {
			t.Errorf("#%d: term = %d, want %d", i, s.markerTerm, tt.wterm)
		}
		if len(s.entries)+1 != tt.wlen {
			t.Errorf("#%d: len = %d, want %d", i, len(s.entries), tt.wlen)
		}
	}
}

func TestLogDBCreateSnapshot(t *testing.T) {
	ents := []pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5}}
	m := getTestMembership([]uint64{1, 2, 3})
	cs := &m

	tests := []struct {
		i uint64

		werr  error
		wsnap pb.Snapshot
	}{
		{4, nil, pb.Snapshot{Index: 4, Term: 4, Membership: *cs}},
		{5, nil, pb.Snapshot{Index: 5, Term: 5, Membership: *cs}},
	}

	for i, tt := range tests {
		v := make([]pb.Entry, len(ents))
		copy(v, ents)
		s := &TestLogDB{
			markerIndex: v[0].Index,
			markerTerm:  v[0].Term,
			entries:     v[1:],
		}
		snap, err := s.getSnapshot(tt.i, cs)
		if cerr := s.CreateSnapshot(snap); cerr != nil {
			t.Fatalf("%v", err)
		}
		if err != tt.werr {
			t.Errorf("#%d: err = %v, want %v", i, err, tt.werr)
		}
		if !reflect.DeepEqual(snap, tt.wsnap) {
			t.Errorf("#%d: snap = %+v, want %+v", i, snap, tt.wsnap)
		}
	}
}

func TestLogDBApplySnapshot(t *testing.T) {
	m := getTestMembership([]uint64{1, 2, 3})
	cs := &m

	tests := []pb.Snapshot{
		{Index: 4, Term: 4, Membership: *cs},
		{Index: 3, Term: 3, Membership: *cs},
	}

	s := &TestLogDB{}
	//Apply Snapshot successful
	i := 0
	tt := tests[i]
	err := s.ApplySnapshot(tt)
	if err != nil {
		t.Errorf("#%d: err = %v, want %v", i, err, nil)
	}

	//Apply Snapshot fails due to ErrSnapOutOfDate
	i = 1
	tt = tests[i]
	err = s.ApplySnapshot(tt)
	if err != ErrSnapshotOutOfDate {
		t.Errorf("#%d: err = %v, want %v", i, err, ErrSnapshotOutOfDate)
	}
}

func TestLogDBAppend(t *testing.T) {
	ents := []pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5}}
	tests := []struct {
		entries []pb.Entry

		werr     error
		wentries []pb.Entry
	}{
		{
			[]pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5}},
			nil,
			[]pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5}},
		},
		{
			[]pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 6}, {Index: 5, Term: 6}},
			nil,
			[]pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 6}, {Index: 5, Term: 6}},
		},
		{
			[]pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5}, {Index: 6, Term: 5}},
			nil,
			[]pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5}, {Index: 6, Term: 5}},
		},
		// truncate incoming entries, truncate the existing entries and append
		{
			[]pb.Entry{{Index: 2, Term: 3}, {Index: 3, Term: 3}, {Index: 4, Term: 5}},
			nil,
			[]pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 5}},
		},
		// truncate the existing entries and append
		{
			[]pb.Entry{{Index: 4, Term: 5}},
			nil,
			[]pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 5}},
		},
		// direct append
		{
			[]pb.Entry{{Index: 6, Term: 5}},
			nil,
			[]pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 5}, {Index: 6, Term: 5}},
		},
	}

	for i, tt := range tests {
		v := make([]pb.Entry, len(ents))
		copy(v, ents)
		s := &TestLogDB{
			markerIndex: v[0].Index,
			markerTerm:  v[0].Term,
			entries:     v[1:],
		}
		plog.Infof("appending #%d", i)
		err := s.Append(tt.entries)
		if err != tt.werr {
			t.Errorf("#%d: err = %v, want %v", i, err, tt.werr)
		}
		if !reflect.DeepEqual(s.entries, tt.wentries[1:]) {
			t.Errorf("#%d: entries = %v, want %v", i, s.entries, tt.wentries[1:])
		}
	}
}
````

## File: internal/raft/logdb_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raft

import (
	"math"

	pb "github.com/lni/dragonboat/v4/raftpb"
)

// TestLogDB is used in raft test only. It is basically a logdb.logreader
// backed by a []entry.
type TestLogDB struct {
	entries     []pb.Entry
	markerIndex uint64
	markerTerm  uint64
	snapshot    pb.Snapshot
	state       pb.State
}

func NewTestLogDB() ILogDB {
	return &TestLogDB{
		entries: make([]pb.Entry, 0),
	}
}

func (db *TestLogDB) SetState(s pb.State) {
	db.state = s
}

func (db *TestLogDB) NodeState() (pb.State, pb.Membership) {
	return db.state, db.snapshot.Membership
}

func (db *TestLogDB) Snapshot() pb.Snapshot {
	return db.snapshot
}

func (db *TestLogDB) ApplySnapshot(ss pb.Snapshot) error {
	if db.snapshot.Index >= ss.Index {
		return ErrSnapshotOutOfDate
	}
	db.snapshot = ss
	db.markerIndex = ss.Index
	db.markerTerm = ss.Term
	db.entries = make([]pb.Entry, 0)
	return nil
}

func (db *TestLogDB) CreateSnapshot(ss pb.Snapshot) error {
	if db.snapshot.Index >= ss.Index {
		return ErrSnapshotOutOfDate
	}
	db.snapshot = ss
	return nil
}

func (db *TestLogDB) getSnapshot(index uint64,
	cs *pb.Membership) (pb.Snapshot, error) {
	if index <= db.snapshot.Index {
		return pb.Snapshot{}, ErrSnapshotOutOfDate
	}

	offset := db.markerIndex
	if index > db.lastIndex() {
		plog.Panicf("snapshot %d is out of bound lastindex(%d)",
			index, db.lastIndex())
	}
	ss := pb.Snapshot{
		Index: index,
		Term:  db.entries[index-offset-1].Term,
	}
	if cs != nil {
		ss.Membership = *cs
	}
	return ss, nil
}

func (db *TestLogDB) GetRange() (uint64, uint64) {
	return db.firstIndex(), db.lastIndex()
}

func (db *TestLogDB) firstIndex() uint64 {
	return db.markerIndex + 1
}

func (db *TestLogDB) lastIndex() uint64 {
	return db.markerIndex + uint64(len(db.entries))
}

func (db *TestLogDB) SetRange(firstIndex uint64, length uint64) {
	panic("not implemented")
}

func (db *TestLogDB) Term(index uint64) (uint64, error) {
	if index == db.markerIndex {
		return db.markerTerm, nil
	}
	ents, err := db.Entries(index, index+1, math.MaxUint64)
	if err != nil {
		return 0, err
	}
	if len(ents) == 0 {
		return 0, nil
	}
	return ents[0].Term, nil
}

func (db *TestLogDB) Append(entries []pb.Entry) error {
	if len(entries) == 0 {
		return nil
	}
	first := db.firstIndex()
	if db.markerIndex+uint64(len(entries)) < first {
		return nil
	}
	if first > entries[0].Index {
		entries = entries[first-entries[0].Index:]
	}
	offset := entries[0].Index - db.markerIndex
	if uint64(len(db.entries)+1) > offset {
		db.entries = db.entries[:offset-1]
	} else if uint64(len(db.entries)+1) < offset {
		plog.Panicf("found a hole last index %d, first incoming index %d",
			db.lastIndex(), entries[0].Index)
	}
	db.entries = append(db.entries, entries...)
	return nil
}

func (db *TestLogDB) Entries(low uint64,
	high uint64, maxSize uint64) ([]pb.Entry, error) {
	if low <= db.markerIndex {
		return nil, ErrCompacted
	}
	if high > db.lastIndex()+1 {
		return nil, ErrUnavailable
	}
	if len(db.entries) == 0 {
		return nil, ErrUnavailable
	}
	ents := db.entries[low-db.markerIndex-1 : high-db.markerIndex-1]
	return limitSize(ents, maxSize), nil
}

func (db *TestLogDB) Compact(index uint64) error {
	if index <= db.markerIndex {
		return ErrCompacted
	}
	if index > db.lastIndex() {
		return ErrUnavailable
	}
	if len(db.entries) == 0 {
		return ErrUnavailable
	}
	term, err := db.Term(index)
	if err != nil {
		return err
	}
	cut := index - db.markerIndex
	db.entries = db.entries[cut:]
	db.markerIndex = index
	db.markerTerm = term
	return nil
}
````

## File: internal/raft/logentry_etcd_test.go
````go
// Copyright 2015 The etcd Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//
// log_etcd_test.go is ported from etcd raft for testing purposes.
// tests have been updated to reflect the fact that we have a 3 stage log in
// dragonboat while etcd raft uses a 2 stage log.
//

package raft

import (
	"reflect"
	"testing"

	"github.com/cockroachdb/errors"

	"github.com/lni/dragonboat/v4/internal/server"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

func getAllEntries(l *entryLog) []pb.Entry {
	ents, err := l.entries(l.firstIndex(), noLimit)
	if err == nil {
		return ents
	}
	// try again if there was a racing compaction
	if errors.Is(err, ErrCompacted) {
		return getAllEntries(l)
	}
	panic(err)
}

func TestFindConflict(t *testing.T) {
	previousEnts := []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}, {Index: 3, Term: 3}}
	tests := []struct {
		ents      []pb.Entry
		wconflict uint64
	}{
		// no conflict, empty ent
		{[]pb.Entry{}, 0},
		// no conflict
		{[]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}, {Index: 3, Term: 3}}, 0},
		{[]pb.Entry{{Index: 2, Term: 2}, {Index: 3, Term: 3}}, 0},
		{[]pb.Entry{{Index: 3, Term: 3}}, 0},
		// no conflict, but has new entries
		{[]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}, {Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 4}}, 4},
		{[]pb.Entry{{Index: 2, Term: 2}, {Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 4}}, 4},
		{[]pb.Entry{{Index: 3, Term: 3}, {Index: 4, Term: 4}, {Index: 5, Term: 4}}, 4},
		{[]pb.Entry{{Index: 4, Term: 4}, {Index: 5, Term: 4}}, 4},
		// conflicts with existing entries
		{[]pb.Entry{{Index: 1, Term: 4}, {Index: 2, Term: 4}}, 1},
		{[]pb.Entry{{Index: 2, Term: 1}, {Index: 3, Term: 4}, {Index: 4, Term: 4}}, 2},
		{[]pb.Entry{{Index: 3, Term: 1}, {Index: 4, Term: 2}, {Index: 5, Term: 4}, {Index: 6, Term: 4}}, 3},
	}

	for i, tt := range tests {
		raftLog := newEntryLog(NewTestLogDB(), server.NewInMemRateLimiter(0))
		raftLog.append(previousEnts)

		gconflict, err := raftLog.getConflictIndex(tt.ents)
		if err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		if gconflict != tt.wconflict {
			t.Errorf("#%d: conflict = %d, want %d", i, gconflict, tt.wconflict)
		}
	}
}

func TestIsUpToDate(t *testing.T) {
	previousEnts := []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}, {Index: 3, Term: 3}}
	raftLog := newEntryLog(NewTestLogDB(), server.NewInMemRateLimiter(0))
	raftLog.append(previousEnts)
	tests := []struct {
		lastIndex uint64
		term      uint64
		wUpToDate bool
	}{
		// greater term, ignore lastIndex
		{raftLog.lastIndex() - 1, 4, true},
		{raftLog.lastIndex(), 4, true},
		{raftLog.lastIndex() + 1, 4, true},
		// smaller term, ignore lastIndex
		{raftLog.lastIndex() - 1, 2, false},
		{raftLog.lastIndex(), 2, false},
		{raftLog.lastIndex() + 1, 2, false},
		// equal term, equal or lager lastIndex wins
		{raftLog.lastIndex() - 1, 3, false},
		{raftLog.lastIndex(), 3, true},
		{raftLog.lastIndex() + 1, 3, true},
	}

	for i, tt := range tests {
		gUpToDate, err := raftLog.upToDate(tt.lastIndex, tt.term)
		if err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		if gUpToDate != tt.wUpToDate {
			t.Errorf("#%d: uptodate = %v, want %v", i, gUpToDate, tt.wUpToDate)
		}
	}
}

func TestAppend(t *testing.T) {
	previousEnts := []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}}
	tests := []struct {
		ents      []pb.Entry
		windex    uint64
		wents     []pb.Entry
		wunstable uint64
	}{
		{
			[]pb.Entry{},
			2,
			[]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}},
			3,
		},
		{
			[]pb.Entry{{Index: 3, Term: 2}},
			3,
			[]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}, {Index: 3, Term: 2}},
			3,
		},
		// conflicts with index 1
		{
			[]pb.Entry{{Index: 1, Term: 2}},
			1,
			[]pb.Entry{{Index: 1, Term: 2}},
			1,
		},
		// conflicts with index 2
		{
			[]pb.Entry{{Index: 2, Term: 3}, {Index: 3, Term: 3}},
			3,
			[]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 3}, {Index: 3, Term: 3}},
			2,
		},
	}

	for i, tt := range tests {
		storage := NewTestLogDB()
		if err := storage.Append(previousEnts); err != nil {
			t.Fatalf("%v", err)
		}
		raftLog := newEntryLog(storage, server.NewInMemRateLimiter(0))

		raftLog.append(tt.ents)
		index := raftLog.lastIndex()
		if index != tt.windex {
			t.Errorf("#%d: lastIndex = %d, want %d", i, index, tt.windex)
		}
		g, err := raftLog.entries(1, noLimit)
		if err != nil {
			t.Fatalf("#%d: unexpected error %v", i, err)
		}
		if !reflect.DeepEqual(g, tt.wents) {
			t.Errorf("#%d: logEnts = %+v, want %+v", i, g, tt.wents)
		}
		if goff := raftLog.inmem.markerIndex; goff != tt.wunstable {
			t.Errorf("#%d: unstable = %d, want %d", i, goff, tt.wunstable)
		}
	}
}

// TestLogMaybeAppend ensures:
// If the given (index, term) matches with the existing log:
// 	1. If an existing entry conflicts with a new one (same index
// 	but different terms), delete the existing entry and all that
// 	follow it
// 	2.Append any new entries not already in the log
// If the given (index, term) does not match with the existing log:
// 	return false
func TestLogMaybeAppend(t *testing.T) {
	previousEnts := []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}, {Index: 3, Term: 3}}
	lastindex := uint64(3)
	lastterm := uint64(3)
	commit := uint64(1)

	tests := []struct {
		logTerm   uint64
		index     uint64
		committed uint64
		ents      []pb.Entry

		wlasti  uint64
		wappend bool
		wcommit uint64
		wpanic  bool
	}{
		// not match: term is different
		{
			lastterm - 1, lastindex, lastindex, []pb.Entry{{Index: lastindex + 1, Term: 4}},
			0, false, commit, false,
		},
		// not match: index out of bound
		{
			lastterm, lastindex + 1, lastindex, []pb.Entry{{Index: lastindex + 2, Term: 4}},
			0, false, commit, false,
		},
		// match with the last existing entry
		{
			lastterm, lastindex, lastindex, nil,
			lastindex, true, lastindex, false,
		},
		{
			lastterm, lastindex, lastindex + 1, nil,
			lastindex, true, lastindex, false, // do not increase commit higher than lastnewi
		},
		{
			lastterm, lastindex, lastindex - 1, nil,
			lastindex, true, lastindex - 1, false, // commit up to the commit in the message
		},
		{
			lastterm, lastindex, 0, nil,
			lastindex, true, commit, false, // commit do not decrease
		},
		{
			0, 0, lastindex, nil,
			0, true, commit, false, // commit do not decrease
		},
		{
			lastterm, lastindex, lastindex, []pb.Entry{{Index: lastindex + 1, Term: 4}},
			lastindex + 1, true, lastindex, false,
		},
		{
			lastterm, lastindex, lastindex + 1, []pb.Entry{{Index: lastindex + 1, Term: 4}},
			lastindex + 1, true, lastindex + 1, false,
		},
		{
			lastterm, lastindex, lastindex + 2, []pb.Entry{{Index: lastindex + 1, Term: 4}},
			lastindex + 1, true, lastindex + 1, false, // do not increase commit higher than lastnewi
		},
		{
			lastterm, lastindex, lastindex + 2, []pb.Entry{{Index: lastindex + 1, Term: 4}, {Index: lastindex + 2, Term: 4}},
			lastindex + 2, true, lastindex + 2, false,
		},
		// match with the the entry in the middle
		{
			lastterm - 1, lastindex - 1, lastindex, []pb.Entry{{Index: lastindex, Term: 4}},
			lastindex, true, lastindex, false,
		},
		{
			lastterm - 2, lastindex - 2, lastindex, []pb.Entry{{Index: lastindex - 1, Term: 4}},
			lastindex - 1, true, lastindex - 1, false,
		},
		{
			lastterm - 3, lastindex - 3, lastindex, []pb.Entry{{Index: lastindex - 2, Term: 4}},
			lastindex - 2, true, lastindex - 2, true, // conflict with existing committed entry
		},
		{
			lastterm - 2, lastindex - 2, lastindex, []pb.Entry{{Index: lastindex - 1, Term: 4}, {Index: lastindex, Term: 4}},
			lastindex, true, lastindex, false,
		},
	}

	for i, tt := range tests {
		raftLog := newEntryLog(NewTestLogDB(), server.NewInMemRateLimiter(0))
		raftLog.append(previousEnts)
		raftLog.committed = commit
		func() {
			defer func() {
				if r := recover(); r != nil {
					if !tt.wpanic {
						t.Errorf("%d: panic = %v, want %v", i, true, tt.wpanic)
					}
				}
			}()
			var glasti uint64
			var gappend bool
			match, err := raftLog.matchTerm(tt.index, tt.logTerm)
			if err != nil {
				t.Fatalf("unexpected error %v", err)
			}
			if match {
				gappend = true
				if _, err := raftLog.tryAppend(tt.index, tt.ents); err != nil {
					t.Fatalf("unexpected error %v", err)
				}
				glasti = tt.index + uint64(len(tt.ents))
				raftLog.commitTo(min(glasti, tt.committed))
			}
			gcommit := raftLog.committed

			if glasti != tt.wlasti {
				t.Errorf("#%d: lastindex = %d, want %d", i, glasti, tt.wlasti)
			}
			if gappend != tt.wappend {
				t.Errorf("#%d: append = %v, want %v", i, gappend, tt.wappend)
			}
			if gcommit != tt.wcommit {
				t.Errorf("#%d: committed = %d, want %d", i, gcommit, tt.wcommit)
			}
			if gappend && len(tt.ents) != 0 {
				gents, err := raftLog.getEntries(raftLog.lastIndex()-uint64(len(tt.ents))+1, raftLog.lastIndex()+1, noLimit)
				if err != nil {
					t.Fatalf("unexpected error %v", err)
				}
				if !reflect.DeepEqual(tt.ents, gents) {
					t.Errorf("%d: appended entries = %v, want %v", i, gents, tt.ents)
				}
			}
		}()
	}
}

func TestHasNextEnts(t *testing.T) {
	snap := pb.Snapshot{
		Term: 1, Index: 3,
	}
	ents := []pb.Entry{
		{Term: 1, Index: 4},
		{Term: 1, Index: 5},
		{Term: 1, Index: 6},
	}
	tests := []struct {
		applied uint64
		hasNext bool
	}{
		{0, true},
		{3, true},
		{4, true},
		{5, false},
	}
	for i, tt := range tests {
		storage := NewTestLogDB()
		if err := storage.ApplySnapshot(snap); err != nil {
			t.Fatalf("%v", err)
		}
		raftLog := newEntryLog(storage, server.NewInMemRateLimiter(0))
		raftLog.append(ents)
		if _, err := raftLog.tryCommit(5, 1); err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		raftLog.commitUpdate(pb.UpdateCommit{Processed: tt.applied})

		hasNext := raftLog.hasEntriesToApply()
		if hasNext != tt.hasNext {
			t.Errorf("#%d: hasNext = %v, want %v", i, hasNext, tt.hasNext)
		}
	}
}

func TestNextEnts(t *testing.T) {
	snap := pb.Snapshot{
		Term: 1, Index: 3,
	}
	ents := []pb.Entry{
		{Term: 1, Index: 4},
		{Term: 1, Index: 5},
		{Term: 1, Index: 6},
	}
	tests := []struct {
		applied uint64
		wents   []pb.Entry
	}{
		{0, ents[:2]},
		{3, ents[:2]},
		{4, ents[1:2]},
		{5, nil},
	}
	for i, tt := range tests {
		storage := NewTestLogDB()
		if err := storage.ApplySnapshot(snap); err != nil {
			t.Fatalf("%v", err)
		}
		raftLog := newEntryLog(storage, server.NewInMemRateLimiter(0))
		raftLog.append(ents)
		if _, err := raftLog.tryCommit(5, 1); err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		raftLog.commitUpdate(pb.UpdateCommit{Processed: tt.applied})

		nents, err := raftLog.entriesToApply()
		if err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		if !reflect.DeepEqual(nents, tt.wents) {
			t.Errorf("#%d: nents = %+v, want %+v", i, nents, tt.wents)
		}
	}
}

func TestCommitTo(t *testing.T) {
	previousEnts := []pb.Entry{{Term: 1, Index: 1}, {Term: 2, Index: 2}, {Term: 3, Index: 3}}
	commit := uint64(2)
	tests := []struct {
		commit  uint64
		wcommit uint64
		wpanic  bool
	}{
		{3, 3, false},
		{1, 2, false}, // never decrease
		{4, 0, true},  // commit out of range -> panic
	}
	for i, tt := range tests {
		func() {
			defer func() {
				if r := recover(); r != nil {
					if !tt.wpanic {
						t.Errorf("%d: panic = %v, want %v", i, true, tt.wpanic)
					}
				}
			}()
			raftLog := newEntryLog(NewTestLogDB(), server.NewInMemRateLimiter(0))
			raftLog.append(previousEnts)
			raftLog.committed = commit
			raftLog.commitTo(tt.commit)
			if raftLog.committed != tt.wcommit {
				t.Errorf("#%d: committed = %d, want %d", i, raftLog.committed, tt.wcommit)
			}
		}()
	}
}

//TestCompaction ensures that the number of log entries is correct after compactions.
func TestCompaction(t *testing.T) {
	tests := []struct {
		lastIndex uint64
		compact   []uint64
		wleft     []int
		wallow    bool
	}{
		// out of upper bound
		{1000, []uint64{1001}, []int{-1}, false},
		{1000, []uint64{300, 500, 800, 900}, []int{700, 500, 200, 100}, true},
		// out of lower bound
		{1000, []uint64{300, 299}, []int{700, -1}, false},
	}

	for i, tt := range tests {
		func() {
			defer func() {
				if r := recover(); r != nil {
					if tt.wallow {
						t.Errorf("%d: allow = %v, want %v: %v", i, false, true, r)
					}
				}
			}()

			storage := NewTestLogDB()
			for i := uint64(1); i <= tt.lastIndex; i++ {
				if err := storage.Append([]pb.Entry{{Index: i}}); err != nil {
					t.Fatalf("%v", err)
				}
			}
			raftLog := newEntryLog(storage, server.NewInMemRateLimiter(0))
			if _, err := raftLog.tryCommit(tt.lastIndex, 0); err != nil {
				t.Fatalf("unexpected error %v", err)
			}
			raftLog.commitUpdate(pb.UpdateCommit{Processed: raftLog.committed})

			for j := 0; j < len(tt.compact); j++ {
				err := storage.Compact(tt.compact[j])
				if err != nil {
					if tt.wallow {
						t.Errorf("#%d.%d allow = %t, want %t", i, j, false, tt.wallow)
					}
					continue
				}
				if len(getAllEntries(raftLog)) != tt.wleft[j] {
					t.Errorf("#%d.%d len = %d, want %d", i, j, len(getAllEntries(raftLog)), tt.wleft[j])
				}
			}
		}()
	}
}

func TestLogRestore(t *testing.T) {
	index := uint64(1000)
	term := uint64(1000)
	storage := NewTestLogDB()
	if err := storage.ApplySnapshot(pb.Snapshot{Index: index, Term: term}); err != nil {
		t.Fatalf("%v", err)
	}
	raftLog := newEntryLog(storage, server.NewInMemRateLimiter(0))

	if len(getAllEntries(raftLog)) != 0 {
		t.Errorf("len = %d, want 0", len(getAllEntries(raftLog)))
	}
	if raftLog.firstIndex() != index+1 {
		t.Errorf("firstIndex = %d, want %d", raftLog.firstIndex(), index+1)
	}
	if raftLog.committed != index {
		t.Errorf("committed = %d, want %d", raftLog.committed, index)
	}
	if raftLog.inmem.markerIndex != index+1 {
		t.Errorf("unstable = %d, want %d", raftLog.inmem.markerIndex, index+1)
	}
	if mustTerm(raftLog.term(index)) != term {
		t.Errorf("term = %d, want %d", mustTerm(raftLog.term(index)), term)
	}
}

func TestIsOutOfBounds(t *testing.T) {
	offset := uint64(100)
	num := uint64(100)
	storage := NewTestLogDB()
	if err := storage.ApplySnapshot(pb.Snapshot{Index: offset}); err != nil {
		t.Fatalf("%v", err)
	}
	l := newEntryLog(storage, server.NewInMemRateLimiter(0))
	for i := uint64(1); i <= num; i++ {
		l.append([]pb.Entry{{Index: i + offset}})
	}

	first := offset + 1
	tests := []struct {
		lo, hi        uint64
		wpanic        bool
		wErrCompacted bool
	}{
		{
			first - 2, first + 1,
			false,
			true,
		},
		{
			first - 1, first + 1,
			false,
			true,
		},
		{
			first, first,
			false,
			false,
		},
		{
			first + num/2, first + num/2,
			false,
			false,
		},
		{
			first + num - 1, first + num - 1,
			false,
			false,
		},
		{
			first + num, first + num,
			false,
			false,
		},
		{
			first + num, first + num + 1,
			true,
			false,
		},
		{
			first + num + 1, first + num + 1,
			true,
			false,
		},
	}

	for i, tt := range tests {
		func() {
			defer func() {
				if r := recover(); r != nil {
					if !tt.wpanic {
						t.Errorf("%d: panic = %v, want %v: %v", i, true, false, r)
					}
				}
			}()
			err := l.checkBound(tt.lo, tt.hi)
			if tt.wpanic {
				t.Errorf("%d: panic = %v, want %v", i, false, true)
			}
			if tt.wErrCompacted && err != ErrCompacted {
				t.Errorf("%d: err = %v, want %v", i, err, ErrCompacted)
			}
			if !tt.wErrCompacted && err != nil {
				t.Errorf("%d: unexpected err %v", i, err)
			}
		}()
	}
}

func TestTerm(t *testing.T) {
	var i uint64
	offset := uint64(100)
	num := uint64(100)

	storage := NewTestLogDB()
	if err := storage.ApplySnapshot(pb.Snapshot{Index: offset, Term: 1}); err != nil {
		t.Fatalf("%v", err)
	}
	l := newEntryLog(storage, server.NewInMemRateLimiter(0))
	for i = 1; i < num; i++ {
		l.append([]pb.Entry{{Index: offset + i, Term: i}})
	}

	tests := []struct {
		index uint64
		w     uint64
	}{
		{offset - 1, 0},
		{offset, 1},
		{offset + num/2, num / 2},
		{offset + num - 1, num - 1},
		{offset + num, 0},
	}

	for j, tt := range tests {
		term := mustTerm(l.term(tt.index))
		if term != tt.w {
			t.Errorf("#%d: at = %d, want %d", j, term, tt.w)
		}
	}
}

func TestTermWithUnstableSnapshot(t *testing.T) {
	storagesnapi := uint64(100)
	unstablesnapi := storagesnapi + 5

	storage := NewTestLogDB()
	if err := storage.ApplySnapshot(pb.Snapshot{Index: storagesnapi, Term: 1}); err != nil {
		t.Fatalf("%v", err)
	}
	l := newEntryLog(storage, server.NewInMemRateLimiter(0))
	l.restore(pb.Snapshot{Index: unstablesnapi, Term: 1})

	tests := []struct {
		index uint64
		w     uint64
	}{
		// cannot get term from storage
		{storagesnapi, 0},
		// cannot get term from the gap between storage ents and unstable snapshot
		{storagesnapi + 1, 0},
		{unstablesnapi - 1, 0},
		// get term from unstable snapshot index
		{unstablesnapi, 1},
	}

	for i, tt := range tests {
		term := mustTerm(l.term(tt.index))
		if term != tt.w {
			t.Errorf("#%d: at = %d, want %d", i, term, tt.w)
		}
	}
}

func TestSlice(t *testing.T) {
	var i uint64
	offset := uint64(100)
	num := uint64(100)
	last := offset + num
	half := offset + num/2
	halfe := pb.Entry{Index: half, Term: half}

	storage := NewTestLogDB()
	if err := storage.ApplySnapshot(pb.Snapshot{Index: offset}); err != nil {
		t.Fatalf("%v", err)
	}
	for i = 1; i < num/2; i++ {
		if err := storage.Append([]pb.Entry{{Index: offset + i, Term: offset + i}}); err != nil {
			t.Fatalf("%v", err)
		}
	}
	l := newEntryLog(storage, server.NewInMemRateLimiter(0))
	for i = num / 2; i < num; i++ {
		l.append([]pb.Entry{{Index: offset + i, Term: offset + i}})
	}

	tests := []struct {
		from  uint64
		to    uint64
		limit uint64

		w      []pb.Entry
		wpanic bool
	}{
		// test no limit
		{offset - 1, offset + 1, noLimit, nil, false},
		{offset, offset + 1, noLimit, nil, false},
		{half - 1, half + 1, noLimit, []pb.Entry{{Index: half - 1, Term: half - 1}, {Index: half, Term: half}}, false},
		{half, half + 1, noLimit, []pb.Entry{{Index: half, Term: half}}, false},
		{last - 1, last, noLimit, []pb.Entry{{Index: last - 1, Term: last - 1}}, false},
		{last, last + 1, noLimit, nil, true},

		// test limit
		{half - 1, half + 1, 0, []pb.Entry{{Index: half - 1, Term: half - 1}}, false},
		{half - 1, half + 1, uint64(halfe.SizeUpperLimit() + 1), []pb.Entry{{Index: half - 1, Term: half - 1}}, false},
		{half - 2, half + 1, uint64(halfe.SizeUpperLimit() + 1), []pb.Entry{{Index: half - 2, Term: half - 2}}, false},
		{half - 1, half + 1, uint64(halfe.SizeUpperLimit() * 2), []pb.Entry{{Index: half - 1, Term: half - 1}, {Index: half, Term: half}}, false},
		{half - 1, half + 2, uint64(halfe.SizeUpperLimit() * 3), []pb.Entry{{Index: half - 1, Term: half - 1}, {Index: half, Term: half}, {Index: half + 1, Term: half + 1}}, false},
		{half, half + 2, uint64(halfe.SizeUpperLimit()), []pb.Entry{{Index: half, Term: half}}, false},
		{half, half + 2, uint64(halfe.SizeUpperLimit() * 2), []pb.Entry{{Index: half, Term: half}, {Index: half + 1, Term: half + 1}}, false},
	}

	for j, tt := range tests {
		func() {
			defer func() {
				if r := recover(); r != nil {
					if !tt.wpanic {
						t.Errorf("%d: panic = %v, want %v: %v", j, true, false, r)
					}
				}
			}()
			g, err := l.getEntries(tt.from, tt.to, tt.limit)
			if tt.from <= offset && err != ErrCompacted {
				t.Fatalf("#%d: err = %v, want %v", j, err, ErrCompacted)
			}
			if tt.from > offset && err != nil {
				t.Fatalf("#%d: unexpected error %v", j, err)
			}
			if !reflect.DeepEqual(g, tt.w) {
				t.Errorf("#%d: from %d to %d = %v, want %v", j, tt.from, tt.to, g, tt.w)
			}
		}()
	}
}

func mustTerm(term uint64, err error) uint64 {
	if err != nil {
		panic(err)
	}
	return term
}

// TestCompactionSideEffects ensures that all the log related functionality works correctly after
// a compaction.
func TestCompactionSideEffects(t *testing.T) {
	var i uint64
	// Populate the log with 1000 entries; 750 in stable storage and 250 in unstable.
	lastIndex := uint64(1000)
	unstableIndex := uint64(750)
	lastTerm := lastIndex
	storage := NewTestLogDB()
	for i = 1; i <= unstableIndex; i++ {
		if err := storage.Append([]pb.Entry{{Term: i, Index: i}}); err != nil {
			t.Fatalf("%v", err)
		}
	}
	raftLog := newEntryLog(storage, server.NewInMemRateLimiter(0))
	for i = unstableIndex; i < lastIndex; i++ {
		raftLog.append([]pb.Entry{{Term: i + 1, Index: i + 1}})
	}

	ok, err := raftLog.tryCommit(lastIndex, lastTerm)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if !ok {
		t.Fatalf("maybeCommit returned false")
	}
	offset := uint64(500)
	if err := storage.Compact(offset); err != nil {
		t.Fatalf("%v", err)
	}
	if raftLog.lastIndex() != lastIndex {
		t.Errorf("lastIndex = %d, want %d", raftLog.lastIndex(), lastIndex)
	}

	for j := offset; j <= raftLog.lastIndex(); j++ {
		if mustTerm(raftLog.term(j)) != j {
			t.Errorf("term(%d) = %d, want %d", j, mustTerm(raftLog.term(j)), j)
		}
	}

	for j := offset; j <= raftLog.lastIndex(); j++ {
		match, err := raftLog.matchTerm(j, j)
		if err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		if !match {
			t.Errorf("matchTerm(%d) = false, want true", j)
		}
	}

	unstableEnts := raftLog.entriesToSave()
	if g := len(unstableEnts); g != 250 {
		t.Errorf("len(unstableEntries) = %d, want = %d", g, 250)
	}
	if len(unstableEnts) == 0 {
		t.Fatalf("len(unstableEnts) == 0")
	}
	if unstableEnts[0].Index != 751 {
		t.Errorf("Index = %d, want = %d", unstableEnts[0].Index, 751)
	}

	prev := raftLog.lastIndex()
	raftLog.append([]pb.Entry{{Index: raftLog.lastIndex() + 1, Term: raftLog.lastIndex() + 1}})
	if raftLog.lastIndex() != prev+1 {
		t.Errorf("lastIndex = %d, want = %d", raftLog.lastIndex(), prev+1)
	}

	ents, err := raftLog.entries(raftLog.lastIndex(), noLimit)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if len(ents) != 1 {
		t.Errorf("len(entries) = %d, want = %d", len(ents), 1)
	}
}

// TestUnstableEnts ensures unstableEntries returns the unstable part of the
// entries correctly.
func TestUnstableEnts(t *testing.T) {
	previousEnts := []pb.Entry{{Term: 1, Index: 1}, {Term: 2, Index: 2}}
	tests := []struct {
		unstable uint64
		wents    []pb.Entry
	}{
		{3, nil},
		{1, previousEnts},
	}

	for i, tt := range tests {
		// append stable entries to storage
		storage := NewTestLogDB()
		if err := storage.Append(previousEnts[:tt.unstable-1]); err != nil {
			t.Fatalf("%v", err)
		}

		// append unstable entries to raftlog
		raftLog := newEntryLog(storage, server.NewInMemRateLimiter(0))
		raftLog.append(previousEnts[tt.unstable-1:])
		ents := raftLog.entriesToSave()
		if l := len(ents); l > 0 {
			if _, err := raftLog.tryCommit(ents[l-1].Index, ents[l-1].Term); err != nil {
				t.Fatalf("unexpected error %v", err)
			}
			cu := pb.UpdateCommit{
				Processed:     ents[l-1].Index,
				LastApplied:   ents[l-1].Index,
				StableLogTo:   ents[l-1].Index,
				StableLogTerm: ents[l-1].Term,
			}
			raftLog.commitUpdate(cu)
			//raftLog.savedLogTo(ents[l-1].Index, ents[l-i].Term)
			//raftLog.appliedLogTo(ents[l-1].Index)
		}
		if !reflect.DeepEqual(ents, tt.wents) {
			t.Errorf("#%d: unstableEnts = %+v, want %+v", i, ents, tt.wents)
		}
		if len(ents) > 0 {
			w := ents[len(ents)-1].Index + 1
			if g := raftLog.inmem.markerIndex; g != w {
				t.Errorf("#%d: unstable = %d, want %d", i, g, w)
			}
		}
	}
}

func TestStableTo(t *testing.T) {
	tests := []struct {
		stablei   uint64
		stablet   uint64
		savedTo   uint64
		wunstable uint64
	}{
		{1, 1, 1, 1},
		{2, 2, 1, 1},
		{2, 1, 0, 1}, // bad term
		{3, 1, 0, 1}, // bad index
	}
	for i, tt := range tests {
		raftLog := newEntryLog(NewTestLogDB(), server.NewInMemRateLimiter(0))
		raftLog.append([]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}})
		cu := pb.UpdateCommit{
			StableLogTo:   tt.stablei,
			StableLogTerm: tt.stablet,
		}
		raftLog.commitUpdate(cu)
		if tt.savedTo > 0 && raftLog.inmem.savedTo != tt.stablei {
			t.Errorf("#%d: stable to %d, want %d", i, raftLog.inmem.savedTo, tt.stablei)
		}
		if raftLog.inmem.markerIndex != tt.wunstable {
			t.Errorf("#%d: unstable = %d, want %d", i, raftLog.inmem.markerIndex, tt.wunstable)
		}
	}
}

func TestStableToWithSnap(t *testing.T) {
	snapi, snapt := uint64(5), uint64(2)
	tests := []struct {
		stablei uint64
		stablet uint64
		newEnts []pb.Entry

		wunstable uint64
	}{
		{snapi + 1, snapt, nil, snapi + 1},
		{snapi, snapt, nil, snapi + 1},
		{snapi - 1, snapt, nil, snapi + 1},

		{snapi + 1, snapt + 1, nil, snapi + 1},
		{snapi, snapt + 1, nil, snapi + 1},
		{snapi - 1, snapt + 1, nil, snapi + 1},

		{snapi + 1, snapt, []pb.Entry{{Index: snapi + 1, Term: snapt}}, snapi + 2},
		{snapi, snapt, []pb.Entry{{Index: snapi + 1, Term: snapt}}, snapi + 1},
		{snapi - 1, snapt, []pb.Entry{{Index: snapi + 1, Term: snapt}}, snapi + 1},

		{snapi + 1, snapt + 1, []pb.Entry{{Index: snapi + 1, Term: snapt}}, snapi + 1},
		{snapi, snapt + 1, []pb.Entry{{Index: snapi + 1, Term: snapt}}, snapi + 1},
		{snapi - 1, snapt + 1, []pb.Entry{{Index: snapi + 1, Term: snapt}}, snapi + 1},
	}
	for i, tt := range tests {
		s := NewTestLogDB()
		if err := s.ApplySnapshot(pb.Snapshot{Index: snapi, Term: snapt}); err != nil {
			t.Fatalf("%v", err)
		}
		raftLog := newEntryLog(s, server.NewInMemRateLimiter(0))
		raftLog.append(tt.newEnts)
		cu := pb.UpdateCommit{
			StableLogTo:   tt.stablei,
			StableLogTerm: tt.stablet,
		}
		raftLog.commitUpdate(cu)
		if raftLog.inmem.savedTo != tt.wunstable-1 {
			t.Errorf("#%d: unstable = %d, want %d", i, raftLog.inmem.savedTo, tt.wunstable)
		}
	}
}
````

## File: internal/raft/logentry_helper.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raft

import (
	"github.com/cockroachdb/errors"

	"github.com/lni/dragonboat/v4/internal/server"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

// LogTestHelper is a helper type used for testing logEntry.
type LogTestHelper struct {
	el *entryLog
}

// NewLog creates and returns a new LogTestHelper instance used for testing
// purpose.
func NewLog(logdb ILogDB) *LogTestHelper {
	return &LogTestHelper{
		el: newEntryLog(logdb, server.NewInMemRateLimiter(0)),
	}
}

// GetConflictIndex ...
func (l *LogTestHelper) GetConflictIndex(ents []pb.Entry) (uint64, error) {
	return l.el.getConflictIndex(ents)
}

// Term ...
func (l *LogTestHelper) Term(index uint64) (uint64, error) {
	return l.el.term(index)
}

// MatchTerm ...
func (l *LogTestHelper) MatchTerm(index uint64, term uint64) (bool, error) {
	return l.el.matchTerm(index, term)
}

// FirstIndex ...
func (l *LogTestHelper) FirstIndex() uint64 {
	return l.el.firstIndex()
}

// LastIndex ...
func (l *LogTestHelper) LastIndex() uint64 {
	return l.el.lastIndex()
}

// UpToDate ...
func (l *LogTestHelper) UpToDate(index uint64, term uint64) (bool, error) {
	return l.el.upToDate(index, term)
}

// Append ...
func (l *LogTestHelper) Append(ents []pb.Entry) error {
	l.el.append(ents)
	return nil
}

// AllEntries ...
func (l *LogTestHelper) AllEntries() []pb.Entry {
	ents, err := l.el.entries(l.el.firstIndex(), noLimit)
	if err == nil {
		return ents
	}
	if errors.Is(err, ErrCompacted) {
		return l.AllEntries()
	}
	panic(err)
}

// Entries ...
func (l *LogTestHelper) Entries(start uint64,
	maxsize uint64) ([]pb.Entry, error) {
	return l.el.entries(start, maxsize)
}

// EntriesToSave ...
func (l *LogTestHelper) EntriesToSave() []pb.Entry {
	return l.el.entriesToSave()
}

// UnstableOffset ...
func (l *LogTestHelper) UnstableOffset() uint64 {
	return l.el.inmem.markerIndex
}

// SetCommitted ...
func (l *LogTestHelper) SetCommitted(v uint64) {
	l.el.committed = v
}

// GetCommitted ...
func (l *LogTestHelper) GetCommitted() uint64 {
	return l.el.committed
}

// TryAppend ...
func (l *LogTestHelper) TryAppend(index uint64, logTerm uint64,
	committed uint64, ents []pb.Entry) (uint64, bool, error) {
	match, err := l.el.matchTerm(index, logTerm)
	if err != nil {
		return 0, false, err
	}
	if match {
		if _, err := l.el.tryAppend(index, ents); err != nil {
			return 0, false, err
		}
		lastIndex := index + uint64(len(ents))
		l.el.commitTo(min(lastIndex, committed))
		return lastIndex, true, nil
	}
	return 0, false, nil
}

// GetEntries ...
func (l *LogTestHelper) GetEntries(low uint64, high uint64,
	maxsize uint64) ([]pb.Entry, error) {
	return l.el.getEntries(low, high, maxsize)
}

// TryCommit ...
func (l *LogTestHelper) TryCommit(index uint64, term uint64) (bool, error) {
	return l.el.tryCommit(index, term)
}

// AppliedTo ...
func (l *LogTestHelper) AppliedTo(index uint64) {
	l.el.commitUpdate(pb.UpdateCommit{Processed: index})
}

// HasEntriesToApply ...
func (l *LogTestHelper) HasEntriesToApply() bool {
	return l.el.hasEntriesToApply()
}

// EntriesToApply ...
func (l *LogTestHelper) EntriesToApply() ([]pb.Entry, error) {
	return l.el.entriesToApply()
}

// CheckBound ...
func (l *LogTestHelper) CheckBound(low uint64, high uint64) error {
	return l.el.checkBound(low, high)
}
````

## File: internal/raft/logentry_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raft

import (
	"math"
	"testing"

	"github.com/lni/dragonboat/v4/internal/server"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

func getTestEntryLog() *entryLog {
	logdb := NewTestLogDB()
	return newEntryLog(logdb, server.NewInMemRateLimiter(0))
}

func TestLogEntryLogCanBeCreated(t *testing.T) {
	logdb := NewTestLogDB()
	if err := logdb.Append([]pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
	}); err != nil {
		t.Fatalf("%v", err)
	}
	first, last := logdb.GetRange()
	if first != 1 || last != 3 {
		t.Errorf("unexpected range")
	}
	el := newEntryLog(logdb, server.NewInMemRateLimiter(0))
	if el.committed != 0 || el.processed != 0 || el.inmem.markerIndex != 4 {
		t.Errorf("unexpected log state %+v", el)
	}
}

func TestLogReturnSnapshotIndexAsFirstIndex(t *testing.T) {
	el := getTestEntryLog()
	ss := pb.Snapshot{Index: 100, Term: 3}
	el.inmem.restore(ss)
	if el.firstIndex() != 101 {
		t.Errorf("unexpected first index %d, want 101", el.firstIndex())
	}
}

func TestLogWithInMemSnapshotOnly(t *testing.T) {
	el := getTestEntryLog()
	ss := pb.Snapshot{Index: 100, Term: 3}
	el.restore(ss)
	if el.firstIndex() != 101 {
		t.Errorf("unexpected first index %d, want 101", el.firstIndex())
	}
	if el.lastIndex() != 100 {
		t.Errorf("unexpected last index %d, want 100", el.lastIndex())
	}
	for i := uint64(0); i < 110; i++ {
		ents, err := el.getEntries(i, i+1, math.MaxUint64)
		if err != ErrCompacted {
			t.Errorf("unexpected err %v", err)
		}
		if len(ents) != 0 {
			t.Errorf("unexpected results %v", ents)
		}
	}
}

func TestLogNoEntriesToApplyAfterSnapshotRestored(t *testing.T) {
	el := getTestEntryLog()
	ss := pb.Snapshot{Index: 100, Term: 3}
	el.restore(ss)
	if el.hasEntriesToApply() {
		t.Errorf("unexpected entry to apply")
	}
}

func TestLogFirstNotAppliedIndexAfterSnapshotRestored(t *testing.T) {
	el := getTestEntryLog()
	ss := pb.Snapshot{Index: 100, Term: 3}
	el.restore(ss)
	idx := el.firstNotAppliedIndex()
	if idx != 101 {
		t.Errorf("unexpected index %d, want 101", idx)
	}
	if el.toApplyIndexLimit() != 101 {
		t.Errorf("unexpected to apply limit %d, want 101", el.toApplyIndexLimit())
	}
}

func TestLogIterateOnReadyToBeAppliedEntries(t *testing.T) {
	ents := make([]pb.Entry, 0)
	for i := uint64(1); i <= 128; i++ {
		ents = append(ents, pb.Entry{Index: i, Term: i})
	}
	ents[10].Cmd = make([]byte, maxEntriesToApplySize)
	ents[20].Cmd = make([]byte, maxEntriesToApplySize)
	ents[30].Cmd = make([]byte, maxEntriesToApplySize*2)
	logdb := NewTestLogDB()
	if err := logdb.Append(ents); err != nil {
		t.Fatalf("%v", err)
	}
	el := newEntryLog(logdb, server.NewInMemRateLimiter(0))
	el.committed = 128
	el.processed = 0
	results := make([]pb.Entry, 0)
	count := 0
	for {
		re, err := el.getEntriesToApply(maxEntriesToApplySize)
		if err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		if len(re) == 0 {
			break
		}
		count++
		results = append(results, re...)
		el.processed = re[len(re)-1].Index
	}
	if len(results) != 128 {
		t.Errorf("failed to get all entries")
	}
	for idx, e := range results {
		if e.Index != uint64(idx+1) {
			t.Errorf("unexpected index")
		}
	}
	if count != 7 {
		t.Errorf("unexpected count %d, want 7", count)
	}
}

func TestLogReturnLastIndexInLogDBWhenNoSnapshotInMem(t *testing.T) {
	logdb := NewTestLogDB()
	if err := logdb.Append([]pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
	}); err != nil {
		t.Fatalf("%v", err)
	}
	el := newEntryLog(logdb, server.NewInMemRateLimiter(0))
	if el.firstIndex() != 1 {
		t.Errorf("unexpected first index, %d", el.firstIndex())
	}
}

func TestLogLastIndexReturnInMemLastIndexWhenPossible(t *testing.T) {
	el := getTestEntryLog()
	el.inmem.merge([]pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
		{Index: 4, Term: 3},
	})
	if el.lastIndex() != 4 {
		t.Errorf("unexpected last index %d", el.lastIndex())
	}
}

func TestLogLastIndexReturnLogDBLastIndexWhenNothingInInMem(t *testing.T) {
	logdb := NewTestLogDB()
	if err := logdb.Append([]pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
	}); err != nil {
		t.Fatalf("%v", err)
	}
	el := newEntryLog(logdb, server.NewInMemRateLimiter(0))
	if el.lastIndex() != 2 {
		t.Errorf("unexpected last index %d", el.lastIndex())
	}
}

func TestLogLastTerm(t *testing.T) {
	el := getTestEntryLog()
	el.inmem.merge([]pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
		{Index: 4, Term: 3},
	})
	term, err := el.lastTerm()
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if term != 3 {
		t.Errorf("unexpected last term %d", term)
	}
	logdb := NewTestLogDB()
	if err := logdb.Append([]pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 5},
	}); err != nil {
		t.Fatalf("%v", err)
	}
	el = newEntryLog(logdb, server.NewInMemRateLimiter(0))
	term, err = el.lastTerm()
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if term != 5 {
		t.Errorf("unexpected last term %d", term)
	}
}

func TestLogTerm(t *testing.T) {
	el := getTestEntryLog()
	el.inmem.merge([]pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
		{Index: 4, Term: 3},
	})
	for idx, ent := range el.inmem.entries {
		term, err := el.term(ent.Index)
		if err != nil {
			t.Errorf("%d, unexpected err %v", idx, err)
		}
		if term != ent.Term {
			t.Errorf("%d, unexpected term %d, want %d", idx, term, ent.Term)
		}
	}
	ents := []pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 5},
	}
	logdb := NewTestLogDB()
	if err := logdb.Append(ents); err != nil {
		t.Fatalf("%v", err)
	}
	el = newEntryLog(logdb, server.NewInMemRateLimiter(0))
	for idx, ent := range ents {
		term, err := el.term(ent.Index)
		if err != nil {
			t.Errorf("%d, unexpected error %v", idx, err)
		}
		if term != ent.Term {
			t.Errorf("%d, unexpected term %d, want %d", idx, term, ent.Term)
		}
	}
}

func TestLogAppend(t *testing.T) {
	el := getTestEntryLog()
	el.append([]pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
		{Index: 4, Term: 3},
	})
	ets := el.entriesToSave()
	if len(ets) != 4 {
		t.Errorf("unexpected length %d", len(ets))
	}
}
func TestLogAppendPanicWhenAppendingCommittedEntry(t *testing.T) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not triggered")
	}()
	el := getTestEntryLog()
	el.committed = 2
	el.append([]pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
		{Index: 4, Term: 3},
	})
}

func TestLogGetEntryFromInMem(t *testing.T) {
	el := getTestEntryLog()
	el.append([]pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
		{Index: 4, Term: 3},
	})
	ents, err := el.getEntries(1, 5, math.MaxUint64)
	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
	if len(ents) != 4 {
		t.Errorf("unexpected length %d", len(ents))
	}
	ents, err = el.getEntries(2, 4, math.MaxUint64)
	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
	if len(ents) != 2 {
		t.Errorf("unexpected length %d", len(ents))
	}
}

func TestLogGetEntryFromLogDB(t *testing.T) {
	ents := []pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
		{Index: 4, Term: 3},
	}
	logdb := NewTestLogDB()
	if err := logdb.Append(ents); err != nil {
		t.Fatalf("%v", err)
	}
	el := newEntryLog(logdb, server.NewInMemRateLimiter(0))
	ents, err := el.getEntries(1, 5, math.MaxUint64)
	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
	if len(ents) != 4 {
		t.Errorf("unexpected length %d", len(ents))
	}
	ents, err = el.getEntries(2, 4, math.MaxUint64)
	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
	if len(ents) != 2 {
		t.Errorf("unexpected length %d", len(ents))
	}
}

func TestLogGetEntryFromLogDBAndInMem(t *testing.T) {
	ents := []pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
		{Index: 4, Term: 3},
	}
	logdb := NewTestLogDB()
	if err := logdb.Append(ents); err != nil {
		t.Fatalf("%v", err)
	}
	el := newEntryLog(logdb, server.NewInMemRateLimiter(0))
	el.append([]pb.Entry{
		{Index: 5, Term: 3},
		{Index: 6, Term: 3},
		{Index: 7, Term: 4},
	})
	ents, err := el.getEntries(1, 8, math.MaxUint64)
	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
	if len(ents) != 7 {
		t.Errorf("unexpected length %d", len(ents))
	}
	ents, err = el.getEntries(2, 7, math.MaxUint64)
	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
	if len(ents) != 5 {
		t.Errorf("unexpected length %d", len(ents))
	}
	if ents[4].Index != 6 || ents[0].Index != 2 {
		t.Errorf("unexpected index")
	}
	ents, err = el.getEntries(1, 5, math.MaxUint64)
	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
	if len(ents) != 4 {
		t.Errorf("unexpected length %d", len(ents))
	}
	if ents[0].Index != 1 || ents[3].Index != 4 {
		t.Errorf("unexpected index")
	}
	// entries() wrapper
	ents, err = el.entries(2, math.MaxUint64)
	if err != nil {
		t.Errorf("entries failed %v", err)
	}
	if len(ents) != 6 {
		t.Errorf("unexpected length")
	}
}

func TestLogSnapshot(t *testing.T) {
	inMemSnapshot := pb.Snapshot{Index: 123, Term: 2}
	logdbSnapshot := pb.Snapshot{Index: 234, Term: 3}
	el := getTestEntryLog()
	el.inmem.restore(inMemSnapshot)
	if err := el.logdb.ApplySnapshot(logdbSnapshot); err != nil {
		t.Fatalf("%v", err)
	}
	ss := el.snapshot()
	if ss.Index != inMemSnapshot.Index {
		t.Errorf("unexpected snapshot index")
	}
	el.inmem.savedSnapshotTo(inMemSnapshot.Index)
	ss = el.snapshot()
	if ss.Index != logdbSnapshot.Index {
		t.Errorf("unexpected snapshot index")
	}
}

func TestLogMatchTerm(t *testing.T) {
	ents := []pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
		{Index: 4, Term: 3},
	}
	logdb := NewTestLogDB()
	if err := logdb.Append(ents); err != nil {
		t.Fatalf("%v", err)
	}
	el := newEntryLog(logdb, server.NewInMemRateLimiter(0))
	el.append([]pb.Entry{
		{Index: 5, Term: 3},
		{Index: 6, Term: 3},
		{Index: 7, Term: 4},
	})
	tests := []struct {
		index uint64
		term  uint64
		match bool
	}{
		{1, 1, true},
		{1, 2, false},
		{4, 4, false},
		{4, 3, true},
		{5, 3, true},
		{5, 4, false},
		{7, 4, true},
		{8, 5, false},
	}
	for idx, tt := range tests {
		match, err := el.matchTerm(tt.index, tt.term)
		if err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		if match != tt.match {
			t.Errorf("%d, incorrect matchTerm result", idx)
		}
	}
}

func TestLogUpToDate(t *testing.T) {
	ents := []pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
		{Index: 4, Term: 3},
	}
	logdb := NewTestLogDB()
	if err := logdb.Append(ents); err != nil {
		t.Fatalf("%v", err)
	}
	el := newEntryLog(logdb, server.NewInMemRateLimiter(0))
	el.append([]pb.Entry{
		{Index: 5, Term: 3},
		{Index: 6, Term: 3},
		{Index: 7, Term: 4},
	})
	tests := []struct {
		index uint64
		term  uint64
		ok    bool
	}{
		{1, 2, false},
		{8, 2, false},
		{1, 4, false},
		{7, 4, true},
		{8, 4, true},
		{8, 5, true},
		{2, 5, true},
	}
	for idx, tt := range tests {
		ok, err := el.upToDate(tt.index, tt.term)
		if err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		if ok != tt.ok {
			t.Errorf("%d, incorrect up to date result", idx)
		}
	}
}

func TestLogGetConflictIndex(t *testing.T) {
	ents := []pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
		{Index: 4, Term: 3},
	}
	logdb := NewTestLogDB()
	if err := logdb.Append(ents); err != nil {
		t.Fatalf("%v", err)
	}
	el := newEntryLog(logdb, server.NewInMemRateLimiter(0))
	el.append([]pb.Entry{
		{Index: 5, Term: 3},
		{Index: 6, Term: 3},
		{Index: 7, Term: 4},
	})
	tests := []struct {
		ents     []pb.Entry
		conflict uint64
	}{
		{[]pb.Entry{}, 0},
		{[]pb.Entry{{Index: 1, Term: 2}}, 1},
		{[]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 1}}, 0},
		{[]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}}, 2},
		{[]pb.Entry{{Index: 6, Term: 3}, {Index: 7, Term: 4}}, 0},
		{[]pb.Entry{{Index: 6, Term: 3}, {Index: 7, Term: 5}}, 7},
		{[]pb.Entry{{Index: 7, Term: 4}, {Index: 8, Term: 4}}, 8},
	}
	for idx, tt := range tests {
		conflict, err := el.getConflictIndex(tt.ents)
		if err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		if conflict != tt.conflict {
			t.Errorf("%d, conflict index %d, want %d", idx, conflict, tt.conflict)
		}
	}
}

func TestLogCommitTo(t *testing.T) {
	ents := []pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
		{Index: 4, Term: 3},
	}
	logdb := NewTestLogDB()
	if err := logdb.Append(ents); err != nil {
		t.Fatalf("%v", err)
	}
	el := newEntryLog(logdb, server.NewInMemRateLimiter(0))
	el.append([]pb.Entry{
		{Index: 5, Term: 3},
		{Index: 6, Term: 3},
		{Index: 7, Term: 4},
	})
	el.commitTo(3)
	if el.committed != 3 {
		t.Errorf("commitedTo failed")
	}
	el.commitTo(2)
	if el.committed != 3 {
		t.Errorf("commitedTo failed")
	}
}

func TestLogCommitToPanicWhenCommitToUnavailableIndex(t *testing.T) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not triggered")
	}()
	ents := []pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
		{Index: 4, Term: 3},
	}
	logdb := NewTestLogDB()
	if err := logdb.Append(ents); err != nil {
		t.Fatalf("%v", err)
	}
	el := newEntryLog(logdb, server.NewInMemRateLimiter(0))
	el.append([]pb.Entry{
		{Index: 5, Term: 3},
		{Index: 6, Term: 3},
		{Index: 7, Term: 4},
	})
	el.commitTo(8)
}

func TestLogRestoreSnapshot(t *testing.T) {
	ents := []pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
		{Index: 4, Term: 3},
	}
	logdb := NewTestLogDB()
	el := newEntryLog(logdb, server.NewInMemRateLimiter(0))
	el.append(ents)
	ss := pb.Snapshot{Index: 100, Term: 10}
	el.restore(ss)
	if el.committed != 100 || el.processed != 100 {
		t.Errorf("committed/applied not updated")
	}
	if el.inmem.markerIndex != 101 {
		t.Errorf("marker index not updated")
	}
	if el.inmem.snapshot.Index != 100 {
		t.Errorf("snapshot index not updated")
	}
}

func TestLogCommitUpdateSetsApplied(t *testing.T) {
	el := getTestEntryLog()
	el.committed = 10
	cu := pb.UpdateCommit{
		Processed: 5,
	}
	el.commitUpdate(cu)
	if el.processed != 5 {
		t.Errorf("applied %d, want 5", el.processed)
	}
}

func TestLogCommitUpdatePanicWhenApplyTwice(t *testing.T) {
	defer func() {
		if r := recover(); r == nil {
			t.Errorf("didn't panic")
		}
	}()
	el := getTestEntryLog()
	el.processed = 6
	el.committed = 10
	cu := pb.UpdateCommit{
		Processed: 5,
	}
	el.commitUpdate(cu)
}

func TestLogCommitUpdatePanicWhenApplyingNotCommitEntry(t *testing.T) {
	defer func() {
		if r := recover(); r == nil {
			t.Errorf("didn't panic")
		}
	}()
	el := getTestEntryLog()
	el.processed = 6
	el.committed = 10
	cu := pb.UpdateCommit{
		Processed: 12,
	}
	el.commitUpdate(cu)
}

func TestGetUncommittedEntries(t *testing.T) {
	el := getTestEntryLog()
	ents := el.getUncommittedEntries()
	if len(ents) != 0 {
		t.Errorf("unexpected length")
	}
	el.append([]pb.Entry{
		{Index: 1, Term: 1},
		{Index: 2, Term: 1},
		{Index: 3, Term: 2},
		{Index: 4, Term: 3},
	})
	tests := []struct {
		committed  uint64
		length     int
		firstIndex uint64
	}{
		{0, 4, 1},
		{1, 3, 2},
		{2, 2, 3},
		{3, 1, 4},
		{4, 0, 0},
	}
	for idx, tt := range tests {
		el.committed = tt.committed
		ents = el.getUncommittedEntries()
		if len(ents) != tt.length {
			t.Errorf("unexpected length")
		}
		if len(ents) > 0 {
			if ents[0].Index != tt.firstIndex {
				t.Errorf("%d, first index %d, want %d", idx, ents[0].Index, tt.firstIndex)
			}
		}
	}
}
````

## File: internal/raft/logentry.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raft

import (
	"github.com/cockroachdb/errors"

	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/settings"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

var (
	maxEntriesToApplySize = settings.Soft.MaxApplyEntrySize
)

// ErrCompacted is the error returned to indicate that the requested entries
// are no longer in the LogDB due to compaction.
var ErrCompacted = errors.New("entry compacted")

// ErrSnapshotOutOfDate is the error returned to indicate that the concerned
// snapshot is considered as out of date.
var ErrSnapshotOutOfDate = errors.New("snapshot out of date")

// ErrUnavailable is the error returned to indicate that requested entries are
// not available in LogDB.
var ErrUnavailable = errors.New("entry unavailable")

// ILogDB is a read-only interface to the underlying persistent storage to
// allow the raft package to access raft state, entries, snapshots stored in
// the persistent storage. Entries stored in the persistent storage accessible
// via ILogDB is usually not required in normal cases.
type ILogDB interface {
	// GetRange returns the range of the entries in LogDB.
	GetRange() (uint64, uint64)
	// SetRange updates the ILogDB to extend the entry range known to the ILogDB.
	SetRange(index uint64, length uint64)
	// NodeState returns the state of the node persistent in LogDB.
	NodeState() (pb.State, pb.Membership)
	// SetState sets the persistent state known to ILogDB.
	SetState(ps pb.State)
	// CreateSnapshot sets the snapshot known to ILogDB
	CreateSnapshot(ss pb.Snapshot) error
	// ApplySnapshot makes the snapshot known to ILogDB and also update the entry
	// range known to ILogDB.
	ApplySnapshot(ss pb.Snapshot) error
	// Term returns the entry term of the specified entry.
	Term(index uint64) (uint64, error)
	// Entries returns entries between [low, high) with total size of entries
	// limited to maxSize bytes.
	Entries(low uint64, high uint64, maxSize uint64) ([]pb.Entry, error)
	// Snapshot returns the metadata for the most recent snapshot known to the
	// LogDB.
	Snapshot() pb.Snapshot
	// Compact performs entry range compaction on ILogDB up to the entry
	// specified by index.
	Compact(index uint64) error
	// Append makes the given entries known to the ILogDB instance. This is
	// usually not how entries are persisted.
	Append(entries []pb.Entry) error
}

// entryLog is the entry log used by Raft. It splits entries into two parts -
// those likely to be access in immediate future and those unlikely to be used
// any time soon in normal fast path.
type entryLog struct {
	logdb     ILogDB
	inmem     inMemory
	committed uint64
	// committed entries already returned as Updated to be applied.
	processed uint64
}

func newEntryLog(logdb ILogDB, rl *server.InMemRateLimiter) *entryLog {
	firstIndex, lastIndex := logdb.GetRange()
	l := &entryLog{
		logdb:     logdb,
		inmem:     newInMemory(lastIndex, rl),
		committed: firstIndex - 1,
		processed: firstIndex - 1,
	}
	return l
}

func (l *entryLog) firstIndex() uint64 {
	index, ok := l.inmem.getSnapshotIndex()
	if ok {
		return index + 1
	}

	index, _ = l.logdb.GetRange()
	return index
}

func (l *entryLog) lastIndex() uint64 {
	if index, ok := l.inmem.getLastIndex(); ok {
		return index
	}
	_, index := l.logdb.GetRange()
	return index

}

func (l *entryLog) termEntryRange() (uint64, uint64) {
	// for firstIndex(), when it is determined by the inmem, what we actually
	// want to return is the snapshot index, l.firstIndex() - 1 is thus required
	// when it is determined by the logdb component, other than actual entries
	// we have a marker entry with known index/term (but not type or data),
	// use l.firstIndex()-1 to include this marker element.
	// as we don't have the type/data of the marker entry, it is only used in
	// term(), we can not pull its value and send it to the RSM for execution.
	return l.firstIndex() - 1, l.lastIndex()
}

func (l *entryLog) entryRange() (uint64, uint64, bool) {
	if l.inmem.snapshot != nil && len(l.inmem.entries) == 0 {
		return 0, 0, false
	}
	return l.firstIndex(), l.lastIndex(), true
}

func (l *entryLog) lastTerm() (uint64, error) {
	t, err := l.term(l.lastIndex())
	if err != nil {
		return 0, err
	}
	return t, nil
}

func (l *entryLog) term(index uint64) (uint64, error) {
	first, last := l.termEntryRange()
	if index < first || index > last {
		return 0, nil
	}
	if t, ok := l.inmem.getTerm(index); ok {
		return t, nil
	}
	t, err := l.logdb.Term(index)
	if err != nil {
		return 0, err
	}
	return t, nil
}

func (l *entryLog) checkBound(low uint64, high uint64) error {
	if low > high {
		plog.Panicf("input low %d > high %d", low, high)
	}
	first, last, ok := l.entryRange()
	if !ok {
		return ErrCompacted
	}
	if low < first {
		return ErrCompacted
	}
	if high > last+1 {
		plog.Panicf("requested range [%d,%d) is out of bound [%d,%d]",
			low, high, first, last)
	}
	return nil
}

func (l *entryLog) getUncommittedEntries() []pb.Entry {
	lastIndex := l.inmem.markerIndex + uint64(len(l.inmem.entries))
	return l.getEntriesFromInMem([]pb.Entry{}, l.committed+1, lastIndex)
}

func (l *entryLog) getEntriesFromLogDB(low uint64,
	high uint64, maxSize uint64) ([]pb.Entry, bool, error) {
	if low >= l.inmem.markerIndex {
		return nil, true, nil
	}

	upperBound := min(high, l.inmem.markerIndex)
	ents, err := l.logdb.Entries(low, upperBound, maxSize)
	if err != nil {
		return nil, false, err
	}
	if uint64(len(ents)) > upperBound-low {
		plog.Panicf("uint64(len(ents)) > upperBound-low")
	}
	return ents, uint64(len(ents)) == upperBound-low, nil
}

func (l *entryLog) getEntriesFromInMem(ents []pb.Entry,
	low uint64, high uint64) []pb.Entry {
	if high <= l.inmem.markerIndex {
		return ents
	}
	lowerBound := max(low, l.inmem.markerIndex)
	inmem := l.inmem.getEntries(lowerBound, high)
	if len(inmem) > 0 {
		if len(ents) > 0 {
			checkEntriesToAppend(ents, inmem)
			return append(ents, inmem...)
		}
		return inmem
	}
	return ents
}

func (l *entryLog) getEntries(low uint64,
	high uint64, maxSize uint64) ([]pb.Entry, error) {
	err := l.checkBound(low, high)
	if err != nil {
		return nil, err
	}
	if low == high {
		return nil, nil
	}
	ents, checkInMem, err := l.getEntriesFromLogDB(low, high, maxSize)
	if err != nil {
		return nil, err
	}
	if !checkInMem {
		return ents, nil
	}
	return limitSize(l.getEntriesFromInMem(ents, low, high), maxSize), nil
}

func (l *entryLog) entries(start uint64, maxSize uint64) ([]pb.Entry, error) {
	if start > l.lastIndex() {
		return nil, nil
	}
	return l.getEntries(start, l.lastIndex()+1, maxSize)
}

// TODO: double check whether the inmem.snapshot can be used in upper layer
func (l *entryLog) snapshot() pb.Snapshot {
	if l.inmem.snapshot != nil {
		return *l.inmem.snapshot
	}
	return l.logdb.Snapshot()
}

func (l *entryLog) firstNotAppliedIndex() uint64 {
	return max(l.processed+1, l.firstIndex())
}

func (l *entryLog) toApplyIndexLimit() uint64 {
	return l.committed + 1
}

func (l *entryLog) hasEntriesToApply() bool {
	return l.toApplyIndexLimit() > l.firstNotAppliedIndex()
}

func (l *entryLog) hasMoreEntriesToApply(appliedTo uint64) bool {
	return l.committed > appliedTo
}

func (l *entryLog) entriesToApply() ([]pb.Entry, error) {
	return l.getEntriesToApply(maxEntriesToApplySize)
}

func (l *entryLog) getEntriesToApply(limit uint64) ([]pb.Entry, error) {
	if l.hasEntriesToApply() {
		ents, err := l.getEntries(l.firstNotAppliedIndex(),
			l.toApplyIndexLimit(), limit)
		if err != nil {
			return nil, err
		}
		return ents, nil
	}
	return nil, nil
}

func (l *entryLog) getCommittedEntries(low uint64,
	high uint64, maxSize uint64) ([]pb.Entry, error) {
	if low < l.firstIndex() || low > l.committed {
		return nil, ErrCompacted
	}
	high = min(high, l.committed+1)
	if low == high {
		return nil, nil
	}
	return l.getEntries(low, high, maxSize)
}

func (l *entryLog) entriesToSave() []pb.Entry {
	return l.inmem.entriesToSave()
}

func (l *entryLog) tryAppend(index uint64, ents []pb.Entry) (bool, error) {
	conflictIndex, err := l.getConflictIndex(ents)
	if err != nil {
		return false, err
	}
	if conflictIndex != 0 {
		if conflictIndex <= l.committed {
			plog.Panicf("entry %d conflicts with committed entry, committed %d",
				conflictIndex, l.committed)
		}
		l.append(ents[conflictIndex-index-1:])
		return true, nil
	}
	return false, nil
}

func (l *entryLog) append(entries []pb.Entry) {
	if len(entries) == 0 {
		return
	}
	if entries[0].Index <= l.committed {
		plog.Panicf("committed entries being changed, committed %d, first idx %d",
			l.committed, entries[0].Index)
	}
	l.inmem.merge(entries)
}

func (l *entryLog) getConflictIndex(entries []pb.Entry) (uint64, error) {
	for _, e := range entries {
		match, err := l.matchTerm(e.Index, e.Term)
		if err != nil {
			return 0, err
		}
		if !match {
			return e.Index, nil
		}
	}
	return 0, nil
}

func (l *entryLog) commitTo(index uint64) {
	if index <= l.committed {
		return
	}
	if index > l.lastIndex() {
		plog.Panicf("invalid commitTo index %d, lastIndex() %d",
			index, l.lastIndex())
	}
	if index < l.committed {
		plog.Panicf("committed value moving backwards index %d, committed %d",
			index, l.committed)
	}
	l.committed = index
}

func (l *entryLog) commitUpdate(cu pb.UpdateCommit) {
	l.inmem.commitUpdate(cu)
	if cu.Processed > 0 {
		if cu.Processed < l.processed || cu.Processed > l.committed {
			plog.Panicf("invalid ApplyReturnedTo %d, current applied %d, committed %d",
				cu.Processed, l.processed, l.committed)
		}
		l.processed = cu.Processed
	}
	if cu.LastApplied > 0 {
		if cu.LastApplied > l.committed {
			plog.Panicf("invalid last applied %d, committed %d",
				cu.LastApplied, l.committed)
		}
		if cu.LastApplied > l.processed {
			plog.Panicf("invalid last applied %d, processed %d",
				cu.LastApplied, l.processed)
		}
		l.inmem.appliedLogTo(cu.LastApplied)
	}
}

func (l *entryLog) matchTerm(index uint64, term uint64) (bool, error) {
	lt, err := l.term(index)
	if err != nil {
		return false, err
	}
	return lt == term, nil
}

func (l *entryLog) upToDate(index uint64, term uint64) (bool, error) {
	lastTerm, err := l.term(l.lastIndex())
	if err != nil {
		return false, err
	}
	if term >= lastTerm {
		if term > lastTerm {
			return true, nil
		}
		return index >= l.lastIndex(), nil
	}
	return false, nil
}

func (l *entryLog) tryCommit(index uint64, term uint64) (bool, error) {
	if index <= l.committed {
		return false, nil
	}
	lterm, err := l.term(index)
	if errors.Is(err, ErrCompacted) {
		lterm = 0
	} else if err != nil {
		return false, err
	}
	if index > l.committed && lterm == term {
		l.commitTo(index)
		return true, nil
	}
	return false, nil
}

func (l *entryLog) restore(s pb.Snapshot) {
	l.inmem.restore(s)
	if s.Index < l.committed {
		plog.Panicf("committed value moving backwards ss index %d, committed %d",
			s.Index, l.committed)
	}
	l.committed = s.Index
	l.processed = s.Index
}
````

## File: internal/raft/monkey.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//go:build dragonboat_monkeytest
// +build dragonboat_monkeytest

package raft

import (
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/logger"
)

var (
	mplog = logger.GetLogger("raft-mt")
)

// DumpRaftInfoToLog prints the raft state to log for debugging purposes.
func (p *Peer) DumpRaftInfoToLog(addrMap map[uint64]string) {
	p.raft.dumpRaftInfoToLog(addrMap)
}

func (p *Peer) GetInMemLogSize() uint64 {
	ents := p.raft.log.inmem.entries
	if len(ents) > 0 {
		if ents[0].Index == p.raft.applied {
			ents = ents[1:]
		}
	}
	return getEntrySliceInMemSize(ents)
}

func (p *Peer) GetRateLimiter() *server.InMemRateLimiter {
	return p.raft.log.inmem.rl
}

func (r *raft) dumpRaftInfoToLog(addrs map[uint64]string) {
	var flag string
	if r.leaderID == r.replicaID {
		flag = "***"
	} else {
		flag = "###"
	}
	mplog.Infof("%s Raft node %s, %d remote nodes",
		flag, r.describe(), len(r.remotes))
	for id, rp := range r.remotes {
		if v, ok := addrs[id]; !ok {
			mplog.Infof("---> node %d is missing", id)
		} else {
			mplog.Infof(" %s,addr:%s,match:%d,next:%d,state:%s,paused:%v,ra:%v,ps:%d",
				ReplicaID(id), v, rp.match, rp.next, rp.state, rp.isPaused(),
				rp.isActive(), rp.snapshotIndex)
		}
	}
}
````

## File: internal/raft/NOTICE
````
Dragonboat Project (https://github.com/lni/dragonboat)
Copyright 2017,2018 Lei Ni (nilei81@gmail.com)

This product includes software developed by Lei Ni (nilei81@gmail.com).
````

## File: internal/raft/NOTICE.etcd
````
CoreOS Project
Copyright 2014 CoreOS, Inc

This product includes software developed at CoreOS, Inc.
(http://www.coreos.com/).
````

## File: internal/raft/peer_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//
// Some tests in this file were ported from etcd raft.
// updates have been made to reflect interface & implementation differences
//
// Copyright 2015 The etcd Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raft

import (
	"bytes"
	"reflect"
	"testing"

	"github.com/stretchr/testify/assert"

	"github.com/lni/dragonboat/v4/config"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

func ne(err error, t *testing.T) {
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
}

// Campaign starts the campaign procedure.
func (p *Peer) Campaign() {
	if err := p.raft.Handle(pb.Message{Type: pb.Election}); err != nil {
		panic(err)
	}
}

func getTestMembership(nodes []uint64) pb.Membership {
	m := pb.Membership{
		Addresses: make(map[uint64]string),
		Removed:   make(map[uint64]bool),
	}
	for _, nid := range nodes {
		m.Addresses[nid] = ""
	}
	return m
}

func TestRaftAPINodeStep(t *testing.T) {
	for i := range pb.MessageType_name {
		s := NewTestLogDB()
		rawNode := Launch(newTestConfig(1, 10, 1), s, nil, []PeerAddress{{ReplicaID: 1}}, true, true)
		rawNode.raft.preVote = true
		rawNode.raft.hasNotAppliedConfigChange = rawNode.raft.testOnlyHasConfigChangeToApply

		msgt := pb.MessageType(i)
		// stepping on non-local messages should be fine
		if !isLocalMessageType(msgt) &&
			msgt != pb.SnapshotReceived && msgt != pb.TimeoutNow {
			ne(rawNode.Handle(pb.Message{Type: msgt, Term: rawNode.raft.term}), t)
		}
	}
}

func TestRaftAPIRequestLeaderTransfer(t *testing.T) {
	s := NewTestLogDB()
	p := Launch(newTestConfig(1, 10, 1), s, nil, []PeerAddress{{ReplicaID: 1}}, true, true)
	ne(p.RequestLeaderTransfer(1), t)
}

func TestRaftAPIRTT(t *testing.T) {
	s := NewTestLogDB()
	p := Launch(newTestConfig(1, 10, 1), s, nil, []PeerAddress{{ReplicaID: 1}}, true, true)
	tick := p.raft.electionTick
	ne(p.Tick(), t)
	if p.raft.electionTick != tick+1 {
		t.Errorf("tick not updated")
	}
	ne(p.QuiescedTick(), t)
	if p.raft.electionTick != tick+2 {
		t.Errorf("tick not updated 2")
	}
}

func TestRaftAPIReportUnreachable(t *testing.T) {
	s := NewTestLogDB()
	p := Launch(newTestConfig(1, 10, 1), s, nil, []PeerAddress{
		{ReplicaID: 1, Address: "1"},
		{ReplicaID: 2, Address: "2"},
	}, true, true)
	if len(p.raft.remotes) != 2 {
		t.Errorf("remotes len %d, want 2", len(p.raft.remotes))
	}
	p.raft.state = leader
	p.raft.remotes[2].state = remoteReplicate
	ne(p.ReportUnreachableNode(2), t)
	if p.raft.remotes[2].state != remoteRetry {
		t.Errorf("remote not set to retry state")
	}
}

func TestRaftAPIReportSnapshotStatus(t *testing.T) {
	s := NewTestLogDB()
	p := Launch(newTestConfig(1, 10, 1), s, nil, []PeerAddress{
		{ReplicaID: 1, Address: "1"},
		{ReplicaID: 2, Address: "2"},
	}, true, true)
	if len(p.raft.remotes) != 2 {
		t.Errorf("remotes len %d, want 2", len(p.raft.remotes))
	}
	p.raft.state = leader
	p.raft.remotes[2].state = remoteSnapshot
	ne(p.ReportSnapshotStatus(2, false), t)
	if p.raft.remotes[2].state != remoteWait {
		t.Errorf("remote not set to wait, %s", p.raft.remotes[2].state)
	}
}

func testRaftAPIProposeAndConfigChange(cct pb.ConfigChangeType, nid uint64, t *testing.T) {
	s := NewTestLogDB()
	var err error
	rawNode := Launch(newTestConfig(1, 10, 1), s, nil, []PeerAddress{{ReplicaID: 1}}, true, true)
	rawNode.raft.hasNotAppliedConfigChange = rawNode.raft.testOnlyHasConfigChangeToApply
	ud, err := rawNode.GetUpdate(true, 0)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if err := s.Append(ud.EntriesToSave); err != nil {
		t.Fatalf("%v", err)
	}
	rawNode.Commit(ud)

	rawNode.Campaign()
	proposed := false
	var (
		lastIndex uint64
		ccdata    []byte
	)
	for {
		ud, err = rawNode.GetUpdate(true, 0)
		if err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		if err := s.Append(ud.EntriesToSave); err != nil {
			t.Fatalf("%v", err)
		}
		// Once we are the leader, propose a command and a ConfigChange.
		if !proposed && rawNode.raft.leaderID == rawNode.raft.replicaID {
			ne(rawNode.ProposeEntries([]pb.Entry{{Cmd: []byte("somedata")}}), t)
			cc := pb.ConfigChange{Type: cct, ReplicaID: nid}
			ccdata, err = cc.Marshal()
			if err != nil {
				t.Fatal(err)
			}
			ne(rawNode.ProposeConfigChange(cc, 128), t)

			proposed = true
		}
		rawNode.Commit(ud)

		// Exit when we have four entries: one ConfigChange, one no-op for the election,
		// our proposed command and proposed ConfigChange.
		_, lastIndex = s.GetRange()
		if lastIndex >= 4 {
			break
		}
	}

	entries, err := s.Entries(lastIndex-1, lastIndex+1, noLimit)
	if err != nil {
		t.Fatal(err)
	}
	if len(entries) != 2 {
		t.Fatalf("len(entries) = %d, want %d", len(entries), 2)
	}
	if !bytes.Equal(entries[0].Cmd, []byte("somedata")) {
		t.Errorf("entries[0].Cmd = %v, want %v", entries[0].Cmd, []byte("somedata"))
	}
	if entries[1].Type != pb.ConfigChangeEntry {
		t.Fatalf("type = %v, want %v", entries[1].Type, pb.ConfigChangeEntry)
	}
	if entries[1].Key != 128 {
		t.Errorf("key not recorded")
	}
	if !bytes.Equal(entries[1].Cmd, ccdata) {
		t.Errorf("data = %v, want %v", entries[1].Cmd, ccdata)
	}
}

func TestRaftAPIProposeAndConfigChange(t *testing.T) {
	testRaftAPIProposeAndConfigChange(pb.AddNode, NoLeader, t)
	testRaftAPIProposeAndConfigChange(pb.AddNode, 2, t)
	testRaftAPIProposeAndConfigChange(pb.RemoveNode, 2, t)
	testRaftAPIProposeAndConfigChange(pb.AddNonVoting, 2, t)
}

func TestGetUpdateIncludeLastAppliedValue(t *testing.T) {
	s := NewTestLogDB()
	rawNode := Launch(newTestConfig(1, 10, 1), s, nil, []PeerAddress{{ReplicaID: 1}}, true, true)
	ud, err := rawNode.GetUpdate(true, 1232)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if ud.LastApplied != 1232 {
		t.Errorf("unexpected last applied value %d, want 1232", ud.LastApplied)
	}
	uc := getUpdateCommit(ud)
	if uc.LastApplied != 1232 {
		t.Errorf("unexpected last applied value %d, want 1232", uc.LastApplied)
	}
}

func TestRaftMoreEntriesToApplyControl(t *testing.T) {
	s := NewTestLogDB()
	rawNode := Launch(newTestConfig(1, 10, 1), s, nil, []PeerAddress{{ReplicaID: 1}}, true, true)
	rawNode.raft.hasNotAppliedConfigChange = rawNode.raft.testOnlyHasConfigChangeToApply
	ud, err := rawNode.GetUpdate(true, 0)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if err := s.Append(ud.EntriesToSave); err != nil {
		t.Fatalf("%v", err)
	}
	rawNode.Commit(ud)

	rawNode.Campaign()
	for {
		ud, err = rawNode.GetUpdate(true, 0)
		if err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		if err := s.Append(ud.EntriesToSave); err != nil {
			t.Fatalf("%v", err)
		}
		if rawNode.raft.leaderID == rawNode.raft.replicaID {
			rawNode.Commit(ud)
			break
		}
		rawNode.Commit(ud)
	}
	cc := pb.ConfigChange{Type: pb.AddNode, ReplicaID: 1}
	ne(rawNode.ProposeConfigChange(cc, 128), t)
	if !rawNode.HasUpdate(true) {
		t.Errorf("HasUpdate returned false")
	}
	ud, err = rawNode.GetUpdate(false, 0)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if len(ud.CommittedEntries) > 0 {
		t.Errorf("unexpected returned %d committed entries", len(ud.CommittedEntries))
	}
	ud, err = rawNode.GetUpdate(true, 0)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if len(ud.CommittedEntries) == 0 {
		t.Errorf("failed to returned committed entries")
	}
}

func TestRaftAPIProposeAddDuplicateNode(t *testing.T) {
	s := NewTestLogDB()
	rawNode := Launch(newTestConfig(1, 10, 1), s, nil, []PeerAddress{{ReplicaID: 1}}, true, true)
	rawNode.raft.hasNotAppliedConfigChange = rawNode.raft.testOnlyHasConfigChangeToApply
	ud, err := rawNode.GetUpdate(true, 0)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if err := s.Append(ud.EntriesToSave); err != nil {
		t.Fatalf("%v", err)
	}
	rawNode.Commit(ud)

	rawNode.Campaign()
	for {
		ud, err = rawNode.GetUpdate(true, 0)
		if err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		if err := s.Append(ud.EntriesToSave); err != nil {
			t.Fatalf("%v", err)
		}
		if rawNode.raft.leaderID == rawNode.raft.replicaID {
			rawNode.Commit(ud)
			break
		}
		rawNode.Commit(ud)
	}

	proposeConfigChangeAndApply := func(cc pb.ConfigChange, key uint64) {
		ne(rawNode.ProposeConfigChange(cc, key), t)
		ud, err = rawNode.GetUpdate(true, 0)
		if err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		if err := s.Append(ud.EntriesToSave); err != nil {
			t.Fatalf("%v", err)
		}
		for _, entry := range ud.CommittedEntries {
			if entry.Type == pb.ConfigChangeEntry {
				var cc pb.ConfigChange
				if err := cc.Unmarshal(entry.Cmd); err != nil {
					t.Fatalf("%v", err)
				}
				ne(rawNode.ApplyConfigChange(cc), t)
			}
		}
		rawNode.Commit(ud)
	}

	cc1 := pb.ConfigChange{Type: pb.AddNode, ReplicaID: 1}
	ccdata1, err := cc1.Marshal()
	if err != nil {
		t.Fatal(err)
	}
	proposeConfigChangeAndApply(cc1, 128)

	// try to add the same node again
	proposeConfigChangeAndApply(cc1, 129)

	// the new node join should be ok
	cc2 := pb.ConfigChange{Type: pb.AddNode, ReplicaID: 2}
	ccdata2, err := cc2.Marshal()
	if err != nil {
		t.Fatal(err)
	}
	proposeConfigChangeAndApply(cc2, 130)

	_, lastIndex := s.GetRange()

	// the last three entries should be: ConfigChange cc1, cc1, cc2
	entries, err := s.Entries(lastIndex-2, lastIndex+1, noLimit)
	if err != nil {
		t.Fatal(err)
	}
	if len(entries) != 3 {
		t.Fatalf("len(entries) = %d, want %d", len(entries), 3)
	}
	if !bytes.Equal(entries[0].Cmd, ccdata1) {
		t.Errorf("entries[0].Cmd = %v, want %v", entries[0].Cmd, ccdata1)
	}
	if !bytes.Equal(entries[2].Cmd, ccdata2) {
		t.Errorf("entries[2].Cmd = %v, want %v", entries[2].Cmd, ccdata2)
	}
	cc3 := pb.ConfigChange{Type: pb.RemoveNode, ReplicaID: 2}
	ne(rawNode.ApplyConfigChange(cc3), t)
	cc4 := pb.ConfigChange{Type: pb.AddNonVoting, ReplicaID: 3}
	ne(rawNode.ApplyConfigChange(cc4), t)
	cc5 := pb.ConfigChange{Type: pb.RemoveNode, ReplicaID: NoLeader}
	ne(rawNode.ApplyConfigChange(cc5), t)
}

func TestRaftAPIRejectConfigChange(t *testing.T) {
	s := NewTestLogDB()
	p := Launch(newTestConfig(1, 10, 1), s, nil, []PeerAddress{{ReplicaID: 1}}, true, true)
	p.raft.setPendingConfigChange()
	if !p.raft.hasPendingConfigChange() {
		t.Errorf("pending config change flag not set")
	}
	ne(p.RejectConfigChange(), t)
	if p.raft.hasPendingConfigChange() {
		t.Errorf("pending config change flag not cleared")
	}
}

func TestRaftAPINotifyRaftLastApplied(t *testing.T) {
	s := NewTestLogDB()
	p := Launch(newTestConfig(1, 10, 1), s, nil, []PeerAddress{{ReplicaID: 1}}, true, true)
	p.NotifyRaftLastApplied(123)
	if p.raft.getApplied() != 123 {
		t.Errorf("applied not set")
	}
}

func TestRaftAPIReadIndex(t *testing.T) {
	msgs := []pb.Message{}
	appendStep := func(r *raft, m pb.Message) error {
		msgs = append(msgs, m)
		return nil
	}
	wrs := []pb.ReadyToRead{{Index: uint64(1), SystemCtx: getTestSystemCtx(12345)}}

	s := NewTestLogDB()
	c := newTestConfig(1, 10, 1)
	rawNode := Launch(c, s, nil, []PeerAddress{{ReplicaID: 1}}, true, true)
	rawNode.raft.hasNotAppliedConfigChange = rawNode.raft.testOnlyHasConfigChangeToApply
	rawNode.raft.readyToRead = wrs
	// ensure the ReadyToReads can be read out
	hasReady := rawNode.HasUpdate(true)
	if !hasReady {
		t.Errorf("HasReady() returns %t, want %t", hasReady, true)
	}
	ud, err := rawNode.GetUpdate(true, 0)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if !reflect.DeepEqual(ud.ReadyToReads, wrs) {
		t.Errorf("ReadyToReads = %d, want %d", ud.ReadyToReads, wrs)
	}
	if err := s.Append(ud.EntriesToSave); err != nil {
		t.Fatalf("%v", err)
	}
	rawNode.Commit(ud)
	// ensure raft.readyToRead is reset after advance
	if len(rawNode.raft.readyToRead) > 0 {
		t.Errorf("readyToRead = %v", rawNode.raft.readyToRead)
	}

	wrequestCtx := getTestSystemCtx(23456)
	rawNode.Campaign()
	for {
		ud, err = rawNode.GetUpdate(true, 0)
		if err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		if err := s.Append(ud.EntriesToSave); err != nil {
			t.Fatalf("%v", err)
		}
		if rawNode.raft.leaderID == rawNode.raft.replicaID {
			rawNode.Commit(ud)
			// Once we are the leader, issue a ReadIndex request
			rawNode.raft.handle = appendStep
			ne(rawNode.ReadIndex(wrequestCtx), t)
			break
		}
		rawNode.Commit(ud)
	}
	// ensure that MTReadIndex message is sent to the underlying raft
	if len(msgs) != 1 {
		t.Fatalf("len(msgs) = %d, want %d", len(msgs), 1)
	}
	if msgs[0].Type != pb.ReadIndex {
		t.Errorf("msg type = %d, want %d", msgs[0].Type, pb.ReadIndex)
	}
	if msgs[0].Hint != wrequestCtx.Low || msgs[0].HintHigh != wrequestCtx.High {
		t.Errorf("data = %d, want %d", msgs[0].Hint, wrequestCtx)
	}
}

func TestRaftAPIStatus(t *testing.T) {
	storage := NewTestLogDB()
	rawNode := Launch(newTestConfig(1, 10, 1), storage, nil, []PeerAddress{{ReplicaID: 1}}, true, true)
	rawNode.raft.hasNotAppliedConfigChange = rawNode.raft.testOnlyHasConfigChangeToApply
	status := getLocalStatus(rawNode.raft)
	if status.ReplicaID != 1 {
		t.Errorf("expected status struct, got nil")
	}
}

func TestRaftAPIInvalidReplicaIDCausePanicInLaunch(t *testing.T) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not called")
	}()
	Launch(config.Config{}, nil, nil, nil, true, true)
}

func TestRaftAPIInvalidInputToLaunchCausePanic(t *testing.T) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not called")
	}()
	storage := NewTestLogDB()
	Launch(newTestConfig(1, 10, 1), storage, nil, []PeerAddress{}, true, true)
}

func TestRaftAPIDuplicatedAddressCausePanicInLaunch(t *testing.T) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not called")
	}()
	storage := NewTestLogDB()
	Launch(newTestConfig(1, 10, 1), storage, nil, []PeerAddress{
		{ReplicaID: 1, Address: "111"},
		{ReplicaID: 2, Address: "111"},
	}, true, true)
}

func TestRaftAPILaunch(t *testing.T) {
	cc := pb.ConfigChange{Type: pb.AddNode, ReplicaID: 1, Initialize: true}
	ccdata, err := cc.Marshal()
	if err != nil {
		t.Fatalf("unexpected marshal error: %v", err)
	}
	wants := []pb.Update{
		{
			ReplicaID: 1,
			State:     pb.State{Term: 1, Commit: 1, Vote: 0},
			EntriesToSave: []pb.Entry{
				{Type: pb.ConfigChangeEntry, Term: 1, Index: 1, Cmd: ccdata},
			},
			CommittedEntries: []pb.Entry{
				{Type: pb.ConfigChangeEntry, Term: 1, Index: 1, Cmd: ccdata},
			},
			UpdateCommit: pb.UpdateCommit{Processed: 1, StableLogTo: 1, StableLogTerm: 1},
			LeaderUpdate: pb.LeaderUpdate{LeaderID: 0, Term: 1},
		},
		{
			ReplicaID:        1,
			State:            pb.State{Term: 2, Commit: 3, Vote: 1},
			EntriesToSave:    []pb.Entry{{Term: 2, Index: 3, Cmd: []byte("foo")}},
			CommittedEntries: []pb.Entry{{Term: 2, Index: 3, Cmd: []byte("foo")}},
			UpdateCommit:     pb.UpdateCommit{Processed: 3, StableLogTo: 3, StableLogTerm: 2},
		},
	}

	storage := NewTestLogDB()
	rawNode := Launch(newTestConfig(1, 10, 1), storage, nil, []PeerAddress{{ReplicaID: 1}}, true, true)
	rawNode.raft.hasNotAppliedConfigChange = rawNode.raft.testOnlyHasConfigChangeToApply
	ud, err := rawNode.GetUpdate(true, 0)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	ud.Messages = nil
	if !reflect.DeepEqual(ud, wants[0]) {
		t.Fatalf("#%d: g = %+v,\n             w   %+v", 1, ud, wants[0])
	} else {
		if err := storage.Append(ud.EntriesToSave); err != nil {
			t.Fatalf("%v", err)
		}
		rawNode.Commit(ud)
	}
	if err := storage.Append(ud.EntriesToSave); err != nil {
		t.Fatalf("%v", err)
	}
	rawNode.Commit(ud)

	rawNode.Campaign()
	ud, err = rawNode.GetUpdate(true, 0)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if err := storage.Append(ud.EntriesToSave); err != nil {
		t.Fatalf("%v", err)
	}
	rawNode.Commit(ud)

	ne(rawNode.ProposeEntries([]pb.Entry{{Cmd: []byte("foo")}}), t)
	ud, err = rawNode.GetUpdate(true, 0)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	ud.Messages = nil
	if !reflect.DeepEqual(ud, wants[1]) {
		t.Errorf("#%d: g = %+v,\n             w   %+v", 2, ud, wants[1])
	} else {
		if err := storage.Append(ud.EntriesToSave); err != nil {
			t.Fatalf("%v", err)
		}
		rawNode.Commit(ud)
	}

	if rawNode.HasUpdate(true) {
		t.Errorf("unexpected Ready")
	}
}

func TestRaftAPIRestart(t *testing.T) {
	entries := []pb.Entry{
		{Term: 1, Index: 1},
		{Term: 1, Index: 2, Cmd: []byte("foo")},
	}
	st := pb.State{Term: 1, Commit: 1}

	want := pb.Update{
		ReplicaID: 1,
		State:     emptyState,
		// commit up to commit index in st
		CommittedEntries: entries[:st.Commit],
		UpdateCommit:     pb.UpdateCommit{Processed: 1},
		FastApply:        true,
		LeaderUpdate:     pb.LeaderUpdate{LeaderID: 0, Term: 1},
	}

	storage := NewTestLogDB()
	storage.SetState(st)
	if err := storage.Append(entries); err != nil {
		t.Fatalf("%v", err)
	}
	rawNode := Launch(newTestConfig(1, 10, 1), storage, nil, nil, true, false)
	rawNode.raft.hasNotAppliedConfigChange = rawNode.raft.testOnlyHasConfigChangeToApply
	ud, err := rawNode.GetUpdate(true, 0)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	ud.Messages = nil
	if !reflect.DeepEqual(ud, want) {
		t.Errorf("g = %+v,\n             w   %+v", ud, want)
	}
	rawNode.Commit(ud)
	if rawNode.HasUpdate(true) {
		t.Errorf("unexpected Ready")
	}
}

func TestRaftAPIRestartFromSnapshot(t *testing.T) {
	snap := pb.Snapshot{
		Membership: getTestMembership([]uint64{1, 2}),
		Index:      2,
		Term:       1,
	}
	entries := []pb.Entry{
		{Term: 1, Index: 3, Cmd: []byte("foo")},
	}
	st := pb.State{Term: 1, Commit: 3}

	want := pb.Update{
		ReplicaID: 1,
		State:     emptyState,
		// commit up to commit index in st
		CommittedEntries: entries,
		UpdateCommit:     pb.UpdateCommit{Processed: 3},
		FastApply:        true,
		LeaderUpdate:     pb.LeaderUpdate{LeaderID: 0, Term: 1},
	}

	s := NewTestLogDB()
	s.SetState(st)
	if err := s.ApplySnapshot(snap); err != nil {
		t.Fatalf("%v", err)
	}
	if err := s.Append(entries); err != nil {
		t.Fatalf("%v", err)
	}
	rawNode := Launch(newTestConfig(1, 10, 1), s, nil, nil, true, false)
	rawNode.raft.hasNotAppliedConfigChange = rawNode.raft.testOnlyHasConfigChangeToApply
	ud, err := rawNode.GetUpdate(true, 0)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	ud.Messages = nil
	if !reflect.DeepEqual(ud, want) {
		t.Errorf("g = %+v,\n             w   %+v", ud, want)
	} else {
		rawNode.Commit(ud)
	}
	if rawNode.HasUpdate(true) {
		t.Errorf("unexpected Ready: %+v", rawNode.HasUpdate(true))
	}
}

func TestRaftAPIStepOnLocalMessageWillPanic(t *testing.T) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not triggered")
	}()
	storage := NewTestLogDB()
	p := Launch(newTestConfig(1, 10, 1), storage, nil, []PeerAddress{{ReplicaID: 1}}, true, true)
	ne(p.Handle(pb.Message{Type: pb.LocalTick}), t)
}

func TestRaftAPIGetUpdateCommit(t *testing.T) {
	ud := pb.Update{
		CommittedEntries: []pb.Entry{
			{Index: 100, Term: 2},
			{Index: 101, Term: 3},
		},
		EntriesToSave: []pb.Entry{
			{Index: 102, Term: 3},
			{Index: 103, Term: 4},
		},
		Snapshot:    pb.Snapshot{Index: 105},
		LastApplied: 99,
	}
	uc := getUpdateCommit(ud)
	if uc.StableSnapshotTo != 105 {
		t.Errorf("stable snapshot to incorrect")
	}
	if uc.Processed != 105 {
		t.Errorf("applied to")
	}
	if uc.StableLogTo != 103 || uc.StableLogTerm != 4 {
		t.Errorf("stable log to/term")
	}
	if uc.LastApplied != 99 {
		t.Errorf("last applied %d, want 99", uc.LastApplied)
	}
}

func TestCheckLaunchRequest(t *testing.T) {
	expectPanicFn := func(f func()) {
		defer func() {
			if r := recover(); r == nil {
				t.Fatalf("panic not triggered")
			}
		}()
		f()
	}
	expectPanicFn(func() {
		checkLaunchRequest(config.Config{}, nil, false, false)
	})
	expectPanicFn(func() {
		checkLaunchRequest(config.Config{ReplicaID: 1}, nil, true, true)
	})
	addr := make([]PeerAddress, 0)
	addr = append(addr, PeerAddress{Address: "1"})
	addr = append(addr, PeerAddress{Address: "1"})
	expectPanicFn(func() {
		checkLaunchRequest(config.Config{ReplicaID: 1}, addr, false, false)
	})
}

func TestValidateUpdate(t *testing.T) {
	tests := []struct {
		commit              uint64
		firstCommittedIndex uint64
		committedLength     uint64
		firstSaveIndex      uint64
		saveLength          uint64
		panic               bool
	}{
		{0, 1, 2, 0, 0, false},
		{1, 1, 2, 0, 0, true},
		{2, 1, 2, 0, 0, false},
		{0, 1, 2, 1, 2, false},
		{0, 1, 2, 1, 1, true},
		{0, 1, 2, 3, 1, false},
		{0, 0, 0, 1, 2, false},
		{0, 1, 2, 0, 0, false},
	}
	for idx, tt := range tests {
		tidx := idx
		ud := pb.Update{}
		ud.Commit = tt.commit
		if tt.committedLength > 0 {
			lastIndex := tt.firstCommittedIndex + tt.committedLength - 1
			for i := tt.firstCommittedIndex; i <= lastIndex; i++ {
				e := pb.Entry{
					Index: i,
				}
				ud.CommittedEntries = append(ud.CommittedEntries, e)
			}
		}
		if tt.saveLength > 0 {
			lastIndex := tt.firstSaveIndex + tt.saveLength - 1
			for i := tt.firstSaveIndex; i <= lastIndex; i++ {
				e := pb.Entry{
					Index: i,
				}
				ud.EntriesToSave = append(ud.EntriesToSave, e)
			}
		}
		func() {
			if tt.panic {
				defer func() {
					if r := recover(); r == nil {
						t.Fatalf("%d, failed to panic", tidx)
					}
				}()
			}
			validateUpdate(ud)
		}()
	}
}

func TestSetFastApply(t *testing.T) {
	tests := []struct {
		hasSnapshot         bool
		firstCommittedIndex uint64
		committedLength     uint64
		firstSaveIndex      uint64
		saveLength          uint64
		fastApply           bool
	}{
		{true, 0, 0, 0, 0, false},
		{true, 0, 0, 1, 2, false},
		{true, 1, 2, 0, 0, false},
		{true, 1, 2, 1, 2, false},
		{true, 1, 2, 1, 1, false},
		{true, 1, 1, 1, 2, false},
		{false, 1, 2, 1, 2, false},
		{false, 1, 2, 0, 0, true},
		{false, 0, 0, 1, 2, true},
		{false, 1, 2, 2, 3, false},
		{false, 1, 1, 2, 3, true},
	}

	for idx, tt := range tests {
		tidx := idx
		ud := pb.Update{FastApply: true}
		if tt.hasSnapshot {
			ud.Snapshot = pb.Snapshot{
				Index: 1,
			}
		}
		if tt.committedLength > 0 {
			lastIndex := tt.firstCommittedIndex + tt.committedLength - 1
			for i := tt.firstCommittedIndex; i <= lastIndex; i++ {
				e := pb.Entry{
					Index: i,
				}
				ud.CommittedEntries = append(ud.CommittedEntries, e)
			}
		}
		if tt.saveLength > 0 {
			lastIndex := tt.firstSaveIndex + tt.saveLength - 1
			for i := tt.firstSaveIndex; i <= lastIndex; i++ {
				e := pb.Entry{
					Index: i,
				}
				ud.EntriesToSave = append(ud.EntriesToSave, e)
			}
		}
		ud = setFastApply(ud)
		if ud.FastApply != tt.fastApply {
			t.Fatalf("%d, fast apply not expected", tidx)
		}
	}
}

func TestRaftAPIQueryRaftLog(t *testing.T) {
	s := NewTestLogDB()
	var err error
	rawNode := Launch(newTestConfig(1, 10, 1), s, nil, []PeerAddress{{ReplicaID: 1}}, true, true)
	rawNode.raft.hasNotAppliedConfigChange = rawNode.raft.testOnlyHasConfigChangeToApply
	ud, err := rawNode.GetUpdate(true, 0)
	assert.NoError(t, err)
	assert.NoError(t, s.Append(ud.EntriesToSave))
	rawNode.Commit(ud)

	rawNode.Campaign()
	var lastIndex uint64
	for {
		ud, err = rawNode.GetUpdate(true, 0)
		assert.NoError(t, err)
		assert.NoError(t, s.Append(ud.EntriesToSave))
		// Once we are the leader, propose a command and a ConfigChange.
		if rawNode.raft.leaderID == rawNode.raft.replicaID {
			ne(rawNode.ProposeEntries([]pb.Entry{{Cmd: []byte("somedata")}}), t)
		}
		rawNode.Commit(ud)
		// Exit when we have four entries: one ConfigChange, one no-op for the election,
		// our proposed command and proposed ConfigChange.
		_, lastIndex = s.GetRange()
		if lastIndex >= 4 {
			break
		}
	}
	entries, err := s.Entries(lastIndex-1, lastIndex+1, noLimit)
	assert.NoError(t, err)
	assert.NoError(t, rawNode.QueryRaftLog(lastIndex-1, lastIndex+1, noLimit))
	assert.NotNil(t, rawNode.raft.logQueryResult)
	ud, err = rawNode.GetUpdate(true, 0)
	assert.NoError(t, err)
	assert.NotNil(t, rawNode.raft.logQueryResult)
	assert.False(t, ud.LogQueryResult.IsEmpty())
	assert.Nil(t, ud.LogQueryResult.Error)
	rawNode.Commit(ud)
	assert.Nil(t, rawNode.raft.logQueryResult)
	assert.Equal(t, entries, ud.LogQueryResult.Entries)
}
````

## File: internal/raft/peer.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
//
// Copyright 2015 The etcd Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
//
// Peer.go is the interface used by the upper layer to access functionalities
// provided by the raft protocol. It translates all incoming requests to raftpb
// messages and pass them to the raft protocol implementation to be handled.
// Such a state machine style design together with the iterative style interface
// here is derived from etcd.
// Compared to etcd raft, we strictly model all inputs to the raft protocol as
// messages including those used to advance the raft state.
//

package raft

import (
	"sort"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/server"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

// PeerAddress is the basic info for a peer in the Raft shard.
type PeerAddress struct {
	Address   string
	ReplicaID uint64
}

// Peer is the interface struct for interacting with the underlying Raft
// protocol implementation.
type Peer struct {
	raft      *raft
	prevState pb.State
}

// Launch starts or restarts a Raft node.
func Launch(config config.Config,
	logdb ILogDB, events server.IRaftEventListener,
	addresses []PeerAddress, initial bool, newNode bool) Peer {
	checkLaunchRequest(config, addresses, initial, newNode)
	plog.Infof("%s created, initial: %t, new: %t",
		dn(config.ShardID, config.ReplicaID), initial, newNode)
	p := Peer{raft: newRaft(config, logdb)}
	p.raft.events = events
	p.prevState = p.raft.raftState()
	if initial && newNode {
		p.raft.becomeFollower(1, NoLeader)
		bootstrap(p.raft, addresses)
	}
	return p
}

// Tick moves the logical clock forward by one tick.
func (p *Peer) Tick() error {
	return p.raft.Handle(pb.Message{
		Type:   pb.LocalTick,
		Reject: false,
	})
}

// QuiescedTick moves the logical clock forward by one tick in quiesced mode.
func (p *Peer) QuiescedTick() error {
	return p.raft.Handle(pb.Message{
		Type:   pb.LocalTick,
		Reject: true,
	})
}

func (p *Peer) QueryRaftLog(firstIndex uint64,
	lastIndex uint64, maxSize uint64) error {
	return p.raft.Handle(pb.Message{
		Type: pb.LogQuery,
		From: firstIndex,
		To:   lastIndex,
		Hint: maxSize,
	})
}

// RequestLeaderTransfer makes a request to transfer the leadership to the
// specified target node.
func (p *Peer) RequestLeaderTransfer(target uint64) error {
	return p.raft.Handle(pb.Message{
		Type: pb.LeaderTransfer,
		To:   p.raft.replicaID,
		Hint: target,
	})
}

// ProposeEntries proposes specified entries in a batched mode using a single
// MTPropose message.
func (p *Peer) ProposeEntries(ents []pb.Entry) error {
	return p.raft.Handle(pb.Message{
		Type:    pb.Propose,
		From:    p.raft.replicaID,
		Entries: ents,
	})
}

// ProposeConfigChange proposes a raft membership change.
func (p *Peer) ProposeConfigChange(cc pb.ConfigChange, key uint64) error {
	data := pb.MustMarshal(&cc)
	return p.raft.Handle(pb.Message{
		Type:    pb.Propose,
		Entries: []pb.Entry{{Type: pb.ConfigChangeEntry, Cmd: data, Key: key}},
	})
}

// ApplyConfigChange applies a raft membership change to the local raft node.
func (p *Peer) ApplyConfigChange(cc pb.ConfigChange) error {
	if cc.ReplicaID == NoLeader {
		p.raft.clearPendingConfigChange()
		return nil
	}
	return p.raft.Handle(pb.Message{
		Type:     pb.ConfigChangeEvent,
		Reject:   false,
		Hint:     cc.ReplicaID,
		HintHigh: uint64(cc.Type),
	})
}

// RejectConfigChange rejects the currently pending raft membership change.
func (p *Peer) RejectConfigChange() error {
	return p.raft.Handle(pb.Message{
		Type:   pb.ConfigChangeEvent,
		Reject: true,
	})
}

// RestoreRemotes applies the remotes info obtained from the specified snapshot.
func (p *Peer) RestoreRemotes(ss pb.Snapshot) error {
	return p.raft.Handle(pb.Message{
		Type:     pb.SnapshotReceived,
		Snapshot: ss,
	})
}

// ReportUnreachableNode marks the specified node as not reachable.
func (p *Peer) ReportUnreachableNode(replicaID uint64) error {
	return p.raft.Handle(pb.Message{
		Type: pb.Unreachable,
		From: replicaID,
	})
}

// ReportSnapshotStatus reports the status of the snapshot to the local raft
// node.
func (p *Peer) ReportSnapshotStatus(replicaID uint64, reject bool) error {
	return p.raft.Handle(pb.Message{
		Type:   pb.SnapshotStatus,
		From:   replicaID,
		Reject: reject,
	})
}

// Handle processes the given message.
func (p *Peer) Handle(m pb.Message) error {
	if IsLocalMessageType(m.Type) {
		panic("local message sent to Step")
	}
	_, rok := p.raft.remotes[m.From]
	_, ook := p.raft.nonVotings[m.From]
	_, wok := p.raft.witnesses[m.From]
	if rok || ook || wok || !isResponseMessageType(m.Type) {
		return p.raft.Handle(m)
	}
	return nil
}

// GetUpdate returns the current state of the Peer.
func (p *Peer) GetUpdate(moreToApply bool,
	lastApplied uint64) (pb.Update, error) {
	ud, err := p.getUpdate(moreToApply, lastApplied)
	if err != nil {
		return pb.Update{}, err
	}
	validateUpdate(ud)
	ud = setFastApply(ud)
	ud.UpdateCommit = getUpdateCommit(ud)
	return ud, nil
}

func setFastApply(ud pb.Update) pb.Update {
	ud.FastApply = true
	if !pb.IsEmptySnapshot(ud.Snapshot) {
		ud.FastApply = false
	}
	if ud.FastApply {
		if len(ud.CommittedEntries) > 0 && len(ud.EntriesToSave) > 0 {
			lastApplyIndex := ud.CommittedEntries[len(ud.CommittedEntries)-1].Index
			lastSaveIndex := ud.EntriesToSave[len(ud.EntriesToSave)-1].Index
			firstSaveIndex := ud.EntriesToSave[0].Index
			if lastApplyIndex >= firstSaveIndex && lastApplyIndex <= lastSaveIndex {
				ud.FastApply = false
			}
		}
	}
	return ud
}

func validateUpdate(ud pb.Update) {
	if ud.Commit > 0 && len(ud.CommittedEntries) > 0 {
		lastIndex := ud.CommittedEntries[len(ud.CommittedEntries)-1].Index
		if lastIndex > ud.Commit {
			plog.Panicf("trying to apply not committed entry: %d, %d",
				ud.Commit, lastIndex)
		}
	}
	if len(ud.CommittedEntries) > 0 && len(ud.EntriesToSave) > 0 {
		lastApply := ud.CommittedEntries[len(ud.CommittedEntries)-1].Index
		lastSave := ud.EntriesToSave[len(ud.EntriesToSave)-1].Index
		if lastApply > lastSave {
			plog.Panicf("trying to apply not saved entry: %d, %d",
				lastApply, lastSave)
		}
	}
}

// RateLimited returns a boolean flag indicating whether the Raft node is rate
// limited.
func (p *Peer) RateLimited() bool {
	return p.raft.rl.RateLimited()
}

// HasUpdate returns a boolean value indicating whether there is any Update
// ready to be processed.
func (p *Peer) HasUpdate(moreToApply bool) bool {
	r := p.raft
	if len(r.log.entriesToSave()) > 0 {
		return true
	}
	if r.logQueryResult != nil {
		return true
	}
	if r.leaderUpdate != nil {
		return true
	}
	if len(r.msgs) > 0 {
		return true
	}
	if moreToApply && r.log.hasEntriesToApply() {
		return true
	}
	if pst := r.raftState(); !pb.IsEmptyState(pst) &&
		!pb.IsStateEqual(pst, p.prevState) {
		return true
	}
	if r.log.inmem.snapshot != nil &&
		!pb.IsEmptySnapshot(*r.log.inmem.snapshot) {
		return true
	}
	if len(r.readyToRead) != 0 {
		return true
	}
	if len(r.droppedEntries) > 0 {
		return true
	}
	if len(r.droppedReadIndexes) > 0 {
		return true
	}
	return false
}

// Commit commits the Update state to mark it as processed.
func (p *Peer) Commit(ud pb.Update) {
	p.raft.msgs = nil
	p.raft.logQueryResult = nil
	p.raft.leaderUpdate = nil
	p.raft.droppedEntries = nil
	p.raft.droppedReadIndexes = nil
	if !pb.IsEmptyState(ud.State) {
		p.prevState = ud.State
	}
	if ud.UpdateCommit.ReadyToRead > 0 {
		p.raft.clearReadyToRead()
	}
	p.entryLog().commitUpdate(ud.UpdateCommit)
}

// ReadIndex starts a ReadIndex operation. The ReadIndex protocol is defined in
// the section 6.4 of the Raft thesis.
func (p *Peer) ReadIndex(ctx pb.SystemCtx) error {
	return p.raft.Handle(pb.Message{
		Type:     pb.ReadIndex,
		Hint:     ctx.Low,
		HintHigh: ctx.High,
	})
}

// NotifyRaftLastApplied passes on the lastApplied index confirmed by the RSM to
// the raft state machine.
func (p *Peer) NotifyRaftLastApplied(lastApplied uint64) {
	p.raft.setApplied(lastApplied)
}

// HasEntryToApply returns a boolean flag indicating whether there are more
// entries ready to be applied.
func (p *Peer) HasEntryToApply() bool {
	return p.entryLog().hasEntriesToApply()
}

func (p *Peer) entryLog() *entryLog {
	return p.raft.log
}

func (p *Peer) getUpdate(moreToApply bool,
	lastApplied uint64) (pb.Update, error) {
	ud := pb.Update{
		ShardID:       p.raft.shardID,
		ReplicaID:     p.raft.replicaID,
		EntriesToSave: p.entryLog().entriesToSave(),
		Messages:      p.raft.msgs,
		LastApplied:   lastApplied,
		FastApply:     true,
	}
	if p.raft.logQueryResult != nil {
		ud.LogQueryResult = *p.raft.logQueryResult
	}
	if p.raft.leaderUpdate != nil {
		ud.LeaderUpdate = *p.raft.leaderUpdate
	}
	for idx := range ud.Messages {
		ud.Messages[idx].ShardID = p.raft.shardID
	}
	if moreToApply {
		toApply, err := p.entryLog().entriesToApply()
		if err != nil {
			return pb.Update{}, err
		}
		ud.CommittedEntries = toApply
	}
	if len(ud.CommittedEntries) > 0 {
		lastIndex := ud.CommittedEntries[len(ud.CommittedEntries)-1].Index
		ud.MoreCommittedEntries = p.entryLog().hasMoreEntriesToApply(lastIndex)
	}
	if pst := p.raft.raftState(); !pb.IsStateEqual(pst, p.prevState) {
		ud.State = pst
	}
	if p.entryLog().inmem.snapshot != nil {
		ud.Snapshot = *p.entryLog().inmem.snapshot
	}
	if len(p.raft.readyToRead) > 0 {
		ud.ReadyToReads = p.raft.readyToRead
	}
	if len(p.raft.droppedEntries) > 0 {
		ud.DroppedEntries = p.raft.droppedEntries
	}
	if len(p.raft.droppedReadIndexes) > 0 {
		ud.DroppedReadIndexes = p.raft.droppedReadIndexes
	}
	return ud, nil
}

func checkLaunchRequest(config config.Config,
	addresses []PeerAddress, initial bool, newNode bool) {
	if config.ReplicaID == 0 {
		panic("config.ReplicaID must not be zero")
	}
	if initial && newNode && len(addresses) == 0 {
		panic("addresses must be specified")
	}
	uniqueAddressList := make(map[string]struct{})
	for _, addr := range addresses {
		uniqueAddressList[addr.Address] = struct{}{}
	}
	if len(uniqueAddressList) != len(addresses) {
		plog.Panicf("duplicated address found %v", addresses)
	}
	if initial && config.IsWitness {
		plog.Panicf("witness can not be used as initial member")
	}
	if initial && config.IsNonVoting {
		plog.Panicf("non-voting can not be used as initial member")
	}
}

func bootstrap(r *raft, addresses []PeerAddress) {
	sort.Slice(addresses, func(i, j int) bool {
		return addresses[i].ReplicaID < addresses[j].ReplicaID
	})
	ents := make([]pb.Entry, len(addresses))
	for i, peer := range addresses {
		plog.Infof("%s added bootstrap ConfigChangeAddNode, %d, %s",
			r.describe(), peer.ReplicaID, peer.Address)
		cc := pb.ConfigChange{
			Type:       pb.AddNode,
			ReplicaID:  peer.ReplicaID,
			Initialize: true,
			Address:    peer.Address,
		}
		ents[i] = pb.Entry{
			Type:  pb.ConfigChangeEntry,
			Term:  1,
			Index: uint64(i + 1),
			Cmd:   pb.MustMarshal(&cc),
		}
	}
	r.log.append(ents)
	r.log.committed = uint64(len(ents))
	for _, peer := range addresses {
		r.addNode(peer.ReplicaID)
	}
}

func getUpdateCommit(ud pb.Update) pb.UpdateCommit {
	uc := pb.UpdateCommit{
		ReadyToRead: uint64(len(ud.ReadyToReads)),
		LastApplied: ud.LastApplied,
	}
	if len(ud.CommittedEntries) > 0 {
		uc.Processed = ud.CommittedEntries[len(ud.CommittedEntries)-1].Index
	}
	if len(ud.EntriesToSave) > 0 {
		lastEntry := ud.EntriesToSave[len(ud.EntriesToSave)-1]
		uc.StableLogTo, uc.StableLogTerm = lastEntry.Index, lastEntry.Term
	}
	if !pb.IsEmptySnapshot(ud.Snapshot) {
		uc.StableSnapshotTo = ud.Snapshot.Index
		uc.Processed = max(uc.Processed, uc.StableSnapshotTo)
	}
	return uc
}
````

## File: internal/raft/raft_etcd_paper_test.go
````go
// Copyright 2015 The etcd Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

/*
This file contains tests which verify that the scenarios described
in the raft paper (https://ramcloud.stanford.edu/raft.pdf) are
handled by the raft implementation correctly. Each test focuses on
several sentences written in the paper. This could help us to prevent
most implementation bugs.

Each test is composed of three parts: init, test and check.
Init part uses simple and understandable way to simulate the init state.
Test part uses Step function to generate the scenario. Check part checks
outgoing messages and state.
*/

//
// raft_paper_test.go is ported from etcd raft for testing purposes.
// updates have been made to reflect the interface & implementation differences
//

package raft

import (
	"fmt"
	"reflect"
	"sort"
	"testing"

	"github.com/lni/dragonboat/v4/logger"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

func TestFollowerUpdateTermFromMessage(t *testing.T) {
	testUpdateTermFromMessage(t, follower)
}
func TestCandidateUpdateTermFromMessage(t *testing.T) {
	testUpdateTermFromMessage(t, candidate)
}
func TestLeaderUpdateTermFromMessage(t *testing.T) {
	testUpdateTermFromMessage(t, leader)
}

// testUpdateTermFromMessage tests that if one server’s current term is
// smaller than the other’s, then it updates its current term to the larger
// value. If a candidate or leader discovers that its term is out of date,
// it immediately reverts to follower state.
// Reference: section 5.1
func testUpdateTermFromMessage(t *testing.T, state State) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	switch state {
	case follower:
		r.becomeFollower(1, 2)
	case candidate:
		r.becomeCandidate()
	case leader:
		r.becomeCandidate()
		r.becomeLeader()
	}

	ne(r.Handle(pb.Message{Type: pb.Replicate, Term: 2}), t)

	if r.term != 2 {
		t.Errorf("term = %d, want %d", r.term, 2)
	}
	if r.state != follower {
		t.Errorf("state = %v, want %v", r.state, follower)
	}
}

// TestRejectStaleTermMessage tests that if a server receives a request with
// a stale term number, it rejects the request.
// Our implementation ignores the request instead.
// Reference: section 5.1
func TestRejectStaleTermMessage(t *testing.T) {
	called := false
	fakeStep := func(r *raft, m pb.Message) error {
		called = true
		return nil
	}
	r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	r.handle = fakeStep
	r.loadState(pb.State{Term: 2})

	ne(r.Handle(pb.Message{Type: pb.Replicate, Term: r.term - 1}), t)

	if called {
		t.Errorf("stepFunc called = %v, want %v", called, false)
	}
}

// TestStartAsFollower tests that when servers start up, they begin as followers.
// Reference: section 5.2
func TestStartAsFollower(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	if r.state != follower {
		t.Errorf("state = %s, want %s", r.state, follower)
	}
}

// TestLeaderBcastBeat tests that if the leader receives a heartbeat tick,
// it will send a msgApp with m.Index = 0, m.LogTerm=0 and empty entries as
// heartbeat to all followers.
// Reference: section 5.2
func TestLeaderBcastBeat(t *testing.T) {
	// heartbeat interval
	hi := 1
	r := newTestRaft(1, []uint64{1, 2, 3}, 10, hi, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	for i := 0; i < 10; i++ {
		r.appendEntries([]pb.Entry{{Index: uint64(i) + 1}})
	}

	for i := 0; i < hi; i++ {
		ne(r.tick(), t)
	}

	msgs := r.readMessages()
	sort.Sort(messageSlice(msgs))
	wmsgs := []pb.Message{
		{From: 1, To: 2, Term: 1, Type: pb.Heartbeat},
		{From: 1, To: 3, Term: 1, Type: pb.Heartbeat},
	}
	if !reflect.DeepEqual(msgs, wmsgs) {
		t.Errorf("msgs = %v, want %v", msgs, wmsgs)
	}
}

func TestFollowerStartElection(t *testing.T) {
	testNonleaderStartElection(t, follower)
}
func TestCandidateStartNewElection(t *testing.T) {
	testNonleaderStartElection(t, candidate)
}

// testNonleaderStartElection tests that if a follower receives no communication
// over election timeout, it begins an election to choose a new leader. It
// increments its current term and transitions to candidate state. It then
// votes for itself and issues RequestVote RPCs in parallel to each of the
// other servers in the shard.
// Reference: section 5.2
// Also if a candidate fails to obtain a majority, it will time out and
// start a new election by incrementing its term and initiating another
// round of RequestVote RPCs.
// Reference: section 5.2
func testNonleaderStartElection(t *testing.T, state State) {
	// election timeout
	et := 10
	r := newTestRaft(1, []uint64{1, 2, 3}, et, 1, NewTestLogDB())
	switch state {
	case follower:
		r.becomeFollower(1, 2)
	case candidate:
		r.becomeCandidate()
	}

	for i := 1; i < 2*et; i++ {
		ne(r.tick(), t)
	}

	if r.term != 2 {
		t.Errorf("term = %d, want 2", r.term)
	}
	if r.state != candidate {
		t.Errorf("state = %s, want %s", r.state, candidate)
	}
	if !r.votes[r.replicaID] {
		t.Errorf("vote for self = false, want true")
	}
	msgs := r.readMessages()
	sort.Sort(messageSlice(msgs))
	wmsgs := []pb.Message{
		{From: 1, To: 2, Term: 2, Type: pb.RequestVote},
		{From: 1, To: 3, Term: 2, Type: pb.RequestVote},
	}
	if !reflect.DeepEqual(msgs, wmsgs) {
		t.Errorf("msgs = %v, want %v", msgs, wmsgs)
	}
}

// TestLeaderElectionInOneRoundRPC tests all cases that may happen in
// leader election during one round of RequestVote RPC:
// a) it wins the election
// b) it loses the election
// c) it is unclear about the result
// Reference: section 5.2
func TestLeaderElectionInOneRoundRPC(t *testing.T) {
	tests := []struct {
		size  int
		votes map[uint64]bool
		state State
	}{
		// win the election when receiving votes from a majority of the servers
		{1, map[uint64]bool{}, leader},
		{3, map[uint64]bool{2: true, 3: true}, leader},
		{3, map[uint64]bool{2: true}, leader},
		{5, map[uint64]bool{2: true, 3: true, 4: true, 5: true}, leader},
		{5, map[uint64]bool{2: true, 3: true, 4: true}, leader},
		{5, map[uint64]bool{2: true, 3: true}, leader},

		// return to follower state if it receives vote denial from a majority
		{3, map[uint64]bool{2: false, 3: false}, follower},
		{5, map[uint64]bool{2: false, 3: false, 4: false, 5: false}, follower},
		{5, map[uint64]bool{2: true, 3: false, 4: false, 5: false}, follower},

		// stay in candidate if it does not obtain the majority
		{3, map[uint64]bool{}, candidate},
		{5, map[uint64]bool{2: true}, candidate},
		{5, map[uint64]bool{2: false, 3: false}, candidate},
		{5, map[uint64]bool{}, candidate},
	}
	for i, tt := range tests {
		r := newTestRaft(1, idsBySize(tt.size), 10, 1, NewTestLogDB())

		ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.Election}), t)
		for id, vote := range tt.votes {
			ne(r.Handle(pb.Message{From: id, To: 1, Type: pb.RequestVoteResp, Reject: !vote}), t)
		}

		if r.state != tt.state {
			t.Errorf("#%d: state = %s, want %s", i, r.state, tt.state)
		}
		if g := r.term; g != 1 {
			t.Errorf("#%d: term = %d, want %d", i, g, 1)
		}
	}
}

// TestFollowerVote tests that each follower will vote for at most one
// candidate in a given term, on a first-come-first-served basis.
// Reference: section 5.2
func TestFollowerVote(t *testing.T) {
	tests := []struct {
		vote    uint64
		nvote   uint64
		wreject bool
	}{
		{NoLeader, 1, false},
		{NoLeader, 2, false},
		{1, 1, false},
		{2, 2, false},
		{1, 2, true},
		{2, 1, true},
	}
	for i, tt := range tests {
		r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
		r.loadState(pb.State{Term: 1, Vote: tt.vote})

		ne(r.Handle(pb.Message{From: tt.nvote, To: 1, Term: 1, Type: pb.RequestVote}), t)

		msgs := r.readMessages()
		wmsgs := []pb.Message{
			{From: 1, To: tt.nvote, Term: 1, Type: pb.RequestVoteResp, Reject: tt.wreject},
		}
		if !reflect.DeepEqual(msgs, wmsgs) {
			t.Errorf("#%d: msgs = %v, want %v", i, msgs, wmsgs)
		}
	}
}

// TestCandidateFallback tests that while waiting for votes,
// if a candidate receives an AppendEntries RPC from another server claiming
// to be leader whose term is at least as large as the candidate's current term,
// it recognizes the leader as legitimate and returns to follower state.
// Reference: section 5.2
func TestCandidateFallback(t *testing.T) {
	tests := []pb.Message{
		{From: 2, To: 1, Term: 1, Type: pb.Replicate},
		{From: 2, To: 1, Term: 2, Type: pb.Replicate},
	}
	for i, tt := range tests {
		r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
		ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.Election}), t)
		if r.state != candidate {
			t.Fatalf("unexpected state = %s, want %s", r.state, candidate)
		}

		ne(r.Handle(tt), t)

		if g := r.state; g != follower {
			t.Errorf("#%d: state = %s, want %s", i, g, follower)
		}
		if g := r.term; g != tt.Term {
			t.Errorf("#%d: term = %d, want %d", i, g, tt.Term)
		}
	}
}

func TestFollowerElectionTimeoutRandomized(t *testing.T) {
	//SetLogger(discardLogger)
	//defer SetLogger(defaultLogger)
	plog.SetLevel(logger.WARNING)
	defer plog.SetLevel(logger.INFO)
	testNonleaderElectionTimeoutRandomized(t, follower)
}
func TestCandidateElectionTimeoutRandomized(t *testing.T) {
	//SetLogger(discardLogger)
	//defer SetLogger(defaultLogger)
	plog.SetLevel(logger.WARNING)
	defer plog.SetLevel(logger.INFO)
	testNonleaderElectionTimeoutRandomized(t, candidate)
}

// testNonleaderElectionTimeoutRandomized tests that election timeout for
// follower or candidate is randomized.
// Reference: section 5.2
func testNonleaderElectionTimeoutRandomized(t *testing.T, state State) {
	et := 10
	r := newTestRaft(1, []uint64{1, 2, 3}, et, 1, NewTestLogDB())
	timeouts := make(map[int]bool)
	for round := 0; round < 50*et; round++ {
		switch state {
		case follower:
			r.becomeFollower(r.term+1, 2)
		case candidate:
			r.becomeCandidate()
		}

		time := 0
		for len(r.readMessages()) == 0 {
			ne(r.tick(), t)
			time++
		}
		timeouts[time] = true
	}

	for d := et + 1; d < 2*et; d++ {
		if !timeouts[d] {
			t.Errorf("timeout in %d ticks should happen", d)
		}
	}
}

func TestFollowersElectioinTimeoutNonconflict(t *testing.T) {
	//SetLogger(discardLogger)
	//defer SetLogger(defaultLogger)
	plog.SetLevel(logger.WARNING)
	defer plog.SetLevel(logger.INFO)
	testNonleadersElectionTimeoutNonconflict(t, follower)
}
func TestCandidatesElectionTimeoutNonconflict(t *testing.T) {
	//SetLogger(discardLogger)
	//defer SetLogger(defaultLogger)
	plog.SetLevel(logger.WARNING)
	defer plog.SetLevel(logger.INFO)
	testNonleadersElectionTimeoutNonconflict(t, candidate)
}

// testNonleadersElectionTimeoutNonconflict tests that in most cases only a
// single server(follower or candidate) will time out, which reduces the
// likelihood of split vote in the new election.
// Reference: section 5.2
func testNonleadersElectionTimeoutNonconflict(t *testing.T, state State) {
	et := 10
	size := 5
	rs := make([]*raft, size)
	ids := idsBySize(size)
	for k := range rs {
		rs[k] = newTestRaft(ids[k], ids, et, 1, NewTestLogDB())
	}
	conflicts := 0
	for round := 0; round < 1000; round++ {
		for _, r := range rs {
			switch state {
			case follower:
				r.becomeFollower(r.term+1, NoLeader)
			case candidate:
				r.becomeCandidate()
			}
		}

		timeoutNum := 0
		for timeoutNum == 0 {
			for _, r := range rs {
				ne(r.tick(), t)
				if len(r.readMessages()) > 0 {
					timeoutNum++
				}
			}
		}
		// several rafts time out at the same tick
		if timeoutNum > 1 {
			conflicts++
		}
	}

	if g := float64(conflicts) / 1000; g > 0.3 {
		t.Errorf("probability of conflicts = %v, want <= 0.3", g)
	}
}

// TestLeaderCommitEntry tests that when the entry has been safely replicated,
// the leader gives out the applied entries, which can be applied to its state
// machine.
// Also, the leader keeps track of the highest index it knows to be committed,
// and it includes that index in future AppendEntries RPCs so that the other
// servers eventually find out.
// Reference: section 5.3
func TestLeaderCommitEntry(t *testing.T) {
	s := NewTestLogDB()
	r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, s)
	r.becomeCandidate()
	r.becomeLeader()
	commitNoopEntry(r, s)
	li := r.log.lastIndex()
	ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("some data")}}}), t)

	for _, m := range r.readMessages() {
		ne(r.Handle(acceptAndReply(m)), t)
	}

	if g := r.log.committed; g != li+1 {
		t.Errorf("committed = %d, want %d", g, li+1)
	}
	wents := []pb.Entry{{Index: li + 1, Term: 1, Cmd: []byte("some data")}}
	g, err := r.log.entriesToApply()
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if !reflect.DeepEqual(g, wents) {
		t.Errorf("nextEnts = %+v, want %+v", g, wents)
	}
	msgs := r.readMessages()
	sort.Sort(messageSlice(msgs))
	for i, m := range msgs {
		if w := uint64(i + 2); m.To != w {
			t.Errorf("to = %x, want %x", m.To, w)
		}
		if m.Type != pb.Replicate {
			t.Errorf("type = %v, want %v", m.Type, pb.Replicate)
		}
		if m.Commit != li+1 {
			t.Errorf("commit = %d, want %d", m.Commit, li+1)
		}
	}
}

// TestLeaderAcknowledgeCommit tests that a log entry is committed once the
// leader that created the entry has replicated it on a majority of the servers.
// Reference: section 5.3
func TestLeaderAcknowledgeCommit(t *testing.T) {
	tests := []struct {
		size      int
		acceptors map[uint64]bool
		wack      bool
	}{
		{1, nil, true},
		{3, nil, false},
		{3, map[uint64]bool{2: true}, true},
		{3, map[uint64]bool{2: true, 3: true}, true},
		{5, nil, false},
		{5, map[uint64]bool{2: true}, false},
		{5, map[uint64]bool{2: true, 3: true}, true},
		{5, map[uint64]bool{2: true, 3: true, 4: true}, true},
		{5, map[uint64]bool{2: true, 3: true, 4: true, 5: true}, true},
	}
	for i, tt := range tests {
		s := NewTestLogDB()
		r := newTestRaft(1, idsBySize(tt.size), 10, 1, s)
		r.becomeCandidate()
		r.becomeLeader()
		commitNoopEntry(r, s)
		li := r.log.lastIndex()
		ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("some data")}}}), t)

		for _, m := range r.readMessages() {
			if tt.acceptors[m.To] {
				ne(r.Handle(acceptAndReply(m)), t)
			}
		}

		if g := r.log.committed > li; g != tt.wack {
			t.Errorf("#%d: ack commit = %v, want %v", i, g, tt.wack)
		}
	}
}

// TestLeaderCommitPrecedingEntries tests that when leader commits a log entry,
// it also commits all preceding entries in the leader’s log, including
// entries created by previous leaders.
// Also, it applies the entry to its local state machine (in log order).
// Reference: section 5.3
func TestLeaderCommitPrecedingEntries(t *testing.T) {
	tests := [][]pb.Entry{
		{},
		{{Term: 2, Index: 1}},
		{{Term: 1, Index: 1}, {Term: 2, Index: 2}},
		{{Term: 1, Index: 1}},
	}
	for i, tt := range tests {
		storage := NewTestLogDB()
		if err := storage.Append(tt); err != nil {
			t.Fatalf("%v", err)
		}
		r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, storage)
		r.loadState(pb.State{Term: 2})
		r.becomeCandidate()
		r.becomeLeader()
		ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("some data")}}}), t)

		for _, m := range r.readMessages() {
			ne(r.Handle(acceptAndReply(m)), t)
		}

		li := uint64(len(tt))
		wents := append(tt, pb.Entry{Term: 3, Index: li + 1}, pb.Entry{Term: 3, Index: li + 2, Cmd: []byte("some data")})
		g, err := r.log.entriesToApply()
		if err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		if !reflect.DeepEqual(g, wents) {
			t.Errorf("#%d: ents = %+v, want %+v", i, g, wents)
		}
	}
}

// TestFollowerCommitEntry tests that once a follower learns that a log entry
// is committed, it applies the entry to its local state machine (in log order).
// Reference: section 5.3
func TestFollowerCommitEntry(t *testing.T) {
	tests := []struct {
		ents   []pb.Entry
		commit uint64
	}{
		{
			[]pb.Entry{
				{Term: 1, Index: 1, Cmd: []byte("some data")},
			},
			1,
		},
		{
			[]pb.Entry{
				{Term: 1, Index: 1, Cmd: []byte("some data")},
				{Term: 1, Index: 2, Cmd: []byte("some data2")},
			},
			2,
		},
		{
			[]pb.Entry{
				{Term: 1, Index: 1, Cmd: []byte("some data2")},
				{Term: 1, Index: 2, Cmd: []byte("some data")},
			},
			2,
		},
		{
			[]pb.Entry{
				{Term: 1, Index: 1, Cmd: []byte("some data")},
				{Term: 1, Index: 2, Cmd: []byte("some data2")},
			},
			1,
		},
	}
	for i, tt := range tests {
		r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
		r.becomeFollower(1, 2)

		ne(r.Handle(pb.Message{From: 2, To: 1, Type: pb.Replicate, Term: 1, Entries: tt.ents, Commit: tt.commit}), t)

		if g := r.log.committed; g != tt.commit {
			t.Errorf("#%d: committed = %d, want %d", i, g, tt.commit)
		}
		wents := tt.ents[:int(tt.commit)]
		g, err := r.log.entriesToApply()
		if err != nil {
			t.Fatalf("unexpected error %v", err)
		}
		if !reflect.DeepEqual(g, wents) {
			t.Errorf("#%d: nextEnts = %v, want %v", i, g, wents)
		}
	}
}

// TestFollowerCheckReplicate tests that if the follower does not find an
// entry in its log with the same index and term as the one in AppendEntries RPC,
// then it refuses the new entries. Otherwise it replies that it accepts the
// append entries.
// Reference: section 5.3
func TestFollowerCheckReplicate(t *testing.T) {
	ents := []pb.Entry{{Term: 1, Index: 1}, {Term: 2, Index: 2}}
	tests := []struct {
		term        uint64
		index       uint64
		windex      uint64
		wreject     bool
		wrejectHint uint64
	}{
		// match with committed entries
		{0, 0, 1, false, 0},
		{ents[0].Term, ents[0].Index, 1, false, 0},
		// match with uncommitted entries
		{ents[1].Term, ents[1].Index, 2, false, 0},

		// unmatch with existing entry
		{ents[0].Term, ents[1].Index, ents[1].Index, true, 2},
		// unexisting entry
		{ents[1].Term + 1, ents[1].Index + 1, ents[1].Index + 1, true, 2},
	}
	for i, tt := range tests {
		storage := NewTestLogDB()
		if err := storage.Append(ents); err != nil {
			t.Fatalf("%v", err)
		}
		r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, storage)
		r.loadState(pb.State{Commit: 1})
		r.becomeFollower(2, 2)

		ne(r.Handle(pb.Message{From: 2, To: 1, Type: pb.Replicate, Term: 2, LogTerm: tt.term, LogIndex: tt.index}), t)

		msgs := r.readMessages()
		wmsgs := []pb.Message{
			{From: 1, To: 2, Type: pb.ReplicateResp, Term: 2, LogIndex: tt.windex, Reject: tt.wreject, Hint: tt.wrejectHint},
		}
		if !reflect.DeepEqual(msgs, wmsgs) {
			t.Errorf("#%d: msgs = %+v, want %+v", i, msgs, wmsgs)
		}
	}
}

// TestFollowerAppendEntries tests that when AppendEntries RPC is valid,
// the follower will delete the existing conflict entry and all that follow it,
// and append any new entries not already in the log.
// Also, it writes the new entry into stable storage.
// Reference: section 5.3
func TestFollowerAppendEntries(t *testing.T) {
	tests := []struct {
		index, term uint64
		ents        []pb.Entry
		wents       []pb.Entry
		wunstable   []pb.Entry
	}{
		{
			2, 2,
			[]pb.Entry{{Term: 3, Index: 3}},
			[]pb.Entry{{Term: 1, Index: 1}, {Term: 2, Index: 2}, {Term: 3, Index: 3}},
			[]pb.Entry{{Term: 3, Index: 3}},
		},
		{
			1, 1,
			[]pb.Entry{{Term: 3, Index: 2}, {Term: 4, Index: 3}},
			[]pb.Entry{{Term: 1, Index: 1}, {Term: 3, Index: 2}, {Term: 4, Index: 3}},
			[]pb.Entry{{Term: 3, Index: 2}, {Term: 4, Index: 3}},
		},
		{
			0, 0,
			[]pb.Entry{{Term: 1, Index: 1}},
			[]pb.Entry{{Term: 1, Index: 1}, {Term: 2, Index: 2}},
			nil,
		},
		{
			0, 0,
			[]pb.Entry{{Term: 3, Index: 1}},
			[]pb.Entry{{Term: 3, Index: 1}},
			[]pb.Entry{{Term: 3, Index: 1}},
		},
	}
	for i, tt := range tests {
		storage := NewTestLogDB()
		if err := storage.Append([]pb.Entry{{Term: 1, Index: 1}, {Term: 2, Index: 2}}); err != nil {
			t.Fatalf("%v", err)
		}
		r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, storage)
		r.becomeFollower(2, 2)

		ne(r.Handle(pb.Message{From: 2, To: 1, Type: pb.Replicate, Term: 2, LogTerm: tt.term, LogIndex: tt.index, Entries: tt.ents}), t)

		if g := getAllEntries(r.log); !reflect.DeepEqual(g, tt.wents) {
			t.Errorf("#%d: ents = %+v, want %+v", i, g, tt.wents)
		}
		if g := r.log.entriesToSave(); !reflect.DeepEqual(g, tt.wunstable) {
			t.Errorf("#%d: unstableEnts = %+v, want %+v", i, g, tt.wunstable)
		}
	}
}

// TestLeaderSyncFollowerLog tests that the leader could bring a follower's log
// into consistency with its own.
// Reference: section 5.3, figure 7
func TestLeaderSyncFollowerLog(t *testing.T) {
	ents := []pb.Entry{
		{},
		{Term: 1, Index: 1}, {Term: 1, Index: 2}, {Term: 1, Index: 3},
		{Term: 4, Index: 4}, {Term: 4, Index: 5},
		{Term: 5, Index: 6}, {Term: 5, Index: 7},
		{Term: 6, Index: 8}, {Term: 6, Index: 9}, {Term: 6, Index: 10},
	}
	term := uint64(8)
	tests := [][]pb.Entry{
		{
			{},
			{Term: 1, Index: 1}, {Term: 1, Index: 2}, {Term: 1, Index: 3},
			{Term: 4, Index: 4}, {Term: 4, Index: 5},
			{Term: 5, Index: 6}, {Term: 5, Index: 7},
			{Term: 6, Index: 8}, {Term: 6, Index: 9},
		},
		{
			{},
			{Term: 1, Index: 1}, {Term: 1, Index: 2}, {Term: 1, Index: 3},
			{Term: 4, Index: 4},
		},
		{
			{},
			{Term: 1, Index: 1}, {Term: 1, Index: 2}, {Term: 1, Index: 3},
			{Term: 4, Index: 4}, {Term: 4, Index: 5},
			{Term: 5, Index: 6}, {Term: 5, Index: 7},
			{Term: 6, Index: 8}, {Term: 6, Index: 9}, {Term: 6, Index: 10}, {Term: 6, Index: 11},
		},
		{
			{},
			{Term: 1, Index: 1}, {Term: 1, Index: 2}, {Term: 1, Index: 3},
			{Term: 4, Index: 4}, {Term: 4, Index: 5},
			{Term: 5, Index: 6}, {Term: 5, Index: 7},
			{Term: 6, Index: 8}, {Term: 6, Index: 9}, {Term: 6, Index: 10},
			{Term: 7, Index: 11}, {Term: 7, Index: 12},
		},
		{
			{},
			{Term: 1, Index: 1}, {Term: 1, Index: 2}, {Term: 1, Index: 3},
			{Term: 4, Index: 4}, {Term: 4, Index: 5}, {Term: 4, Index: 6}, {Term: 4, Index: 7},
		},
		{
			{},
			{Term: 1, Index: 1}, {Term: 1, Index: 2}, {Term: 1, Index: 3},
			{Term: 2, Index: 4}, {Term: 2, Index: 5}, {Term: 2, Index: 6},
			{Term: 3, Index: 7}, {Term: 3, Index: 8}, {Term: 3, Index: 9}, {Term: 3, Index: 10}, {Term: 3, Index: 11},
		},
	}
	for i, tt := range tests {
		leadStorage := NewTestLogDB()
		if err := leadStorage.Append(ents); err != nil {
			t.Fatalf("%v", err)
		}
		lead := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, leadStorage)
		lead.loadState(pb.State{Commit: lead.log.lastIndex(), Term: term})
		followerStorage := NewTestLogDB()
		if err := followerStorage.Append(tt); err != nil {
			t.Fatalf("%v", err)
		}
		follower := newTestRaft(2, []uint64{1, 2, 3}, 10, 1, followerStorage)
		follower.loadState(pb.State{Term: term - 1})
		// It is necessary to have a three-node shard.
		// The second may have more up-to-date log than the first one, so the
		// first node needs the vote from the third node to become the leader.
		n := newNetwork(lead, follower, nopStepper)
		n.send(pb.Message{From: 1, To: 1, Type: pb.Election})
		// The election occurs in the term after the one we loaded with
		// lead.loadState above.
		n.send(pb.Message{From: 3, To: 1, Type: pb.RequestVoteResp, Term: term + 1})

		n.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{}}})

		if g := diffu(ltoa(lead.log), ltoa(follower.log)); g != "" {
			t.Errorf("#%d: log diff:\n%s", i, g)
		}
	}
}

// TestVoteRequest tests that the vote request includes information about the candidate’s log
// and are sent to all of the other nodes.
// Reference: section 5.4.1
func TestVoteRequest(t *testing.T) {
	tests := []struct {
		ents  []pb.Entry
		wterm uint64
	}{
		{[]pb.Entry{{Term: 1, Index: 1}}, 2},
		{[]pb.Entry{{Term: 1, Index: 1}, {Term: 2, Index: 2}}, 3},
	}
	for j, tt := range tests {
		r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
		ne(r.Handle(pb.Message{
			From: 2, To: 1, Type: pb.Replicate, Term: tt.wterm - 1, LogTerm: 0, LogIndex: 0, Entries: tt.ents,
		}), t)
		r.readMessages()

		for i := uint64(1); i < r.electionTimeout*2; i++ {
			ne(r.nonLeaderTick(), t)
		}

		msgs := r.readMessages()
		sort.Sort(messageSlice(msgs))
		if len(msgs) != 2 {
			t.Fatalf("#%d: len(msg) = %d, want %d", j, len(msgs), 2)
		}
		for i, m := range msgs {
			if m.Type != pb.RequestVote {
				t.Errorf("#%d: msgType = %d, want %d", i, m.Type, pb.RequestVote)
			}
			if m.To != uint64(i+2) {
				t.Errorf("#%d: to = %d, want %d", i, m.To, i+2)
			}
			if m.Term != tt.wterm {
				t.Errorf("#%d: term = %d, want %d", i, m.Term, tt.wterm)
			}
			windex, wlogterm := tt.ents[len(tt.ents)-1].Index, tt.ents[len(tt.ents)-1].Term
			if m.LogIndex != windex {
				t.Errorf("#%d: index = %d, want %d", i, m.LogIndex, windex)
			}
			if m.LogTerm != wlogterm {
				t.Errorf("#%d: logterm = %d, want %d", i, m.LogTerm, wlogterm)
			}
		}
	}
}

// TestVoter tests the voter denies its vote if its own log is more up-to-date
// than that of the candidate.
// Reference: section 5.4.1
func TestVoter(t *testing.T) {
	tests := []struct {
		ents    []pb.Entry
		logterm uint64
		index   uint64

		wreject bool
	}{
		// same logterm
		{[]pb.Entry{{Term: 1, Index: 1}}, 1, 1, false},
		{[]pb.Entry{{Term: 1, Index: 1}}, 1, 2, false},
		{[]pb.Entry{{Term: 1, Index: 1}, {Term: 1, Index: 2}}, 1, 1, true},
		// candidate higher logterm
		{[]pb.Entry{{Term: 1, Index: 1}}, 2, 1, false},
		{[]pb.Entry{{Term: 1, Index: 1}}, 2, 2, false},
		{[]pb.Entry{{Term: 1, Index: 1}, {Term: 1, Index: 2}}, 2, 1, false},
		// voter higher logterm
		{[]pb.Entry{{Term: 2, Index: 1}}, 1, 1, true},
		{[]pb.Entry{{Term: 2, Index: 1}}, 1, 2, true},
		{[]pb.Entry{{Term: 2, Index: 1}, {Term: 1, Index: 2}}, 1, 1, true},
	}
	for i, tt := range tests {
		storage := NewTestLogDB()
		if err := storage.Append(tt.ents); err != nil {
			t.Fatalf("%v", err)
		}
		r := newTestRaft(1, []uint64{1, 2}, 10, 1, storage)

		ne(r.Handle(pb.Message{From: 2, To: 1, Type: pb.RequestVote, Term: 3, LogTerm: tt.logterm, LogIndex: tt.index}), t)

		msgs := r.readMessages()
		if len(msgs) != 1 {
			t.Fatalf("#%d: len(msg) = %d, want %d", i, len(msgs), 1)
		}
		m := msgs[0]
		if m.Type != pb.RequestVoteResp {
			t.Errorf("#%d: msgType = %d, want %d", i, m.Type, pb.RequestVoteResp)
		}
		if m.Reject != tt.wreject {
			t.Errorf("#%d: reject = %t, want %t", i, m.Reject, tt.wreject)
		}
	}
}

// TestLeaderOnlyCommitsLogFromCurrentTerm tests that only log entries from the leader’s
// current term are committed by counting replicas.
// Reference: section 5.4.2
func TestLeaderOnlyCommitsLogFromCurrentTerm(t *testing.T) {
	ents := []pb.Entry{{Term: 1, Index: 1}, {Term: 2, Index: 2}}
	tests := []struct {
		index   uint64
		wcommit uint64
	}{
		// do not commit log entries in previous terms
		{1, 0},
		{2, 0},
		// commit log in current term
		{3, 3},
	}
	for i, tt := range tests {
		storage := NewTestLogDB()
		if err := storage.Append(ents); err != nil {
			t.Fatalf("%v", err)
		}
		r := newTestRaft(1, []uint64{1, 2}, 10, 1, storage)
		r.loadState(pb.State{Term: 2})
		// become leader at term 3
		r.becomeCandidate()
		r.becomeLeader()
		r.readMessages()
		// propose a entry to current term
		ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{}}}), t)

		ne(r.Handle(pb.Message{From: 2, To: 1, Type: pb.ReplicateResp, Term: r.term, LogIndex: tt.index}), t)
		if r.log.committed != tt.wcommit {
			t.Errorf("#%d: commit = %d, want %d", i, r.log.committed, tt.wcommit)
		}
	}
}

func TestLeaderStartReplication(t *testing.T) {
	s := NewTestLogDB()
	r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, s)
	r.becomeCandidate()
	r.becomeLeader()
	commitNoopEntry(r, s)
	li := r.log.lastIndex()

	ents := []pb.Entry{{Cmd: []byte("some data")}}
	ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: ents}), t)
	if g := r.log.lastIndex(); g != li+1 {
		t.Errorf("lastIndex = %d, want %d", g, li+1)
	}
	if g := r.log.committed; g != li {
		t.Errorf("committed = %d, want %d", g, li)
	}
	wents := []pb.Entry{{Index: li + 1, Term: 1, Cmd: []byte("some data")}}
	wmsgs := []pb.Message{
		{From: 1, To: 2, Term: 1, Type: pb.Replicate, LogIndex: li, LogTerm: 1, Entries: wents, Commit: li},
		{From: 1, To: 3, Term: 1, Type: pb.Replicate, LogIndex: li, LogTerm: 1, Entries: wents, Commit: li},
	}
	msgs := r.readMessages()
	sort.Sort(messageSlice(msgs))
	if !reflect.DeepEqual(msgs, wmsgs) {
		t.Errorf("msgs = %+v, want %+v", msgs, wmsgs)
	}
	r.log.inmem.savedTo = li
	if g := r.log.entriesToSave(); !reflect.DeepEqual(g, wents) {
		t.Errorf("ents = %+v, want %+v", g, wents)
	}
}

type messageSlice []pb.Message

func (s messageSlice) Len() int           { return len(s) }
func (s messageSlice) Less(i, j int) bool { return fmt.Sprint(s[i]) < fmt.Sprint(s[j]) }
func (s messageSlice) Swap(i, j int)      { s[i], s[j] = s[j], s[i] }

func commitNoopEntry(r *raft, s ILogDB) {
	if r.state != leader {
		panic("it should only be used when it is the leader")
	}
	r.broadcastReplicateMessage()
	// simulate the response of Replicate
	msgs := r.readMessages()
	for _, m := range msgs {
		if m.Type != pb.Replicate || len(m.Entries) != 1 || m.Entries[0].Cmd != nil {
			panic("not a message to append noop entry")
		}
		if err := r.Handle(acceptAndReply(m)); err != nil {
			panic(err)
		}
	}
	// ignore further messages to refresh followers' commit index
	r.readMessages()
	if err := s.Append(r.log.entriesToSave()); err != nil {
		panic(err)
	}
	term, err := r.log.lastTerm()
	if err != nil {
		panic(err)
	}
	r.log.commitUpdate(pb.UpdateCommit{
		Processed:     r.log.committed,
		StableLogTo:   r.log.lastIndex(),
		StableLogTerm: term,
	})
}

func acceptAndReply(m pb.Message) pb.Message {
	if m.Type != pb.Replicate {
		panic("type should be Replicate")
	}
	return pb.Message{
		From:     m.To,
		To:       m.From,
		Term:     m.Term,
		Type:     pb.ReplicateResp,
		LogIndex: m.LogIndex + uint64(len(m.Entries)),
	}
}
````

## File: internal/raft/raft_etcd_test.go
````go
// Copyright 2015 The etcd Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//
// raft_etcd_test.go is ported from etcd rafto.
// updates have been made to reflect interface & implementation differences
// between dragonboat and etcd raft. some irrelevant tests were removed.
//

package raft

import (
	"bytes"
	"fmt"
	"io"
	"math"
	"math/rand"
	"os"
	"os/exec"
	"reflect"
	"sort"
	"strings"
	"testing"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/server"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

var (
	testRateLimit = uint64(1024 * 128)
)

func (r *raft) testOnlyHasConfigChangeToApply() bool {
	entries, err := r.log.getEntriesToApply(noLimit)
	if err != nil {
		panic(err)
	}
	if r.log.committed > r.log.processed && len(entries) > 0 {
		return countConfigChange(entries) > 0
	}

	return false
}

func diffu(a, b string) string {
	if a == b {
		return ""
	}
	aname, bname := mustTemp("base", a), mustTemp("other", b)
	defer func() {
		if err := os.Remove(aname); err != nil {
			panic(err)
		}
	}()
	defer func() {
		if err := os.Remove(bname); err != nil {
			panic(err)
		}
	}()
	cmd := exec.Command("diff", "-u", aname, bname)
	buf, err := cmd.CombinedOutput()
	if err != nil {
		if _, ok := err.(*exec.ExitError); ok {
			// do nothing
			return string(buf)
		}
		panic(err)
	}
	return string(buf)
}

func mustTemp(pre, body string) string {
	f, err := fileutil.CreateTemp("", pre)
	if err != nil {
		panic(err)
	}
	_, err = io.Copy(f, strings.NewReader(body))
	if err != nil {
		panic(err)
	}
	if err := f.Close(); err != nil {
		panic(err)
	}
	return f.Name()
}

func ltoa(l *entryLog) string {
	s := fmt.Sprintf("committed: %d\n", l.committed)
	s += fmt.Sprintf("applied:  %d\n", l.processed)
	for i, e := range getAllEntries(l) {
		s += fmt.Sprintf("#%d: %+v\n", i, e)
	}
	return s
}

// nextEnts returns the appliable entries and updates the applied index
func nextEnts(r *raft, s ILogDB) (ents []pb.Entry) {
	// Transfer all unstable entries to "stable" storage.
	if err := s.Append(r.log.entriesToSave()); err != nil {
		panic(err)
	}
	term, err := r.log.lastTerm()
	if err != nil {
		panic(err)
	}
	r.log.commitUpdate(pb.UpdateCommit{
		StableLogTo:   r.log.lastIndex(),
		StableLogTerm: term,
	})
	ents, err = r.log.entriesToApply()
	if err != nil {
		panic(err)
	}
	r.log.commitUpdate(pb.UpdateCommit{
		Processed: r.log.committed,
	})
	return ents
}

type stateMachine interface {
	Handle(m pb.Message) error
	readMessages() []pb.Message
}

func (r *raft) readMessages() []pb.Message {
	msgs := r.msgs
	r.msgs = make([]pb.Message, 0)

	return msgs
}

func checkLeaderTransferState(t *testing.T, r *raft, state State, lead uint64) {
	if r.state != state || r.leaderID != lead {
		t.Fatalf("after transferring, node has state %v lead %v, want state %v lead %v", r.state, r.leaderID, state, lead)
	}
	if r.leaderTransferTarget != NoNode {
		t.Fatalf("after transferring, node has leadTransferee %v, want leadTransferee %v", r.leaderTransferTarget, NoNode)
	}
}

// TestLeaderTransferToUpToDateNode verifies transferring should succeed
// if the transferee has the most up-to-date log entries when transfer starts.
func TestLeaderTransferToUpToDateNode(t *testing.T) {
	nt := newNetwork(nil, nil, nil)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	lead := nt.peers[1].(*raft)
	if lead.leaderID != 1 {
		t.Fatalf("after election leader is %x, want 1", lead.leaderID)
	}

	// Transfer leadership to 2.
	nt.send(pb.Message{From: 2, To: 1, Hint: 2, Type: pb.LeaderTransfer})
	checkLeaderTransferState(t, lead, follower, 2)
	// After some log replication, transfer leadership back to 1.
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{}}})
	nt.send(pb.Message{From: 1, To: 2, Hint: 1, Type: pb.LeaderTransfer})
	checkLeaderTransferState(t, lead, leader, 1)
}

// TestLeaderTransferToUpToDateNodeFromFollower verifies transferring should succeed
// if the transferee has the most up-to-date log entries when transfer starts.
// Not like TestLeaderTransferToUpToDateNode, where the leader transfer message
// is sent to the leader, in this test case every leader transfer message is sent
// to the follower.
func TestLeaderTransferToUpToDateNodeFromFollower(t *testing.T) {
	nt := newNetwork(nil, nil, nil)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	lead := nt.peers[1].(*raft)
	if lead.leaderID != 1 {
		t.Fatalf("after election leader is %x, want 1", lead.leaderID)
	}
	// Transfer leadership to 2.
	nt.send(pb.Message{From: 2, To: 2, Hint: 2, Type: pb.LeaderTransfer})
	checkLeaderTransferState(t, lead, follower, 2)
	// After some log replication, transfer leadership back to 1.
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{}}})
	nt.send(pb.Message{From: 1, To: 1, Hint: 1, Type: pb.LeaderTransfer})
	checkLeaderTransferState(t, lead, leader, 1)
}

// TestLeaderTransferWithPreVote ensures transferring leader still works
// even the current leader is still under its leader lease
func TestLeaderTransferWithPreVote(t *testing.T) {
	nt := newNetwork(nil, nil, nil)
	for i := uint64(1); i < 4; i++ {
		r := nt.peers[i].(*raft)
		r.checkQuorum = true
		r.preVote = true
		setRandomizedElectionTimeout(r, r.electionTimeout+i)
	}
	// Letting peer 2 electionElapsed reach to timeout so that it can vote for peer 1
	f := nt.peers[2].(*raft)
	for i := uint64(0); i < f.electionTimeout; i++ {
		ne(f.tick(), t)
	}
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	lead := nt.peers[1].(*raft)
	if lead.leaderID != 1 {
		t.Fatalf("after election leader is %x, want 1", lead.leaderID)
	}
	// Transfer leadership to 2.
	nt.send(pb.Message{From: 2, To: 1, Hint: 2, Type: pb.LeaderTransfer})
	checkLeaderTransferState(t, lead, follower, 2)
	// After some log replication, transfer leadership back to 1.
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{}}})
	nt.send(pb.Message{From: 1, To: 2, Hint: 1, Type: pb.LeaderTransfer})
	checkLeaderTransferState(t, lead, leader, 1)
}

// TestLeaderTransferWithCheckQuorum ensures transferring leader still works
// even the current leader is still under its leader lease
func TestLeaderTransferWithCheckQuorum(t *testing.T) {
	nt := newNetwork(nil, nil, nil)
	for i := uint64(1); i < 4; i++ {
		r := nt.peers[i].(*raft)
		r.checkQuorum = true
		setRandomizedElectionTimeout(r, r.electionTimeout+i)
	}
	// Letting peer 2 electionElapsed reach to timeout so that it can vote for peer 1
	f := nt.peers[2].(*raft)
	for i := uint64(0); i < f.electionTimeout; i++ {
		ne(f.tick(), t)
	}
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	lead := nt.peers[1].(*raft)
	if lead.leaderID != 1 {
		t.Fatalf("after election leader is %x, want 1", lead.leaderID)
	}
	// Transfer leadership to 2.
	nt.send(pb.Message{From: 2, To: 1, Hint: 2, Type: pb.LeaderTransfer})
	checkLeaderTransferState(t, lead, follower, 2)
	// After some log replication, transfer leadership back to 1.
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{}}})
	nt.send(pb.Message{From: 1, To: 2, Hint: 1, Type: pb.LeaderTransfer})
	checkLeaderTransferState(t, lead, leader, 1)
}

func TestLeaderTransferToSlowFollower(t *testing.T) {
	nt := newNetwork(nil, nil, nil)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	nt.isolate(3)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{}}})
	nt.recover()
	lead := nt.peers[1].(*raft)
	if lead.remotes[3].match != 1 {
		t.Fatalf("node 1 has match %x for node 3, want %x", lead.remotes[3].match, 1)
	}
	// Transfer leadership to 3 when node 3 is lack of log.
	// this will not actually transfer the leadership. on receiving the
	// MsgLeaderTransfer, dragonboat's raft implementation won't force a MsgAppend msg
	// to be sent
	nt.send(pb.Message{From: 3, To: 1, Hint: 3, Type: pb.LeaderTransfer})
	if lead.state != leader || lead.leaderID != 1 {
		t.Errorf("leadership transferred to unexpected node")
	}
	if !lead.leaderTransfering() {
		t.Errorf("leader transfer flag is gone")
	}
	lead.abortLeaderTransfer()
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{}}})
	nt.send(pb.Message{From: 3, To: 1, Hint: 3, Type: pb.LeaderTransfer})
	checkLeaderTransferState(t, lead, follower, 3)
}

func TestLeaderTransferAfterSnapshot(t *testing.T) {
	nt := newNetwork(nil, nil, nil)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	nt.isolate(3)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{}}})
	lead := nt.peers[1].(*raft)
	nextEnts(lead, nt.storage[1])
	m := getTestMembership(lead.nodesSorted())
	ss, err := nt.storage[1].(*TestLogDB).getSnapshot(lead.log.processed, &m)
	if err != nil {
		t.Fatalf("failed to get snapshot")
	}
	if err := nt.storage[1].CreateSnapshot(ss); err != nil {
		t.Fatalf("%v", err)
	}
	if err := nt.storage[1].Compact(lead.log.processed); err != nil {
		t.Fatalf("%v", err)
	}
	nt.recover()
	if lead.remotes[3].match != 1 {
		t.Fatalf("node 1 has match %x for node 3, want %x", lead.remotes[3].match, 1)
	}
	// Transfer leadership to 3 when node 3 is lack of snapshot.
	nt.send(pb.Message{From: 3, To: 1, Hint: 3, Type: pb.LeaderTransfer})
	// Send pb.HeartbeatResp to leader to trigger a snapshot for node 3.
	nt.send(pb.Message{From: 3, To: 1, Type: pb.HeartbeatResp})
	checkLeaderTransferState(t, lead, follower, 3)
}

func TestLeaderTransferToSelf(t *testing.T) {
	nt := newNetwork(nil, nil, nil)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	lead := nt.peers[1].(*raft)
	// Transfer leadership to self, there will be noop.
	nt.send(pb.Message{From: 1, To: 1, Hint: 1, Type: pb.LeaderTransfer})
	checkLeaderTransferState(t, lead, leader, 1)
}

func TestLeaderTransferToNonExistingNode(t *testing.T) {
	nt := newNetwork(nil, nil, nil)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	lead := nt.peers[1].(*raft)
	// Transfer leadership to non-existing node, there will be noop.
	nt.send(pb.Message{From: 4, To: 1, Hint: 4, Type: pb.LeaderTransfer})
	checkLeaderTransferState(t, lead, leader, 1)
}

func TestLeaderTransferTimeout(t *testing.T) {
	nt := newNetwork(nil, nil, nil)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	nt.isolate(3)
	lead := nt.peers[1].(*raft)
	// Transfer leadership to isolated node, wait for timeout.
	nt.send(pb.Message{From: 3, To: 1, Hint: 3, Type: pb.LeaderTransfer})
	if lead.leaderTransferTarget != 3 {
		t.Fatalf("wait transferring, leadTransferee = %v, want %v", lead.leaderTransferTarget, 3)
	}
	for i := uint64(0); i < lead.heartbeatTimeout; i++ {
		ne(lead.tick(), t)
	}
	if lead.leaderTransferTarget != 3 {
		t.Fatalf("wait transferring, leadTransferee = %v, want %v", lead.leaderTransferTarget, 3)
	}
	for i := uint64(0); i < lead.electionTimeout; i++ {
		ne(lead.tick(), t)
	}
	checkLeaderTransferState(t, lead, leader, 1)
}

func TestLeaderTransferIgnoreProposal(t *testing.T) {
	nt := newNetwork(nil, nil, nil)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	nt.isolate(3)
	lead := nt.peers[1].(*raft)
	// Transfer leadership to isolated node to let transfer pending, then send proposal.
	nt.send(pb.Message{From: 3, To: 1, Hint: 3, Type: pb.LeaderTransfer})
	if lead.leaderTransferTarget != 3 {
		t.Fatalf("wait transferring, leadTransferee = %v, want %v", lead.leaderTransferTarget, 3)
	}
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{}}})
	matched := lead.remotes[2].match
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{}}})
	if lead.remotes[2].match != matched {
		t.Fatalf("node 1 has match %x, want %x", lead.remotes[2].match, matched)
	}
}

func TestLeaderTransferReceiveHigherTermVote(t *testing.T) {
	nt := newNetwork(nil, nil, nil)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	nt.isolate(3)
	lead := nt.peers[1].(*raft)
	// Transfer leadership to isolated node to let transfer pending.
	nt.send(pb.Message{From: 3, To: 1, Hint: 3, Type: pb.LeaderTransfer})
	if lead.leaderTransferTarget != 3 {
		t.Fatalf("wait transferring, leadTransferee = %v, want %v", lead.leaderTransferTarget, 3)
	}
	nt.send(pb.Message{From: 2, To: 2, Type: pb.Election, LogIndex: 1, Term: 2})
	checkLeaderTransferState(t, lead, follower, 2)
}

func TestLeaderTransferRemoveNode(t *testing.T) {
	nt := newNetwork(nil, nil, nil)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	nt.ignore(pb.TimeoutNow)
	lead := nt.peers[1].(*raft)
	// The leadTransferee is removed when leadship transferring.
	nt.send(pb.Message{From: 3, To: 1, Hint: 3, Type: pb.LeaderTransfer})
	if lead.leaderTransferTarget != 3 {
		t.Fatalf("wait transferring, leadTransferee = %v, want %v", lead.leaderTransferTarget, 3)
	}
	lead.removeNode(3)
	checkLeaderTransferState(t, lead, leader, 1)
}

func TestNewLeaderTransferCanNotOverrideOngoingLeaderTransfer(t *testing.T) {
	nt := newNetwork(nil, nil, nil)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	nt.isolate(3)
	lead := nt.peers[1].(*raft)
	nt.send(pb.Message{From: 3, To: 1, Hint: 3, Type: pb.LeaderTransfer})
	if lead.leaderTransferTarget != 3 {
		t.Fatalf("wait transferring, leadTransferee = %v, want %v", lead.leaderTransferTarget, 3)
	}
	ot := lead.electionTick
	nt.send(pb.Message{From: 1, To: 1, Hint: 1, Type: pb.LeaderTransfer})
	// the above MsgLeaderTransfer should be ignored
	if lead.leaderTransferTarget != 3 {
		t.Fatalf("wait transferring, leadTransferee = %v, want %v", lead.leaderTransferTarget, 3)
	}
	if lead.electionTick != ot {
		t.Fatalf("election tick changed unexpectedly")
	}
}

// TestLeaderTransferSecondTransferToSameNode verifies second transfer leader request
// to the same node should not extend the timeout while the first one is pending.
func TestLeaderTransferSecondTransferToSameNode(t *testing.T) {
	nt := newNetwork(nil, nil, nil)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	nt.isolate(3)
	lead := nt.peers[1].(*raft)
	nt.send(pb.Message{From: 3, To: 1, Hint: 3, Type: pb.LeaderTransfer})
	if lead.leaderTransferTarget != 3 {
		t.Fatalf("wait transferring, leadTransferee = %v, want %v", lead.leaderTransferTarget, 3)
	}
	for i := uint64(0); i < lead.heartbeatTimeout; i++ {
		ne(lead.tick(), t)
	}
	// Second transfer leadership request to the same node.
	nt.send(pb.Message{From: 3, To: 1, Hint: 3, Type: pb.LeaderTransfer})
	for i := uint64(0); i < lead.electionTimeout-lead.heartbeatTimeout; i++ {
		ne(lead.tick(), t)
	}
	checkLeaderTransferState(t, lead, leader, 1)
}

// TestRemoteResumeByHeartbeatResp ensures raft.heartbeat reset progress.paused by heartbeat response.
func TestRemoteResumeByHeartbeatResp(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	r.remotes[2].retryToWait()

	ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.LeaderHeartbeat}), t)
	if r.remotes[2].state != remoteWait {
		t.Errorf("st = %s, want true", r.remotes[2].state)
	}

	r.remotes[2].becomeReplicate()
	ne(r.Handle(pb.Message{From: 2, To: 1, Type: pb.HeartbeatResp}), t)
	if r.remotes[2].state == remoteWait {
		t.Errorf("paused = %s, want false", r.remotes[2].state)
	}
}

func TestRemotePaused(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("somedata")}}}), t)
	ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("somedata")}}}), t)
	ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("somedata")}}}), t)

	ms := r.readMessages()
	if len(ms) != 1 {
		t.Errorf("len(ms) = %d, want 1", len(ms))
	}
}

func TestLeaderElection(t *testing.T) {
	testLeaderElection(t)
}

/*
func TestLeaderElectionPreVote(t *testing.T) {
	testLeaderElection(t, true)
}
*/

func testLeaderElection(t *testing.T) {
	var cfg func(config.Config)
	tests := []struct {
		*network
		state   State
		expTerm uint64
	}{
		{newNetworkWithConfig(cfg, nil, nil, nil), leader, 1},
		{newNetworkWithConfig(cfg, nil, nil, nopStepper), leader, 1},
		{newNetworkWithConfig(cfg, nil, nopStepper, nopStepper), candidate, 1},
		{newNetworkWithConfig(cfg, nil, nopStepper, nopStepper, nil), candidate, 1},
		{newNetworkWithConfig(cfg, nil, nopStepper, nopStepper, nil, nil), leader, 1},

		// three logs further along than 0, but in the same term so rejections
		// are returned instead of the votes being ignored.
		{newNetworkWithConfig(cfg,
			nil, entsWithConfig(cfg, 1), entsWithConfig(cfg, 1), entsWithConfig(cfg, 1, 1), nil),
			follower, 1},
	}

	for i, tt := range tests {
		tt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
		sm := tt.peers[1].(*raft)
		var expState State
		var expTerm uint64
		expState = tt.state
		expTerm = tt.expTerm

		if sm.state != expState {
			t.Errorf("#%d: state = %s, want %s", i, sm.state, expState)
		}
		if g := sm.term; g != expTerm {
			t.Errorf("#%d: term = %d, want %d", i, g, expTerm)
		}
	}
}

func TestLeaderCycle(t *testing.T) {
	testLeaderCycle(t)
}

// testLeaderCycle verifies that each node in a shard can campaign
// and be elected in turn. This ensures that elections (including
// pre-vote) work when not starting from a clean slate (as they do in
// TestLeaderElection)
func testLeaderCycle(t *testing.T) {
	var cfg func(config.Config)
	n := newNetworkWithConfig(cfg, nil, nil, nil)
	for campaignerID := uint64(1); campaignerID <= 3; campaignerID++ {
		n.send(pb.Message{From: campaignerID, To: campaignerID, Type: pb.Election})

		for _, peer := range n.peers {
			sm := peer.(*raft)
			if sm.replicaID == campaignerID && sm.state != leader {
				t.Errorf("campaigning node %d state = %v, want leader",
					sm.replicaID, sm.state)
			} else if sm.replicaID != campaignerID && sm.state != follower {
				t.Errorf("after campaign of node %d, "+
					"node %d had state = %v, want follower",
					campaignerID, sm.replicaID, sm.state)
			}
		}
	}
}

// TestLeaderElectionOverwriteNewerLogs tests a scenario in which a
// newly-elected leader does *not* have the newest (i.e. highest term)
// log entries, and must overwrite higher-term log entries with
// lower-term ones.
func TestLeaderElectionOverwriteNewerLogs(t *testing.T) {
	testLeaderElectionOverwriteNewerLogs(t)
}

func testLeaderElectionOverwriteNewerLogs(t *testing.T) {
	var cfg func(config.Config)
	// This network represents the results of the following sequence of
	// events:
	// - Node 1 won the election in term 1.
	// - Node 1 replicated a log entry to node 2 but died before sending
	//   it to other nodes.
	// - Node 3 won the second election in term 2.
	// - Node 3 wrote an entry to its logs but died without sending it
	//   to any other nodes.
	//
	// At this point, nodes 1, 2, and 3 all have uncommitted entries in
	// their logs and could win an election at term 3. The winner's log
	// entry overwrites the losers'. (TestLeaderSyncFollowerLog tests
	// the case where older log entries are overwritten, so this test
	// focuses on the case where the newer entries are lost).
	n := newNetworkWithConfig(cfg,
		entsWithConfig(cfg, 1),     // Node 1: Won first election
		entsWithConfig(cfg, 1),     // Node 2: Got logs from node 1
		entsWithConfig(cfg, 2),     // Node 3: Won second election
		votedWithConfig(cfg, 3, 2), // Node 4: Voted but didn't get logs
		votedWithConfig(cfg, 3, 2)) // Node 5: Voted but didn't get logs

	// Node 1 campaigns. The election fails because a quorum of nodes
	// know about the election that already happened at term 2. Node 1's
	// term is pushed ahead to 2.
	n.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	sm1 := n.peers[1].(*raft)
	if sm1.state != follower {
		t.Errorf("state = %s, want follower", sm1.state)
	}
	if sm1.term != 2 {
		t.Errorf("term = %d, want 2", sm1.term)
	}

	// Node 1 campaigns again with a higher term. This time it succeeds.
	n.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	if sm1.state != leader {
		t.Errorf("state = %s, want leader", sm1.state)
	}
	if sm1.term != 3 {
		t.Errorf("term = %d, want 3", sm1.term)
	}

	// Now all nodes agree on a log entry with term 1 at index 1 (and
	// term 3 at index 2).
	for i := range n.peers {
		sm := n.peers[i].(*raft)
		entries := getAllEntries(sm.log)
		if len(entries) != 2 {
			t.Fatalf("node %d: len(entries) == %d, want 2", i, len(entries))
		}
		if entries[0].Term != 1 {
			t.Errorf("node %d: term at index 1 == %d, want 1", i, entries[0].Term)
		}
		if entries[1].Term != 3 {
			t.Errorf("node %d: term at index 2 == %d, want 3", i, entries[1].Term)
		}
	}
}

func TestVoteFromAnyState(t *testing.T) {
	testVoteFromAnyState(t, pb.RequestVote)
}

func testVoteFromAnyState(t *testing.T, vt pb.MessageType) {
	for st := State(0); st < numStates; st++ {
		r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
		r.term = 1

		switch st {
		case follower:
			r.becomeFollower(r.term, 3)
		case candidate:
			r.becomeCandidate()
		case leader:
			r.becomeCandidate()
			r.becomeLeader()
		}

		// Note that setting our state above may have advanced r.term
		// past its initial value.
		origTerm := r.term
		newTerm := r.term + 1

		msg := pb.Message{
			From:     2,
			To:       1,
			Type:     vt,
			Term:     newTerm,
			LogTerm:  newTerm,
			LogIndex: 42,
		}
		ne(r.Handle(msg), t)
		if len(r.msgs) != 1 {
			t.Errorf("%s,%s: %d response messages, want 1: %+v", vt, st, len(r.msgs), r.msgs)
		} else {
			resp := r.msgs[0]
			if resp.Type != pb.RequestVoteResp {
				t.Errorf("%s,%s: response message is %s, want %s",
					vt, st, resp.Type, pb.RequestVoteResp)
			}
			if resp.Reject {
				t.Errorf("%s,%s: unexpected rejection", vt, st)
			}
		}

		// If this was a real vote, we reset our state and term.
		if vt == pb.RequestVote {
			if r.state != follower {
				t.Errorf("%s,%s: state %s, want %s", vt, st, r.state, follower)
			}
			if r.term != newTerm {
				t.Errorf("%s,%s: term %d, want %d", vt, st, r.term, newTerm)
			}
			if r.vote != 2 {
				t.Errorf("%s,%s: vote %d, want 2", vt, st, r.vote)
			}
		} else {
			// In a prevote, nothing changes.
			if r.state != st {
				t.Errorf("%s,%s: state %s, want %s", vt, st, r.state, st)
			}
			if r.term != origTerm {
				t.Errorf("%s,%s: term %d, want %d", vt, st, r.term, origTerm)
			}
			// if st == follower or statePreCandidate, r hasn't voted yet.
			// In candidate or leader, it's voted for itself.
			if r.vote != NoLeader && r.vote != 1 {
				t.Errorf("%s,%s: vote %d, want %d or 1", vt, st, r.vote, NoLeader)
			}
		}
	}
}

func TestLogReplication(t *testing.T) {
	tests := []struct {
		*network
		msgs       []pb.Message
		wcommitted uint64
	}{
		{
			newNetwork(nil, nil, nil),
			[]pb.Message{
				{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("somedata")}}},
			},
			2,
		},
		{
			newNetwork(nil, nil, nil),
			[]pb.Message{
				{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("somedata")}}},
				{From: 1, To: 2, Type: pb.Election},
				{From: 1, To: 2, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("somedata")}}},
			},
			4,
		},
	}

	for i, tt := range tests {
		tt.send(pb.Message{From: 1, To: 1, Type: pb.Election})

		for _, m := range tt.msgs {
			tt.send(m)
		}

		for j, x := range tt.peers {
			sm := x.(*raft)

			if sm.log.committed != tt.wcommitted {
				t.Errorf("#%d.%d: committed = %d, want %d", i, j, sm.log.committed, tt.wcommitted)
			}

			ents := []pb.Entry{}
			for _, e := range nextEnts(sm, tt.storage[j]) {
				if e.Cmd != nil {
					ents = append(ents, e)
				}
			}
			props := []pb.Message{}
			for _, m := range tt.msgs {
				if m.Type == pb.Propose {
					props = append(props, m)
				}
			}
			for k, m := range props {
				if !bytes.Equal(ents[k].Cmd, m.Entries[0].Cmd) {
					t.Errorf("#%d.%d: data = %d, want %d", i, j, ents[k].Cmd, m.Entries[0].Cmd)
				}
			}
		}
	}
}

func TestSingleNodeCommit(t *testing.T) {
	tt := newNetwork(nil)
	tt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	tt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("some data")}}})
	tt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("some data")}}})

	sm := tt.peers[1].(*raft)
	if sm.log.committed != 3 {
		t.Errorf("committed = %d, want %d", sm.log.committed, 3)
	}
}

// TestCannotCommitWithoutNewTermEntry tests the entries cannot be committed
// when leader changes, no new proposal comes in and ChangeTerm proposal is
// filtered.
func TestCannotCommitWithoutNewTermEntry(t *testing.T) {
	tt := newNetwork(nil, nil, nil, nil, nil)
	tt.send(pb.Message{From: 1, To: 1, Type: pb.Election})

	// 0 cannot reach 2,3,4
	tt.cut(1, 3)
	tt.cut(1, 4)
	tt.cut(1, 5)

	tt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("some data")}}})
	tt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("some data")}}})

	sm := tt.peers[1].(*raft)
	if sm.log.committed != 1 {
		t.Errorf("committed = %d, want %d", sm.log.committed, 1)
	}

	// network recovery
	tt.recover()
	// avoid committing ChangeTerm proposal
	tt.ignore(pb.Replicate)

	// elect 2 as the new leader with term 2
	tt.send(pb.Message{From: 2, To: 2, Type: pb.Election})

	// no log entries from previous term should be committed
	sm = tt.peers[2].(*raft)
	if sm.log.committed != 1 {
		t.Errorf("committed = %d, want %d", sm.log.committed, 1)
	}

	tt.recover()
	// send heartbeat; reset wait
	tt.send(pb.Message{From: 2, To: 2, Type: pb.LeaderHeartbeat})
	// append an entry at current term
	tt.send(pb.Message{From: 2, To: 2, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("some data")}}})
	// expect the committed to be advanced
	if sm.log.committed != 5 {
		t.Errorf("committed = %d, want %d", sm.log.committed, 5)
	}
}

// TestCommitWithoutNewTermEntry tests the entries could be committed
// when leader changes, no new proposal comes in.
func TestCommitWithoutNewTermEntry(t *testing.T) {
	tt := newNetwork(nil, nil, nil, nil, nil)
	tt.send(pb.Message{From: 1, To: 1, Type: pb.Election})

	// 0 cannot reach 2,3,4
	tt.cut(1, 3)
	tt.cut(1, 4)
	tt.cut(1, 5)

	tt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("some data")}}})
	tt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("some data")}}})

	sm := tt.peers[1].(*raft)
	if sm.log.committed != 1 {
		t.Errorf("committed = %d, want %d", sm.log.committed, 1)
	}

	// network recovery
	tt.recover()

	// elect 1 as the new leader with term 2
	// after append a ChangeTerm entry from the current term, all entries
	// should be committed
	tt.send(pb.Message{From: 2, To: 2, Type: pb.Election})

	if sm.log.committed != 4 {
		t.Errorf("committed = %d, want %d", sm.log.committed, 4)
	}
}

func TestDuelingCandidates(t *testing.T) {
	a := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	b := newTestRaft(2, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	c := newTestRaft(3, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())

	nt := newNetwork(a, b, c)
	nt.cut(1, 3)

	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	nt.send(pb.Message{From: 3, To: 3, Type: pb.Election})

	// 1 becomes leader since it receives votes from 1 and 2
	sm := nt.peers[1].(*raft)
	if sm.state != leader {
		t.Errorf("state = %s, want %s", sm.state, leader)
	}

	// 3 stays as candidate since it receives a vote from 3 and a rejection from 2
	sm = nt.peers[3].(*raft)
	if sm.state != candidate {
		t.Errorf("state = %s, want %s", sm.state, candidate)
	}

	nt.recover()

	// candidate 3 now increases its term and tries to vote again
	// we expect it to disrupt the leader 1 since it has a higher term
	// 3 will be follower again since both 1 and 2 rejects its vote request since 3 does not have a long enough log
	nt.send(pb.Message{From: 3, To: 3, Type: pb.Election})

	wlog := &entryLog{
		logdb:     &TestLogDB{entries: []pb.Entry{{Cmd: nil, Term: 1, Index: 1}}},
		committed: 1,
		inmem:     inMemory{markerIndex: 2},
	}
	tests := []struct {
		sm       *raft
		state    State
		term     uint64
		entryLog *entryLog
	}{
		{a, follower, 2, wlog},
		{b, follower, 2, wlog},
		{c, follower, 2, newEntryLog(NewTestLogDB(), server.NewInMemRateLimiter(0))},
	}

	for i, tt := range tests {
		if g := tt.sm.state; g != tt.state {
			t.Errorf("#%d: state = %s, want %s", i, g, tt.state)
		}
		if g := tt.sm.term; g != tt.term {
			t.Errorf("#%d: term = %d, want %d", i, g, tt.term)
		}
		base := ltoa(tt.entryLog)
		if sm, ok := nt.peers[1+uint64(i)].(*raft); ok {
			l := ltoa(sm.log)
			if g := diffu(base, l); g != "" {
				t.Errorf("#%d: diff:\n%s", i, g)
			}
		} else {
			t.Logf("#%d: empty log", i)
		}
	}
}

func TestDuelingPreCandidates(t *testing.T) {
	a := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	b := newTestRaft(2, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	c := newTestRaft(3, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	a.preVote = true
	b.preVote = true
	c.preVote = true
	nt := newNetwork(a, b, c)
	nt.cut(1, 3)

	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	nt.send(pb.Message{From: 3, To: 3, Type: pb.Election})

	// 1 becomes leader since it receives votes from 1 and 2
	sm := nt.peers[1].(*raft)
	if sm.state != leader {
		t.Errorf("state = %s, want %s", sm.state, leader)
	}

	// 3 campaigns then reverts to follower when its PreVote is rejected
	sm = nt.peers[3].(*raft)
	if sm.state != follower {
		t.Errorf("state = %s, want %s", sm.state, follower)
	}

	nt.recover()

	// Candidate 3 now increases its term and tries to vote again.
	// With PreVote, it does not disrupt the leader.
	nt.send(pb.Message{From: 3, To: 3, Type: pb.Election})

	wlog := &entryLog{
		logdb:     &TestLogDB{entries: []pb.Entry{{Cmd: nil, Term: 1, Index: 1}}},
		committed: 1,
		inmem:     inMemory{markerIndex: 2},
	}
	tests := []struct {
		sm       *raft
		state    State
		term     uint64
		entryLog *entryLog
	}{
		{a, leader, 1, wlog},
		{b, follower, 1, wlog},
		{c, follower, 1, newEntryLog(NewTestLogDB(), server.NewInMemRateLimiter(0))},
	}

	for i, tt := range tests {
		if g := tt.sm.state; g != tt.state {
			t.Errorf("#%d: state = %s, want %s", i, g, tt.state)
		}
		if g := tt.sm.term; g != tt.term {
			t.Errorf("#%d: term = %d, want %d", i, g, tt.term)
		}
		base := ltoa(tt.entryLog)
		if sm, ok := nt.peers[1+uint64(i)].(*raft); ok {
			l := ltoa(sm.log)
			if g := diffu(base, l); g != "" {
				t.Errorf("#%d: diff:\n%s", i, g)
			}
		} else {
			t.Logf("#%d: empty log", i)
		}
	}
}

func TestCandidateConcede(t *testing.T) {
	tt := newNetwork(nil, nil, nil)
	tt.isolate(1)

	tt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	tt.send(pb.Message{From: 3, To: 3, Type: pb.Election})

	// heal the partition
	tt.recover()
	// send heartbeat; reset wait
	tt.send(pb.Message{From: 3, To: 3, Type: pb.LeaderHeartbeat})

	data := []byte("force follower")
	// send a proposal to 3 to flush out a MsgAppend to 1
	tt.send(pb.Message{From: 3, To: 3, Type: pb.Propose, Entries: []pb.Entry{{Cmd: data}}})
	// send heartbeat; flush out commit
	tt.send(pb.Message{From: 3, To: 3, Type: pb.LeaderHeartbeat})

	a := tt.peers[1].(*raft)
	if g := a.state; g != follower {
		t.Errorf("state = %s, want %s", g, follower)
	}
	if g := a.term; g != 1 {
		t.Errorf("term = %d, want %d", g, 1)
	}
	wantLog := ltoa(&entryLog{
		logdb: &TestLogDB{
			entries: []pb.Entry{{Cmd: nil, Term: 1, Index: 1}, {Term: 1, Index: 2, Cmd: data}},
		},
		inmem:     inMemory{markerIndex: 3},
		committed: 2,
	})
	for i, p := range tt.peers {
		if sm, ok := p.(*raft); ok {
			l := ltoa(sm.log)
			if g := diffu(wantLog, l); g != "" {
				t.Errorf("#%d: diff:\n%s", i, g)
			}
		} else {
			t.Logf("#%d: empty log", i)
		}
	}
}

func TestSingleNodeCandidate(t *testing.T) {
	tt := newNetwork(nil)
	tt.send(pb.Message{From: 1, To: 1, Type: pb.Election})

	sm := tt.peers[1].(*raft)
	if sm.state != leader {
		t.Errorf("state = %d, want %d", sm.state, leader)
	}
}

func TestOldMessages(t *testing.T) {
	tt := newNetwork(nil, nil, nil)
	// make 0 leader @ term 3
	tt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	tt.send(pb.Message{From: 2, To: 2, Type: pb.Election})
	tt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	// pretend we're an old leader trying to make progress; this entry is expected to be ignored.
	tt.send(pb.Message{From: 2, To: 1, Type: pb.Replicate, Term: 2, Entries: []pb.Entry{{Index: 3, Term: 2}}})
	// commit a new entry
	tt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("somedata")}}})

	ilog := &entryLog{
		logdb: &TestLogDB{
			entries: []pb.Entry{
				{Cmd: nil, Term: 1, Index: 1},
				{Cmd: nil, Term: 2, Index: 2}, {Cmd: nil, Term: 3, Index: 3},
				{Cmd: []byte("somedata"), Term: 3, Index: 4},
			},
		},
		inmem:     inMemory{markerIndex: 5},
		committed: 4,
	}
	base := ltoa(ilog)
	for i, p := range tt.peers {
		if sm, ok := p.(*raft); ok {
			l := ltoa(sm.log)
			if g := diffu(base, l); g != "" {
				t.Errorf("#%d: diff:\n%s", i, g)
			}
		} else {
			t.Logf("#%d: empty log", i)
		}
	}
}

// TestOldMessagesReply - optimization - reply with new term.

func TestProposal(t *testing.T) {
	tests := []struct {
		*network
		success bool
	}{
		{newNetwork(nil, nil, nil), true},
		{newNetwork(nil, nil, nopStepper), true},
		{newNetwork(nil, nopStepper, nopStepper), false},
		{newNetwork(nil, nopStepper, nopStepper, nil), false},
		{newNetwork(nil, nopStepper, nopStepper, nil, nil), true},
	}

	for j, tt := range tests {
		success := tt.success
		jj := j
		ttt := tt
		send := func(m pb.Message) {
			defer func() {
				// only recover is we expect it to panic so
				// panics we don't expect go up.
				if !success {
					e := recover()
					if e != nil {
						t.Logf("#%d: err: %s", jj, e)
					}
				}
			}()
			ttt.send(m)
		}

		data := []byte("somedata")

		// promote 0 the leader
		send(pb.Message{From: 1, To: 1, Type: pb.Election})
		send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: data}}})

		wantLog := newEntryLog(NewTestLogDB(), server.NewInMemRateLimiter(0))
		if tt.success {
			wantLog = &entryLog{
				logdb: &TestLogDB{
					entries: []pb.Entry{{Cmd: nil, Term: 1, Index: 1}, {Term: 1, Index: 2, Cmd: data}},
				},
				inmem:     inMemory{markerIndex: 3},
				committed: 2}
		}
		base := ltoa(wantLog)
		for i, p := range tt.peers {
			if sm, ok := p.(*raft); ok {
				l := ltoa(sm.log)
				if g := diffu(base, l); g != "" {
					t.Errorf("#%d: diff:\n%s", i, g)
				}
			} else {
				t.Logf("#%d: empty log", i)
			}
		}
		sm := tt.peers[1].(*raft)
		if g := sm.term; g != 1 {
			t.Errorf("#%d: term = %d, want %d", j, g, 1)
		}
	}
}

func TestProposalByProxy(t *testing.T) {
	data := []byte("somedata")
	tests := []*network{
		newNetwork(nil, nil, nil),
		newNetwork(nil, nil, nopStepper),
	}

	for j, tt := range tests {
		// promote 0 the leader
		tt.send(pb.Message{From: 1, To: 1, Type: pb.Election})

		// propose via follower
		tt.send(pb.Message{From: 2, To: 2, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("somedata")}}})

		wantLog := &entryLog{
			logdb: &TestLogDB{
				entries: []pb.Entry{{Cmd: nil, Term: 1, Index: 1}, {Term: 1, Cmd: data, Index: 2}},
			},
			inmem:     inMemory{markerIndex: 3},
			committed: 2}
		base := ltoa(wantLog)
		for i, p := range tt.peers {
			if sm, ok := p.(*raft); ok {
				l := ltoa(sm.log)
				if g := diffu(base, l); g != "" {
					t.Errorf("#%d: diff:\n%s", i, g)
				}
			} else {
				t.Logf("#%d: empty log", i)
			}
		}
		sm := tt.peers[1].(*raft)
		if g := sm.term; g != 1 {
			t.Errorf("#%d: term = %d, want %d", j, g, 1)
		}
	}
}

func TestCommit(t *testing.T) {
	tests := []struct {
		matches []uint64
		logs    []pb.Entry
		smTerm  uint64
		w       uint64
	}{
		// single
		{[]uint64{1}, []pb.Entry{{Index: 1, Term: 1}}, 1, 1},
		{[]uint64{1}, []pb.Entry{{Index: 1, Term: 1}}, 2, 0},
		{[]uint64{2}, []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}}, 2, 2},
		{[]uint64{1}, []pb.Entry{{Index: 1, Term: 2}}, 2, 1},

		// odd
		// quorum 1, term according to log is 1, node's term is 1, advance the
		// committed value to 1
		{[]uint64{2, 1, 1}, []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}}, 1, 1},
		// quorum 1, term according to log is 1, node's term is 2, do nothing,
		// committed value remains 0
		{[]uint64{2, 1, 1}, []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 1}}, 2, 0},
		{[]uint64{2, 1, 2}, []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}}, 2, 2},
		{[]uint64{2, 1, 2}, []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 1}}, 2, 0},

		// even
		{[]uint64{2, 1, 1, 1}, []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}}, 1, 1},
		{[]uint64{2, 1, 1, 1}, []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 1}}, 2, 0},
		{[]uint64{2, 1, 1, 2}, []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}}, 1, 1},
		{[]uint64{2, 1, 1, 2}, []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 1}}, 2, 0},
		{[]uint64{2, 1, 2, 2}, []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}}, 2, 2},
		{[]uint64{2, 1, 2, 2}, []pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 1}}, 2, 0},
	}

	for i, tt := range tests {
		storage := NewTestLogDB()
		if err := storage.Append(tt.logs); err != nil {
			t.Fatalf("%v", err)
		}
		storage.(*TestLogDB).state = pb.State{Term: tt.smTerm}

		sm := newTestRaft(1, []uint64{1}, 5, 1, storage)
		for j := 0; j < len(tt.matches); j++ {
			sm.setRemote(uint64(j)+1, tt.matches[j], tt.matches[j]+1)
		}
		sm.state = leader
		sm.tryCommit()
		if g := sm.log.committed; g != tt.w {
			t.Errorf("#%d: committed = %d, want %d", i, g, tt.w)
		}
	}
}

func TestPastElectionTimeout(t *testing.T) {
	tests := []struct {
		elapse       int
		wprobability float64
		round        bool
	}{
		{5, 0, false},
		{10, 0.1, true},
		{13, 0.4, true},
		{15, 0.6, true},
		{18, 0.9, true},
		{20, 1, false},
	}

	for i, tt := range tests {
		sm := newTestRaft(1, []uint64{1}, 10, 1, NewTestLogDB())
		sm.electionTick = uint64(tt.elapse)
		c := 0
		for j := 0; j < 10000; j++ {
			sm.setRandomizedElectionTimeout()
			if sm.timeForElection() {
				c++
			}
		}
		got := float64(c) / 10000.0
		if tt.round {
			got = math.Floor(got*10+0.5) / 10.0
		}
		if got != tt.wprobability {
			t.Errorf("#%d: probability = %v, want %v", i, got, tt.wprobability)
		}
	}
}

// ensure that the Step function ignores the message from old term and does not pass it to the
// actual stepX function.
func TestStepIgnoreOldTermMsg(t *testing.T) {
	called := false
	fakeStep := func(r *raft, m pb.Message) error {
		called = true
		return nil
	}
	sm := newTestRaft(1, []uint64{1}, 10, 1, NewTestLogDB())
	sm.handle = fakeStep
	sm.term = 2
	ne(sm.Handle(pb.Message{Type: pb.Replicate, Term: sm.term - 1}), t)
	if called {
		t.Errorf("stepFunc called = %v , want %v", called, false)
	}
}

// TestHandleMTReplicate ensures:
//  1. Reply false if log doesn’t contain an entry at prevLogIndex whose term matches prevLogTerm.
//  2. If an existing entry conflicts with a new one (same index but different terms),
//     delete the existing entry and all that follow it; append any new entries not already in the log.
//  3. If leaderCommit > commitIndex, set commitIndex = min(leaderCommit, index of last new entry).
func TestHandleMTReplicate(t *testing.T) {
	tests := []struct {
		m       pb.Message
		wIndex  uint64
		wCommit uint64
		wReject bool
	}{
		// lni
		// LogTerm and Index are the previous Log term/index.
		// Term is the term of the node. Commit is the log committed value (leader commit).

		// Ensure 1
		{pb.Message{Type: pb.Replicate, Term: 2, LogTerm: 3, LogIndex: 2, Commit: 3}, 2, 0, true}, // previous log mismatch
		{pb.Message{Type: pb.Replicate, Term: 2, LogTerm: 3, LogIndex: 3, Commit: 3}, 2, 0, true}, // previous log non-exist

		// Ensure 2
		{pb.Message{Type: pb.Replicate, Term: 2, LogTerm: 1, LogIndex: 1, Commit: 1}, 2, 1, false},
		// conflict found, delete all existing logs, append the new one
		{pb.Message{Type: pb.Replicate, Term: 2, LogTerm: 0, LogIndex: 0, Commit: 1, Entries: []pb.Entry{{Index: 1, Term: 2}}}, 1, 1, false},
		// no conflict, both new logs appended, leaderCommit determines the commit value, last index in log is 4
		{pb.Message{Type: pb.Replicate, Term: 2, LogTerm: 2, LogIndex: 2, Commit: 3, Entries: []pb.Entry{{Index: 3, Term: 2}, {Index: 4, Term: 2}}}, 4, 3, false},
		// no conflict, one log appeneded, last index in log determines the commit value 3, last index in log is 3
		{pb.Message{Type: pb.Replicate, Term: 2, LogTerm: 2, LogIndex: 2, Commit: 4, Entries: []pb.Entry{{Index: 3, Term: 2}}}, 3, 3, false},
		// no conflict, no new entry. last index in log determines the commit value (2), last index in log is 2.
		{pb.Message{Type: pb.Replicate, Term: 2, LogTerm: 1, LogIndex: 1, Commit: 4, Entries: []pb.Entry{{Index: 2, Term: 2}}}, 2, 2, false},

		// Ensure 3
		{pb.Message{Type: pb.Replicate, Term: 1, LogTerm: 1, LogIndex: 1, Commit: 3}, 2, 1, false},                                           // match entry 1, commit up to last new entry 1
		{pb.Message{Type: pb.Replicate, Term: 1, LogTerm: 1, LogIndex: 1, Commit: 3, Entries: []pb.Entry{{Index: 2, Term: 2}}}, 2, 2, false}, // match entry 1, commit up to last new entry 2
		{pb.Message{Type: pb.Replicate, Term: 2, LogTerm: 2, LogIndex: 2, Commit: 3}, 2, 2, false},                                           // match entry 2, commit up to last new entry 2
		{pb.Message{Type: pb.Replicate, Term: 2, LogTerm: 2, LogIndex: 2, Commit: 4}, 2, 2, false},                                           // commit up to log.last()
	}

	for i, tt := range tests {
		storage := NewTestLogDB()
		if err := storage.Append([]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}}); err != nil {
			t.Fatalf("%v", err)
		}
		sm := newTestRaft(1, []uint64{1}, 10, 1, storage)
		sm.becomeFollower(2, NoLeader)

		ne(sm.handleReplicateMessage(tt.m), t)
		if sm.log.lastIndex() != tt.wIndex {
			t.Errorf("#%d: lastIndex = %d, want %d", i, sm.log.lastIndex(), tt.wIndex)
		}
		if sm.log.committed != tt.wCommit {
			t.Errorf("#%d: committed = %d, want %d", i, sm.log.committed, tt.wCommit)
		}
		m := sm.readMessages()
		if len(m) != 1 {
			t.Fatalf("#%d: msg = nil, want 1", i)
		}
		if m[0].Reject != tt.wReject {
			t.Errorf("#%d: reject = %v, want %v", i, m[0].Reject, tt.wReject)
		}
	}
}

// TestHandleHeartbeat ensures that the follower commits to the commit in the message.
func TestHandleHeartbeat(t *testing.T) {
	commit := uint64(2)
	tests := []struct {
		m       pb.Message
		wCommit uint64
	}{
		{pb.Message{From: 2, To: 1, Type: pb.Heartbeat, Term: 2, Commit: commit + 1}, commit + 1},
		{pb.Message{From: 2, To: 1, Type: pb.Heartbeat, Term: 2, Commit: commit - 1}, commit}, // do not decrease commit
	}

	// heatbeat from the leader tells the follower to performan the commitTo operation.
	// other than boundary checkings, nothing else got checked.
	for i, tt := range tests {
		storage := NewTestLogDB()
		if err := storage.Append([]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}, {Index: 3, Term: 3}}); err != nil {
			t.Fatalf("%v", err)
		}
		sm := newTestRaft(1, []uint64{1, 2}, 5, 1, storage)
		sm.becomeFollower(2, 2)
		sm.log.commitTo(commit)
		ne(sm.handleHeartbeatMessage(tt.m), t)
		if sm.log.committed != tt.wCommit {
			t.Errorf("#%d: committed = %d, want %d", i, sm.log.committed, tt.wCommit)
		}
		m := sm.readMessages()
		if len(m) != 1 {
			t.Fatalf("#%d: msg = nil, want 1", i)
		}
		if m[0].Type != pb.HeartbeatResp {
			t.Errorf("#%d: type = %v, want MsgHeartbeatResp", i, m[0].Type)
		}
	}
}

// TestHandleHeartbeatResp ensures that we re-send log entries when we get a heartbeat response.
func TestHandleHeartbeatResp(t *testing.T) {
	storage := NewTestLogDB()
	if err := storage.Append([]pb.Entry{{Index: 1, Term: 1}, {Index: 2, Term: 2}, {Index: 3, Term: 3}}); err != nil {
		t.Fatalf("%v", err)
	}
	sm := newTestRaft(1, []uint64{1, 2}, 5, 1, storage)
	sm.becomeCandidate()
	sm.becomeLeader()
	sm.log.commitTo(sm.log.lastIndex())
	// A heartbeat response from a node that is behind; re-send MTReplicate
	ne(sm.Handle(pb.Message{From: 2, Type: pb.HeartbeatResp}), t)
	msgs := sm.readMessages()
	if len(msgs) != 1 {
		t.Fatalf("len(msgs) = %d, want 1", len(msgs))
	}
	if msgs[0].Type != pb.Replicate {
		t.Errorf("type = %v, want MTReplicate", msgs[0].Type)
	}
	// A second heartbeat response generates another MTReplicate re-send
	ne(sm.Handle(pb.Message{From: 2, Type: pb.HeartbeatResp}), t)
	msgs = sm.readMessages()
	if len(msgs) != 1 {
		t.Fatalf("len(msgs) = %d, want 1", len(msgs))
	}
	if msgs[0].Type != pb.Replicate {
		t.Errorf("type = %v, want MTReplicate", msgs[0].Type)
	}
	// Once we have an MTReplicateResp, heartbeats no longer send MTReplicate.
	ne(sm.Handle(pb.Message{
		From:     2,
		Type:     pb.ReplicateResp,
		LogIndex: msgs[0].LogIndex + uint64(len(msgs[0].Entries)),
	}), t)
	// Consume the message sent in response to MTReplicateResp
	sm.readMessages()

	ne(sm.Handle(pb.Message{From: 2, Type: pb.HeartbeatResp}), t)
	msgs = sm.readMessages()
	if len(msgs) != 0 {
		t.Fatalf("len(msgs) = %d, want 0: %+v", len(msgs), msgs)
	}
}

// TestMTReplicateRespWaitReset verifies the resume behavior of a leader
// MTReplicateResp.
func TestMTReplicateRespWaitReset(t *testing.T) {
	sm := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	sm.becomeCandidate()
	sm.becomeLeader()

	// The new leader has just emitted a new Term 4 entry; consume those messages
	// from the outgoing queue.
	sm.broadcastReplicateMessage()
	sm.readMessages()

	// Node 2 acks the first entry, making it committed.
	ne(sm.Handle(pb.Message{
		From:     2,
		Type:     pb.ReplicateResp,
		LogIndex: 1,
	}), t)
	if sm.log.committed != 1 {
		t.Fatalf("expected committed to be 1, got %d", sm.log.committed)
	}
	// Also consume the MTReplicate messages that update Commit on the followers.
	sm.readMessages()

	// A new command is now proposed on node 1.
	ne(sm.Handle(pb.Message{
		From:    1,
		Type:    pb.Propose,
		Entries: []pb.Entry{{}},
	}), t)

	// The command is broadcast to all nodes not in the wait state.
	// Node 2 left the wait state due to its MsgAppendResp, but node 3 is still waiting.
	msgs := sm.readMessages()
	if len(msgs) != 1 {
		t.Fatalf("expected 1 message, got %d: %+v", len(msgs), msgs)
	}
	if msgs[0].Type != pb.Replicate || msgs[0].To != 2 {
		t.Errorf("expected MTReplicate to node 2, got %v to %d", msgs[0].Type, msgs[0].To)
	}
	if len(msgs[0].Entries) != 1 || msgs[0].Entries[0].Index != 2 {
		t.Errorf("expected to send entry 2, but got %v", msgs[0].Entries)
	}

	// lni
	// initially node 3 is in the probe state, the received MTReplicateResp message makes
	// the node to enter replicate state.
	if sm.remotes[3].state != remoteWait {
		t.Fatalf("expected state probe, got %d (%s)", sm.remotes[3].state, sm.remotes[3].state)
	}

	// Now Node 3 acks the first entry. This releases the wait and entry 2 is sent.
	ne(sm.Handle(pb.Message{
		From:     3,
		Type:     pb.ReplicateResp,
		LogIndex: 1,
	}), t)

	// lni
	// now node 3 should be in replicate state
	if sm.remotes[3].state != remoteReplicate {
		t.Fatalf("expected state replicate, got %d (%s)", sm.remotes[3].state, sm.remotes[3].state)
	}

	msgs = sm.readMessages()
	if len(msgs) != 1 {
		t.Fatalf("expected 1 message, got %d: %+v", len(msgs), msgs)
	}
	if msgs[0].Type != pb.Replicate || msgs[0].To != 3 {
		t.Errorf("expected MTReplicate to node 3, got %v to %d", msgs[0].Type, msgs[0].To)
	}
	if len(msgs[0].Entries) != 1 || msgs[0].Entries[0].Index != 2 {
		t.Errorf("expected to send entry 2, but got %v", msgs[0].Entries)
	}
}

func TestRecvMsgVote(t *testing.T) {
	testRecvMsgVote(t, pb.RequestVote)
}

func testRecvMsgVote(t *testing.T, msgType pb.MessageType) {
	tests := []struct {
		state   State
		i, term uint64
		voteFor uint64
		wreject bool
	}{
		{follower, 0, 0, NoLeader, true},
		{follower, 0, 1, NoLeader, true},
		{follower, 0, 2, NoLeader, true},
		{follower, 0, 3, NoLeader, false},

		{follower, 1, 0, NoLeader, true},
		{follower, 1, 1, NoLeader, true},
		{follower, 1, 2, NoLeader, true},
		{follower, 1, 3, NoLeader, false},

		{follower, 2, 0, NoLeader, true},
		{follower, 2, 1, NoLeader, true},
		{follower, 2, 2, NoLeader, false},
		{follower, 2, 3, NoLeader, false},

		{follower, 3, 0, NoLeader, true},
		{follower, 3, 1, NoLeader, true},
		{follower, 3, 2, NoLeader, false},
		{follower, 3, 3, NoLeader, false},

		{follower, 3, 2, 2, false},
		{follower, 3, 2, 1, true},

		{leader, 3, 3, 1, true},
		//{statePreCandidate, 3, 3, 1, true},
		{candidate, 3, 3, 1, true},
	}

	for i, tt := range tests {
		sm := newTestRaft(1, []uint64{1, 2}, 10, 1, NewTestLogDB())
		sm.state = tt.state
		sm.vote = tt.voteFor
		sm.log = &entryLog{
			logdb: &TestLogDB{entries: []pb.Entry{{Index: 1, Term: 2}, {Index: 2, Term: 2}}},
			inmem: inMemory{markerIndex: 3},
		}

		ne(sm.Handle(pb.Message{Type: msgType, From: 2, LogIndex: tt.i, LogTerm: tt.term}), t)

		msgs := sm.readMessages()
		if g := len(msgs); g != 1 {
			t.Fatalf("#%d: len(msgs) = %d, want 1", i, g)
		}
		if g := msgs[0].Reject; g != tt.wreject {
			t.Errorf("#%d, m.Reject = %v, want %v", i, g, tt.wreject)
		}
	}
}

func TestStateTransition(t *testing.T) {
	tests := []struct {
		from   State
		to     State
		wallow bool
		wterm  uint64
		wlead  uint64
	}{
		{follower, follower, true, 1, NoLeader},
		//{follower, statePreCandidate, true, 0, NoLeader},
		{follower, candidate, true, 1, NoLeader},
		{follower, leader, false, 0, NoLeader},

		//{statePreCandidate, follower, true, 0, NoLeader},
		//{statePreCandidate, statePreCandidate, true, 0, NoLeader},
		//{statePreCandidate, candidate, true, 1, NoLeader},
		//{statePreCandidate, leader, true, 0, 1},

		{candidate, follower, true, 0, NoLeader},
		//{candidate, statePreCandidate, true, 0, NoLeader},
		{candidate, candidate, true, 1, NoLeader},
		{candidate, leader, true, 0, 1},

		{leader, follower, true, 1, NoLeader},
		//{leader, statePreCandidate, false, 0, NoLeader},
		{leader, candidate, false, 1, NoLeader},
		{leader, leader, true, 0, 1},
	}

	for i, tt := range tests {
		func() {
			defer func() {
				// lni
				// when tt.wallow is false, panic is expected, recover from
				// it and assert tt.wallow to be true.
				if r := recover(); r != nil {
					if tt.wallow {
						t.Errorf("%d: allow = %v, want %v", i, false, true)
					}
				}
			}()

			sm := newTestRaft(1, []uint64{1}, 10, 1, NewTestLogDB())
			sm.state = tt.from

			switch tt.to {
			case follower:
				sm.becomeFollower(tt.wterm, tt.wlead)
			case candidate:
				sm.becomeCandidate()
			case leader:
				sm.becomeLeader()
			}

			if sm.term != tt.wterm {
				t.Errorf("%d: term = %d, want %d", i, sm.term, tt.wterm)
			}
			if sm.leaderID != tt.wlead {
				t.Errorf("%d: lead = %d, want %d", i, sm.leaderID, tt.wlead)
			}
		}()
	}
}

func TestAllServerStepdown(t *testing.T) {
	tests := []struct {
		state State

		wstate State
		wterm  uint64
		windex uint64
	}{
		{follower, follower, 3, 0},
		//{statePreCandidate, follower, 3, 0},
		{candidate, follower, 3, 0},
		{leader, follower, 3, 1},
	}

	tmsgTypes := [...]pb.MessageType{pb.RequestVote, pb.Replicate}
	tterm := uint64(3)

	for i, tt := range tests {
		sm := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
		switch tt.state {
		case follower:
			sm.becomeFollower(1, NoLeader)
		case candidate:
			sm.becomeCandidate()
		case leader:
			sm.becomeCandidate()
			sm.becomeLeader()
		}

		for j, msgType := range tmsgTypes {
			ne(sm.Handle(pb.Message{From: 2, Type: msgType, Term: tterm, LogTerm: tterm}), t)

			if sm.state != tt.wstate {
				t.Errorf("#%d.%d state = %v , want %v", i, j, sm.state, tt.wstate)
			}
			if sm.term != tt.wterm {
				t.Errorf("#%d.%d term = %v , want %v", i, j, sm.term, tt.wterm)
			}
			if sm.log.lastIndex() != tt.windex {
				t.Errorf("#%d.%d index = %v , want %v", i, j, sm.log.lastIndex(), tt.windex)
			}
			if uint64(len(getAllEntries(sm.log))) != tt.windex {
				t.Errorf("#%d.%d len(ents) = %v , want %v", i, j, len(getAllEntries(sm.log)), tt.windex)
			}
			wlead := uint64(2)
			if msgType == pb.RequestVote {
				wlead = NoLeader
			}
			if sm.leaderID != wlead {
				t.Errorf("#%d, sm.lead = %d, want %d", i, sm.leaderID, NoLeader)
			}
		}
	}
}

func TestLeaderStepdownWhenQuorumActive(t *testing.T) {
	sm := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())

	sm.checkQuorum = true

	sm.becomeCandidate()
	sm.becomeLeader()

	for i := uint64(0); i < sm.electionTimeout+1; i++ {
		ne(sm.Handle(pb.Message{From: 2, Type: pb.HeartbeatResp, Term: sm.term}), t)
		ne(sm.tick(), t)
	}

	if sm.state != leader {
		t.Errorf("state = %v, want %v", sm.state, leader)
	}
}

func TestLeaderStepdownWhenQuorumLost(t *testing.T) {
	sm := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())

	sm.checkQuorum = true

	sm.becomeCandidate()
	sm.becomeLeader()

	for i := uint64(0); i < sm.electionTimeout+1; i++ {
		ne(sm.tick(), t)
	}

	if sm.state != follower {
		t.Errorf("state = %v, want %v", sm.state, follower)
	}
}

func TestLeaderSupersedingWithCheckQuorum(t *testing.T) {
	a := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	b := newTestRaft(2, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	c := newTestRaft(3, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())

	a.checkQuorum = true
	b.checkQuorum = true
	c.checkQuorum = true

	nt := newNetwork(a, b, c)
	setRandomizedElectionTimeout(b, b.electionTimeout+1)

	for i := uint64(0); i < b.electionTimeout; i++ {
		ne(b.tick(), t)
	}
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})

	if a.state != leader {
		t.Errorf("state = %s, want %s", a.state, leader)
	}

	if c.state != follower {
		t.Errorf("state = %s, want %s", c.state, follower)
	}

	nt.send(pb.Message{From: 3, To: 3, Type: pb.Election})
	// Peer b rejected c's vote since its electionRTT had not reached to electionTimeout
	if c.state != candidate {
		t.Errorf("state = %s, want %s", c.state, candidate)
	}

	// Letting b's electionRTT reach to electionTimeout
	for i := uint64(0); i < b.electionTimeout; i++ {
		ne(b.tick(), t)
	}
	nt.send(pb.Message{From: 3, To: 3, Type: pb.Election})

	if c.state != leader {
		t.Errorf("state = %s, want %s", c.state, leader)
	}
}

// lni
// check this three tests
func TestLeaderElectionWithCheckQuorum(t *testing.T) {
	a := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	b := newTestRaft(2, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	c := newTestRaft(3, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())

	a.checkQuorum = true
	b.checkQuorum = true
	c.checkQuorum = true

	nt := newNetwork(a, b, c)
	setRandomizedElectionTimeout(a, a.electionTimeout+1)
	setRandomizedElectionTimeout(b, b.electionTimeout+2)

	// Immediately after creation, votes are cast regardless of the
	// election timeout.
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})

	if a.state != leader {
		t.Errorf("state = %s, want %s", a.state, leader)
	}

	if c.state != follower {
		t.Errorf("state = %s, want %s", c.state, follower)
	}

	// need to reset randomizedElectionTimeout larger than electionTimeout again,
	// because the value might be reset to electionTimeout since the last state changes
	setRandomizedElectionTimeout(a, a.electionTimeout+1)
	setRandomizedElectionTimeout(b, b.electionTimeout+2)
	for i := uint64(0); i < a.electionTimeout; i++ {
		ne(a.tick(), t)
	}
	for i := uint64(0); i < b.electionTimeout; i++ {
		ne(b.tick(), t)
	}
	nt.send(pb.Message{From: 3, To: 3, Type: pb.Election})

	if a.state != follower {
		t.Errorf("state = %s, want %s", a.state, follower)
	}

	if c.state != leader {
		t.Errorf("state = %s, want %s", c.state, leader)
	}
}

func TestFreeStuckCandidateWithCheckQuorum(t *testing.T) {
	testFreeStuckCandidateWithCheckQuorum(t)
}

/*
func TestFreeStuckCandidateWithCheckQuorumAndPreVote(t *testing.T) {
	testFreeStuckCandidateWithCheckQuorum(t, true)
}
*/

// TestFreeStuckCandidateWithCheckQuorum ensures that a candidate with a higher term
// can disrupt the leader even if the leader still "officially" holds the lease, The
// leader is expected to step down and adopt the candidate's term
func testFreeStuckCandidateWithCheckQuorum(t *testing.T) {
	a := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	b := newTestRaft(2, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	c := newTestRaft(3, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())

	a.checkQuorum = true
	b.checkQuorum = true
	c.checkQuorum = true
	nt := newNetwork(a, b, c)
	setRandomizedElectionTimeout(b, b.electionTimeout+1)

	for i := uint64(0); i < b.electionTimeout; i++ {
		ne(b.tick(), t)
	}
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})

	nt.isolate(1)
	nt.send(pb.Message{From: 3, To: 3, Type: pb.Election})

	if b.state != follower {
		t.Errorf("state = %s, want %s", b.state, follower)
	}

	if c.state != candidate {
		t.Errorf("state = %s, want %s", c.state, candidate)
	}

	if c.term != b.term+1 {
		t.Errorf("term = %d, want %d", c.term, b.term+1)
	}

	// Vote again for safety
	nt.send(pb.Message{From: 3, To: 3, Type: pb.Election})

	if b.state != follower {
		t.Errorf("state = %s, want %s", b.state, follower)
	}

	if c.state != candidate {
		t.Errorf("state = %s, want %s", c.state, candidate)
	}

	if c.term != b.term+2 {
		t.Errorf("term = %d, want %d", c.term, b.term+2)
	}

	nt.recover()
	nt.send(pb.Message{From: 1, To: 3, Type: pb.Heartbeat, Term: a.term})

	// Disrupt the leader so that the stuck peer is freed
	if a.state != follower {
		t.Errorf("state = %s, want %s", a.state, follower)
	}

	if c.term != a.term {
		t.Errorf("term = %d, want %d", c.term, a.term)
	}

	// Vote again, should become leader this time
	nt.send(pb.Message{From: 3, To: 3, Type: pb.Election})
	if c.state != leader {
		t.Errorf("peer 3 state: %s, want %s", c.state, leader)
	}
}

func TestNonPromotableVoterWithCheckQuorum(t *testing.T) {
	a := newTestRaft(1, []uint64{1, 2}, 10, 1, NewTestLogDB())
	b := newTestRaft(2, []uint64{1}, 10, 1, NewTestLogDB())

	a.checkQuorum = true
	b.checkQuorum = true

	nt := newNetwork(a, b)
	setRandomizedElectionTimeout(b, b.electionTimeout+1)
	// Need to remove 2 again to make it a non-promotable node since newNetwork overwritten some internal states
	b.deleteRemote(2)

	if !b.selfRemoved() {
		t.Fatalf("promotable = %v, want false", b.selfRemoved())
	}

	for i := uint64(0); i < b.electionTimeout; i++ {
		ne(b.tick(), t)
	}
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})

	if a.state != leader {
		t.Errorf("state = %s, want %s", a.state, leader)
	}

	if b.state != follower {
		t.Errorf("state = %s, want %s", b.state, follower)
	}

	if b.leaderID != 1 {
		t.Errorf("lead = %d, want 1", b.leaderID)
	}
}

func TestReadOnlyOptionSafe(t *testing.T) {
	a := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	b := newTestRaft(2, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	c := newTestRaft(3, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())

	nt := newNetwork(a, b, c)
	setRandomizedElectionTimeout(b, b.electionTimeout+1)

	for i := uint64(0); i < b.electionTimeout; i++ {
		ne(b.tick(), t)
	}
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})

	if a.state != leader {
		t.Fatalf("state = %s, want %s", a.state, leader)
	}

	tests := []struct {
		sm        *raft
		proposals int
		wri       uint64
		wctx      pb.SystemCtx
	}{
		{a, 10, 11, getTestSystemCtx(10001)},
		{b, 10, 21, getTestSystemCtx(10002)},
		{c, 10, 31, getTestSystemCtx(10003)},
		{a, 10, 41, getTestSystemCtx(10004)},
		{b, 10, 51, getTestSystemCtx(10005)},
		{c, 10, 61, getTestSystemCtx(10006)},
	}

	for i, tt := range tests {
		for j := 0; j < tt.proposals; j++ {
			nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{}}})
		}

		nt.send(pb.Message{From: tt.sm.replicaID, To: tt.sm.replicaID, Type: pb.ReadIndex, Hint: tt.wctx.Low, HintHigh: tt.wctx.High})

		r := tt.sm
		if len(r.readyToRead) == 0 {
			t.Errorf("#%d: len(readyToRead) = 0, want non-zero", i)
		}
		rs := r.readyToRead[0]
		if rs.Index != tt.wri {
			t.Errorf("#%d: readIndex = %d, want %d", i, rs.Index, tt.wri)
		}

		if rs.SystemCtx != tt.wctx {
			t.Errorf("#%d: requestCtx = %d, want %d", i, rs.SystemCtx, tt.wctx)
		}
		r.readyToRead = nil
	}
}

func TestLeaderAppResp(t *testing.T) {
	// initial progress: match = 0; next = 3
	tests := []struct {
		index  uint64
		reject bool
		// progress
		wmatch uint64
		wnext  uint64
		// message
		wmsgNum    int
		windex     uint64
		wcommitted uint64
	}{
		{3, true, 0, 3, 0, 0, 0},  // stale resp; no replies
		{2, true, 0, 2, 1, 1, 0},  // denied resp; leader does not commit; decrease next and send probing msg, which is just a r.sendAppend()
		{2, false, 2, 4, 2, 2, 2}, // accept resp; leader commits; broadcast with commit index
		{0, false, 0, 3, 0, 0, 0}, // ignore heartbeat replies
	}

	for i, tt := range tests {
		// sm term is 1 after it becomes the leader.
		// thus the last log term must be 1 to be committed.
		sm := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
		sm.log = &entryLog{
			logdb: &TestLogDB{entries: []pb.Entry{{Index: 1, Term: 0}, {Index: 2, Term: 1}}},
			inmem: inMemory{markerIndex: 3},
		}
		sm.becomeCandidate()
		sm.becomeLeader()
		sm.readMessages()
		ne(sm.Handle(pb.Message{From: 2, Type: pb.ReplicateResp, LogIndex: tt.index, Term: sm.term, Reject: tt.reject, Hint: tt.index}), t)

		p := sm.remotes[2]
		if p.match != tt.wmatch {
			t.Errorf("#%d match = %d, want %d", i, p.match, tt.wmatch)
		}
		if p.next != tt.wnext {
			t.Errorf("#%d next = %d, want %d", i, p.next, tt.wnext)
		}

		msgs := sm.readMessages()

		if len(msgs) != tt.wmsgNum {
			t.Errorf("#%d msgNum = %d, want %d", i, len(msgs), tt.wmsgNum)
		}
		for j, msg := range msgs {
			if msg.LogIndex != tt.windex {
				t.Errorf("#%d.%d index = %d, want %d", i, j, msg.LogIndex, tt.windex)
			}
			if msg.Commit != tt.wcommitted {
				t.Errorf("#%d.%d commit = %d, want %d", i, j, msg.Commit, tt.wcommitted)
			}
		}
	}
}

// When the leader receives a heartbeat tick, it should
// send a MsgAppend with m.Index = 0, m.LogTerm=0 and empty entries.
func TestBcastBeat(t *testing.T) {
	offset := uint64(1000)
	// make a state machine with log.offset = 1000
	s := pb.Snapshot{
		Index:      offset,
		Term:       1,
		Membership: getTestMembership([]uint64{1, 2, 3}),
	}
	storage := NewTestLogDB()
	if err := storage.ApplySnapshot(s); err != nil {
		t.Fatalf("%v", err)
	}
	sm := newTestRaft(1, nil, 10, 1, storage)
	sm.term = 1

	sm.becomeCandidate()
	sm.becomeLeader()
	for i := 0; i < 10; i++ {
		sm.appendEntries([]pb.Entry{{Index: uint64(i) + 1}})
	}
	// slow follower
	sm.remotes[2].match, sm.remotes[2].next = 5, 6
	// normal follower
	sm.remotes[3].match, sm.remotes[3].next = sm.log.lastIndex(), sm.log.lastIndex()+1

	ne(sm.Handle(pb.Message{Type: pb.LeaderHeartbeat}), t)
	msgs := sm.readMessages()
	if len(msgs) != 2 {
		t.Fatalf("len(msgs) = %v, want 2", len(msgs))
	}
	wantCommitMap := map[uint64]uint64{
		2: min(sm.log.committed, sm.remotes[2].match),
		3: min(sm.log.committed, sm.remotes[3].match),
	}
	for i, m := range msgs {
		if m.Type != pb.Heartbeat {
			t.Fatalf("#%d: type = %v, want = %v", i, m.Type, pb.Heartbeat)
		}
		if m.LogIndex != 0 {
			t.Fatalf("#%d: prevIndex = %d, want %d", i, m.LogIndex, 0)
		}
		if m.LogTerm != 0 {
			t.Fatalf("#%d: prevTerm = %d, want %d", i, m.LogTerm, 0)
		}
		if wantCommitMap[m.To] == 0 {
			t.Fatalf("#%d: unexpected to %d", i, m.To)
		} else {
			if m.Commit != wantCommitMap[m.To] {
				t.Fatalf("#%d: commit = %d, want %d", i, m.Commit, wantCommitMap[m.To])
			}
			delete(wantCommitMap, m.To)
		}
		if len(m.Entries) != 0 {
			t.Fatalf("#%d: len(entries) = %d, want 0", i, len(m.Entries))
		}
	}
}

// tests the output of the state machine when receiving MsgLeaderHeartbeat
func TestRecvMsgLeaderHeartbeat(t *testing.T) {
	tests := []struct {
		state State
		wMsg  int
	}{
		// leader should send MsgHeartbeat
		{leader, 2},
		// candidate and follower should ignore MsgLeaderHeartbeat
		{candidate, 0},
		{follower, 0},
	}

	for i, tt := range tests {
		sm := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
		sm.log = &entryLog{logdb: &TestLogDB{entries: []pb.Entry{{Index: 1, Term: 0}, {Index: 2, Term: 1}}}}
		sm.term = 1
		sm.state = tt.state
		ne(sm.Handle(pb.Message{From: 1, To: 1, Type: pb.LeaderHeartbeat}), t)

		msgs := sm.readMessages()
		if len(msgs) != tt.wMsg {
			t.Errorf("%d: len(msgs) = %d, want %d", i, len(msgs), tt.wMsg)
		}
		for _, m := range msgs {
			if m.Type != pb.Heartbeat {
				t.Errorf("%d: msg.type = %v, want %v", i, m.Type, pb.Heartbeat)
			}
		}
	}
}

func TestLeaderIncreaseNext(t *testing.T) {
	previousEnts := []pb.Entry{{Term: 1, Index: 1}, {Term: 1, Index: 2}, {Term: 1, Index: 3}}
	tests := []struct {
		// progress
		state remoteStateType
		next  uint64

		wnext uint64
	}{
		// state replicate, optimistically increase next
		// previous entries + noop entry + propose + 1
		{remoteReplicate, 2, uint64(len(previousEnts) + 1 + 1 + 1)},
		// state probe, not optimistically increase next
		{remoteRetry, 2, 2},
	}

	for i, tt := range tests {
		sm := newTestRaft(1, []uint64{1, 2}, 10, 1, NewTestLogDB())
		sm.log.append(previousEnts)
		sm.becomeCandidate()
		sm.becomeLeader()
		sm.remotes[2].state = tt.state
		sm.remotes[2].next = tt.next
		ne(sm.Handle(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("somedata")}}}), t)

		p := sm.remotes[2]
		if p.next != tt.wnext {
			t.Errorf("#%d next = %d, want %d", i, p.next, tt.wnext)
		}
	}
}

func TestSendAppendForRemoteRetry(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 10, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	r.readMessages()
	r.remotes[2].becomeRetry()

	// each round is a heartbeat
	for i := 0; i < 3; i++ {
		if i == 0 {
			// we expect that raft will only send out one msgAPP on the first
			// loop. After that, the follower is paused until a heartbeat response is
			// received.
			r.appendEntries([]pb.Entry{{Cmd: []byte("somedata")}})
			r.sendReplicateMessage(2)
			msg := r.readMessages()
			if len(msg) != 1 {
				t.Errorf("len(msg) = %d, want %d", len(msg), 1)
			}
			if msg[0].LogIndex != 0 {
				t.Errorf("index = %d, want %d", msg[0].LogIndex, 0)
			}
		}

		if r.remotes[2].state != remoteWait {
			t.Errorf("paused = %s, want remoteWait", r.remotes[2].state)
		}
		for j := 0; j < 10; j++ {
			r.appendEntries([]pb.Entry{{Cmd: []byte("somedata")}})
			r.sendReplicateMessage(2)
			if l := len(r.readMessages()); l != 0 {
				t.Errorf("len(msg) = %d, want %d", l, 0)
			}
		}

		// do a heartbeat
		for j := uint64(0); j < r.heartbeatTimeout; j++ {
			ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.LeaderHeartbeat}), t)
		}
		if r.remotes[2].state != remoteWait {
			t.Errorf("paused = %v, want remoteWait", r.remotes[2].state)
		}

		// consume the heartbeat
		msg := r.readMessages()
		if len(msg) != 1 {
			t.Errorf("len(msg) = %d, want %d", len(msg), 1)
		}
		if msg[0].Type != pb.Heartbeat {
			t.Errorf("type = %v, want %v", msg[0].Type, pb.Heartbeat)
		}
	}

	// a heartbeat response will allow another message to be sent
	ne(r.Handle(pb.Message{From: 2, To: 1, Type: pb.HeartbeatResp}), t)
	msg := r.readMessages()
	if len(msg) != 1 {
		t.Errorf("len(msg) = %d, want %d", len(msg), 1)
	}
	if msg[0].LogIndex != 0 {
		t.Errorf("index = %d, want %d", msg[0].LogIndex, 0)
	}
	if r.remotes[2].state != remoteWait {
		t.Errorf("paused = %s, want remoteWait", r.remotes[2].state)
	}
}

// optimisitically send entries out
func TestSendAppendForRemoteReplicate(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 10, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	r.readMessages()
	r.remotes[2].becomeReplicate()

	for i := 0; i < 10; i++ {
		r.appendEntries([]pb.Entry{{Cmd: []byte("somedata")}})
		r.sendReplicateMessage(2)
		msgs := r.readMessages()
		if len(msgs) != 1 {
			t.Errorf("len(msg) = %d, want %d", len(msgs), 1)
		}
	}
}

// paused, no msgapp when entries are locally appended
func TestSendAppendForRemoteSnapshot(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 10, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	r.readMessages()
	r.remotes[2].becomeSnapshot(10)

	for i := 0; i < 10; i++ {
		r.appendEntries([]pb.Entry{{Cmd: []byte("somedata")}})
		r.sendReplicateMessage(2)
		msgs := r.readMessages()
		if len(msgs) != 0 {
			t.Errorf("len(msg) = %d, want %d", len(msgs), 0)
		}
	}
}

// MsgUnreachable puts remote peer into probe state.
func TestRecvMsgUnreachable(t *testing.T) {
	previousEnts := []pb.Entry{{Term: 1, Index: 1}, {Term: 1, Index: 2}, {Term: 1, Index: 3}}
	s := NewTestLogDB()
	if err := s.Append(previousEnts); err != nil {
		t.Fatalf("%v", err)
	}
	r := newTestRaft(1, []uint64{1, 2}, 10, 1, s)
	r.becomeCandidate()
	r.becomeLeader()
	r.readMessages()
	// set node 2 to state replicate
	r.remotes[2].match = 3
	r.remotes[2].becomeReplicate()
	r.remotes[2].tryUpdate(5)

	ne(r.Handle(pb.Message{From: 2, To: 1, Type: pb.Unreachable}), t)

	if r.remotes[2].state != remoteRetry {
		t.Errorf("state = %s, want %s", r.remotes[2].state, remoteRetry)
	}
	if wnext := r.remotes[2].match + 1; r.remotes[2].next != wnext {
		t.Errorf("next = %d, want %d", r.remotes[2].next, wnext)
	}
}

func getNodesFromMembership(ss pb.Membership) []uint64 {
	v := make([]uint64, 0)
	for nid := range ss.Addresses {
		v = append(v, nid)
	}
	return v
}

func sliceEqual(s1 []uint64, s2 []uint64) bool {
	if len(s1) != len(s2) {
		return false
	}
	sort.Slice(s1, func(i, j int) bool { return s1[i] < s1[j] })
	sort.Slice(s2, func(i, j int) bool { return s2[i] < s2[j] })
	for idx, v := range s1 {
		if v != s2[idx] {
			return false
		}
	}

	return true
}

// nodes info is in the snapshot.
func TestRestore(t *testing.T) {
	s := pb.Snapshot{
		Index:      11, // magic number
		Term:       11, // magic number
		Membership: getTestMembership([]uint64{1, 2, 3}),
	}

	storage := NewTestLogDB()
	sm := newTestRaft(1, []uint64{1, 2}, 10, 1, storage)
	ok, err := sm.restore(s)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if !ok {
		t.Fatal("restore fail, want succeed")
	}

	if sm.log.lastIndex() != s.Index {
		t.Errorf("log.lastIndex = %d, want %d", sm.log.lastIndex(), s.Index)
	}
	if mustTerm(sm.log.term(s.Index)) != s.Term {
		t.Errorf("log.lastTerm = %d, want %d", mustTerm(sm.log.term(s.Index)), s.Term)
	}

	if sliceEqual(sm.nodesSorted(), getNodesFromMembership(s.Membership)) {
		t.Errorf("snapshot remotes info restored too earlier: %v", sm.nodesSorted())
	}

	sm.restoreRemotes(s)
	sg := sm.nodesSorted()
	if !sliceEqual(sg, getNodesFromMembership(s.Membership)) {
		t.Errorf("sm.Nodes = %+v, want %+v", sg, getNodesFromMembership(s.Membership))
	}
	ok, err = sm.restore(s)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if ok {
		t.Fatal("restore succeed, want fail")
	}
}

func TestRestoreIgnoreSnapshot(t *testing.T) {
	previousEnts := []pb.Entry{{Term: 1, Index: 1}, {Term: 1, Index: 2}, {Term: 1, Index: 3}}
	commit := uint64(1)
	storage := NewTestLogDB()
	sm := newTestRaft(1, []uint64{1, 2}, 10, 1, storage)
	sm.log.append(previousEnts)
	sm.log.commitTo(commit)

	s := pb.Snapshot{
		Index:      commit,
		Term:       1,
		Membership: getTestMembership([]uint64{1, 2}),
	}

	// ignore snapshot
	ok, err := sm.restore(s)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if ok {
		t.Errorf("restore = %t, want %t", ok, false)
	}
	if sm.log.committed != commit {
		t.Errorf("commit = %d, want %d", sm.log.committed, commit)
	}

	// lni
	// s.Index, s.term match the log. restore is not required.
	// but it will fast forward the committed value.
	// ignore snapshot and fast forward commit
	s.Index = commit + 1
	ok, err = sm.restore(s)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if ok {
		t.Errorf("restore = %t, want %t", ok, false)
	}
	if sm.log.committed != commit+1 {
		t.Errorf("commit = %d, want %d", sm.log.committed, commit+1)
	}
}

func TestProvideSnap(t *testing.T) {
	// restore the state machine from a snapshot so it has a compacted log and a snapshot
	s := pb.Snapshot{
		Index:      11, // magic number
		Term:       11, // magic number
		Membership: getTestMembership([]uint64{1, 2}),
	}
	storage := NewTestLogDB()
	sm := newTestRaft(1, []uint64{1}, 10, 1, storage)
	if _, err := sm.restore(s); err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	sm.restoreRemotes(s)

	sm.becomeCandidate()
	sm.becomeLeader()

	// force set the next of node 2, so that node 2 needs a snapshot
	sm.remotes[2].next = sm.log.firstIndex()
	ne(sm.Handle(pb.Message{From: 2, To: 1, Type: pb.ReplicateResp, LogIndex: sm.remotes[2].next - 1, Reject: true}), t)

	msgs := sm.readMessages()
	if len(msgs) != 1 {
		t.Fatalf("len(msgs) = %d, want 1", len(msgs))
	}
	m := msgs[0]
	if m.Type != pb.InstallSnapshot {
		t.Errorf("m.Type = %v, want %v", m.Type, pb.InstallSnapshot)
	}
}

func TestIgnoreProvidingSnap(t *testing.T) {
	// restore the state machine from a snapshot so it has a compacted log and a snapshot
	s := pb.Snapshot{
		Index:      11, // magic number
		Term:       11, // magic number
		Membership: getTestMembership([]uint64{1, 2}),
	}
	storage := NewTestLogDB()
	sm := newTestRaft(1, []uint64{1}, 10, 1, storage)
	if _, err := sm.restore(s); err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	sm.restoreRemotes(s)

	sm.becomeCandidate()
	sm.becomeLeader()

	// force set the next of node 2, so that node 2 needs a snapshot
	// change node 2 to be inactive, expect node 1 ignore sending snapshot to 2
	sm.remotes[2].next = sm.log.firstIndex() - 1
	sm.remotes[2].active = false

	ne(sm.Handle(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("somedata")}}}), t)

	msgs := sm.readMessages()
	if len(msgs) != 0 {
		t.Errorf("len(msgs) = %d, want 0", len(msgs))
	}
}

func TestRestoreFromSnapMsg(t *testing.T) {
	s := pb.Snapshot{
		Index:      11, // magic number
		Term:       11, // magic number
		Membership: getTestMembership([]uint64{1, 2}),
	}
	m := pb.Message{Type: pb.InstallSnapshot, From: 1, Term: 2, Snapshot: s}

	sm := newTestRaft(2, []uint64{1, 2}, 10, 1, NewTestLogDB())
	ne(sm.Handle(m), t)

	if sm.leaderID != uint64(1) {
		t.Errorf("sm.lead = %d, want 1", sm.leaderID)
	}

	// TODO(bdarnell): what should this test?
}

func TestSlowNodeRestore(t *testing.T) {
	nt := newNetwork(nil, nil, nil)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})

	nt.isolate(3)
	for j := 0; j <= 100; j++ {
		nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{}}})
	}
	lead := nt.peers[1].(*raft)
	nextEnts(lead, nt.storage[1])
	m := getTestMembership(lead.nodesSorted())
	ss, err := nt.storage[1].(*TestLogDB).getSnapshot(lead.log.processed, &m)
	if err != nil {
		t.Fatalf("failed to get snapshot")
	}
	if err := nt.storage[1].CreateSnapshot(ss); err != nil {
		t.Fatalf("%v", err)
	}
	if err := nt.storage[1].Compact(lead.log.processed); err != nil {
		t.Fatalf("%v", err)
	}
	follower := nt.peers[3].(*raft)
	nt.recover()
	// send heartbeats so that the leader can learn everyone is active.
	// node 3 will only be considered as active when node 1 receives a reply from it.
	for {
		nt.send(pb.Message{From: 1, To: 1, Type: pb.LeaderHeartbeat})
		if lead.remotes[3].active {
			break
		}
	}

	// trigger a snapshot
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{}}})
	// trigger a commit
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{}}})
	if follower.log.committed != lead.log.committed {
		t.Errorf("follower.committed = %d, want %d", follower.log.committed, lead.log.committed)
	}
}

// TestStepConfig tests that when raft step msgProp in EntryConfChange type,
// it appends the entry to log and sets pendingConfigChange to be true.
func TestStepConfig(t *testing.T) {
	// a raft that cannot make progress
	r := newTestRaft(1, []uint64{1, 2}, 10, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	index := r.log.lastIndex()
	ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Type: pb.ConfigChangeEntry}}}), t)
	if g := r.log.lastIndex(); g != index+1 {
		t.Errorf("index = %d, want %d", g, index+1)
	}
	if !r.pendingConfigChange {
		t.Errorf("pendingConfigChange = %v, want true", r.pendingConfigChange)
	}
}

// TestStepIgnoreConfig tests that if raft step the second msgProp in
// EntryConfChange type when the first one is uncommitted, the node will set
// the proposal to noop and keep its original state.
func TestStepIgnoreConfig(t *testing.T) {
	// a raft that cannot make progress
	r := newTestRaft(1, []uint64{1, 2}, 10, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Type: pb.ConfigChangeEntry}}}), t)
	index := r.log.lastIndex()
	pendingConfigChange := r.pendingConfigChange
	ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Type: pb.ConfigChangeEntry}}}), t)
	wents := []pb.Entry{{Type: pb.ApplicationEntry, Term: 1, Index: 3, Cmd: nil}}
	ents, err := r.log.entries(index+1, noLimit)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if !reflect.DeepEqual(ents, wents) {
		t.Errorf("ents = %+v, want %+v", ents, wents)
	}
	if r.pendingConfigChange != pendingConfigChange {
		t.Errorf("pendingConfigChange = %v, want %v", r.pendingConfigChange, pendingConfigChange)
	}
}

// TestRecoverPendingConfig tests that new leader recovers its pendingConfigChange flag
// based on uncommitted entries.
func TestRecoverPendingConfig(t *testing.T) {
	tests := []struct {
		entType  pb.EntryType
		wpending bool
	}{
		{pb.ApplicationEntry, false},
		{pb.ConfigChangeEntry, true},
	}
	for i, tt := range tests {
		r := newTestRaft(1, []uint64{1, 2}, 10, 1, NewTestLogDB())
		r.appendEntries([]pb.Entry{{Type: tt.entType}})
		r.becomeCandidate()
		r.becomeLeader()
		if r.pendingConfigChange != tt.wpending {
			t.Errorf("#%d: pendingConfigChange = %v, want %v", i, r.pendingConfigChange, tt.wpending)
		}
	}
}

// TestRecoverDoublePendingConfig tests that new leader will panic if
// there exist two uncommitted config entries.
func TestRecoverDoublePendingConfig(t *testing.T) {
	func() {
		defer func() {
			if err := recover(); err == nil {
				t.Errorf("expect panic, but nothing happens")
			}
		}()
		r := newTestRaft(1, []uint64{1, 2}, 10, 1, NewTestLogDB())
		r.appendEntries([]pb.Entry{{Type: pb.ConfigChangeEntry}})
		r.appendEntries([]pb.Entry{{Type: pb.ConfigChangeEntry}})
		r.becomeCandidate()
		r.becomeLeader()
	}()
}

// TestAddNode tests that addNode could update pendingConfigChange and nodes correctly.
func TestAddNode(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 10, 1, NewTestLogDB())
	r.pendingConfigChange = true
	r.addNode(2)
	if r.pendingConfigChange {
		t.Errorf("pendingConfigChange = %v, want false", r.pendingConfigChange)
	}
	nodes := r.nodesSorted()
	wnodes := []uint64{1, 2}
	if !reflect.DeepEqual(nodes, wnodes) {
		t.Errorf("nodes = %v, want %v", nodes, wnodes)
	}
}

// TestRemoveNode tests that removeNode could update pendingConfigChange, nodes and
// and removed list correctly.
func TestRemoveNode(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 10, 1, NewTestLogDB())
	r.pendingConfigChange = true
	r.removeNode(2)
	if r.pendingConfigChange {
		t.Errorf("pendingConfigChange = %v, want false", r.pendingConfigChange)
	}
	w := []uint64{1}
	if g := r.nodesSorted(); !reflect.DeepEqual(g, w) {
		t.Errorf("nodes = %v, want %v", g, w)
	}

	// remove all nodes from shard
	r.removeNode(1)
	w = []uint64{}
	if g := r.nodesSorted(); !reflect.DeepEqual(g, w) {
		t.Errorf("nodes = %v, want %v", g, w)
	}
}

// lni
// promotoable probably means the local node has not been removed?
func TestPromotable(t *testing.T) {
	id := uint64(1)
	tests := []struct {
		peers []uint64
		wp    bool
	}{
		{[]uint64{1}, true},
		{[]uint64{1, 2, 3}, true},
		{[]uint64{}, false},
		{[]uint64{2, 3}, false},
	}
	for i, tt := range tests {
		r := newTestRaft(id, tt.peers, 5, 1, NewTestLogDB())
		if g := !r.selfRemoved(); g != tt.wp {
			t.Errorf("#%d: promotable = %v, want %v", i, g, tt.wp)
		}
	}
}

func TestRaftNodes(t *testing.T) {
	tests := []struct {
		ids  []uint64
		wids []uint64
	}{
		{
			[]uint64{1, 2, 3},
			[]uint64{1, 2, 3},
		},
		{
			[]uint64{3, 2, 1},
			[]uint64{1, 2, 3},
		},
	}
	for i, tt := range tests {
		r := newTestRaft(1, tt.ids, 10, 1, NewTestLogDB())
		if !reflect.DeepEqual(r.nodesSorted(), tt.wids) {
			t.Errorf("#%d: nodes = %+v, want %+v", i, r.nodesSorted(), tt.wids)
		}
	}
}

func TestCampaignWhileLeader(t *testing.T) {
	testCampaignWhileLeader(t)
}

func testCampaignWhileLeader(t *testing.T) {
	s := NewTestLogDB()
	peers := []uint64{1}
	cfg := newTestConfig(1, 5, 1)
	r := newRaft(cfg, s)
	r.setTestPeers(peers)
	if r.state != follower {
		t.Errorf("expected new node to be follower but got %s", r.state)
	}
	// We don't call campaign() directly because it comes after the check
	// for our current state.
	ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.Election}), t)
	if r.state != leader {
		t.Errorf("expected single-node election to become leader but got %s", r.state)
	}
	term := r.term
	ne(r.Handle(pb.Message{From: 1, To: 1, Type: pb.Election}), t)
	if r.state != leader {
		t.Errorf("expected to remain leader but got %s", r.state)
	}
	if r.term != term {
		t.Errorf("expected to remain in term %v but got %v", term, r.term)
	}
}

// TestCommitAfterRemoveNode verifies that pending commands can become
// committed when a config change reduces the quorum requirements.
func TestCommitAfterRemoveNode(t *testing.T) {
	// Create a shard with two nodes.
	s := NewTestLogDB()
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, s)
	r.becomeCandidate()
	r.becomeLeader()

	// Begin to remove the second node.
	cc := pb.ConfigChange{
		Type:      pb.RemoveNode,
		ReplicaID: 2,
	}
	ccData, err := cc.Marshal()
	if err != nil {
		t.Fatal(err)
	}
	ne(r.Handle(pb.Message{
		Type: pb.Propose,
		Entries: []pb.Entry{
			{Type: pb.ConfigChangeEntry, Cmd: ccData},
		},
	}), t)
	// Stabilize the log and make sure nothing is committed yet.
	if ents := nextEnts(r, s); len(ents) > 0 {
		t.Fatalf("unexpected committed entries: %v", ents)
	}
	ccIndex := r.log.lastIndex()

	// While the config change is pending, make another proposal.
	ne(r.Handle(pb.Message{
		Type: pb.Propose,
		Entries: []pb.Entry{
			{Type: pb.ApplicationEntry, Cmd: []byte("hello")},
		},
	}), t)

	// Node 2 acknowledges the config change, committing it.
	ne(r.Handle(pb.Message{
		Type:     pb.ReplicateResp,
		From:     2,
		LogIndex: ccIndex,
	}), t)
	ents := nextEnts(r, s)
	if len(ents) != 2 {
		t.Fatalf("expected two committed entries, got %v", ents)
	}
	if ents[0].Type != pb.ApplicationEntry || ents[0].Cmd != nil {
		t.Fatalf("expected ents[0] to be empty, but got %v", ents[0])
	}
	if ents[1].Type != pb.ConfigChangeEntry {
		t.Fatalf("expected ents[1] to be EntryConfChange, got %v", ents[1])
	}

	// Apply the config change. This reduces quorum requirements so the
	// pending command can now commit.
	r.removeNode(2)
	ents = nextEnts(r, s)
	if len(ents) != 1 || ents[0].Type != pb.ApplicationEntry ||
		string(ents[0].Cmd) != "hello" {
		t.Fatalf("expected one committed ApplicationEntry, got %v", ents)
	}
}

var (
	testingSnap = pb.Snapshot{
		Index:      11, // magic number
		Term:       11, // magic number
		Membership: getTestMembership([]uint64{1, 2}),
	}
)

func TestSendingSnapshotSetPendingSnapshot(t *testing.T) {
	storage := NewTestLogDB()
	sm := newTestRaft(1, []uint64{1}, 10, 1, storage)
	if _, err := sm.restore(testingSnap); err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	sm.restoreRemotes(testingSnap)

	sm.becomeCandidate()
	sm.becomeLeader()

	// force set the next of node 1, so that
	// node 1 needs a snapshot
	sm.remotes[2].next = sm.log.firstIndex()

	ne(sm.Handle(pb.Message{From: 2, To: 1, Type: pb.ReplicateResp, LogIndex: sm.remotes[2].next - 1, Reject: true}), t)
	if sm.remotes[2].snapshotIndex != 11 {
		t.Fatalf("PendingSnapshot = %d, want 11", sm.remotes[2].snapshotIndex)
	}
}

func TestPendingSnapshotPauseReplication(t *testing.T) {
	storage := NewTestLogDB()
	sm := newTestRaft(1, []uint64{1, 2}, 10, 1, storage)
	if _, err := sm.restore(testingSnap); err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	sm.restoreRemotes(testingSnap)

	sm.becomeCandidate()
	sm.becomeLeader()

	sm.remotes[2].becomeSnapshot(11)

	ne(sm.Handle(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("somedata")}}}), t)
	msgs := sm.readMessages()
	if len(msgs) != 0 {
		t.Fatalf("len(msgs) = %d, want 0", len(msgs))
	}
}

func TestSnapshotFailure(t *testing.T) {
	storage := NewTestLogDB()
	sm := newTestRaft(1, []uint64{1, 2}, 10, 1, storage)
	if _, err := sm.restore(testingSnap); err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	sm.restoreRemotes(testingSnap)

	sm.becomeCandidate()
	sm.becomeLeader()

	sm.remotes[2].next = 1
	sm.remotes[2].becomeSnapshot(11)

	ne(sm.Handle(pb.Message{From: 2, To: 1, Type: pb.SnapshotStatus, Reject: true}), t)
	if sm.remotes[2].snapshotIndex != 0 {
		t.Fatalf("PendingSnapshot = %d, want 0", sm.remotes[2].snapshotIndex)
	}
	if sm.remotes[2].next != 1 {
		t.Fatalf("Next = %d, want 1", sm.remotes[2].next)
	}
	if sm.remotes[2].state != remoteWait {
		t.Errorf("Paused = %s, want remoteWait", sm.remotes[2].state)
	}
}

func TestSnapshotSucceed(t *testing.T) {
	storage := NewTestLogDB()
	sm := newTestRaft(1, []uint64{1, 2}, 10, 1, storage)
	if _, err := sm.restore(testingSnap); err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	sm.restoreRemotes(testingSnap)

	sm.becomeCandidate()
	sm.becomeLeader()

	sm.remotes[2].next = 1
	sm.remotes[2].becomeSnapshot(11)

	ne(sm.Handle(pb.Message{From: 2, To: 1, Type: pb.SnapshotStatus, Reject: false}), t)
	if sm.remotes[2].snapshotIndex != 0 {
		t.Fatalf("PendingSnapshot = %d, want 0", sm.remotes[2].snapshotIndex)
	}
	if sm.remotes[2].next != 12 {
		t.Fatalf("Next = %d, want 12", sm.remotes[2].next)
	}
	if sm.remotes[2].state != remoteWait {
		t.Errorf("Paused = %s, want remoteWait", sm.remotes[2].state)
	}
}

func TestSnapshotAbort(t *testing.T) {
	storage := NewTestLogDB()
	sm := newTestRaft(1, []uint64{1, 2}, 10, 1, storage)
	if _, err := sm.restore(testingSnap); err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	sm.restoreRemotes(testingSnap)

	sm.becomeCandidate()
	sm.becomeLeader()

	sm.remotes[2].next = 1
	sm.remotes[2].becomeSnapshot(11)

	// A successful msgAppResp that has a higher/equal index than the
	// pending snapshot should abort the pending snapshot.
	ne(sm.Handle(pb.Message{From: 2, To: 1, Type: pb.ReplicateResp, LogIndex: 11}), t)
	if sm.remotes[2].snapshotIndex != 0 {
		t.Fatalf("PendingSnapshot = %d, want 0", sm.remotes[2].snapshotIndex)
	}
	if sm.remotes[2].next != 12 {
		t.Fatalf("Next = %d, want 12", sm.remotes[2].next)
	}
}

func entsWithConfig(configFunc func(config.Config), terms ...uint64) *raft {
	storage := NewTestLogDB()
	for i, term := range terms {
		if err := storage.Append([]pb.Entry{{Index: uint64(i + 1), Term: term}}); err != nil {
			panic(err)
		}
	}
	cfg := newTestConfig(1, 5, 1)
	if configFunc != nil {
		configFunc(cfg)
	}
	sm := newRaft(cfg, storage)
	sm.reset(terms[len(terms)-1], true)
	return sm
}

// votedWithConfig creates a raft state machine with Vote and Term set
// to the given value but no log entries (indicating that it voted in
// the given term but has not received any logs).
func votedWithConfig(configFunc func(config.Config), vote, term uint64) *raft {
	storage := NewTestLogDB()
	storage.SetState(pb.State{Vote: vote, Term: term})
	cfg := newTestConfig(1, 5, 1)
	if configFunc != nil {
		configFunc(cfg)
	}
	sm := newRaft(cfg, storage)
	sm.reset(term, true)
	return sm
}

type network struct {
	peers   map[uint64]stateMachine
	storage map[uint64]ILogDB
	dropm   map[connem]float64
	ignorem map[pb.MessageType]bool
}

// newNetwork initializes a network from peers.
// A nil node will be replaced with a new *stateMachine.
// A *stateMachine will get its k, id.
// When using stateMachine, the address list is always [1, n].
func newNetwork(peers ...stateMachine) *network {
	return newNetworkWithConfig(nil, peers...)
}

// newNetworkWithConfig is like newNetwork but calls the given func to
// modify the configuration of any state machines it creates.
func newNetworkWithConfig(configFunc func(config.Config), peers ...stateMachine) *network {
	size := len(peers)
	peerAddrs := idsBySize(size)

	npeers := make(map[uint64]stateMachine, size)
	nstorage := make(map[uint64]ILogDB, size)

	for j, p := range peers {
		id := peerAddrs[j]
		switch v := p.(type) {
		case nil:
			nstorage[id] = NewTestLogDB()
			cfg := newTestConfig(id, 10, 1)
			if configFunc != nil {
				configFunc(cfg)
			}
			sm := newRaft(cfg, nstorage[id])
			sm.setTestPeers(peerAddrs)
			sm.hasNotAppliedConfigChange = sm.testOnlyHasConfigChangeToApply
			npeers[id] = sm
		case *raft:
			nonVotings := make(map[uint64]bool)
			witnesses := make(map[uint64]bool)
			for i := range v.nonVotings {
				nonVotings[i] = true
			}
			for i := range v.witnesses {
				witnesses[i] = true
			}

			v.replicaID = id
			v.remotes = make(map[uint64]*remote)
			v.nonVotings = make(map[uint64]*remote)
			v.witnesses = make(map[uint64]*remote)
			for i := 0; i < size; i++ {
				if _, ok := nonVotings[peerAddrs[i]]; ok {
					v.nonVotings[peerAddrs[i]] = &remote{}
				} else if _, ok := witnesses[peerAddrs[i]]; ok {
					v.witnesses[peerAddrs[i]] = &remote{}
				} else {
					v.remotes[peerAddrs[i]] = &remote{}
				}
			}
			v.reset(v.term, true)
			npeers[id] = v
		case *blackHole:
			npeers[id] = v
		default:
			panic(fmt.Sprintf("unexpected state machine type: %T", p))
		}
	}
	return &network{
		peers:   npeers,
		storage: nstorage,
		dropm:   make(map[connem]float64),
		ignorem: make(map[pb.MessageType]bool),
	}
}

func (nw *network) send(msgs ...pb.Message) {
	for len(msgs) > 0 {
		m := msgs[0]
		p := nw.peers[m.To]
		if err := p.Handle(m); err != nil {
			panic(err)
		}
		msgs = append(msgs[1:], nw.filter(p.readMessages())...)
	}
}

func (nw *network) drop(from, to uint64, perc float64) {
	nw.dropm[connem{from, to}] = perc
}

func (nw *network) cut(one, other uint64) {
	nw.drop(one, other, 1)
	nw.drop(other, one, 1)
}

func (nw *network) isolate(id uint64) {
	for i := 0; i < len(nw.peers); i++ {
		nid := uint64(i) + 1
		if nid != id {
			nw.drop(id, nid, 1.0)
			nw.drop(nid, id, 1.0)
		}
	}
}

func (nw *network) ignore(t pb.MessageType) {
	nw.ignorem[t] = true
}

func (nw *network) recover() {
	nw.dropm = make(map[connem]float64)
	nw.ignorem = make(map[pb.MessageType]bool)
}

func (nw *network) filter(msgs []pb.Message) []pb.Message {
	mm := []pb.Message{}
	for _, m := range msgs {
		if nw.ignorem[m.Type] {
			continue
		}
		switch m.Type {
		case pb.Election:
			// hups never go over the network, so don't drop them but panic
			panic("unexpected msgHup")
		default:
			perc := nw.dropm[connem{m.From, m.To}]
			if n := rand.Float64(); n < perc {
				continue
			}
		}
		mm = append(mm, m)
	}
	return mm
}

type connem struct {
	from, to uint64
}

type blackHole struct{}

func (blackHole) Handle(pb.Message) error    { return nil }
func (blackHole) readMessages() []pb.Message { return nil }

var nopStepper = &blackHole{}

func idsBySize(size int) []uint64 {
	ids := make([]uint64, size)
	for i := 0; i < size; i++ {
		ids[i] = 1 + uint64(i)
	}
	return ids
}

// setRandomizedElectionTimeout set up the value by caller instead of choosing
// by system, in some test scenario we need to fill in some expected value to
// ensure the certainty
func setRandomizedElectionTimeout(r *raft, v uint64) {
	r.randomizedElectionTimeout = v
}

func newTestConfig(id uint64, election, heartbeat int) config.Config {
	return newRateLimitedTestConfig(id, election, heartbeat, 0)
}

func newRateLimitedTestConfig(id uint64, election, heartbeat int, maxLogSize int) config.Config {
	return config.Config{
		ReplicaID:       id,
		ElectionRTT:     uint64(election),
		HeartbeatRTT:    uint64(heartbeat),
		MaxInMemLogSize: uint64(maxLogSize),
	}
}

func newTestRaft(id uint64, peers []uint64, election, heartbeat int, logdb ILogDB) *raft {
	r := newRaft(newTestConfig(id, election, heartbeat), logdb)
	if len(r.remotes) == 0 {
		for _, p := range peers {
			r.remotes[p] = &remote{next: 1}
		}
	}
	r.hasNotAppliedConfigChange = r.testOnlyHasConfigChangeToApply
	return r
}

func newRateLimitedTestRaft(id uint64, peers []uint64, election, heartbeat int, logdb ILogDB) *raft {
	cfg := config.Config{
		ReplicaID:       id,
		ElectionRTT:     uint64(election),
		HeartbeatRTT:    uint64(heartbeat),
		MaxInMemLogSize: testRateLimit,
	}
	r := newRaft(cfg, logdb)
	if len(r.remotes) == 0 {
		for _, p := range peers {
			r.remotes[p] = &remote{next: 1}
		}
	}
	r.hasNotAppliedConfigChange = r.testOnlyHasConfigChangeToApply
	return r
}

func newTestNonVoting(id uint64, peers []uint64, nonVotings []uint64, election, heartbeat int, logdb ILogDB) *raft {
	found := false
	for _, p := range nonVotings {
		if p == id {
			found = true
		}
	}
	if !found {
		panic("nonVoting node id not included in the nonVotings list")
	}
	cfg := newTestConfig(id, election, heartbeat)
	cfg.IsNonVoting = true
	r := newRaft(cfg, logdb)
	if len(r.remotes) == 0 {
		for _, p := range peers {
			r.remotes[p] = &remote{next: 1}
		}
	}
	if len(r.nonVotings) == 0 {
		for _, p := range nonVotings {
			r.nonVotings[p] = &remote{next: 1}
		}
	}
	r.hasNotAppliedConfigChange = r.testOnlyHasConfigChangeToApply
	return r
}

func newTestWitness(id uint64, peers []uint64, witnesses []uint64, election, heartbeat int, logdb ILogDB) *raft {
	cfg := newTestConfig(id, election, heartbeat)
	cfg.IsWitness = true
	r := newRaft(cfg, logdb)
	if len(r.remotes) == 0 {
		for _, p := range peers {
			r.remotes[p] = &remote{next: 1}
		}
	}
	if len(r.witnesses) == 0 {
		for _, p := range witnesses {
			r.witnesses[p] = &remote{next: 1}
		}
	}
	r.hasNotAppliedConfigChange = r.testOnlyHasConfigChangeToApply
	return r
}
````

## File: internal/raft/raft_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raft

import (
	"math"
	"reflect"
	"sort"
	"testing"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/settings"

	"github.com/stretchr/testify/assert"

	"github.com/lni/dragonboat/v4/internal/server"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

//
// Most test here are more focused on individual features/actions in our raft
// implementation. They are more like unit tests. raft_etcd_test.go contains
// tests ported from etcd raft, those tests are more like raft protocol level
// integration tests.
//

func TestInitializeRaft(t *testing.T) {
	m := pb.Membership{
		Addresses: map[uint64]string{
			5: "",
			6: "",
			7: "",
		},
		NonVotings: map[uint64]string{
			3: "",
			4: "",
		},
		Witnesses: map[uint64]string{
			1: "",
			2: "",
		},
		Removed: make(map[uint64]bool),
	}

	logdb := &TestLogDB{
		entries: make([]pb.Entry, 0),
		snapshot: pb.Snapshot{
			Membership: m,
		},
	}

	node := newRaft(newTestConfig(1, 10, 1), logdb)
	if len(node.remotes) != 3 {
		t.Errorf("remotes length not expected: %d", len(node.remotes))
	}
	if len(node.nonVotings) != 2 {
		t.Errorf("nonVotings length not expected: %d", len(node.nonVotings))
	}
	if len(node.witnesses) != 2 {
		t.Errorf("witnesses length not expected: %d", len(node.witnesses))
	}
}

func TestMustBeLeaderPanicWhenNotLeader(t *testing.T) {
	tests := []struct {
		st          State
		shouldPanic bool
	}{
		{follower, true},
		{candidate, true},
		{leader, false},
		{nonVoting, true},
	}
	for idx, tt := range tests {
		r := raft{state: tt.st}
		func() {
			defer func() {
				r := recover()
				if r == nil {
					if tt.shouldPanic {
						t.Errorf("%d, failed to panic", idx)
					}
				}
			}()
			r.mustBeLeader()
		}()
	}
}

func TestConfigViolationWillPanic(t *testing.T) {
	tests := []struct {
		name       string
		config     config.Config
		shouldFail bool
	}{
		{"Zero node id", newTestConfig(0, 10, 1), true},
		{"Zero heartbeat", newTestConfig(1, 10, 0), true},
		{"Zero election rtt", newTestConfig(1, 0, 1), true},
		{"Too low election rtt", newTestConfig(1, 3, 2), true},
		{"Good config", newTestConfig(1, 10, 1), false},
		{"Rate limit too small", newRateLimitedTestConfig(1, 10, 1, 15), true},
		{"Good rate limit config", newRateLimitedTestConfig(1, 10, 1, settings.EntryNonCmdFieldsSize+5), false},
	}

	for _, test := range tests {
		func() {
			defer func() {
				if r := recover(); test.shouldFail == (r == nil) {
					t.Errorf("Test %v failed: panic expectation is %v however get recover result %v",
						test.name, test.shouldFail, r)
				}
			}()
			newRaft(test.config, NewTestLogDB())
		}()
	}
}

func TestNilLogdbWillPanic(t *testing.T) {
	defer func() {
		if r := recover(); r == nil {
			t.Errorf("Should have panic with nil logdb.")
		}
	}()
	newRaft(newTestConfig(1, 10, 1), nil)
}

func TestTryCommitResetsMatchArray(t *testing.T) {
	p := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	p.becomeCandidate()
	p.becomeLeader()
	p.matched = nil
	p.tryCommit()
	if len(p.matched) != 3 {
		t.Errorf("tryCommit didn't resize the match array")
	}
}

func TestOneNodeWithHigherTermAndOneNodeWithMostRecentLogCanCompleteElection(t *testing.T) {
	a := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	b := newTestRaft(2, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	c := newTestRaft(3, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	a.becomeFollower(1, NoLeader)
	b.becomeFollower(1, NoLeader)
	c.becomeFollower(1, NoLeader)
	a.checkQuorum = true
	b.checkQuorum = true
	c.checkQuorum = true
	// cause a network partition to isolate node 3
	nt := newNetwork(a, b, c)
	nt.cut(1, 3)
	nt.cut(2, 3)
	// start a few elections to bump the term value
	for i := 0; i < 4; i++ {
		nt.send(pb.Message{From: 3, To: 3, Type: pb.Election})
	}
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("some data")}}})
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("some data2")}}})
	sm := nt.peers[1].(*raft)
	if sm.log.committed != 3 {
		t.Errorf("peer 1 committed index: %d, want %d", sm.log.committed, 3)
	}
	sm = nt.peers[2].(*raft)
	if sm.log.committed != 3 {
		t.Errorf("peer 2 committed index: %d, want %d", sm.log.committed, 3)
	}
	sm = nt.peers[1].(*raft)
	if sm.state != leader {
		t.Errorf("peer 1 state: %s, want %s", sm.state, leader)
	}
	sm = nt.peers[2].(*raft)
	if sm.state != follower {
		t.Errorf("peer 2 state: %s, want %s", sm.state, follower)
	}
	sm = nt.peers[3].(*raft)
	if sm.state != candidate {
		t.Errorf("peer 3 state: %s, want %s", sm.state, candidate)
	}
	// check whether the term values are expected
	// a.Term == 1
	// b.Term == 1
	// c.Term == 100
	sm = nt.peers[1].(*raft)
	if sm.term != 2 {
		t.Errorf("peer 1 term: %d, want %d", sm.term, 2)
	}
	sm = nt.peers[2].(*raft)
	if sm.term != 2 {
		t.Errorf("peer 2 term: %d, want %d", sm.term, 2)
	}
	sm = nt.peers[3].(*raft)
	if sm.term != 5 {
		t.Errorf("peer 3 term: %d, want %d", sm.term, 5)
	}
	nt.recover()
	nt.cut(1, 2)
	nt.cut(1, 3)
	for i := 0; i <= 2; i++ {
		// call for election
		nt.send(pb.Message{From: 3, To: 3, Type: pb.Election})
		nt.send(pb.Message{From: 2, To: 2, Type: pb.Election})

		// do we have a leader
		sma := nt.peers[2].(*raft)
		smb := nt.peers[3].(*raft)
		if sma.state != leader && smb.state != leader {
			if i == 2 {
				t.Errorf("no leader")
			}
		} else {
			break
		}
	}
	sm = nt.peers[2].(*raft)
	if sm.state != leader {
		t.Errorf("peer 2 state: %s, want %s", sm.state, leader)
	}
	sm = nt.peers[3].(*raft)
	if sm.state != follower {
		t.Errorf("peer 3 state: %s, want %s", sm.state, follower)
	}
}

func TestRaftHelperMethods(t *testing.T) {
	v := ReplicaID(100)
	v2 := ShardID(100)
	if v != "n00100" || v2 != "c00100" {
		t.Errorf("unexpected node id / shard id value")
	}
	r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	r.becomeFollower(2, 3)
	r.checkHandlerMap()
	addrMap := make(map[uint64]string)
	addrMap[1] = "address1"
	addrMap[2] = "address2"
	addrMap[3] = "address3"
	status := getLocalStatus(r)
	if status.IsLeader() || !status.IsFollower() || status.ReplicaID != 1 {
		t.Errorf("unexpected status value")
	}
}

func TestBecomePreVoteCandidateFromCandidate(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	r.preVote = true
	r.becomeFollower(2, 3)
	r.becomeCandidate()
	r.becomePreVoteCandidate()
	if r.term != 3 {
		t.Errorf("term changed")
	}
	if r.state != preVoteCandidate {
		t.Errorf("state not set to preVoteCandidate")
	}
}

func TestBecomePreVoteCandidate(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	r.preVote = true
	r.becomeFollower(2, 3)
	if r.term != 2 {
		t.Errorf("term not set")
	}
	r.becomePreVoteCandidate()
	if r.term != 2 {
		t.Errorf("term changed")
	}
	if r.state != preVoteCandidate {
		t.Errorf("state not set to preVoteCandidate")
	}
	if r.leaderID != NoLeader {
		t.Errorf("leader unexpectedly set")
	}
}

func TestBecomeFollowerDragonboat(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	r.electionTick = 100
	r.becomeFollower(2, 3)
	if r.term != 2 {
		t.Errorf("term not set")
	}
	if r.leaderID != 3 {
		t.Errorf("leader id not set")
	}
	if r.state != follower {
		t.Errorf("not become follower")
	}
	if r.electionTick != 0 {
		t.Errorf("election tick not reset, %d", r.electionTick)
	}
}

func TestBecomeFollowerKE(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	r.electionTick = 100
	r.electionTimeout = 101
	r.becomeFollowerKE(2, 3)
	if r.term != 2 {
		t.Errorf("term not set")
	}
	if r.leaderID != 3 {
		t.Errorf("leader id not set")
	}
	if r.state != follower {
		t.Errorf("not become follower")
	}
	if r.electionTick != 100 {
		t.Errorf("election tick changed, %d", r.electionTick)
	}
	if r.electionTimeout != 101 {
		t.Errorf("election timeout changed, %d", r.electionTimeout)
	}
}

func TestBecomeCandidatePanicWhenNodeIsLeader(t *testing.T) {
	ready := false
	defer func() {
		if r := recover(); r != nil {
			if !ready {
				t.Errorf("panic too early")
			}
			return
		}
		t.Errorf("not panic")
	}()
	r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	r.becomeFollower(2, 3)
	r.becomeCandidate()
	r.becomeLeader()
	ready = true
	r.becomeCandidate()
}

func TestBecomeLeaderPanicWhenNodeIsFollower(t *testing.T) {
	ready := false
	defer func() {
		if r := recover(); r != nil {
			if !ready {
				t.Errorf("panic too early")
			}
			return
		}
		t.Errorf("not panic")
	}()
	r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	r.becomeFollower(2, 3)
	ready = true
	r.becomeLeader()
}

func TestBecomeCandidateDragonboat(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	r.becomeFollower(2, 3)
	r.electionTick = 100
	term := r.term
	r.becomeCandidate()
	if r.term != term+1 {
		t.Errorf("term didn't increase")
	}
	if r.state != candidate {
		t.Errorf("not in candidate state")
	}
	if r.vote != r.replicaID {
		t.Errorf("vote not set")
	}
	if r.electionTick != 0 {
		t.Errorf("election tick not reset, %d", r.electionTick)
	}
}

func TestNonVotingWillNotStartElection(t *testing.T) {
	p := newTestNonVoting(1, nil, []uint64{1}, 10, 1, NewTestLogDB())
	if !p.isNonVoting() {
		t.Errorf("not a non-voting node")
	}
	if len(p.remotes) != 0 {
		t.Errorf("p.romotes len: %d", len(p.remotes))
	}
	for i := uint64(0); i < p.randomizedElectionTimeout*10; i++ {
		ne(p.tick(), t)
	}
	// gRequestVote won't be sent
	if len(p.msgs) != 0 {
		t.Errorf("unexpected msg found %+v", p.msgs)
	}
}

func TestNonVotingsPreVoteWillNotBeCounted(t *testing.T) {
	p := newTestNonVoting(1, nil, []uint64{1, 2}, 10, 1, NewTestLogDB())
	p.preVote = true
	p.addNode(1)
	p.becomeCandidate()
	ne(p.Handle(pb.Message{From: 2, To: 1, Type: pb.RequestPreVoteResp}), t)
	if len(p.votes) != 0 {
		t.Errorf("vote from nonvoting not dropped")
	}
}

func TestNonVotingsVoteWillNotBeCounted(t *testing.T) {
	p := newTestNonVoting(1, nil, []uint64{1, 2}, 10, 1, NewTestLogDB())
	p.addNode(1)
	p.becomeCandidate()
	ne(p.Handle(pb.Message{From: 2, To: 1, Type: pb.RequestVoteResp}), t)
	if len(p.votes) != 0 {
		t.Errorf("vote from nonvoting not dropped")
	}
}

func TestNonVotingCanBePromotedToVotingMember(t *testing.T) {
	p := newTestNonVoting(1, nil, []uint64{1}, 10, 1, NewTestLogDB())
	if !p.isNonVoting() {
		t.Errorf("not a non-voting node")
	}
	p.addNode(1)
	if p.isNonVoting() {
		t.Errorf("not promoted to regular node")
	}
	if len(p.remotes) != 1 {
		t.Errorf("remotes len: %d, want 1", len(p.remotes))
	}
	if len(p.nonVotings) != 0 {
		t.Errorf("nonVotings len: %d, want 0", len(p.nonVotings))
	}
}

func TestNonVotingCanActAsRegularNodeAfterPromotion(t *testing.T) {
	p := newTestNonVoting(1, nil, []uint64{1}, 10, 1, NewTestLogDB())
	if !p.isNonVoting() {
		t.Errorf("not a non-voting node")
	}
	p.addNode(1)
	if p.isNonVoting() {
		t.Errorf("not promoted to regular node")
	}
	for i := uint64(0); i <= p.randomizedElectionTimeout; i++ {
		ne(p.tick(), t)
	}
	if p.state != leader {
		t.Errorf("failed to start election")
	}
}

// nonVotings will not be asked to vote, however a node used to be a non-voting node
// may not realize its own promotion and thus can still receive RequestVote from
// candidates that are aware of such promotion. in this case, nonVotings have to
// cast their votes
func TestNonVotingCanVote(t *testing.T) {
	testNonVotingCanVote(t, false)
	testNonVotingCanVote(t, true)
}

func testNonVotingCanVote(t *testing.T, preVote bool) {
	a := newTestNonVoting(1, nil, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	b := newTestNonVoting(2, nil, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	c := newTestNonVoting(3, nil, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	if preVote {
		a.preVote = true
		b.preVote = true
		c.preVote = true
	}
	if !c.isNonVoting() {
		t.Errorf("not nonvoting")
	}
	a.addNode(1)
	b.addNode(2)
	if a.isNonVoting() || b.isNonVoting() {
		t.Errorf("unepected nonvoting")
	}
	if a.state != follower {
		t.Errorf("not a follower")
	}
	if b.state != follower {
		t.Errorf("not a follower")
	}
	nt := newNetwork(a, b, c)
	nt.isolate(1)
	nt.send(pb.Message{From: 2, To: 2, Type: pb.Election})
	if !b.isLeader() {
		t.Errorf("not leader")
	}
	if b.term == a.term || a.leaderID != NoLeader {
		t.Errorf("a not isolated")
	}
}

func TestNonVotingReplication(t *testing.T) {
	p1 := newTestNonVoting(1, nil, []uint64{1, 2}, 10, 1, NewTestLogDB())
	p2 := newTestNonVoting(2, nil, []uint64{1, 2}, 10, 1, NewTestLogDB())
	p1.addNode(1)
	p2.addNode(1)
	if p1.isNonVoting() {
		t.Errorf("p1 is still nonvoting")
	}
	if !p2.isNonVoting() {
		t.Errorf("p2 is not nonvoting")
	}
	nt := newNetwork(p1, p2)
	if len(p1.remotes) != 1 {
		t.Errorf("remotes len: %d, want 1", len(p1.remotes))
	}
	for i := uint64(0); i <= p1.randomizedElectionTimeout; i++ {
		ne(p1.tick(), t)
	}
	if p1.state != leader {
		t.Errorf("failed to start election")
	}
	committed := p1.log.committed
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("test-data")}}})
	if committed+1 != p1.log.committed {
		t.Errorf("entry not committed")
	}
	// the no-op blank entry appended after p1 becomes the leader is also replicated
	if committed+1 != p2.log.committed {
		t.Errorf("entry not committed on nonvoting: %d", p2.log.committed)
	}
	if committed+1 != p1.nonVotings[2].match {
		t.Errorf("match value not expected: %d", p1.nonVotings[2].match)
	}
}

func TestNonVotingCanPropose(t *testing.T) {
	p1 := newTestNonVoting(1, nil, []uint64{1, 2}, 10, 1, NewTestLogDB())
	p2 := newTestNonVoting(2, nil, []uint64{1, 2}, 10, 1, NewTestLogDB())
	p1.addNode(1)
	p2.addNode(1)
	if p1.isNonVoting() {
		t.Errorf("p1 is still nonvoting")
	}
	if !p2.isNonVoting() {
		t.Errorf("p2 is not nonvoting")
	}
	nt := newNetwork(p1, p2)
	if len(p1.remotes) != 1 {
		t.Errorf("remotes len: %d, want 1", len(p1.remotes))
	}
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	if p1.state != leader {
		t.Errorf("failed to start election")
	}
	for i := uint64(0); i <= p1.randomizedElectionTimeout; i++ {
		ne(p1.tick(), t)
		nt.send(pb.Message{From: 1, To: 1, Type: pb.NoOP})
	}
	if !p2.isNonVoting() {
		t.Errorf("not nonvoting")
	}
	committed := p1.log.committed
	for i := 0; i < 10; i++ {
		nt.send(pb.Message{From: 2, To: 2, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("test-data")}}})
	}
	if committed+10 != p1.log.committed {
		t.Errorf("entry not committed")
	}
	// the no-op blank entry appended after p1 becomes the leader is also replicated
	if committed+10 != p2.log.committed {
		t.Errorf("entry not committed on nonvoting: %d", p2.log.committed)
	}
	if committed+10 != p1.nonVotings[2].match {
		t.Errorf("match value not expected: %d", p1.nonVotings[2].match)
	}
}

func TestNonVotingCanReadIndexQuorum1(t *testing.T) {
	p1 := newTestNonVoting(1, nil, []uint64{1, 2}, 10, 1, NewTestLogDB())
	p2 := newTestNonVoting(2, nil, []uint64{1, 2}, 10, 1, NewTestLogDB())
	p1.addNode(1)
	p2.addNode(1)
	if p1.isNonVoting() {
		t.Errorf("p1 is still nonvoting")
	}
	if !p2.isNonVoting() {
		t.Errorf("p2 is not nonvoting")
	}
	nt := newNetwork(p1, p2)
	if len(p1.remotes) != 1 {
		t.Errorf("remotes len: %d, want 1", len(p1.remotes))
	}
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	if p1.state != leader {
		t.Errorf("failed to start election")
	}
	for i := uint64(0); i <= p1.randomizedElectionTimeout; i++ {
		ne(p1.tick(), t)
		nt.send(pb.Message{From: 1, To: 1, Type: pb.NoOP})
	}
	if !p2.isNonVoting() {
		t.Errorf("not nonvoting")
	}
	committed := p1.log.committed
	for i := 0; i < 10; i++ {
		nt.send(pb.Message{From: 2, To: 2, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("test-data")}}})
	}
	if committed+10 != p1.log.committed {
		t.Errorf("entry not committed")
	}
	nt.send(pb.Message{From: 2, To: 2, Type: pb.ReadIndex, Hint: 12345})
	if len(p2.readyToRead) != 1 {
		t.Fatalf("ready to read len is 0")
	}
	if p2.readyToRead[0].Index != p1.log.committed {
		t.Errorf("unexpected ready to read index")
	}
}

func TestNonVotingCanReadIndexQuorum2(t *testing.T) {
	p1 := newTestRaft(1, []uint64{1, 2}, 10, 1, NewTestLogDB())
	p2 := newTestRaft(2, []uint64{1, 2}, 10, 1, NewTestLogDB())
	p3 := newTestNonVoting(3, []uint64{1, 2}, []uint64{3}, 10, 1, NewTestLogDB())
	p1.addNonVoting(3)
	p2.addNonVoting(3)
	nt := newNetwork(p1, p2, p3)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	if p1.state != leader {
		t.Errorf("failed to start election")
	}
	if p2.state != follower {
		t.Errorf("not a follower")
	}
	if !p3.isNonVoting() {
		t.Errorf("not a non-voting node")
	}
	for i := uint64(0); i <= p1.randomizedElectionTimeout; i++ {
		ne(p1.tick(), t)
		nt.send(pb.Message{From: 1, To: 1, Type: pb.NoOP})
	}
	committed := p1.log.committed
	for i := 0; i < 10; i++ {
		nt.send(pb.Message{From: 2, To: 2, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("test-data")}}})
	}
	if committed+10 != p1.log.committed {
		t.Errorf("entry not committed")
	}
	nt.send(pb.Message{From: 3, To: 3, Type: pb.ReadIndex, Hint: 12345})
	if len(p3.readyToRead) != 1 {
		t.Fatalf("ready to read len is not 1")
	}
	if p3.readyToRead[0].Index != p1.log.committed {
		t.Errorf("unexpected ready to read index")
	}
}

func TestNonVotingCanReceiveSnapshot(t *testing.T) {
	members := pb.Membership{
		Addresses:  make(map[uint64]string),
		NonVotings: make(map[uint64]string),
		Removed:    make(map[uint64]bool),
	}
	members.Addresses[1] = "a1"
	members.Addresses[2] = "a2"
	ss := pb.Snapshot{
		Index:      20,
		Term:       20,
		Membership: members,
	}
	p1 := newTestNonVoting(3, []uint64{1}, []uint64{2, 3}, 10, 1, NewTestLogDB())
	if !p1.isNonVoting() {
		t.Errorf("not a non-voting node")
	}
	ne(p1.Handle(pb.Message{From: 2, To: 1, Type: pb.InstallSnapshot, Snapshot: ss}), t)
	if p1.log.committed != 20 {
		t.Errorf("snapshot not applied")
	}
}

func TestNonVotingCanReceiveHeartbeatMessage(t *testing.T) {
	p1 := newTestNonVoting(2, []uint64{1}, []uint64{2}, 10, 1, NewTestLogDB())
	m := pb.Message{
		From:     1,
		To:       2,
		Type:     pb.Replicate,
		LogIndex: 0,
		LogTerm:  0,
		Commit:   0,
		Entries:  make([]pb.Entry, 0),
	}
	m.Entries = append(m.Entries, pb.Entry{Index: 1, Term: 1, Cmd: []byte("test-data1")})
	m.Entries = append(m.Entries, pb.Entry{Index: 2, Term: 1, Cmd: []byte("test-data2")})
	m.Entries = append(m.Entries, pb.Entry{Index: 3, Term: 1, Cmd: []byte("test-data3")})
	ne(p1.Handle(m), t)
	if p1.log.lastIndex() != 3 {
		t.Errorf("last index unexpected: %d, want 3", p1.log.lastIndex())
	}
	if p1.log.committed != 0 {
		t.Errorf("unexpected committed value %d, want 0", p1.log.committed)
	}
	hbm := pb.Message{
		Type:   pb.Heartbeat,
		Commit: 3,
		From:   1,
		To:     2,
	}
	ne(p1.Handle(hbm), t)
	if p1.log.committed != 3 {
		t.Errorf("unexpected committed value %d, want 3", p1.log.committed)
	}
}

func TestNonVotingCanBeRestored(t *testing.T) {
	members := pb.Membership{
		Addresses:  make(map[uint64]string),
		NonVotings: make(map[uint64]string),
		Removed:    make(map[uint64]bool),
	}
	members.Addresses[1] = "a1"
	members.Addresses[2] = "a2"
	members.NonVotings[3] = "a3"
	ss := pb.Snapshot{
		Index:      20,
		Term:       20,
		Membership: members,
	}
	p1 := newTestNonVoting(3, []uint64{1, 2}, []uint64{3}, 10, 1, NewTestLogDB())
	ok, err := p1.restore(ss)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if !ok {
		t.Errorf("failed to restore")
	}
}

func TestNonVotingCanBePromotedBySnapshot(t *testing.T) {
	members := pb.Membership{
		Addresses:  make(map[uint64]string),
		NonVotings: make(map[uint64]string),
		Removed:    make(map[uint64]bool),
	}
	members.Addresses[1] = "a1"
	members.Addresses[2] = "a2"
	ss := pb.Snapshot{
		Index:      20,
		Term:       20,
		Membership: members,
	}
	p1 := newTestNonVoting(1, nil, []uint64{1, 2}, 10, 1, NewTestLogDB())
	if !p1.isNonVoting() {
		t.Errorf("not a non-voting node")
	}
	ok, err := p1.restore(ss)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if !ok {
		t.Errorf("failed to restore")
	}
	p1.restoreRemotes(ss)
	if p1.isNonVoting() {
		t.Errorf("nonvoting not promoted")
	}
}

func TestCorrectNonVotingCanBePromotedBySnapshot(t *testing.T) {
	members := pb.Membership{
		Addresses:  make(map[uint64]string),
		NonVotings: make(map[uint64]string),
		Removed:    make(map[uint64]bool),
	}
	members.NonVotings[1] = "a1"
	members.Addresses[2] = "a2"
	members.Addresses[3] = "a3"
	ss := pb.Snapshot{
		Index:      20,
		Term:       20,
		Membership: members,
	}
	p1 := newTestNonVoting(1, []uint64{2}, []uint64{1, 3}, 10, 1, NewTestLogDB())
	if !p1.isNonVoting() {
		t.Errorf("not a non-voting node")
	}
	_, ok := p1.nonVotings[1]
	if !ok {
		t.Errorf("not a non-voting node")
	}
	_, ok = p1.nonVotings[3]
	if !ok {
		t.Errorf("not a non-voting node")
	}
	p1.restoreRemotes(ss)
	if !p1.isNonVoting() {
		t.Errorf("nonvoting p1 unexpectedly promoted")
	}
}

func TestNonVotingCanNotMoveNodeBackToNonVotingBySnapshot(t *testing.T) {
	members := pb.Membership{
		Addresses:  make(map[uint64]string),
		NonVotings: make(map[uint64]string),
		Removed:    make(map[uint64]bool),
	}
	members.Addresses[1] = "a1"
	members.Addresses[2] = "a2"
	members.NonVotings[3] = "a3"
	ss := pb.Snapshot{
		Index:      20,
		Term:       20,
		Membership: members,
	}
	p1 := newTestRaft(3, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	defer func() {
		if r := recover(); r == nil {
			panic("restore didn't cause panic")
		}
	}()
	ok, err := p1.restore(ss)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if ok {
		t.Errorf("restore unexpectedly completed")
	}
}

func TestNonVotingCanBeAdded(t *testing.T) {
	p1 := newTestRaft(1, []uint64{1}, 10, 1, NewTestLogDB())
	if len(p1.nonVotings) != 0 {
		t.Errorf("unexpected nonvoting record")
	}
	p1.addNonVoting(2)
	if len(p1.nonVotings) != 1 {
		t.Errorf("nonvoting not added")
	}
	if p1.isNonVoting() {
		t.Errorf("unexpectedly changed to nonvoting")
	}
}

func TestNonVotingCanBeRemoved(t *testing.T) {
	p1 := newTestNonVoting(1, nil, []uint64{1, 2}, 10, 1, NewTestLogDB())
	if len(p1.nonVotings) != 2 {
		t.Errorf("unexpected nonvoting count")
	}
	p1.removeNode(2)
	if len(p1.nonVotings) != 1 {
		t.Errorf("nonvoting not removed")
	}
	_, ok := p1.nonVotings[2]
	if ok {
		t.Errorf("nonvoting node 2 not removed")
	}
}

func TestWitnessCanNotBecomeNonVoting(t *testing.T) {
	_, witness, _ := setUpLeaderAndWitness(t)
	defer func() {
		if r := recover(); r == nil {
			t.Fatalf("witness became follower")
		}
	}()
	witness.becomeNonVoting(1, 1)
}

func TestWitnessCanNotBecomeFollower(t *testing.T) {
	_, witness, _ := setUpLeaderAndWitness(t)
	defer func() {
		if r := recover(); r == nil {
			t.Fatalf("witness became follower")
		}
	}()
	witness.becomeFollower(1, 1)
}

func TestWitnessCanNotBecomeCandidate(t *testing.T) {
	_, witness, _ := setUpLeaderAndWitness(t)
	defer func() {
		if r := recover(); r == nil {
			t.Fatalf("witness became candidate")
		}
	}()
	witness.becomeCandidate()
}

func TestWitnessWillNotStartElection(t *testing.T) {
	p := newTestWitness(1, nil, []uint64{1}, 10, 1, NewTestLogDB())
	if !p.isWitness() {
		t.Errorf("not a witness")
	}
	if len(p.remotes) != 0 {
		t.Errorf("p.romotes len: %d", len(p.remotes))
	}
	for i := uint64(0); i < p.randomizedElectionTimeout*10; i++ {
		ne(p.tick(), t)
	}
	// RequestVote won't be sent
	if len(p.msgs) != 0 {
		t.Errorf("unexpected msg found %+v", p.msgs)
	}
}

func TestWitnessWillVoteInElection(t *testing.T) {
	p := newTestWitness(1, nil, []uint64{1}, 10, 1, NewTestLogDB())
	if !p.isWitness() {
		t.Errorf("not a witness")
	}
	ne(p.Handle(pb.Message{From: 2, To: 1, Type: pb.RequestVote, LogTerm: 100, LogIndex: 100}), t)
	if len(p.msgs) != 1 {
		t.Fatalf("witness is not voting")
	}
	if p.msgs[0].Type != pb.RequestVoteResp {
		t.Errorf("witness didn't send vote resp")
	}
}

func TestWitnessCannotBePromotedToFullMember(t *testing.T) {
	defer func() {
		if r := recover(); r == nil {
			t.Errorf("Should panic while promoting from witness")
		}
	}()
	replicaID := uint64(1)
	p := newTestWitness(replicaID, nil, []uint64{1}, 10, 1, NewTestLogDB())
	if !p.isWitness() {
		t.Errorf("not an witness")
	}
	p.addNode(replicaID)
}

func TestNonWitnessWouldPanicWhenRemoteSnapshotAssumeAsWitness(t *testing.T) {
	members := pb.Membership{
		Addresses:  make(map[uint64]string),
		NonVotings: make(map[uint64]string),
		Removed:    make(map[uint64]bool),
	}
	members.Addresses[1] = "a1"
	members.Addresses[2] = "a2"
	ss := pb.Snapshot{
		Index:      20,
		Term:       20,
		Membership: members,
	}
	p1 := newTestNonVoting(1, []uint64{1}, []uint64{1}, 10, 1, NewTestLogDB())
	if !p1.isNonVoting() {
		t.Errorf("not a non-voting node")
	}
	ok, err := p1.restore(ss)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if !ok {
		t.Errorf("failed to restore")
	}
	p1.restoreRemotes(ss)
	if p1.isNonVoting() {
		t.Errorf("nonvoting not promoted")
	}

	p1.witnesses[2] = &remote{}
	defer func() {
		if r := recover(); r == nil {
			panic("assumed witness not promotion not causing panic")
		}
	}()
	p1.restoreRemotes(ss)
}

func TestWitnessReplication(t *testing.T) {
	leader, witness, nt := setUpLeaderAndWitness(t)
	committed := leader.log.committed
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("test-data")}}})
	expectedIndex := committed + 1
	if expectedIndex != leader.log.committed {
		t.Errorf("entry not committed on leader: %d", witness.log.committed)
	}
	// the no-op blank entry appended after p1 becomes the leader is also replicated
	if expectedIndex != witness.log.committed {
		t.Errorf("entry not committed on witness: %d", witness.log.committed)
	}
	if expectedIndex != leader.witnesses[2].match {
		t.Errorf("match value expected: %d, actual: %d", expectedIndex, leader.witnesses[2].match)
	}
}

func TestApplicationMessageSentToWitnessIsEmpty(t *testing.T) {
	_, witness, _ := setUpLeaderAndWitness(t)
	expectedEntry := pb.Entry{
		Type:  pb.MetadataEntry,
		Term:  1,
		Index: 1,
		Cmd:   nil,
	}
	witnessEntries, err := witness.log.getEntries(1, 2, math.MaxUint64)
	if err != nil {
		t.Errorf("Encounter error during get entries: %v", err)
	}
	if !reflect.DeepEqual(expectedEntry, witnessEntries[0]) {
		t.Errorf("Found entry not matching. Expected: %v, actual: %v", expectedEntry, witnessEntries[0])
	}
}

func TestConfigChangeMessageSentToWitnessIsEmpty(t *testing.T) {
	leader, witness, nt := setUpLeaderAndWitness(t)
	configChangEntry := pb.Entry{
		Term:  1,
		Index: 2,
		Type:  pb.ConfigChangeEntry,
		Cmd:   []byte("test-data"),
	}
	leader.log.append([]pb.Entry{configChangEntry})
	// Send config change to witness.
	leader.broadcastReplicateMessage()
	if len(leader.msgs) != 1 {
		t.Errorf("Expecting 1 election message, actually get %v", len(leader.msgs))
	}
	nt.send(leader.msgs[0])
	witnessEntries, err := witness.log.getEntries(1, 3, math.MaxUint64)
	if err != nil {
		t.Errorf("Encounter error during get entries: %v", err)
	}
	if !reflect.DeepEqual(configChangEntry, witnessEntries[1]) {
		t.Errorf("Found entry not matching. Expected: %v, actual: %v", configChangEntry, witnessEntries[1])
	}
}

func TestWitnessSnapshot(t *testing.T) {
	leader, _, _ := setUpLeaderAndWitness(t)
	ss := pb.Snapshot{Index: 10, Term: 2}
	if err := leader.log.logdb.ApplySnapshot(ss); err != nil {
		t.Errorf("apply snapshot failed %v", err)
	}
	msg := pb.Message{}
	if idx := leader.makeInstallSnapshotMessage(2, &msg); idx != 10 {
		t.Errorf("unexpected index %d", idx)
	}
	if msg.Type != pb.InstallSnapshot || msg.Snapshot.Index != 10 ||
		msg.Snapshot.Term != 2 || !msg.Snapshot.Witness || msg.Snapshot.Dummy {
		t.Errorf("unexpected message values")
	}
}

func TestNonWitnessCanNotAddItselfAsWitness(t *testing.T) {
	p := newTestRaft(1, []uint64{1}, 10, 1, NewTestLogDB())
	defer func() {
		if r := recover(); r == nil {
			panic("added non witness node as witness")
		}
	}()
	p.addWitness(1)
}

func TestWitnessCanNotBeAddedAsNode(t *testing.T) {
	_, witness, _ := setUpLeaderAndWitness(t)
	defer func() {
		if r := recover(); r == nil {
			t.Fatalf("witness added as a node")
		}
	}()
	witness.addNode(2)
}

func setUpLeaderAndWitness(t *testing.T) (*raft, *raft, *network) {
	leader := newTestRaft(1, []uint64{1, 2}, 10, 1, NewTestLogDB())
	witness := newTestWitness(2, nil, []uint64{2}, 10, 1, NewTestLogDB())
	leader.addWitness(2)
	witness.addNode(1)
	if !witness.isWitness() {
		t.Errorf("Assumed witness is not witness")
	}
	nt := newNetwork(leader, witness)
	if len(leader.remotes) != 1 {
		t.Errorf("remotes len: %d, want 1", len(leader.remotes))
	}
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	if !leader.isLeader() {
		t.Errorf("failed to start election")
	}
	for i := uint64(0); i <= leader.randomizedElectionTimeout; i++ {
		ne(leader.tick(), t)
		nt.send(pb.Message{From: 1, To: 1, Type: pb.NoOP})
	}
	if !witness.isWitness() {
		t.Errorf("not witness")
	}
	return leader, witness, nt
}

func TestWitnessCannotReadIndex(t *testing.T) {
	witness := newTestWitness(1, nil, []uint64{1}, 10, 1, NewTestLogDB())
	nt := newNetwork(witness)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.ReadIndex, Hint: 12345})
	if len(witness.readyToRead) != 0 {
		t.Errorf("ready to read len is not 0")
	}
}

func TestWitnessCanReceiveSnapshot(t *testing.T) {
	members := pb.Membership{
		Addresses: make(map[uint64]string),
		Witnesses: make(map[uint64]string),
		Removed:   make(map[uint64]bool),
	}
	members.Addresses[1] = "a1"
	members.Addresses[2] = "a2"
	ss := pb.Snapshot{
		Index:      20,
		Term:       20,
		Membership: members,
	}
	p1 := newTestWitness(3, []uint64{1}, []uint64{2}, 10, 1, NewTestLogDB())
	if !p1.isWitness() {
		t.Errorf("not a witness")
	}
	ne(p1.Handle(pb.Message{From: 2, To: 1, Type: pb.InstallSnapshot, Snapshot: ss}), t)
	if p1.log.committed != 20 {
		t.Errorf("snapshot not applied")
	}
	if len(p1.msgs) != 1 {
		t.Fatalf("failed to send ss resp")
	}
	if p1.msgs[len(p1.msgs)-1].LogIndex != 20 {
		t.Errorf("unexpected log index")
	}
}

func TestWitnessCanReceiveHeartbeatMessage(t *testing.T) {
	p1 := newTestWitness(2, []uint64{1}, []uint64{2}, 10, 1, NewTestLogDB())
	m := pb.Message{
		From:     1,
		To:       2,
		Type:     pb.Replicate,
		LogIndex: 0,
		LogTerm:  0,
		Commit:   0,
		Entries:  make([]pb.Entry, 0),
	}
	m.Entries = append(m.Entries, pb.Entry{Index: 1, Term: 1, Type: pb.MetadataEntry})
	m.Entries = append(m.Entries, pb.Entry{Index: 2, Term: 1, Type: pb.MetadataEntry})
	m.Entries = append(m.Entries, pb.Entry{Index: 3, Term: 1, Type: pb.MetadataEntry})

	ne(p1.Handle(m), t)
	if p1.log.lastIndex() != 3 {
		t.Errorf("last index unexpected: %d, want 3", p1.log.lastIndex())
	}
	if p1.log.committed != 0 {
		t.Errorf("unexpected committed value %d, want 0", p1.log.committed)
	}
	hbm := pb.Message{
		Type:   pb.Heartbeat,
		Commit: 3,
		From:   1,
		To:     2,
	}
	ne(p1.Handle(hbm), t)
	if p1.log.committed != 3 {
		t.Errorf("unexpected committed value %d, want 3", p1.log.committed)
	}
}

func TestWitnessCanBeRestored(t *testing.T) {
	members := pb.Membership{
		Addresses: make(map[uint64]string),
		Witnesses: make(map[uint64]string),
		Removed:   make(map[uint64]bool),
	}
	members.Addresses[1] = "a1"
	members.Addresses[2] = "a2"
	members.Witnesses[3] = "a3"
	ss := pb.Snapshot{
		Index:      20,
		Term:       20,
		Membership: members,
	}
	p1 := newTestWitness(3, []uint64{1, 2}, []uint64{3}, 10, 1, NewTestLogDB())
	ok, err := p1.restore(ss)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if !ok {
		t.Errorf("failed to restore")
	}
}

func TestWitnessCanNotMoveNodeBackToWitnessBySnapshot(t *testing.T) {
	members := pb.Membership{
		Addresses: make(map[uint64]string),
		Witnesses: make(map[uint64]string),
		Removed:   make(map[uint64]bool),
	}
	members.Addresses[1] = "a1"
	members.Addresses[2] = "a2"
	members.Witnesses[3] = "a3"
	ss := pb.Snapshot{
		Index:      20,
		Term:       20,
		Membership: members,
	}
	p1 := newTestRaft(3, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	defer func() {
		if r := recover(); r == nil {
			panic("restore didn't cause panic")
		}
	}()
	ok, err := p1.restore(ss)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if ok {
		t.Errorf("restore unexpectedly completed")
	}
}

func TestWitnessCanBeAdded(t *testing.T) {
	p1 := newTestRaft(1, []uint64{1}, 10, 1, NewTestLogDB())
	if len(p1.witnesses) != 0 {
		t.Errorf("unexpected witness record")
	}
	p1.addWitness(2)
	if len(p1.witnesses) != 1 {
		t.Errorf("witness not added")
	}
	if p1.isWitness() {
		t.Errorf("unexpectedly changed to nonvoting")
	}
}

func TestWitnessCanBeRemoved(t *testing.T) {
	p1 := newTestWitness(1, []uint64{1}, []uint64{2}, 10, 1, NewTestLogDB())
	if len(p1.witnesses) != 1 {
		t.Errorf("unexpected witness count")
	}
	p1.removeNode(2)
	if len(p1.witnesses) != 0 {
		t.Errorf("witness not removed")
	}
}

func TestFollowerTick(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(10, 2)
	for i := 0; i < 9; i++ {
		if r.timeForElection() {
			t.Errorf("time for election unexpected became true")
		}
		ne(r.tick(), t)
	}
	if len(r.msgs) != 1 {
		t.Fatalf("unexpected message count %+v", r.msgs)
	}
	if r.msgs[0].Type != pb.RequestVote {
		t.Errorf("gRequestVote not sent")
	}
}

func TestLeaderTick(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	for i := 0; i < 10; i++ {
		ne(r.tick(), t)
	}
	if len(r.msgs) != 10 {
		t.Errorf("unexpected msg count")
	}
	for _, msg := range r.msgs {
		if msg.Type != pb.Heartbeat {
			t.Errorf("unexpected msg type")
		}
	}
}

func TestTimeForElection(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 5, 1, NewTestLogDB())
	if r.randomizedElectionTimeout < 5 || r.randomizedElectionTimeout > 10 {
		t.Errorf("unexpected randomized election timeout %d",
			r.randomizedElectionTimeout)
	}
	r.electionTick = r.randomizedElectionTimeout - 1
	if r.timeForElection() {
		t.Errorf("unexpected time for election result")
	}
	r.electionTick = r.randomizedElectionTimeout
	if !r.timeForElection() {
		t.Errorf("time for election result didn't return true")
	}
}

func TestLeaderChecksQuorumEveryElectionTick(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	r.checkQuorum = true
	for i := 0; i < 5; i++ {
		ne(r.tick(), t)
	}
	if r.state == leader {
		t.Errorf("leader didn't step down")
	}
}

func TestQuiescedTick(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	for i := 0; i < 200; i++ {
		r.quiescedTick()
	}
	if len(r.msgs) != 0 {
		t.Errorf("unexpectedly generated outgoing message")
	}
	r = newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(10, 2)
	for i := 0; i < 200; i++ {
		r.quiescedTick()
	}
	if len(r.msgs) != 0 {
		t.Errorf("unexpectedly generated outgoing message")
	}
}

func TestSetRandomizedElectionTimeout(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(10, 2)
	for i := 0; i < 100; i++ {
		r.setRandomizedElectionTimeout()
		if r.randomizedElectionTimeout < 5 && r.randomizedElectionTimeout > 10 {
			t.Errorf("unexpected randomizedElectionTimeout value, %d", r.randomizedElectionTimeout)
		}
	}
}

func TestMultiNodeShardCampaign(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeFollower(10, 2)
	ne(r.campaign(), t)
	if len(r.msgs) != 2 {
		t.Fatalf("unexpected message count")
	}
	for _, msg := range r.msgs {
		if msg.Type != pb.RequestVote {
			t.Errorf("unexpected message %+v", msg)
		}
	}
	if r.state != candidate {
		t.Errorf("unexpected state %s", r.state)
	}
	if r.vote != 1 {
		t.Errorf("vote not recorded")
	}
}

func TestSingleNodeShardCampaign(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 5, 1, NewTestLogDB())
	ne(r.campaign(), t)
	if len(r.msgs) != 0 {
		t.Fatalf("unexpected message count")
	}
	if r.state != leader {
		t.Errorf("didn't become leader")
	}
}

func TestNoOPWithHigherTermIsSentToLeaderWhenLeaderHasLowerTerm(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(10, 2)
	r.checkQuorum = true
	ne(r.Handle(pb.Message{Type: pb.Heartbeat, Term: 2}), t)
	msgs := r.msgs
	if len(msgs) == 0 {
		t.Fatalf("no message sent")
	}
	m := msgs[0]
	if m.Type != pb.NoOP {
		t.Errorf("no gNoOP sent")
	}
	if m.Term != 10 {
		t.Errorf("gNoOP not sent with high term")
	}
}

func TestIsLeaderMessage(t *testing.T) {
	msgs := []struct {
		mt     pb.MessageType
		leader bool
	}{
		{pb.Election, false},
		{pb.LeaderHeartbeat, false},
		{pb.Propose, false},
		{pb.SnapshotStatus, false},
		{pb.Unreachable, false},
		{pb.CheckQuorum, false},
		{pb.BatchedReadIndex, false},
		{pb.LocalTick, false},
		{pb.Replicate, true},
		{pb.ReplicateResp, false},
		{pb.RequestVote, false},
		{pb.RequestVoteResp, false},
		{pb.InstallSnapshot, true},
		{pb.Heartbeat, true},
		{pb.HeartbeatResp, false},
		{pb.ReadIndex, false},
		{pb.ReadIndexResp, true},
		{pb.Quiesce, false},
		{pb.ConfigChangeEvent, false},
		{pb.Ping, false},
		{pb.Pong, false},
		{pb.SnapshotReceived, false},
		{pb.LeaderTransfer, false},
		{pb.TimeoutNow, true},
		{pb.NoOP, false},
	}

	for _, tt := range msgs {
		if res := isLeaderMessage(tt.mt); res != tt.leader {
			t.Errorf("%s, result %t, want %t", tt.mt, res, tt.leader)
		}
	}
}

func TestNoOPIsSentOnSmallTermRejectedRequestPreVote(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.preVote = true
	r.becomeFollower(10, 2)
	ne(r.Handle(pb.Message{Type: pb.RequestPreVote, Term: 9, From: 2}), t)
	if len(r.msgs) != 1 {
		t.Fatalf("no message sent")
	}
	m := pb.Message{Type: pb.NoOP, To: 2, From: 1, Term: 10}
	if !reflect.DeepEqual(m, r.msgs[0]) {
		t.Errorf("not expected message")
	}
}

func TestPreVoteRespWithHigherTerm(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.preVote = true
	r.becomeFollower(10, 2)
	ne(r.Handle(pb.Message{Type: pb.RequestPreVoteResp, Term: 11, From: 2}), t)
	if r.term != 10 {
		t.Errorf("term unexpected changed")
	}
	ne(r.Handle(pb.Message{Type: pb.RequestPreVoteResp, Term: 20, From: 2, Reject: true}), t)
	if r.term != 20 {
		t.Errorf("term not set")
	}
}

func TestDropRequestVoteMessageFromHighTermNode(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.preVote = true
	r.becomeFollower(10, 2)

	tests := []struct {
		checkQuorum bool
		term        uint64
		drop        bool
	}{
		{false, 20, false},
		{true, 20, true},
		{true, 10, false},
	}
	for _, mt := range []pb.MessageType{pb.RequestVote, pb.RequestPreVote} {
		for idx, tt := range tests {
			r.checkQuorum = tt.checkQuorum
			m := pb.Message{
				Type: mt,
				Term: tt.term,
				From: 2,
			}
			if r.dropRequestVoteFromHighTermNode(m) != tt.drop {
				t.Errorf("%d, got %t, want %t", idx, r.dropRequestVoteFromHighTermNode(m), tt.drop)
			}
		}
	}
}

func TestOnMessageTermNotMatched(t *testing.T) {
	tests := []struct {
		checkQuorum bool
		term        uint64
		msgType     pb.MessageType
		leaderID    uint64
		notMatched  bool
		msgCount    int
	}{
		{false, 20, pb.Replicate, 3, false, 0},
		{false, 20, pb.ReplicateResp, NoLeader, false, 0},
		{false, 10, pb.Replicate, 2, false, 0},
		{false, 10, pb.ReplicateResp, 2, false, 0},
		{true, 5, pb.Replicate, 2, true, 1},
		{true, 5, pb.ReplicateResp, 2, true, 0},
		{false, 5, pb.ReplicateResp, 2, true, 0},
	}
	for idx, tt := range tests {
		r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
		r.becomeFollower(10, 2)
		r.checkQuorum = tt.checkQuorum
		msg := pb.Message{
			Type: tt.msgType,
			Term: tt.term,
			From: 3,
		}
		if r.onMessageTermNotMatched(msg) != tt.notMatched {
			t.Fatalf("%d, incorrect not matched result", idx)
		}
		if r.leaderID != tt.leaderID {
			t.Errorf("%d, leader ID %d, want %d", idx, r.leaderID, tt.leaderID)
		}
		if len(r.msgs) != tt.msgCount {
			t.Errorf("%d, msg count %d, want %d", idx, len(r.msgs), tt.msgCount)
		}
	}
}

func testZeroTermRequestVoteMessageCausePanic(t *testing.T, r *raft) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not triggered")
	}()
	r.finalizeMessageTerm(pb.Message{Type: pb.RequestVote})
}

func testNonZeroTermOtherMessageCausePanic(t *testing.T, r *raft) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not triggered")
	}()
	r.finalizeMessageTerm(pb.Message{Type: pb.NoOP, Term: 1})
}

func TestFinalizeMessageTerm(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(10, 2)
	testZeroTermRequestVoteMessageCausePanic(t, r)
	testNonZeroTermOtherMessageCausePanic(t, r)
	msg := r.finalizeMessageTerm(pb.Message{Type: pb.NoOP})
	if msg.Term != 10 {
		t.Errorf("term not set")
	}
}

func TestSendSetMessageFromAndTerm(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(10, 2)
	r.send(pb.Message{Type: pb.NoOP})
	if len(r.msgs) != 1 {
		t.Errorf("message not sent")
	}
	m := r.msgs[0]
	if m.From != 1 || m.Term != 10 {
		t.Errorf("from or term not set")
	}
}

func TestMakeInstallSnapshotMessage(t *testing.T) {
	st := NewTestLogDB()
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, st)
	r.becomeCandidate()
	r.becomeLeader()
	ss := pb.Snapshot{Index: 100, Term: 2}
	if err := st.ApplySnapshot(ss); err != nil {
		t.Errorf("apply snapshot failed %v", err)
	}
	msg := pb.Message{}
	if idx := r.makeInstallSnapshotMessage(2, &msg); idx != 100 {
		t.Errorf("unexpected index %d", idx)
	}
	if msg.Type != pb.InstallSnapshot || msg.Snapshot.Index != 100 ||
		msg.Snapshot.Term != 2 {
		t.Errorf("unexpected message values")
	}
}

func TestMakeReplicateMessage(t *testing.T) {
	st := NewTestLogDB()
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, st)
	r.becomeCandidate()
	r.becomeLeader()
	noop := pb.Entry{}
	ents := []pb.Entry{
		{Index: 2, Term: 1, Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
		{Index: 3, Term: 1, Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
	}
	r.appendEntries(ents)
	sz := noop.SizeUpperLimit() + ents[0].SizeUpperLimit() + ents[1].SizeUpperLimit() + 1
	msg, err := r.makeReplicateMessage(2, 1, uint64(sz))
	if err != nil {
		t.Errorf("make Replicate failed %v", err)
	}
	if msg.Type != pb.Replicate || msg.To != 2 {
		t.Errorf("failed to make Replicate")
	}
	// NoOP entry plus two above
	if len(msg.Entries) != 3 {
		t.Errorf("unexpected entry list length, got %d, want 3, %+v",
			len(msg.Entries), msg.Entries)
	}
	noopEntry := pb.Entry{Index: 1, Term: 1, Type: pb.ApplicationEntry}
	noopEntrySize := uint64(noopEntry.SizeUpperLimit())
	msg, err = r.makeReplicateMessage(2, 1, noopEntrySize+uint64(ents[1].SizeUpperLimit()))
	if err != nil {
		t.Errorf("failed to get gReplicate")
	}
	if len(msg.Entries) != 2 {
		t.Errorf("unexpected entry list length, %+v", msg)
	}
}

func TestBroadcastReplicateMessage(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	r.broadcastReplicateMessage()
	count := 0
	for _, msg := range r.msgs {
		if msg.Type == pb.Replicate {
			count++
		}
	}
	if count != 2 {
		t.Errorf("unexpected gReplicate count %d", count)
	}
}

func TestBroadcastHeartbeatMessage(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	r.broadcastHeartbeatMessage()
	count := 0
	for _, msg := range r.msgs {
		if msg.Type == pb.Heartbeat {
			count++
		}
	}
	if count != 2 {
		t.Errorf("unexpected gReplicate count %d", count)
	}
}

func TestBroadcastHeartbeatMessageWithHint(t *testing.T) {
	ctx := pb.SystemCtx{
		Low:  101,
		High: 1001,
	}
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	r.broadcastHeartbeatMessageWithHint(ctx)
	count := 0
	for _, msg := range r.msgs {
		if msg.Type == pb.Heartbeat {
			count++
		}
		if msg.Hint != ctx.Low || msg.HintHigh != ctx.High {
			t.Errorf("ctx not carried in the message")
		}
	}
	if count != 2 {
		t.Errorf("unexpected gReplicate count %d", count)
	}
}

func TestSendTimeoutNowMessage(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.sendTimeoutNowMessage(2)
	if len(r.msgs) != 1 {
		t.Fatalf("msg count is not 1")
	}
	if r.msgs[0].Type != pb.TimeoutNow || r.msgs[0].To != 2 {
		t.Errorf("unexpected message, %+v", r.msgs[0])
	}
}

func TestSendHeartbeatMessage(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	hint := pb.SystemCtx{Low: 100, High: 200}
	match := uint64(100)
	r.remotes[2].match = match
	r.log.committed = 200
	r.sendHeartbeatMessage(2, hint, match)
	msgs := r.msgs
	if len(msgs) != 1 {
		t.Fatalf("unexpected msgs list length")
	}
	m := msgs[0]
	if m.Type != pb.Heartbeat || m.Commit != 100 || m.Hint != 100 || m.HintHigh != 200 {
		t.Errorf("unexpected msg %+v", m)
	}
}

func TestQuorumValue(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 5, 1, NewTestLogDB())
	if r.quorum() != 1 {
		t.Errorf("quorum %d", r.quorum())
	}
	r = newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	if r.quorum() != 2 {
		t.Errorf("quorum %d", r.quorum())
	}
	r = newTestRaft(1, []uint64{1, 2, 3, 4, 5}, 5, 1, NewTestLogDB())
	if r.quorum() != 3 {
		t.Errorf("quorum %d", r.quorum())
	}
}

func TestIsSingleNodeQuorum(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 5, 1, NewTestLogDB())
	if !r.isSingleNodeQuorum() {
		t.Errorf("is single node returned incorrect result")
	}
	r = newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	if r.isSingleNodeQuorum() {
		t.Errorf("is single node returned incorrect result")
	}
}

func TestAppendEntries(t *testing.T) {
	st := NewTestLogDB()
	r := newTestRaft(1, []uint64{1}, 5, 1, st)
	r.becomeCandidate()
	r.becomeLeader()
	ents := []pb.Entry{
		{Index: 2, Term: 1, Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
		{Index: 3, Term: 1, Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
	}
	r.appendEntries(ents)
	if r.log.committed != 3 {
		t.Errorf("unexpected committed value %d", r.log.committed)
	}
	if r.remotes[1].match != 3 {
		t.Errorf("remotes' match is not updated %d", r.remotes[1].match)
	}
}

// got hit twice on ResetRemotes, just unbelievable...
func TestResetRemotes(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	ents := []pb.Entry{
		{Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
		{Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
	}
	r.appendEntries(ents)
	r.remotes[1].next = 100
	r.remotes[1].match = 50
	r.remotes[2].next = 200
	r.remotes[2].match = 52
	r.remotes[3].next = 300
	r.remotes[3].match = 53
	r.resetRemotes()
	if r.remotes[1].next != 4 || r.remotes[2].next != 4 || r.remotes[3].next != 4 {
		t.Errorf("unexpected next value, %+v", r.remotes)
	}
	if r.remotes[1].match != 3 {
		t.Errorf("unexpected match value, %d", r.remotes[1].match)
	}
}

func TestFollowerSelfRemoved(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	if r.selfRemoved() {
		t.Errorf("unexpectedly self removed")
	}
	delete(r.remotes, 1)
	if !r.selfRemoved() {
		t.Errorf("self removed not report removed")
	}
}

func TestNonVotingSelfRemoved(t *testing.T) {
	r := newTestNonVoting(1, []uint64{}, []uint64{1}, 5, 1, NewTestLogDB())
	if r.selfRemoved() {
		t.Errorf("unexpectedly self removed")
	}
	delete(r.nonVotings, 1)
	if !r.selfRemoved() {
		t.Errorf("self removed not report removed")
	}
}

func TestWitnessSelfRemoved(t *testing.T) {
	r := newTestWitness(1, []uint64{}, []uint64{1}, 5, 1, NewTestLogDB())
	if r.selfRemoved() {
		t.Errorf("unexpectedly self removed")
	}
	delete(r.witnesses, 1)
	if !r.selfRemoved() {
		t.Errorf("self removed not report removed")
	}
}

func TestFullMemberWithOneWitnessCouldMakeProgressWithOneMemberDrop(t *testing.T) {
	p1 := newTestRaft(1, []uint64{1, 2, 3, 4}, 10, 1, NewTestLogDB())
	p2 := newTestRaft(1, []uint64{1, 2, 3, 4}, 10, 1, NewTestLogDB())
	p3 := newTestRaft(1, []uint64{1, 2, 3, 4}, 10, 1, NewTestLogDB())
	p4 := newTestWitness(1, []uint64{1, 2, 3, 4}, []uint64{4}, 10, 1, NewTestLogDB())
	nt := newNetwork(p1, p2, p3, p4)

	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	if !p1.isLeader() {
		t.Errorf("p1 should be leader")
	}
	if !p4.isWitness() {
		t.Errorf("p4 should be witness")
	}

	committed := p1.log.committed
	nt.send(pb.Message{From: 2, To: 2, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("test-data")}}})

	peers := []*raft{p1, p2, p3, p4}
	for _, p := range peers {
		if p.log.committed != committed+1 {
			t.Errorf("new propose should have committed for member %v", p.replicaID)
		}
	}

	// Partition a full member
	nt.isolate(3)
	nt.send(pb.Message{From: 2, To: 2, Type: pb.Propose, Entries: []pb.Entry{{Cmd: []byte("test-data")}}})

	for _, p := range peers {
		// Only p3 will lag behind.
		if p.log.committed != committed+2 && p.replicaID != 3 {
			t.Errorf("new propose should have committed for member %v", p.replicaID)
		}
	}
}

func TestSetRemote(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.setRemote(4, 100, 101)
	if len(r.remotes) != 4 {
		t.Errorf("remote not set")
	}
	_, ok := r.remotes[4]
	if !ok {
		t.Errorf("node 4 not added")
	}
}

func TestDeleteRemote(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.deleteRemote(3)
	_, ok := r.remotes[3]
	if ok {
		t.Errorf("node 3 not deleted")
	}
	if len(r.remotes) != 2 {
		t.Errorf("remote not deleted")
	}
}

func TestCampaignSendExpectedMessages(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	ents := []pb.Entry{
		{Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
		{Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
	}
	r.appendEntries(ents)
	r.becomeFollower(r.term+1, NoLeader)
	ne(r.campaign(), t)
	msgs := r.msgs
	if len(msgs) != 2 {
		t.Errorf("unexpected message count")
	}
	for idx, m := range msgs {
		if m.Type != pb.RequestVote || m.LogIndex != 3 || m.LogTerm != 1 {
			t.Errorf("%d, unexpected msg %+v", idx, m)
		}
	}
}

func TestHandleVoteResp(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	v1 := r.handleVoteResp(1, false, false)
	v2 := r.handleVoteResp(2, true, false)
	v3 := r.handleVoteResp(3, false, false)
	v4 := r.handleVoteResp(2, false, false)
	if v1 != 1 || v2 != 1 || v3 != 2 || v4 != 2 {
		t.Errorf("unexpected count")
	}
}

func TestCanGrantVote(t *testing.T) {
	from := uint64(2)
	term := uint64(1)
	tests := []struct {
		vote uint64
		mt   uint64
		ok   bool
	}{
		{NoNode, 1, true},
		{NoNode, 2, true},
		{from, 1, true},
		{from, 2, true},
		{3, 1, false},
		{3, 5, true},
	}

	for idx, tt := range tests {
		r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
		r.term = term
		r.vote = tt.vote
		msg := pb.Message{From: from, Term: tt.mt}
		ok := r.canGrantVote(msg)
		if ok != tt.ok {
			t.Errorf("%d, unexpected can grant vote result", idx)
		}
	}
}

func TestElectionTickResetAfterGrantVote(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(2, 2)
	r.electionTick = 101
	m := pb.Message{
		Type: pb.RequestVote,
		From: 2,
		To:   1,
		Term: 3,
	}
	ne(r.Handle(m), t)
	if r.vote != 2 {
		t.Fatalf("failed to grant the vote")
	}
	if r.electionTick != 0 {
		t.Errorf("electionTick not reset")
	}
}

func TestHasConfigChangeToApply(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.hasNotAppliedConfigChange = nil
	r.log.committed = 10
	r.setApplied(5)
	if !r.hasConfigChangeToApply() {
		t.Errorf("unexpected r.hasConfigChangeToApply result")
	}
	r.setApplied(10)
	if r.hasConfigChangeToApply() {
		t.Errorf("unexpected r.hasConfigChangeToApply result")
	}
	r.setApplied(12)
	if r.hasConfigChangeToApply() {
		t.Errorf("unexpected r.hasConfigChangeToApply result")
	}
}

func TestPendingConfigChangeFlag(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	if r.hasPendingConfigChange() {
		t.Errorf("unexpected value")
	}
	r.setPendingConfigChange()
	if !r.hasPendingConfigChange() {
		t.Errorf("flag not set")
	}
	r.clearPendingConfigChange()
	if r.hasPendingConfigChange() {
		t.Errorf("unexpected value")
	}
}

func TestGetPendingConfigChangeCount(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	for i := 0; i < 5; i++ {
		ents := []pb.Entry{
			{Type: pb.ApplicationEntry, Cmd: make([]byte, maxEntriesToApplySize)},
			{Type: pb.ConfigChangeEntry},
		}
		r.appendEntries(ents)
	}
	count := r.getPendingConfigChangeCount()
	if count != 5 {
		t.Errorf("count %d, want 5", count)
	}
}

func TestIsRequestLeaderMessage(t *testing.T) {
	tests := []struct {
		msgType   pb.MessageType
		reqMsg    bool
		leaderMsg bool
	}{
		{pb.Propose, true, false},
		{pb.ReadIndex, true, false},
		{pb.Replicate, false, true},
		{pb.ReplicateResp, false, false},
		{pb.RequestVote, false, false},
		{pb.RequestVoteResp, false, false},
		{pb.Heartbeat, false, true},
		{pb.HeartbeatResp, false, false},
		{pb.InstallSnapshot, false, true},
		{pb.ReadIndexResp, false, true},
		{pb.TimeoutNow, false, true},
		{pb.LeaderTransfer, true, false},
	}
	for _, tt := range tests {
		if isRequestMessage(tt.msgType) != tt.reqMsg {
			t.Errorf("incorrect is request message result %s", tt.msgType)
		}
		if isLeaderMessage(tt.msgType) != tt.leaderMsg {
			t.Errorf("incorrect is leader message result %s", tt.msgType)
		}
	}
}

func TestAddNodeDragonboat(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	ents := []pb.Entry{
		{Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
		{Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
		{Type: pb.ConfigChangeEntry},
	}
	r.setPendingConfigChange()
	r.appendEntries(ents)
	if !r.hasPendingConfigChange() {
		t.Errorf("pending config change flag not set")
	}
	r.addNode(3)
	if r.hasPendingConfigChange() {
		t.Errorf("pending config change flag not cleared")
	}
	if len(r.remotes) != 3 {
		t.Errorf("remotes not expanded")
	}
	if r.remotes[3].match != 0 || r.remotes[3].next != 5 {
		t.Errorf("unexpected remotes %+v", r.remotes[3])
	}
}

func TestRemoveNodeDragonboat(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.setPendingConfigChange()
	r.removeNode(2)
	if r.hasPendingConfigChange() {
		t.Errorf("pending config change flag not cleared")
	}
}

func TestHasCommittedEntryAtCurrentTerm(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(1, NoLeader)
	if r.hasCommittedEntryAtCurrentTerm() {
		t.Errorf("unexpectedly set hasCommittedEntryAtCurrentTerm")
	}
	r.becomeCandidate()
	r.becomeLeader()
	if r.hasCommittedEntryAtCurrentTerm() {
		t.Errorf("unexpected hasCommittedEntryAtCurrentTerm result")
	}
	r.remotes[2].tryUpdate(r.log.lastIndex())
	ok := r.tryCommit()
	if !ok {
		t.Errorf("failed to commit")
	}
	if !r.hasCommittedEntryAtCurrentTerm() {
		t.Errorf("unexpected hasCommittedEntryAtCurrentTerm result")
	}
}

func TestHandleLeaderCheckQuorum(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	ne(r.handleLeaderCheckQuorum(pb.Message{}), t)
	if r.state != follower {
		t.Errorf("node didn't step down")
	}
	r = newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	r.remotes[1].setActive()
	r.remotes[2].setActive()
	ne(r.handleLeaderCheckQuorum(pb.Message{}), t)
	if r.state != leader {
		t.Errorf("node didn't step down")
	}
}

func TestReadyToReadList(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	if len(r.readyToRead) != 0 {
		t.Errorf("unexpected initial count")
	}
	r.addReadyToRead(100, pb.SystemCtx{})
	r.addReadyToRead(200, pb.SystemCtx{})
	r.addReadyToRead(300, pb.SystemCtx{})
	if len(r.readyToRead) != 3 {
		t.Errorf("unexpected count")
	}
	r.clearReadyToRead()
	if len(r.readyToRead) != 0 {
		t.Errorf("unexpected count")
	}
}

func TestRestoreSnapshotIgnoreDelayedSnapshot(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	ents := []pb.Entry{
		{Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
		{Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
	}
	r.appendEntries(ents)
	ss := pb.Snapshot{Index: 3, Term: 1}
	ok, err := r.restore(ss)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if ok {
		t.Errorf("nothing to restore")
	}
}

func TestSnapshotCommitEntries(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	ents := []pb.Entry{
		{Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
		{Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
	}
	r.appendEntries(ents)
	if r.log.committed != 0 {
		t.Errorf("unexpected commit, %d", r.log.committed)
	}
	ss := pb.Snapshot{Index: 2, Term: 1}
	ok, err := r.restore(ss)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if ok {
		t.Errorf("not expect to restore")
	}
	if r.log.committed != 2 {
		t.Errorf("commit not moved")
	}
}

func TestSnapshotCanBeRestored(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	ents := []pb.Entry{
		{Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
		{Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
	}
	r.appendEntries(ents)
	ss := pb.Snapshot{Index: 4, Term: 1}
	ok, err := r.restore(ss)
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if !ok {
		t.Errorf("snapshot unexpectedly ignored")
	}
	if r.log.lastIndex() != 4 {
		t.Errorf("last index not moved, %d", r.log.lastIndex())
	}
	term, err := r.log.lastTerm()
	if err != nil {
		t.Fatalf("unexpected error %v", err)
	}
	if term != 1 {
		t.Errorf("last term %d", term)
	}
}

func TestRestoreRemote(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	ents := []pb.Entry{
		{Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
		{Type: pb.ApplicationEntry, Cmd: make([]byte, 16)},
	}
	r.appendEntries(ents)
	ss := pb.Snapshot{}
	ss.Membership.Addresses = make(map[uint64]string)
	ss.Membership.Addresses[1] = ""
	ss.Membership.Addresses[2] = ""
	ss.Membership.Addresses[3] = ""
	ss.Membership.NonVotings = make(map[uint64]string)
	ss.Membership.NonVotings[4] = ""
	ss.Membership.NonVotings[5] = ""
	ss.Membership.Witnesses = make(map[uint64]string)
	ss.Membership.Witnesses[6] = ""
	ss.Membership.Witnesses[7] = ""
	r.restoreRemotes(ss)
	if len(r.remotes) != 3 {
		t.Errorf("remotes length unexpected %d", len(r.remotes))
	}
	if len(r.nonVotings) != 2 {
		t.Errorf("nonVotings length unexpected %d", len(r.nonVotings))
	}
	if len(r.witnesses) != 2 {
		t.Errorf("witnesses length unexpected %d", len(r.witnesses))
	}
	if len(r.nodesSorted()) != 7 {
		t.Errorf("total node length unexpected %d", len(r.nodesSorted()))
	}

	if len(r.matched) != 5 {
		t.Errorf("matchValue not reset as %d", len(r.matched))
	}
	for nid, rm := range r.votingMembers() {
		if nid == 1 {
			if rm.match != 3 {
				t.Errorf("match not moved, %d", rm.match)
			}
		}
		if rm.next != 4 {
			t.Errorf("next not moved, %d", rm.next)
		}
	}
}

func TestAppliedValueCanBeSet(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 5, 1, NewTestLogDB())
	r.setApplied(12345)
	if r.getApplied() != 12345 {
		t.Errorf("applied value not set")
	}
}

func TestUnrolledBubbleSortMatchValue(t *testing.T) {
	tests := []struct {
		vals []uint64
	}{
		{[]uint64{1, 1, 1}},
		{[]uint64{1, 1, 2}},
		{[]uint64{1, 2, 2}},
		{[]uint64{2, 3, 1}},
		{[]uint64{3, 2, 1}},
		{[]uint64{3, 3, 1}},
	}
	for idx, tt := range tests {
		r := &raft{matched: make([]uint64, len(tt.vals))}
		copy(r.matched, tt.vals)
		r.sortMatchValues()
		vals := tt.vals
		sort.Slice(tt.vals, func(i, j int) bool {
			return vals[i] < vals[j]
		})
		if !reflect.DeepEqual(tt.vals, r.matched) {
			t.Errorf("%d, sort failed, %v, want %v", idx, r.matched, tt.vals)
		}
	}
}

func TestDoubleCheckTermMatched(t *testing.T) {
	r := raft{term: 10}
	r.doubleCheckTermMatched(0)
	r.doubleCheckTermMatched(10)
	tt := func() {
		defer func() {
			if r := recover(); r == nil {
				t.Errorf("panic not triggered")
			}
		}()
		r.doubleCheckTermMatched(1)
	}
	tt()
}

func TestEnterRetryState(t *testing.T) {
	tests := []struct {
		initType  remoteStateType
		finalType remoteStateType
	}{
		{remoteRetry, remoteRetry},
		{remoteReplicate, remoteRetry},
		{remoteSnapshot, remoteSnapshot},
	}
	for idx, tt := range tests {
		r := raft{}
		rm := &remote{state: tt.initType}
		r.enterRetryState(rm)
		if rm.state != tt.finalType {
			t.Errorf("%d, unexpected type %s, want %s", idx, r.state, tt.finalType)
		}
	}
}

func TestHandleCandidatePropose(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	msg := pb.Message{
		Type:    pb.Replicate,
		Entries: []pb.Entry{{Cmd: []byte("test-data")}},
	}
	ne(r.handleCandidatePropose(msg), t)
	if len(r.msgs) != 0 {
		t.Errorf("unexpectedly sent message")
	}
}

func TestCandidateBecomeFollowerOnRecivingLeaderMessage(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	tests := []struct {
		fn      func(msg pb.Message) error
		msgType pb.MessageType
	}{
		{r.handleCandidateReplicate, pb.Replicate},
		{r.handleCandidateInstallSnapshot, pb.InstallSnapshot},
		{r.handleCandidateHeartbeat, pb.Heartbeat},
	}
	for _, tt := range tests {
		r.becomeCandidate()
		r.term = 2
		if r.leaderID != NoLeader {
			t.Errorf("leader not cleared")
		}
		msg := pb.Message{
			Type: tt.msgType,
			From: 2,
		}
		ne(tt.fn(msg), t)
		if r.state != follower {
			t.Errorf("not a follower")
		}
		if r.term != 2 {
			t.Errorf("term changed, term %d want 2", r.term)
		}
		if r.leaderID != 2 {
			t.Errorf("leader id not set")
		}
	}
}

func TestHandleCandidateHeartbeat(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	msg := pb.Message{
		Type:   pb.Heartbeat,
		From:   2,
		Commit: 3,
	}
	ents := make([]pb.Entry, 0)
	for i := uint64(0); i < uint64(10); i++ {
		ents = append(ents, pb.Entry{Index: i, Term: 1})
	}
	// yes, this is a bit hacky
	r.log.inmem.merge(ents)
	ne(r.handleCandidateHeartbeat(msg), t)
	if r.log.committed != 3 {
		t.Errorf("committed %d, want 3", r.log.committed)
	}
}

func TestHandleCandidateInstallSnapshot(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	ents := make([]pb.Entry, 0)
	for i := uint64(0); i < uint64(10); i++ {
		ents = append(ents, pb.Entry{Index: i, Term: 1})
	}
	r.log.inmem.merge(ents)
	ss := pb.Snapshot{
		Index: 10,
		Term:  1,
	}
	m := pb.Message{From: 2, To: 1, Type: pb.InstallSnapshot, Snapshot: ss}
	ne(r.handleCandidateInstallSnapshot(m), t)
	if r.log.committed != 10 {
		t.Errorf("gSnapshot not applied")
	}
}

func TestHandleCandidateReplicate(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	ents := make([]pb.Entry, 0)
	for i := uint64(1); i < uint64(10); i++ {
		ents = append(ents, pb.Entry{Index: i, Term: 1})
	}
	m := pb.Message{
		LogIndex: 0,
		LogTerm:  0,
		Entries:  ents,
		Type:     pb.Replicate,
		Term:     0,
	}
	ne(r.handleCandidateReplicate(m), t)
	if r.log.lastIndex() != ents[len(ents)-1].Index {
		t.Errorf("entries not appended, last index %d, want %d",
			r.log.lastIndex(), ents[len(ents)-1].Index)
	}
}

func TestHandleCandidateRequestVoteResp(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	for i := uint64(1); i <= uint64(3); i++ {
		m := pb.Message{
			Type:   pb.RequestVoteResp,
			From:   i,
			Reject: false,
		}
		ne(r.handleCandidateRequestVoteResp(m), t)
	}
	if r.state != leader {
		t.Errorf("didn't become leader")
	}
	count := 0
	for _, msg := range r.msgs {
		if msg.Type == pb.Replicate {
			count++
		}
	}
	if count != 2 {
		t.Errorf("gReplicate count %d, want 2", count)
	}
}

func TestHandleCandidateRequestVoteRespRejected(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	for i := uint64(2); i <= uint64(3); i++ {
		m := pb.Message{
			Type:   pb.RequestVoteResp,
			From:   i,
			Reject: true,
		}
		ne(r.handleCandidateRequestVoteResp(m), t)
	}
	if r.state != follower {
		t.Errorf("didn't become follower")
	}
	if len(r.msgs) != 0 {
		t.Errorf("unexpectedly sent message")
	}
}

func TestFollowerResetElectionTickOnReceivingLeaderMessage(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	tests := []struct {
		fn      func(msg pb.Message) error
		msgType pb.MessageType
	}{
		{r.handleFollowerReplicate, pb.Replicate},
		{r.handleFollowerInstallSnapshot, pb.InstallSnapshot},
		{r.handleFollowerHeartbeat, pb.Heartbeat},
		{r.handleFollowerReadIndexResp, pb.ReadIndexResp},
	}
	for _, tt := range tests {
		r.becomeFollower(r.term, 2)
		r.electionTick = 2
		m := pb.Message{
			Type: tt.msgType,
			From: 3,
		}
		ne(tt.fn(m), t)
		if r.leaderID != 3 {
			t.Errorf("leader id not set")
		}
		if r.electionTick != 0 {
			t.Errorf("election tick not reset")
		}
	}
}

func TestFollowerRedirectReadIndexMessageToLeader(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(10, 2)
	m := pb.Message{
		Type: pb.ReadIndex,
	}
	ne(r.handleFollowerReadIndex(m), t)
	if len(r.msgs) != 1 {
		t.Fatalf("failed to redirect the message")
	}
	if r.msgs[0].Type != pb.ReadIndex || r.msgs[0].To != 2 {
		t.Errorf("unexpected message sent %v", r.msgs[0])
	}
	if r.msgs[0].Term != 0 {
		t.Errorf("term unexpectedly set")
	}
}

func TestFollowerRedirectProposeMessageToLeader(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(10, 2)
	m := pb.Message{
		Type: pb.Propose,
	}
	ne(r.handleFollowerPropose(m), t)
	if len(r.msgs) != 1 {
		t.Fatalf("failed to redirect the message")
	}
	if r.msgs[0].Type != pb.Propose || r.msgs[0].To != 2 {
		t.Errorf("unexpected message sent %v", r.msgs[0])
	}
	if r.msgs[0].Term != 0 {
		t.Errorf("term unexpectedly set")
	}
}

func TestFollowerRedirectLeaderTransferMessageToLeader(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(10, 2)
	m := pb.Message{
		Type: pb.LeaderTransfer,
	}
	ne(r.handleFollowerLeaderTransfer(m), t)
	if len(r.msgs) != 1 {
		t.Fatalf("failed to redirect the message")
	}
	if r.msgs[0].Type != pb.LeaderTransfer || r.msgs[0].To != 2 {
		t.Errorf("unexpected message sent %v", r.msgs[0])
	}
	if r.msgs[0].Term != 0 {
		t.Errorf("term unexpectedly set")
	}
}

func TestHandleFollowerReplicate(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(0, 2)
	ents := make([]pb.Entry, 0)
	for i := uint64(1); i < uint64(10); i++ {
		ents = append(ents, pb.Entry{Index: i, Term: 1})
	}
	m := pb.Message{
		LogIndex: 0,
		LogTerm:  0,
		Entries:  ents,
		Type:     pb.Replicate,
		Term:     0,
	}
	ne(r.handleFollowerReplicate(m), t)
	if r.log.lastIndex() != ents[len(ents)-1].Index {
		t.Errorf("entries not appended, last index %d, want %d",
			r.log.lastIndex(), ents[len(ents)-1].Index)
	}
}

func TestHandleFollowerHeartbeat(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(0, 2)
	msg := pb.Message{
		Type:   pb.Heartbeat,
		From:   2,
		Commit: 3,
	}
	ents := make([]pb.Entry, 0)
	for i := uint64(0); i < uint64(10); i++ {
		ents = append(ents, pb.Entry{Index: i, Term: 1})
	}
	r.log.inmem.merge(ents)
	ne(r.handleFollowerHeartbeat(msg), t)
	if r.log.committed != 3 {
		t.Errorf("committed %d, want 3", r.log.committed)
	}
}

func TestHandleFollowerInstallSnapshot(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(0, 2)
	ents := make([]pb.Entry, 0)
	for i := uint64(0); i < uint64(10); i++ {
		ents = append(ents, pb.Entry{Index: i, Term: 1})
	}
	r.log.inmem.merge(ents)
	ss := pb.Snapshot{
		Index: 10,
		Term:  1,
	}
	m := pb.Message{From: 2, To: 1, Type: pb.InstallSnapshot, Snapshot: ss}
	ne(r.handleFollowerInstallSnapshot(m), t)
	if r.log.committed != 10 {
		t.Errorf("gSnapshot not applied")
	}
}

func TestHandleFollowerReadIndexResp(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(0, 2)
	msg := pb.Message{
		Type:     pb.ReadIndexResp,
		Hint:     101,
		HintHigh: 1002,
		LogIndex: 100,
	}
	ne(r.handleFollowerReadIndexResp(msg), t)
	if len(r.readyToRead) != 1 {
		t.Fatalf("ready to read not updated")
	}
	rr := r.readyToRead[0]
	if rr.Index != 100 || rr.SystemCtx.Low != 101 || rr.SystemCtx.High != 1002 {
		t.Errorf("unexpected ready to read content")
	}
}

func TestHandleFollowerTimeoutNow(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(0, 2)
	m := pb.Message{
		Type: pb.TimeoutNow,
	}
	ne(r.handleFollowerTimeoutNow(m), t)
	if r.state != candidate {
		t.Errorf("not become candidate")
	}
	if len(r.msgs) != 1 {
		t.Fatalf("no message sent")
	}
	if r.msgs[0].Type != pb.RequestVote {
		t.Errorf("no gRequestVote sent")
	}
}

func TestHandleFollowerLeaderTransfer(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(0, 2)
	m := pb.Message{
		Type: pb.LeaderTransfer,
	}
	ne(r.handleFollowerLeaderTransfer(m), t)
	if len(r.msgs) != 1 {
		t.Fatalf("gLeaderTransfer not redirected")
	}
	if r.msgs[0].Type != pb.LeaderTransfer || r.msgs[0].To != 2 {
		t.Errorf("unexpected msg")
	}
}

func TestLeaderIgnoreElection(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	msg := pb.Message{
		Type: pb.Election,
	}
	ne(r.handleNodeElection(msg), t)
	if len(r.msgs) != 0 {
		t.Errorf("unexpected message sent")
	}
	if r.state != leader {
		t.Errorf("no longer a leader")
	}
}

func TestElectionIgnoredWhenConfigChangeIsPending(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(2, 2)
	r.log.committed = 10
	r.setApplied(5)
	r.hasNotAppliedConfigChange = nil
	if !r.hasConfigChangeToApply() {
		t.Fatalf("no config change to apply")
	}
	msg := pb.Message{
		Type: pb.Election,
	}
	ne(r.handleNodeElection(msg), t)
	if len(r.msgs) != 0 {
		t.Errorf("unexpected message sent")
	}
	if r.state != follower {
		t.Errorf("state change to %s", r.state)
	}
}

func TestHandleElection(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(2, 2)
	msg := pb.Message{
		Type: pb.Election,
	}
	ne(r.handleNodeElection(msg), t)
	if len(r.msgs) != 1 {
		t.Fatalf("unexpected message count %d", len(r.msgs))
	}
	if r.msgs[0].Type != pb.RequestVote || r.msgs[0].To != 2 {
		t.Errorf("didn't send out gRequestVote")
	}
	if r.state != candidate {
		t.Errorf("not a candidate")
	}
}

func TestRequestVoteMessageWontResetElectionTick(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2}, 5, 1, NewTestLogDB())
	r.becomeFollower(2, 2)
	r.electionTick = 101
	r.electionTimeout = 102
	msg := pb.Message{
		Type: pb.RequestVote,
		From: 2,
		To:   1,
		Term: 3,
	}
	r.onMessageTermNotMatched(msg)
	if r.electionTick != 101 || r.electionTimeout != 102 {
		t.Errorf("election tick changed")
	}
	msg = pb.Message{
		Type: pb.Replicate,
		Term: 5,
	}
	r.onMessageTermNotMatched(msg)
	if r.electionTick != 0 {
		t.Fatalf("election tick not reset")
	}
}

func TestLeaderStepDownAfterRemoved(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 5, 1, NewTestLogDB())
	r.becomeFollower(2, 2)
	msg := pb.Message{
		Type: pb.Election,
	}
	ne(r.handleNodeElection(msg), t)
	if r.state != leader {
		t.Errorf("not a leader")
	}
	r.removeNode(2)
	if r.state != leader {
		t.Errorf("no longer a leader, %s", r.state)
	}
	r.removeNode(1)
	if r.state != follower {
		t.Errorf("not a follower, %s", r.state)
	}
	if r.leaderID != NoLeader {
		t.Errorf("unexpected leader id %d", r.leaderID)
	}
}

func TestLeaderStepDownAfterRemovedBySnapshot(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 5, 1, NewTestLogDB())
	r.becomeFollower(2, 2)
	msg := pb.Message{
		Type: pb.Election,
	}
	ne(r.handleNodeElection(msg), t)
	if r.state != leader {
		t.Errorf("not a leader")
	}
	ss := pb.Snapshot{
		Membership: pb.Membership{
			Addresses: map[uint64]string{2: "a2", 1: "a1"},
		},
	}
	r.restoreRemotes(ss)
	if r.state != leader {
		t.Errorf("no longer a leader, %s", r.state)
	}
	ss = pb.Snapshot{
		Membership: pb.Membership{
			Addresses: map[uint64]string{2: "a2"},
		},
	}
	r.restoreRemotes(ss)
	if r.state != follower {
		t.Errorf("not a follower, %s", r.state)
	}
	if r.leaderID != NoLeader {
		t.Errorf("unexpected leader id %d", r.leaderID)
	}
}

func TestHandleLeaderHeartbeatMessage(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	msg := pb.Message{
		Type: pb.LeaderHeartbeat,
	}
	ne(r.handleLeaderHeartbeat(msg), t)
	count := 0
	for _, msg := range r.msgs {
		if msg.Type == pb.Heartbeat && (msg.To == 2 || msg.To == 3) {
			count++
		}
	}
	if count != 2 {
		t.Errorf("didn't send heartbeat to all other nodes")
	}
}

func TestLeaderStepDownWithoutQuorum(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	msg := pb.Message{
		Type: pb.CheckQuorum,
	}
	ne(r.handleLeaderCheckQuorum(msg), t)
	if r.state == leader {
		t.Errorf("leader didn't step down")
	}
}

func TestLeaderIgnoreCheckQuorumWhenHasQuorum(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	msg := pb.Message{
		Type: pb.CheckQuorum,
	}
	for _, rp := range r.remotes {
		rp.setActive()
	}
	ne(r.handleLeaderCheckQuorum(msg), t)
	if r.state != leader {
		t.Errorf("leader unexpectedly stepped down")
	}
}

func TestHandleLeaderUnreachable(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	msg := pb.Message{
		Type: pb.Unreachable,
	}
	rp, ok := r.remotes[2]
	if !ok {
		t.Errorf("rp not found")
	}
	rp.state = remoteReplicate
	ne(r.handleLeaderUnreachable(msg, rp), t)
	if rp.state != remoteRetry {
		t.Errorf("not in retry state")
	}
}

func TestSnapshotStatusMessageIgnoredWhenNotInSnapshotState(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	msg := pb.Message{
		Type: pb.SnapshotStatus,
	}
	rp, ok := r.remotes[2]
	if !ok {
		t.Errorf("rp not found")
	}
	rp.state = remoteReplicate
	ne(r.handleLeaderSnapshotStatus(msg, rp), t)
	if rp.state == remoteRetry {
		t.Errorf("unexpectedly in retry state")
	}
}

func TestHandleLeaderSnapshotStatus(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	msg := pb.Message{
		Type: pb.SnapshotStatus,
	}
	rp, ok := r.remotes[2]
	if !ok {
		t.Errorf("rp not found")
	}
	rp.state = remoteSnapshot
	ne(r.handleLeaderSnapshotStatus(msg, rp), t)
	if rp.state != remoteWait {
		t.Errorf("not in wait state")
	}
	if !rp.isPaused() {
		t.Errorf("not paused")
	}
}

func TestHandleLeaderTransfer(t *testing.T) {
	tests := []struct {
		target       uint64
		transferring bool
		match        bool
		ignored      bool
	}{
		{1, false, false, true},
		{1, true, false, true},
		{2, false, true, false},
		{2, false, false, true},
		{2, true, false, true},
	}
	for idx, tt := range tests {
		r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
		r.becomeCandidate()
		r.becomeLeader()
		msg := pb.Message{
			Type: pb.LeaderTransfer,
			Hint: tt.target,
		}
		if tt.transferring {
			r.leaderTransferTarget = 3
		}
		rp, ok := r.remotes[tt.target]
		if !ok {
			t.Fatalf("failed to get remote")
		}
		if tt.match {
			rp.match = r.log.lastIndex()
		}
		ne(r.handleLeaderTransfer(msg), t)
		if tt.ignored {
			if len(r.msgs) != 0 {
				t.Errorf("unexpectedly sent msg")
			}
		} else {
			if len(r.msgs) != 1 {
				t.Fatalf("%d, unexpected msg count %d, want 1", idx, len(r.msgs))
			}
			if r.msgs[0].Type != pb.TimeoutNow || r.msgs[0].To != tt.target {
				t.Errorf("unexpected msg %v", r.msgs[0])
			}
		}
	}
}

func TestHandleLeaderHeartbeatResp(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	msg := pb.Message{
		Type: pb.HeartbeatResp,
		From: 2,
	}
	rp, ok := r.remotes[2]
	if !ok {
		t.Fatalf("failed to get remote")
	}
	rp.setNotActive()
	//rp.pause()
	ne(r.handleLeaderHeartbeatResp(msg, rp), t)
	if !rp.isActive() {
		t.Errorf("not active")
	}
}

func TestLeaderReadIndexOnSingleNodeShard(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	msg := pb.Message{
		Type:     pb.ReadIndex,
		Hint:     101,
		HintHigh: 1002,
	}
	ne(r.handleLeaderReadIndex(msg), t)
	if len(r.msgs) != 0 {
		t.Errorf("unexpected msg sent")
	}
	if len(r.readyToRead) != 1 {
		t.Fatalf("readyToRead has unexpected rec")
	}
	if r.readyToRead[0].Index != r.log.committed ||
		r.readyToRead[0].SystemCtx.Low != 101 ||
		r.readyToRead[0].SystemCtx.High != 1002 {
		t.Errorf("unexpected ready to read stat")
	}
	if len(r.readIndex.pending) != 0 || len(r.readIndex.queue) != 0 {
		t.Errorf("unexpected readIndex content")
	}
}

func TestLeaderIgnoreReadIndexWhenShardCommittedIsUnknown(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	msg := pb.Message{
		Type:     pb.ReadIndex,
		Hint:     101,
		HintHigh: 1002,
	}
	ne(r.handleLeaderReadIndex(msg), t)
	if len(r.msgs) != 0 {
		t.Errorf("unexpected msg sent")
	}
	if len(r.readyToRead) != 0 {
		t.Errorf("readyToRead has unexpected rec")
	}
	if len(r.readIndex.pending) != 0 || len(r.readIndex.queue) != 0 {
		t.Errorf("unexpected readIndex content")
	}
}

func TestHandleLeaderReadIndex(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeFollower(1, NoLeader)
	if r.hasCommittedEntryAtCurrentTerm() {
		t.Errorf("unexpectedly set hasCommittedEntryAtCurrentTerm")
	}
	r.becomeCandidate()
	r.becomeLeader()
	if r.hasCommittedEntryAtCurrentTerm() {
		t.Errorf("unexpected hasCommittedEntryAtCurrentTerm result")
	}
	r.remotes[2].tryUpdate(r.log.lastIndex())
	ok := r.tryCommit()
	if !ok {
		t.Errorf("failed to commit")
	}
	if !r.hasCommittedEntryAtCurrentTerm() {
		t.Errorf("unexpected hasCommittedEntryAtCurrentTerm result")
	}
	msg := pb.Message{
		Type:     pb.ReadIndex,
		Hint:     101,
		HintHigh: 1002,
	}
	count := 0
	ne(r.handleLeaderReadIndex(msg), t)
	for _, m := range r.msgs {
		if m.Type == pb.Heartbeat && (m.To == 2 || m.To == 3) &&
			m.Hint == 101 && m.HintHigh == 1002 {
			count++
		}
	}
	if count != 2 {
		t.Errorf("expected heartbeat messages not sent")
	}
	if len(r.readIndex.pending) != 1 || len(r.readIndex.queue) != 1 {
		t.Errorf("readIndex not updated")
	}
}

func TestWitnessReadIndex(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 5, 1, NewTestLogDB())

	r.becomeFollower(1, NoLeader)
	if r.hasCommittedEntryAtCurrentTerm() {
		t.Errorf("unexpectedly set hasCommittedEntryAtCurrentTerm")
	}
	r.becomeCandidate()
	r.becomeLeader()

	r.addWitness(2)

	msg := pb.Message{
		Type:     pb.ReadIndex,
		Hint:     101,
		HintHigh: 1002,
		From:     2,
	}

	ne(r.handleLeaderReadIndex(msg), t)

	if len(r.readIndex.pending) != 0 || len(r.readIndex.queue) != 0 {
		t.Errorf("readIndex updated unexpectedly")
	}
}

func TestVotingMemberLengthMismatchWillResetMatchArray(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeFollower(1, NoLeader)
	r.becomeCandidate()
	r.becomeLeader()
	r.remotes[2].tryUpdate(r.log.lastIndex())
	if len(r.matched) != 3 {
		t.Errorf("Match array length unexpected %v", len(r.matched))
	}
	// Changing the number of total voting members
	r.witnesses[4] = &remote{}
	ok := r.tryCommit()
	if ok {
		t.Errorf("Should fail commit")
	}
	if len(r.matched) != 4 {
		t.Errorf("Match array should already be reset to 4 however get %v", len(r.matched))
	}
}

func testNodeUpdatesItsRateLimiterHeartbeat(isLeader bool, t *testing.T) {
	r := newRateLimitedTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	if isLeader {
		r.becomeCandidate()
		r.becomeLeader()
	} else {
		r.becomeFollower(0, 2)
	}
	hbt := r.rl.GetTick()
	for i := uint64(0); i < r.electionTimeout; i++ {
		ne(r.tick(), t)
	}
	if r.rl.GetTick() != hbt+1 {
		t.Errorf("rl heartbeat not updated, %d want %d",
			r.rl.GetTick(), hbt+1)
	}
}

func TestLeaderNodeUpdatesItsRateLimiterHeartbeat(t *testing.T) {
	testNodeUpdatesItsRateLimiterHeartbeat(true, t)
}

func TestFollowerNodeUpdatesItsRateLimiterHeartbeat(t *testing.T) {
	testNodeUpdatesItsRateLimiterHeartbeat(false, t)
}

func TestResetClearsFollowerRateLimitState(t *testing.T) {
	r := newRateLimitedTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	rl := r.rl
	ne(r.handleLeaderRateLimit(pb.Message{From: 2, Hint: testRateLimit + 1}), t)
	if !rl.RateLimited() {
		t.Errorf("not rate limited")
	}
	r.reset(2, true)
	for i := uint64(0); i <= server.ChangeTickThreashold; i++ {
		rl.Tick()
	}
	if rl.RateLimited() {
		t.Errorf("still rate limited")
	}
}

func TestLeaderRateLimitMessageIsHandledByLeader(t *testing.T) {
	r := newRateLimitedTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	rl := r.rl
	if rl.RateLimited() {
		t.Errorf("unexpectedly already rate limited")
	}
	ne(r.handleLeaderRateLimit(pb.Message{From: 2, Hint: testRateLimit + 1}), t)
	if !rl.RateLimited() {
		t.Errorf("not rate limited")
	}
}

func TestRateLimitMessageIsNeverSentByLeader(t *testing.T) {
	r := newRateLimitedTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeCandidate()
	r.becomeLeader()
	ents := []pb.Entry{
		{Type: pb.ApplicationEntry, Cmd: make([]byte, testRateLimit+1)},
	}
	r.appendEntries(ents)
	rl := r.rl
	if !rl.RateLimited() {
		t.Errorf("not rate limited")
	}
	for i := uint64(0); i < r.electionTimeout; i++ {
		ne(r.tick(), t)
	}
	for _, msg := range r.msgs {
		if msg.Type == pb.RateLimit {
			t.Fatalf("rate limit message unexpected sent")
		}
	}
}

func testRateLimitMessageIsSentByNonLeader(leaderID uint64,
	rateLimitSent bool, t *testing.T) {
	r := newRateLimitedTestRaft(1, []uint64{1, 2, 3}, 5, 1, NewTestLogDB())
	r.becomeFollower(2, leaderID)
	ents := []pb.Entry{
		{Type: pb.ApplicationEntry, Cmd: make([]byte, testRateLimit+1)},
	}
	r.appendEntries(ents)
	rl := r.rl
	if !rl.RateLimited() {
		t.Errorf("not rate limited")
	}
	for i := uint64(0); i < r.electionTimeout; i++ {
		ne(r.tick(), t)
	}
	sent := false
	for _, msg := range r.msgs {
		if msg.Type == pb.RateLimit {
			sent = true
		}
	}
	if sent != rateLimitSent {
		t.Fatalf("sent %t, want %t", sent, rateLimitSent)
	}
}

func TestRateLimitMessageIsSentByNonLeader(t *testing.T) {
	testRateLimitMessageIsSentByNonLeader(2, true, t)
	testRateLimitMessageIsSentByNonLeader(NoLeader, false, t)
}

func TestInMemoryEntriesSliceCanBeResized(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 5, 1, NewTestLogDB())
	oldcap := cap(r.log.inmem.entries)
	if oldcap != 0 {
		t.Errorf("unexpected cap val: %d", oldcap)
	}
	r.log.inmem.shrunk = true
	for i := uint64(0); i < inMemGcTimeout; i++ {
		ne(r.tick(), t)
	}
	if uint64(cap(r.log.inmem.entries)) != entrySliceSize {
		t.Errorf("not resized")
	}
}

func TestFirstQuiescedTickResizesInMemoryEntriesSlice(t *testing.T) {
	r := newTestRaft(1, []uint64{1}, 5, 1, NewTestLogDB())
	oldcap := cap(r.log.inmem.entries)
	if oldcap != 0 {
		t.Errorf("unexpected cap val: %d", oldcap)
	}
	r.quiescedTick()
	if uint64(cap(r.log.inmem.entries)) != entrySliceSize {
		t.Errorf("not resized, cap: %d", oldcap)
	}
	r.log.inmem.entries = make([]pb.Entry, 0)
	r.quiescedTick()
	if cap(r.log.inmem.entries) != 0 {
		t.Errorf("unexpectedly resized again")
	}
}

func TestDelayedSnapshotAckCanBeSet(t *testing.T) {
	p := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	p.becomeCandidate()
	p.becomeLeader()
	rp := p.remotes[3]
	rp.state = remoteSnapshot
	ne(p.handleLeaderSnapshotStatus(pb.Message{
		Type:   pb.SnapshotStatus,
		Hint:   10,
		Reject: true,
	}, rp), t)
	if !rp.delayed.rejected || rp.delayed.ctick != 10 {
		t.Errorf("delayed snapshot ack not set")
	}
}

func TestCheckDelayedSnapshotAck(t *testing.T) {
	p := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	p.becomeCandidate()
	p.becomeLeader()
	if p.snapshotting {
		t.Errorf("unexpected snapshotting flag")
	}
	rp := p.remotes[3]
	rp.state = remoteSnapshot
	rp.snapshotIndex = 100
	ne(p.handleLeaderSnapshotStatus(pb.Message{
		Type:   pb.SnapshotStatus,
		Hint:   10,
		Reject: true,
	}, rp), t)
	if !p.snapshotting {
		t.Errorf("snapshotting flag not set")
	}
	for i := 0; i < 10; i++ {
		ne(p.tick(), t)
		if i != 9 {
			if !p.snapshotting {
				t.Errorf("snapshotting flag not set")
			}
		} else {
			if p.snapshotting {
				t.Errorf("snapshotting flag not cleared")
			}
		}
	}
	if rp.delayed.rejected || rp.delayed.ctick != 0 {
		t.Errorf("pending not cleared")
	}
	if rp.state != remoteWait || rp.snapshotIndex != 0 {
		t.Errorf("not in remote wait state, %s", rp.state)
	}
}

func TestElectionWithPreVote(t *testing.T) {
	a := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	b := newTestRaft(2, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	c := newTestRaft(3, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	a.preVote = true
	b.preVote = true
	c.preVote = true

	nt := newNetwork(a, b, c)
	nt.send(pb.Message{From: 1, To: 1, Type: pb.Election})
	if a.state != leader {
		t.Errorf("state = %s, want %s", a.state, leader)
	}
	if b.state != follower {
		t.Errorf("state = %s, want %s", b.state, follower)
	}
	if c.state != follower {
		t.Errorf("state = %s, want %s", c.state, follower)
	}
}

func TestInconsistentRaftConfig(t *testing.T) {
	tests := []struct {
		mt      pb.MessageType
		prevote bool
		result  bool
	}{
		{pb.RequestVote, true, false},
		{pb.RequestVote, false, false},
		{pb.RequestPreVote, true, false},
		{pb.RequestPreVote, false, true},
		{pb.RequestPreVoteResp, true, false},
		{pb.RequestPreVoteResp, false, true},
	}

	for idx, tt := range tests {
		r := raft{preVote: tt.prevote}
		result := r.inconsistentRaftConfig(pb.Message{Type: tt.mt})
		if result != tt.result {
			t.Errorf("%d, unexpected result", idx)
		}
	}
}

func TestCastVoteToDifferentNodesIsAllowed(t *testing.T) {
	a := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	a.preVote = true
	a.becomeFollower(10, 3)
	ne(a.Handle(pb.Message{Type: pb.RequestPreVote, From: 2, Term: 11}), t)
	if len(a.msgs) != 1 {
		t.Fatalf("no message")
	}
	expected := pb.Message{Type: pb.RequestPreVoteResp, From: 1, To: 2, Term: 11}
	if !reflect.DeepEqual(expected, a.msgs[0]) {
		t.Errorf("unexpected msg")
	}
	a.msgs = nil
	ne(a.Handle(pb.Message{Type: pb.RequestPreVote, From: 3, Term: 11}), t)
	if len(a.msgs) != 1 {
		t.Fatalf("no message")
	}
	expected = pb.Message{Type: pb.RequestPreVoteResp, From: 1, To: 3, Term: 11}
	if !reflect.DeepEqual(expected, a.msgs[0]) {
		t.Errorf("unexpected msg")
	}
}

func TestHandleLogQuery(t *testing.T) {
	p := newTestRaft(1, []uint64{1}, 10, 1, NewTestLogDB())
	assert.NoError(t, p.Handle(pb.Message{From: 1, To: 1, Type: pb.Election}))
	assert.Equal(t, leader, p.state)
	committed := p.log.committed
	for i := 0; i < 10; i++ {
		assert.NoError(t, p.Handle(pb.Message{
			From:    1,
			To:      1,
			Type:    pb.Propose,
			Entries: []pb.Entry{{Cmd: []byte("test-data")}},
		}))
	}
	assert.Equal(t, committed+10, p.log.committed)
	assert.NoError(t, p.Handle(pb.Message{
		Type: pb.LogQuery,
		From: 1,
		To:   committed + 11,
		Hint: math.MaxUint64,
	}))
	entries, err := p.log.getEntries(1, 12, math.MaxUint64)
	assert.NoError(t, err)
	expected := &pb.LogQueryResult{
		FirstIndex: p.log.firstIndex(),
		LastIndex:  p.log.committed + 1,
		Error:      nil,
		Entries:    entries,
	}
	assert.Equal(t, expected, p.logQueryResult)
}

func TestHandleLogQueryWillPanicWhenRepeatedlyCalled(t *testing.T) {
	p := newTestRaft(1, []uint64{1}, 10, 1, NewTestLogDB())
	assert.NoError(t, p.Handle(pb.Message{From: 1, To: 1, Type: pb.Election}))
	assert.Equal(t, leader, p.state)
	committed := p.log.committed
	for i := 0; i < 10; i++ {
		assert.NoError(t, p.Handle(pb.Message{
			From:    1,
			To:      1,
			Type:    pb.Propose,
			Entries: []pb.Entry{{Cmd: []byte("test-data")}},
		}))
	}
	assert.Equal(t, committed+10, p.log.committed)
	assert.Nil(t, p.logQueryResult)
	assert.NoError(t, p.Handle(pb.Message{
		Type: pb.LogQuery,
		From: 1,
		To:   committed + 11,
		Hint: math.MaxUint64,
	}))
	defer func() {
		if r := recover(); r == nil {
			t.Fatalf("panic not triggered")
		}
	}()
	assert.NotNil(t, p.logQueryResult)
	assert.NoError(t, p.Handle(pb.Message{
		Type: pb.LogQuery,
		From: 1,
		To:   committed + 11,
		Hint: math.MaxUint64,
	}))
}

func TestHandleLogQueryCanHandleRangeError(t *testing.T) {
	p := newTestRaft(1, []uint64{1}, 10, 1, NewTestLogDB())
	assert.NoError(t, p.Handle(pb.Message{From: 1, To: 1, Type: pb.Election}))
	assert.Equal(t, leader, p.state)
	committed := p.log.committed
	for i := 0; i < 10; i++ {
		assert.NoError(t, p.Handle(pb.Message{
			From:    1,
			To:      1,
			Type:    pb.Propose,
			Entries: []pb.Entry{{Cmd: []byte("test-data")}},
		}))
	}
	assert.Equal(t, committed+10, p.log.committed)
	assert.NoError(t, p.Handle(pb.Message{
		Type: pb.LogQuery,
		From: 13,
		To:   14,
		Hint: math.MaxUint64,
	}))
	expected := &pb.LogQueryResult{
		FirstIndex: p.log.firstIndex(),
		LastIndex:  p.log.committed + 1,
		Error:      ErrCompacted,
		Entries:    nil,
	}
	assert.Equal(t, expected, p.logQueryResult)
}

func TestSetLeaderIDWillSetLeaderInfo(t *testing.T) {
	r := raft{term: 200}
	r.setLeaderID(100)
	assert.Equal(t, uint64(100), r.leaderID)
	assert.Equal(t, &pb.LeaderUpdate{LeaderID: 100, Term: 200}, r.leaderUpdate)
}
````

## File: internal/raft/raft.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

/*
Package raft is a distributed consensus package that implements the Raft
protocol.

This package is internally used by Dragonboat, applications are not expected
to import this package.
*/
package raft

import (
	"fmt"
	"math"
	"sort"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/logutil"
	"github.com/lni/goutils/random"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/logger"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

var (
	plog = logger.GetLogger("raft")
)

const (
	// NoLeader is the flag used to indcate that there is no leader or the leader
	// is unknown.
	NoLeader uint64 = 0
	// NoNode is the flag used to indicate that the node id field is not set.
	NoNode          uint64 = 0
	noLimit         uint64 = math.MaxUint64
	numMessageTypes uint64 = 29
)

var (
	emptyState     = pb.State{}
	maxEntrySize   = settings.Soft.MaxEntrySize
	inMemGcTimeout = settings.Soft.InMemGCTimeout
)

// State is the state of a raft node defined in the raft thesis.
type State uint64

const (
	follower State = iota
	candidate
	preVoteCandidate
	leader
	nonVoting
	witness
	numStates
)

var stateNames = [...]string{
	"Follower",
	"Candidate",
	"PreVoteCandidate",
	"Leader",
	"NonVoting",
	"Witness",
}

func (st State) String() string {
	return stateNames[uint64(st)]
}

// ReplicaID returns a human friendly form of ReplicaID for logging purposes.
func ReplicaID(replicaID uint64) string {
	return logutil.ReplicaID(replicaID)
}

// ShardID returns a human friendly form of ShardID for logging purposes.
func ShardID(shardID uint64) string {
	return logutil.ShardID(shardID)
}

type handlerFunc func(pb.Message) error
type stepFunc func(*raft, pb.Message) error

// Status is the struct that captures the status of a raft node.
type Status struct {
	ReplicaID uint64
	ShardID   uint64
	Applied   uint64
	LeaderID  uint64
	NodeState State
	pb.State
}

// IsLeader returns a boolean value indicating whether the node is leader.
func (s *Status) IsLeader() bool {
	return s.NodeState == leader
}

// IsFollower returns a boolean value indicating whether the node is a follower.
func (s *Status) IsFollower() bool {
	return s.NodeState == follower
}

// getLocalStatus gets a copy of the current raft status.
func getLocalStatus(r *raft) Status {
	return Status{
		ReplicaID: r.replicaID,
		ShardID:   r.shardID,
		NodeState: r.state,
		Applied:   r.log.processed,
		LeaderID:  r.leaderID,
		State:     r.raftState(),
	}
}

//
// Struct raft implements the raft protocol published in Diego Ongarno's PhD
// thesis. Almost all features covered in Diego Ongarno's thesis have been
// implemented, including -
//  * leader election with pre-vote
//  * log replication
//  * flow control
//  * membership configuration change
//  * snapshotting and streaming
//  * log compaction
//  * ReadIndex protocol for read-only queries
//  * leadership transfer
//  * non-voting members
//  * witness members
//  * idempotent updates
//  * quorum check
//  * batching
//  * pipelining
//  * witness
//
// Features currently being worked on -
//  * pre-vote
//

//
// This implementation made references to etcd raft's design in the following
// aspects:
//  * it models the raft protocol state as a state machine
//  * restricting to at most one pending leadership change request at a time
//  * replication flow control
//
// Copyright 2015 The etcd Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

//
// When compared with etcd raft, this implementation is quite different,
// including in areas that we made reference to etcd raft -
// * brand new implementation
// * better bootstrapping procedure
// * log entries are partitioned based on whether they are required in
//   immediate future rather than whether they have been persisted to disk
// * zero disk read when replicating raft log entries
// * committed entries are applied in a fully asynchronous manner
// * snapshots are applied in a fully asynchronous manner
// * replication messages can be asynchronously serialized and sent
// * pagination support when applying committed entries
// * making proposals are fully batched
// * ReadIndex protocol implementation are fully batched
// * unsafe read-only queries that rely on local clock is not supported
// * non-voting members are implemented as a special raft state
// * non-voting members can initiate both new proposal and ReadIndex requests
// * simplified flow control
//

var dn = logutil.DescribeNode

type raft struct {
	handlers                  [numStates][numMessageTypes]handlerFunc
	events                    server.IRaftEventListener
	hasNotAppliedConfigChange func() bool
	votes                     map[uint64]bool
	handle                    stepFunc
	log                       *entryLog
	rl                        *server.InMemRateLimiter
	remotes                   map[uint64]*remote
	nonVotings                map[uint64]*remote
	witnesses                 map[uint64]*remote
	logQueryResult            *pb.LogQueryResult
	leaderUpdate              *pb.LeaderUpdate
	readIndex                 *readIndex
	matched                   []uint64
	msgs                      []pb.Message
	droppedReadIndexes        []pb.SystemCtx
	droppedEntries            []pb.Entry
	readyToRead               []pb.ReadyToRead
	prevLeader                server.LeaderInfo
	state                     State
	leaderTransferTarget      uint64
	leaderID                  uint64
	shardID                   uint64
	replicaID                 uint64
	term                      uint64
	applied                   uint64
	vote                      uint64
	tickCount                 uint64
	electionTick              uint64
	heartbeatTick             uint64
	heartbeatTimeout          uint64
	electionTimeout           uint64
	randomizedElectionTimeout uint64
	snapshotting              bool
	checkQuorum               bool
	quiesce                   bool
	isLeaderTransferTarget    bool
	pendingConfigChange       bool
	preVote                   bool
}

func newRaft(c config.Config, logdb ILogDB) *raft {
	if err := c.Validate(); err != nil {
		panic(err)
	}
	if logdb == nil {
		panic("logdb is nil")
	}
	rl := server.NewInMemRateLimiter(c.MaxInMemLogSize)
	r := &raft{
		shardID:          c.ShardID,
		replicaID:        c.ReplicaID,
		leaderID:         NoLeader,
		msgs:             make([]pb.Message, 0),
		droppedEntries:   make([]pb.Entry, 0),
		log:              newEntryLog(logdb, rl),
		remotes:          make(map[uint64]*remote),
		nonVotings:       make(map[uint64]*remote),
		witnesses:        make(map[uint64]*remote),
		electionTimeout:  c.ElectionRTT,
		heartbeatTimeout: c.HeartbeatRTT,
		checkQuorum:      c.CheckQuorum,
		preVote:          c.PreVote,
		readIndex:        newReadIndex(),
		rl:               rl,
	}
	plog.Infof("%s raft log rate limit enabled: %t, %d",
		dn(r.shardID, r.replicaID), r.rl.Enabled(), c.MaxInMemLogSize)
	st, members := logdb.NodeState()
	for p := range members.Addresses {
		r.remotes[p] = &remote{next: 1}
	}
	for p := range members.NonVotings {
		r.nonVotings[p] = &remote{next: 1}
	}
	for p := range members.Witnesses {
		r.witnesses[p] = &remote{next: 1}
	}
	r.resetMatchValueArray()
	if !pb.IsEmptyState(st) {
		r.loadState(st)
	}
	// Set node initial state.
	if c.IsNonVoting {
		r.state = nonVoting
		r.becomeNonVoting(r.term, NoLeader)
	} else if c.IsWitness {
		r.state = witness
		r.becomeWitness(r.term, NoLeader)
	} else {
		// see first paragraph section 5.2 of the raft paper
		r.becomeFollower(r.term, NoLeader)
	}
	r.initializeHandlerMap()
	r.checkHandlerMap()
	r.handle = defaultHandle
	return r
}

func (r *raft) setTestPeers(peers []uint64) {
	if len(r.remotes) == 0 {
		for _, p := range peers {
			r.remotes[p] = &remote{next: 1}
		}
	}
}

func (r *raft) setApplied(applied uint64) {
	r.applied = applied
}

func (r *raft) getApplied() uint64 {
	return r.applied
}

func (r *raft) resetMatchValueArray() {
	r.matched = make([]uint64, r.numVotingMembers())
}

func (r *raft) describe() string {
	li := r.log.lastIndex()
	t, err := r.log.term(li)
	if err != nil && !errors.Is(err, ErrCompacted) {
		plog.Panicf("%s failed to get term, %v", dn(r.shardID, r.replicaID), err)
	}
	// first, last, term, committed, applied
	fmtstr := "[f:%d,l:%d,t:%d,c:%d,a:%d] %s t%d"
	return fmt.Sprintf(fmtstr,
		r.log.firstIndex(), r.log.lastIndex(), t, r.log.committed,
		r.log.processed, dn(r.shardID, r.replicaID), r.term)
}

func (r *raft) isCandidate() bool {
	return r.state == candidate
}

func (r *raft) isLeader() bool {
	return r.state == leader
}

func (r *raft) isNonVoting() bool {
	return r.state == nonVoting
}

func (r *raft) isWitness() bool {
	return r.state == witness
}

func (r *raft) mustBeLeader() {
	if !r.isLeader() {
		plog.Panicf("%s is not leader", r.describe())
	}
}

func (r *raft) setLeaderID(leaderID uint64) {
	r.leaderID = leaderID
	r.leaderUpdate = &pb.LeaderUpdate{
		LeaderID: leaderID,
		Term:     r.term,
	}
	if r.events != nil {
		if (r.term == 0 && leaderID == NoLeader) ||
			leaderID != r.prevLeader.LeaderID || r.term != r.prevLeader.Term {
			info := server.LeaderInfo{
				ShardID:   r.shardID,
				ReplicaID: r.replicaID,
				LeaderID:  leaderID,
				Term:      r.term,
			}
			r.prevLeader = info
			r.events.LeaderUpdated(info)
		}
	}
}

func (r *raft) leaderTransfering() bool {
	return r.leaderTransferTarget != NoNode && r.isLeader()
}

func (r *raft) abortLeaderTransfer() {
	r.leaderTransferTarget = NoNode
}

func (r *raft) numVotingMembers() int {
	return len(r.remotes) + len(r.witnesses)
}

func (r *raft) quorum() int {
	return r.numVotingMembers()/2 + 1
}

func (r *raft) isSingleNodeQuorum() bool {
	return r.quorum() == 1
}

func (r *raft) leaderHasQuorum() bool {
	c := 0

	for nid, member := range r.votingMembers() {
		if nid == r.replicaID || member.isActive() {
			c++
			member.setNotActive()
		}
	}
	return c >= r.quorum()
}

func (r *raft) nodes() []uint64 {
	nodes := make([]uint64, 0, r.numVotingMembers()+len(r.nonVotings))
	for id := range r.remotes {
		nodes = append(nodes, id)
	}
	for id := range r.nonVotings {
		nodes = append(nodes, id)
	}
	for id := range r.witnesses {
		nodes = append(nodes, id)
	}
	return nodes
}

func (r *raft) nodesSorted() []uint64 {
	nodes := r.nodes()
	sort.Slice(nodes, func(i, j int) bool { return nodes[i] < nodes[j] })
	return nodes
}

func (r *raft) votingMembers() map[uint64]*remote {
	nodes := make(map[uint64]*remote, r.numVotingMembers())
	for id, rm := range r.remotes {
		nodes[id] = rm
	}
	for id, wt := range r.witnesses {
		nodes[id] = wt
	}
	return nodes
}

func (r *raft) raftState() pb.State {
	return pb.State{
		Term:   r.term,
		Vote:   r.vote,
		Commit: r.log.committed,
	}
}

func (r *raft) loadState(st pb.State) {
	if st.Commit < r.log.committed || st.Commit > r.log.lastIndex() {
		plog.Panicf("%s got out of range state, st.commit %d, range[%d,%d]",
			r.describe(), st.Commit, r.log.committed, r.log.lastIndex())
	}
	r.log.committed = st.Commit
	r.term = st.Term
	r.vote = st.Vote
}

func (r *raft) restore(ss pb.Snapshot) (bool, error) {
	if ss.Index <= r.log.committed {
		plog.Warningf("%s, restore aborted, ss.Index <= committed", r.describe())
		return false, nil
	}
	if !r.isNonVoting() {
		for nid := range ss.Membership.NonVotings {
			if nid == r.replicaID {
				plog.Panicf("%s converting to nonVoting, index %d, committed %d, %+v",
					r.describe(), ss.Index, r.log.committed, ss)
			}
		}
	}
	if !r.isWitness() {
		for nid := range ss.Membership.Witnesses {
			if nid == r.replicaID {
				plog.Panicf("%s converting to witness, index %d, committed %d, %+v",
					r.describe(), ss.Index, r.log.committed, ss)
			}
		}
	}
	// p52 of the raft thesis
	match, err := r.log.matchTerm(ss.Index, ss.Term)
	if err != nil {
		return false, err
	}
	if match {
		// a snapshot at index X implies that X has been committed
		r.log.commitTo(ss.Index)
		return false, nil
	}
	plog.Infof("%s starts to restore snapshot index %d term %d",
		r.describe(), ss.Index, ss.Term)
	r.log.restore(ss)
	return true, nil
}

func (r *raft) restoreRemotes(ss pb.Snapshot) {
	r.remotes = make(map[uint64]*remote)
	for id := range ss.Membership.Addresses {
		if id == r.replicaID && r.isNonVoting() {
			r.becomeFollower(r.term, r.leaderID)
		}
		if _, ok := r.witnesses[id]; ok {
			plog.Panicf("Assumed witness could not promote to full member")
		}
		match := uint64(0)
		next := r.log.lastIndex() + 1
		if id == r.replicaID {
			match = next - 1
		}
		r.setRemote(id, match, next)
		plog.Debugf("%s restored remote progress of %s [%s]",
			r.describe(), ReplicaID(id), r.remotes[id])
	}
	if r.selfRemoved() && r.isLeader() {
		r.becomeFollower(r.term, NoLeader)
	}
	r.nonVotings = make(map[uint64]*remote)
	for id := range ss.Membership.NonVotings {
		match := uint64(0)
		next := r.log.lastIndex() + 1
		if id == r.replicaID {
			match = next - 1
		}
		r.setNonVoting(id, match, next)
		plog.Debugf("%s restored nonVoting progress of %s [%s]",
			r.describe(), ReplicaID(id), r.nonVotings[id])
	}
	r.witnesses = make(map[uint64]*remote)
	for id := range ss.Membership.Witnesses {
		match := uint64(0)
		next := r.log.lastIndex() + 1
		if id == r.replicaID {
			match = next - 1
		}
		r.setWitness(id, match, next)
		plog.Debugf("%s restored witness progress of %s [%s]",
			r.describe(), ReplicaID(id), r.witnesses[id])
	}
	r.resetMatchValueArray()
}

//
// tick related functions
//

func (r *raft) timeForElection() bool {
	return r.electionTick >= r.randomizedElectionTimeout
}

func (r *raft) timeForHeartbeat() bool {
	return r.heartbeatTick >= r.heartbeatTimeout
}

// p69 of the raft thesis mentions that check quorum is performed when an
// election timeout elapses
func (r *raft) timeForCheckQuorum() bool {
	return r.electionTick >= r.electionTimeout
}

// p29 of the raft thesis mentions that leadership transfer should abort
// when an election timeout elapses
func (r *raft) timeToAbortLeaderTransfer() bool {
	return r.leaderTransfering() && r.electionTick >= r.electionTimeout
}

func (r *raft) timeForRateLimitCheck() bool {
	return r.tickCount%r.electionTimeout == 0
}

func (r *raft) timeForInMemGC() bool {
	return r.tickCount%inMemGcTimeout == 0
}

func (r *raft) tick() error {
	r.quiesce = false
	r.tickCount++
	// this is to work around the language limitation described in
	// https://github.com/golang/go/issues/9618
	if r.timeForInMemGC() {
		r.log.inmem.tryResize()
	}
	if r.isLeader() {
		return r.leaderTick()
	}
	return r.nonLeaderTick()
}

func (r *raft) nonLeaderTick() error {
	if r.isLeader() {
		panic("noleader tick called on leader node")
	}
	r.electionTick++
	if r.timeForRateLimitCheck() {
		if r.rl.Enabled() {
			r.rl.Tick()
			r.sendRateLimitMessage()
		}
	}
	// section 4.2.1 of the raft thesis
	// non-voting member or witness will not participate in election
	if r.isNonVoting() || r.isWitness() {
		return nil
	}
	// 6th paragraph section 5.2 of the raft paper
	if !r.selfRemoved() && r.timeForElection() {
		r.electionTick = 0
		if err := r.Handle(pb.Message{
			From: r.replicaID,
			Type: pb.Election,
		}); err != nil {
			return err
		}
	}
	return nil
}

func (r *raft) leaderTick() error {
	r.mustBeLeader()
	r.electionTick++
	if r.timeForRateLimitCheck() {
		if r.rl.Enabled() {
			r.rl.Tick()
		}
	}
	timeToAbortLeaderTransfer := r.timeToAbortLeaderTransfer()
	if r.timeForCheckQuorum() {
		r.electionTick = 0
		if r.checkQuorum {
			if err := r.Handle(pb.Message{
				From: r.replicaID,
				Type: pb.CheckQuorum,
			}); err != nil {
				return err
			}
		}
	}
	if timeToAbortLeaderTransfer {
		r.abortLeaderTransfer()
	}
	r.heartbeatTick++
	if r.timeForHeartbeat() {
		r.heartbeatTick = 0
		if err := r.Handle(pb.Message{
			From: r.replicaID,
			Type: pb.LeaderHeartbeat,
		}); err != nil {
			return err
		}
	}
	return r.checkPendingSnapshotAck()
}

func (r *raft) quiescedTick() {
	if !r.quiesce {
		r.quiesce = true
		r.log.inmem.resize()
	}
	r.electionTick++
}

func (r *raft) setRandomizedElectionTimeout() {
	randTime := random.LockGuardedRand.Uint64() % r.electionTimeout
	r.randomizedElectionTimeout = r.electionTimeout + randTime
}

//
// send and broadcast functions
//

func (r *raft) finalizeMessageTerm(m pb.Message) pb.Message {
	if m.Term == 0 && m.Type == pb.RequestVote {
		plog.Panicf("%s sending RequestVote with 0 term", r.describe())
	}
	if m.Term > 0 &&
		!isRequestVoteMessage(m.Type) && m.Type != pb.RequestPreVoteResp {
		plog.Panicf("%s term unexpectedly set for message type %d",
			r.describe(), m.Type)
	}
	if !isRequestMessage(m.Type) &&
		!isRequestVoteMessage(m.Type) && m.Type != pb.RequestPreVoteResp {
		m.Term = r.term
	}
	return m
}

func (r *raft) send(m pb.Message) {
	m.From = r.replicaID
	m = r.finalizeMessageTerm(m)
	r.msgs = append(r.msgs, m)
}

func (r *raft) sendRateLimitMessage() {
	if r.isLeader() {
		plog.Panicf("leader node called sendRateLimitMessage")
	}
	if r.leaderID == NoLeader {
		plog.Infof("%s rate limit message skipped, no leader", r.describe())
		return
	}
	if !r.rl.Enabled() {
		return
	}
	mv := uint64(0)
	if r.rl.RateLimited() {
		inmemSz := r.rl.Get()
		notCommitedSz := getEntrySliceSize(r.log.getUncommittedEntries())
		mv = max(inmemSz-notCommitedSz, 0)
	}
	r.send(pb.Message{
		Type: pb.RateLimit,
		To:   r.leaderID,
		Hint: mv,
	})
}

func (r *raft) makeInstallSnapshotMessage(to uint64, m *pb.Message) uint64 {
	m.To = to
	m.Type = pb.InstallSnapshot
	snapshot := r.log.snapshot()
	if pb.IsEmptySnapshot(snapshot) {
		plog.Panicf("%s got an empty snapshot", r.describe())
	}
	// For witness, snapshot message will be marked as dummy snapshot.
	if _, ok := r.witnesses[to]; ok {
		snapshot = makeWitnessSnapshot(snapshot)
	}
	m.Snapshot = snapshot
	return snapshot.Index
}

func makeWitnessSnapshot(snapshot pb.Snapshot) pb.Snapshot {
	result := snapshot
	result.Filepath = ""
	result.FileSize = 0
	result.Files = nil
	result.Witness = true
	result.Dummy = false
	return result
}

func (r *raft) makeReplicateMessage(to uint64,
	next uint64, maxSize uint64) (pb.Message, error) {
	term, err := r.log.term(next - 1)
	if err != nil {
		return pb.Message{}, err
	}
	entries, err := r.log.entries(next, maxSize)
	if err != nil {
		return pb.Message{}, err
	}
	if len(entries) > 0 {
		lastIndex := entries[len(entries)-1].Index
		expected := next - 1 + uint64(len(entries))
		if lastIndex != expected {
			plog.Panicf("%s expected last index in Replicate %d, got %d",
				r.describe(), expected, lastIndex)
		}
	}
	// Don't send actual log entry to witness as they won't replicate real message,
	// unless there is a config change.
	if _, ok := r.witnesses[to]; ok {
		entries = makeMetadataEntries(entries)
	}
	return pb.Message{
		To:       to,
		Type:     pb.Replicate,
		LogIndex: next - 1,
		LogTerm:  term,
		Entries:  entries,
		Commit:   r.log.committed,
	}, nil
}

func makeMetadataEntries(entries []pb.Entry) []pb.Entry {
	me := make([]pb.Entry, 0, len(entries))
	for _, ent := range entries {
		if ent.Type != pb.ConfigChangeEntry {
			me = append(me, pb.Entry{
				Type:  pb.MetadataEntry,
				Index: ent.Index,
				Term:  ent.Term,
			})
		} else {
			me = append(me, ent)
		}
	}
	return me
}

func (r *raft) sendReplicateMessage(to uint64) {
	var rp *remote
	if v, ok := r.remotes[to]; ok {
		rp = v
	} else if v, ok := r.nonVotings[to]; ok {
		rp = v
	} else {
		rp, ok = r.witnesses[to]
		if !ok {
			plog.Panicf("%s failed to get the remote instance", r.describe())
		}
	}
	if rp.isPaused() {
		return
	}
	m, err := r.makeReplicateMessage(to, rp.next, maxEntrySize)
	if err != nil {
		// log not available due to compaction, send snapshot
		if !rp.isActive() {
			plog.Warningf("%s, %s is not active, sending snapshot is skipped",
				r.describe(), ReplicaID(to))
			return
		}
		index := r.makeInstallSnapshotMessage(to, &m)
		plog.Infof("%s is sending snapshot (%d) to %s, r.Next %d, r.Match %d, %v",
			r.describe(), index, ReplicaID(to), rp.next, rp.match, err)
		rp.becomeSnapshot(index)
	} else if len(m.Entries) > 0 {
		lastIndex := m.Entries[len(m.Entries)-1].Index
		rp.progress(lastIndex)
	}
	r.send(m)
}

func (r *raft) broadcastReplicateMessage() {
	r.mustBeLeader()
	for nid := range r.nonVotings {
		if nid == r.replicaID {
			plog.Panicf("%s nonVoting is broadcasting Replicate msg", r.describe())
		}
	}
	for _, nid := range r.nodes() {
		if nid != r.replicaID {
			r.sendReplicateMessage(nid)
		}
	}
}

func (r *raft) sendHeartbeatMessage(to uint64,
	hint pb.SystemCtx, match uint64) {
	commit := min(match, r.log.committed)
	r.send(pb.Message{
		To:       to,
		Type:     pb.Heartbeat,
		Commit:   commit,
		Hint:     hint.Low,
		HintHigh: hint.High,
	})
}

// p72 of the raft thesis describe how to use Heartbeat message in the ReadIndex
// protocol.
func (r *raft) broadcastHeartbeatMessage() {
	r.mustBeLeader()
	if r.readIndex.hasPendingRequest() {
		ctx := r.readIndex.peepCtx()
		r.broadcastHeartbeatMessageWithHint(ctx)
	} else {
		r.broadcastHeartbeatMessageWithHint(pb.SystemCtx{})
	}
}

func (r *raft) broadcastHeartbeatMessageWithHint(ctx pb.SystemCtx) {
	zeroCtx := pb.SystemCtx{}
	for id, rm := range r.votingMembers() {
		if id != r.replicaID {
			r.sendHeartbeatMessage(id, ctx, rm.match)
		}
	}
	if ctx == zeroCtx {
		for id, rm := range r.nonVotings {
			r.sendHeartbeatMessage(id, zeroCtx, rm.match)
		}
	}
}

func (r *raft) sendTimeoutNowMessage(replicaID uint64) {
	r.send(pb.Message{
		Type: pb.TimeoutNow,
		To:   replicaID,
	})
}

//
// log append and commit
//

func (r *raft) sortMatchValues() {
	// unrolled bubble sort, sort.Slice is not allocation free
	if len(r.matched) == 3 {
		if r.matched[0] > r.matched[1] {
			v := r.matched[0]
			r.matched[0] = r.matched[1]
			r.matched[1] = v
		}
		if r.matched[1] > r.matched[2] {
			v := r.matched[1]
			r.matched[1] = r.matched[2]
			r.matched[2] = v
		}
		if r.matched[0] > r.matched[1] {
			v := r.matched[0]
			r.matched[0] = r.matched[1]
			r.matched[1] = v
		}
	} else if len(r.matched) == 1 {
		return
	} else {
		sort.Slice(r.matched, func(i, j int) bool {
			return r.matched[i] < r.matched[j]
		})
	}
}

func (r *raft) tryCommit() bool {
	r.mustBeLeader()
	if r.numVotingMembers() != len(r.matched) {
		r.resetMatchValueArray()
	}
	idx := 0
	for _, v := range r.remotes {
		r.matched[idx] = v.match
		idx++
	}
	for _, v := range r.witnesses {
		r.matched[idx] = v.match
		idx++
	}
	r.sortMatchValues()
	q := r.matched[r.numVotingMembers()-r.quorum()]
	// see p8 raft paper
	// "Raft never commits log entries from previous terms by counting replicas.
	// Only log entries from the leader’s current term are committed by counting
	// replicas"
	ok, err := r.log.tryCommit(q, r.term)
	if err != nil {
		// when trying to commit an entry by counting replicate responses,
		// r.log.tryCommit is not suppose to return any error. its internal call to
		// r.log.term() will always be able to find the entry in the in-mem log, as
		// entries are only moved to logdb after they are applied which happens
		// after being committed.
		panic(err)
	}

	return ok
}

func (r *raft) appendEntries(entries []pb.Entry) {
	lastIndex := r.log.lastIndex()
	for i := range entries {
		entries[i].Term = r.term
		entries[i].Index = lastIndex + 1 + uint64(i)
	}
	r.log.append(entries)
	r.remotes[r.replicaID].tryUpdate(r.log.lastIndex())
	if r.isSingleNodeQuorum() {
		r.tryCommit()
	}
}

//
// state transition related functions
//

func (r *raft) toFollowerState(term uint64, leaderID uint64,
	resetElectionTimeout bool) {
	if r.isWitness() {
		panic("transitioning to follower from witness state")
	}
	r.state = follower
	r.reset(term, resetElectionTimeout)
	r.setLeaderID(leaderID)
	plog.Infof("%s became follower", r.describe())
}

func (r *raft) becomeNonVoting(term uint64, leaderID uint64) {
	if !r.isNonVoting() {
		panic("transitioning to nonVoting state from other states")
	}
	if r.isWitness() {
		panic("transitioning to nonVoting from witness state")
	}
	r.reset(term, true)
	r.setLeaderID(leaderID)
	plog.Infof("%s became nonVoting", r.describe())
}

func (r *raft) becomeWitness(term uint64, leaderID uint64) {
	if !r.isWitness() {
		panic("transitioning to witness state from non-witness")
	}
	r.reset(term, true)
	r.setLeaderID(leaderID)
	plog.Infof("%s became witness", r.describe())
}

func (r *raft) becomeFollower(term uint64, leaderID uint64) {
	r.toFollowerState(term, leaderID, true)
}

func (r *raft) becomeFollowerKE(term uint64, leaderID uint64) {
	r.toFollowerState(term, leaderID, false)
}

func (r *raft) becomePreVoteCandidate() {
	if !r.preVote {
		panic("becomePreVoteCandidate called when preVote not enabled")
	}
	if r.isLeader() {
		panic("transitioning to candidate state from leader")
	}
	if r.isNonVoting() {
		panic("nonVoting is becoming candidate")
	}
	if r.isWitness() {
		panic("witness is becoming candidate")
	}
	r.state = preVoteCandidate
	r.reset(r.term, true)
	r.setLeaderID(NoLeader)
	plog.Warningf("%s became PreVote candidate", r.describe())
}

func (r *raft) becomeCandidate() {
	if r.isLeader() {
		panic("transitioning to candidate state from leader")
	}
	if r.isNonVoting() {
		panic("nonVoting is becoming candidate")
	}
	if r.isWitness() {
		panic("witness is becoming candidate")
	}
	r.state = candidate
	// 2nd paragraph section 5.2 of the raft paper
	r.reset(r.term+1, true)
	r.setLeaderID(NoLeader)
	r.vote = r.replicaID
	plog.Warningf("%s became candidate", r.describe())
}

func (r *raft) becomeLeader() {
	// need a state transition machine
	if !r.isLeader() && !r.isCandidate() {
		plog.Panicf("transitioning to leader state from %v", r.state.String())
	}
	r.state = leader
	r.reset(r.term, true)
	r.setLeaderID(r.replicaID)
	r.preLeaderPromotionHandleConfigChange()
	plog.Infof("%s became leader", r.describe())
	// p72 of the raft thesis
	r.appendEntries([]pb.Entry{{Type: pb.ApplicationEntry, Cmd: nil}})
}

func (r *raft) reset(term uint64, resetElectionTimeout bool) {
	if r.term != term {
		r.term = term
		r.vote = NoLeader
	}
	if r.rl.Enabled() {
		r.rl.Reset()
	}
	if resetElectionTimeout {
		r.electionTick = 0
		r.setRandomizedElectionTimeout()
	}
	r.votes = make(map[uint64]bool)
	r.heartbeatTick = 0
	r.readIndex = newReadIndex()
	r.clearPendingConfigChange()
	r.abortLeaderTransfer()
	r.resetRemotes()
	r.resetNonVotings()
	r.resetWitnesses()
	r.resetMatchValueArray()
}

func (r *raft) preLeaderPromotionHandleConfigChange() {
	n := r.getPendingConfigChangeCount()
	if n > 1 {
		plog.Panicf("%s multiple uncommitted config change entries", r.describe())
	} else if n == 1 {
		plog.Infof("%s becoming leader with pending ConfigChange", r.describe())
		r.setPendingConfigChange()
	}
}

// see section 5.3 of the raft paper
// "When a leader first comes to power, it initializes all nextIndex values to
// the index just after the last one in its log"
func (r *raft) resetRemotes() {
	for id := range r.remotes {
		r.remotes[id] = &remote{
			next: r.log.lastIndex() + 1,
		}
		if id == r.replicaID {
			r.remotes[id].match = r.log.lastIndex()
		}
	}
}

func (r *raft) resetNonVotings() {
	for id := range r.nonVotings {
		r.nonVotings[id] = &remote{
			next: r.log.lastIndex() + 1,
		}
		if id == r.replicaID {
			r.nonVotings[id].match = r.log.lastIndex()
		}
	}
}

func (r *raft) resetWitnesses() {
	for id := range r.witnesses {
		r.witnesses[id] = &remote{
			next: r.log.lastIndex() + 1,
		}
		if id == r.replicaID {
			r.witnesses[id].match = r.log.lastIndex()
		}
	}
}

//
// election related functions
//

func (r *raft) handleVoteResp(from uint64, rejected bool, preVote bool) int {
	mname := "RequestVoteResp"
	if preVote {
		mname = "RequestPreVoteResp"
	}
	if rejected {
		plog.Warningf("%s received %s rejection from %s",
			r.describe(), mname, ReplicaID(from))
	} else {
		plog.Warningf("%s received %s from %s",
			r.describe(), mname, ReplicaID(from))
	}
	votedFor := 0
	if _, ok := r.votes[from]; !ok {
		r.votes[from] = !rejected
	}
	for _, v := range r.votes {
		if v {
			votedFor++
		}
	}
	return votedFor
}

func (r *raft) preVoteCampaign() error {
	r.becomePreVoteCandidate()
	r.handleVoteResp(r.replicaID, false, true)
	if r.isSingleNodeQuorum() {
		return r.campaign()
	}
	index := r.log.lastIndex()
	lastTerm, err := r.log.lastTerm()
	if err != nil {
		return err
	}
	for k := range r.votingMembers() {
		if k == r.replicaID {
			continue
		}
		r.send(pb.Message{
			Term:     r.term + 1,
			To:       k,
			Type:     pb.RequestPreVote,
			LogIndex: index,
			LogTerm:  lastTerm,
		})
		plog.Warningf("%s sent RequestPreVote to %s", r.describe(), ReplicaID(k))
	}
	return nil
}

func (r *raft) campaign() error {
	r.becomeCandidate()
	term := r.term
	if r.events != nil {
		info := server.CampaignInfo{
			ShardID:   r.shardID,
			ReplicaID: r.replicaID,
			Term:      term,
		}
		r.events.CampaignLaunched(info)
	}
	r.handleVoteResp(r.replicaID, false, false)
	if r.isSingleNodeQuorum() {
		r.becomeLeader()
		return nil
	}
	var hint uint64
	if r.isLeaderTransferTarget {
		hint = r.replicaID
		r.isLeaderTransferTarget = false
	}
	index := r.log.lastIndex()
	lastTerm, err := r.log.lastTerm()
	if err != nil {
		return err
	}
	for k := range r.votingMembers() {
		if k == r.replicaID {
			continue
		}
		r.send(pb.Message{
			Term:     term,
			To:       k,
			Type:     pb.RequestVote,
			LogIndex: index,
			LogTerm:  lastTerm,
			Hint:     hint,
		})
		plog.Warningf("%s sent RequestVote to %s", r.describe(), ReplicaID(k))
	}
	return nil
}

//
// membership management
//

func (r *raft) selfRemoved() bool {
	if r.isNonVoting() {
		_, ok := r.nonVotings[r.replicaID]
		return !ok
	}
	if r.isWitness() {
		_, ok := r.witnesses[r.replicaID]
		return !ok
	}
	_, ok := r.remotes[r.replicaID]
	return !ok
}

func (r *raft) addNode(replicaID uint64) {
	r.clearPendingConfigChange()
	if replicaID == r.replicaID && r.isWitness() {
		plog.Panicf("%s is witness", r.describe())
	}
	if _, ok := r.remotes[replicaID]; ok {
		// already a voting member
		return
	}
	if rp, ok := r.nonVotings[replicaID]; ok {
		// promoting to full member with inherited progress info
		r.deleteNonVoting(replicaID)
		r.remotes[replicaID] = rp
		// local peer promoted, become follower
		if replicaID == r.replicaID {
			r.becomeFollower(r.term, r.leaderID)
		}
	} else if _, ok := r.witnesses[replicaID]; ok {
		panic("could not promote witness to full member")
	} else {
		r.setRemote(replicaID, 0, r.log.lastIndex()+1)
	}
}

func (r *raft) addNonVoting(replicaID uint64) {
	r.clearPendingConfigChange()
	if replicaID == r.replicaID && !r.isNonVoting() {
		plog.Panicf("%s is not a nonVoting", r.describe())
	}
	if _, ok := r.nonVotings[replicaID]; ok {
		return
	}
	r.setNonVoting(replicaID, 0, r.log.lastIndex()+1)
}

func (r *raft) addWitness(replicaID uint64) {
	r.clearPendingConfigChange()
	if replicaID == r.replicaID && !r.isWitness() {
		plog.Panicf("%s is not witness", r.describe())
	}
	if _, ok := r.witnesses[replicaID]; ok {
		return
	}
	r.setWitness(replicaID, 0, r.log.lastIndex()+1)
}

func (r *raft) removeNode(replicaID uint64) {
	r.deleteRemote(replicaID)
	r.deleteNonVoting(replicaID)
	r.deleteWitness(replicaID)
	r.clearPendingConfigChange()
	// step down as leader once it is removed
	if r.replicaID == replicaID && r.isLeader() {
		r.becomeFollower(r.term, NoLeader)
	}
	if r.leaderTransfering() && r.leaderTransferTarget == replicaID {
		r.abortLeaderTransfer()
	}
	if r.isLeader() && r.numVotingMembers() > 0 {
		if ok := r.tryCommit(); ok {
			r.broadcastReplicateMessage()
		}
	}
}

func (r *raft) deleteRemote(replicaID uint64) {
	delete(r.remotes, replicaID)
}

func (r *raft) deleteNonVoting(replicaID uint64) {
	delete(r.nonVotings, replicaID)
}

func (r *raft) deleteWitness(replicaID uint64) {
	delete(r.witnesses, replicaID)
}

func (r *raft) setRemote(replicaID uint64, match uint64, next uint64) {
	plog.Debugf("%s set remote %s, match %d, next %d",
		r.describe(), ReplicaID(replicaID), match, next)
	r.remotes[replicaID] = &remote{
		next:  next,
		match: match,
	}
}

func (r *raft) setNonVoting(replicaID uint64, match uint64, next uint64) {
	plog.Debugf("%s set nonVoting %s, match %d, next %d",
		r.describe(), ReplicaID(replicaID), match, next)
	r.nonVotings[replicaID] = &remote{
		next:  next,
		match: match,
	}
}

func (r *raft) setWitness(replicaID uint64, match uint64, next uint64) {
	plog.Debugf("%s set witness %s, match %d, next %d",
		r.describe(), ReplicaID(replicaID), match, next)
	r.witnesses[replicaID] = &remote{
		next:  next,
		match: match,
	}
}

// helper methods required for the membership change implementation
//
// p33-35 of the raft thesis describes a simple membership change protocol which
// requires only one node can be added or removed at a time. its safety is
// guarded by the fact that when there is only one node to be added or removed
// at a time, the old and new quorum are guaranteed to overlap.
// the protocol described in the raft thesis requires the membership change
// entry to be executed as soon as it is appended. this also introduces an extra
// troublesome step to roll back to an old membership configuration when
// necessary.
// similar to etcd raft, we treat such membership change entry as regular
// entries that are only executed after being committed (by the old quorum).
// to do that, however, we need to further restrict the leader to only has at
// most one pending not applied membership change entry in its log. this is to
// avoid the situation that two pending membership change entries are committed
// in one go with the same quorum while they actually require different quorums.
// consider the following situation -
// for a 3 nodes shard with existing members X, Y and Z, let's say we first
// propose a membership change to add a new node A, before A gets committed and
// applied, say we propose another membership change to add a new node B. When
// B gets committed, A will be committed as well, both will be using the 3 node
// membership quorum meaning both entries concerning A and B will become
// committed when any two of the X, Y, Z shard have them replicated. this thus
// violates the safety requirement as B will require 3 out of the 4 nodes (X,
// Y, Z, A) to have it replicated before it can be committed.
// we use the following pendingConfigChange flag to help tracking whether there
// is already a pending membership change entry in the log waiting to be
// executed.
func (r *raft) setPendingConfigChange() {
	r.pendingConfigChange = true
}

func (r *raft) hasPendingConfigChange() bool {
	return r.pendingConfigChange
}

func (r *raft) clearPendingConfigChange() {
	r.pendingConfigChange = false
}

func (r *raft) getPendingConfigChangeCount() int {
	idx := r.log.committed + 1
	count := 0
	for {
		ents, err := r.log.entries(idx, maxEntriesToApplySize)
		if err != nil {
			plog.Panicf("%s failed to get entries %v", r.describe(), err)
		}
		if len(ents) == 0 {
			return count
		}
		count += countConfigChange(ents)
		idx = ents[len(ents)-1].Index + 1
	}
}

//
// handler for various message types
//

func (r *raft) handleHeartbeatMessage(m pb.Message) error {
	r.log.commitTo(m.Commit)
	r.send(pb.Message{
		To:       m.From,
		Type:     pb.HeartbeatResp,
		Hint:     m.Hint,
		HintHigh: m.HintHigh,
	})
	return nil
}

func (r *raft) handleInstallSnapshotMessage(m pb.Message) error {
	plog.Debugf("%s called handleInstallSnapshotMessage with snapshot from %s",
		r.describe(), ReplicaID(m.From))
	index, term := m.Snapshot.Index, m.Snapshot.Term
	resp := pb.Message{
		To:   m.From,
		Type: pb.ReplicateResp,
	}
	ok, err := r.restore(m.Snapshot)
	if err != nil {
		return err
	}
	if ok {
		plog.Debugf("%s restored snapshot %d term %d", r.describe(), index, term)
		resp.LogIndex = r.log.lastIndex()
	} else {
		plog.Debugf("%s rejected snapshot %d term %d", r.describe(), index, term)
		resp.LogIndex = r.log.committed
		if r.events != nil {
			info := server.SnapshotInfo{
				ShardID:   r.shardID,
				ReplicaID: r.replicaID,
				Index:     m.Snapshot.Index,
				Term:      m.Snapshot.Term,
				From:      m.From,
			}
			r.events.SnapshotRejected(info)
		}
	}
	r.send(resp)
	return nil
}

func (r *raft) handleReplicateMessage(m pb.Message) error {
	resp := pb.Message{
		To:   m.From,
		Type: pb.ReplicateResp,
	}
	if m.LogIndex < r.log.committed {
		resp.LogIndex = r.log.committed
		r.send(resp)
		return nil
	}
	ok, err := r.log.matchTerm(m.LogIndex, m.LogTerm)
	if err != nil {
		return err
	}
	if ok {
		if _, err := r.log.tryAppend(m.LogIndex, m.Entries); err != nil {
			return err
		}
		lastIdx := m.LogIndex + uint64(len(m.Entries))
		r.log.commitTo(min(lastIdx, m.Commit))
		resp.LogIndex = lastIdx
	} else {
		plog.Debugf("%s rejected Replicate index %d term %d from %s",
			r.describe(), m.LogIndex, m.Term, ReplicaID(m.From))
		resp.Reject = true
		resp.LogIndex = m.LogIndex
		resp.Hint = r.log.lastIndex()
		if r.events != nil {
			info := server.ReplicationInfo{
				ShardID:   r.shardID,
				ReplicaID: r.replicaID,
				Index:     m.LogIndex,
				Term:      m.LogTerm,
				From:      m.From,
			}
			r.events.ReplicationRejected(info)
		}
	}
	r.send(resp)
	return nil
}

//
// Step related functions
//

func isPreVoteMessage(t pb.MessageType) bool {
	return t == pb.RequestPreVote || t == pb.RequestPreVoteResp
}

func isRequestVoteMessage(t pb.MessageType) bool {
	return t == pb.RequestVote || t == pb.RequestPreVote
}

func isRequestMessage(t pb.MessageType) bool {
	return t == pb.Propose || t == pb.ReadIndex || t == pb.LeaderTransfer
}

func isLeaderMessage(t pb.MessageType) bool {
	return t == pb.Replicate || t == pb.InstallSnapshot ||
		t == pb.Heartbeat || t == pb.TimeoutNow || t == pb.ReadIndexResp
}

func (r *raft) dropRequestVoteFromHighTermNode(m pb.Message) bool {
	if !isRequestVoteMessage(m.Type) || !r.checkQuorum || m.Term <= r.term {
		return false
	}
	// see p42 of the raft thesis
	if m.Hint == m.From {
		plog.Debugf("%s, RequestVote with leader transfer hint received from %s",
			r.describe(), ReplicaID(m.From))
		return false
	}
	if r.isLeader() && !r.quiesce && r.electionTick >= r.electionTimeout {
		panic("r.electionTick >= r.electionTimeout on leader")
	}
	// we got a RequestVote with higher term, but we recently had heartbeat msg
	// from leader within the minimum election timeout and that leader is known
	// to have quorum. we thus drop such RequestVote to minimize interruption by
	// network partitioned nodes with higher term.
	// this idea is from the last paragraph of the section 6 of the raft paper
	if r.leaderID != NoLeader && r.electionTick < r.electionTimeout {
		return true
	}
	return false
}

func isPreVoteMessageWithExpectedHigherTerm(m pb.Message) bool {
	return m.Type == pb.RequestPreVote ||
		(m.Type == pb.RequestPreVoteResp && !m.Reject)
}

// onMessageTermNotMatched handles the situation in which the incoming
// message has a term value different from local node's term. it returns a
// boolean flag indicating whether the message should be ignored.
// see the 3rd paragraph, section 5.1 of the raft paper for details.
func (r *raft) onMessageTermNotMatched(m pb.Message) bool {
	if m.Term == 0 || m.Term == r.term {
		return false
	}
	if r.dropRequestVoteFromHighTermNode(m) {
		plog.Warningf("%s dropped RequestVote at term %d from %s, leader available",
			r.describe(), m.Term, ReplicaID(m.From))
		return true
	}
	if m.Term > r.term {
		if !isPreVoteMessageWithExpectedHigherTerm(m) {
			plog.Warningf("%s received %s with higher term (%d) from %s",
				r.describe(), m.Type, m.Term, ReplicaID(m.From))
			leaderID := NoLeader
			if isLeaderMessage(m.Type) {
				leaderID = m.From
			}
			if r.isNonVoting() {
				r.becomeNonVoting(m.Term, leaderID)
			} else if r.isWitness() {
				r.becomeWitness(m.Term, leaderID)
			} else {
				if m.Type == pb.RequestVote {
					plog.Warningf("%s become followerKE after receiving higher term from %s",
						r.describe(), ReplicaID(m.From))
					// not to reset the electionTick value to avoid the risk of having the
					// local node not being to campaign at all. if the local node generates
					// the tick much slower than other nodes (e.g. bad config, hardware
					// clock issue, bad scheduling, overloaded etc.), it may lose the chance
					// to ever start a campaign unless we keep its electionTick value here.
					r.becomeFollowerKE(m.Term, leaderID)
				} else {
					plog.Warningf("%s become follower after receiving higher term from %s",
						r.describe(), ReplicaID(m.From))
					r.becomeFollower(m.Term, leaderID)
				}
			}
		}
	} else if m.Term < r.term {
		if m.Type == pb.RequestPreVote ||
			(isLeaderMessage(m.Type) && (r.checkQuorum || r.preVote)) {
			// see test TestFreeStuckCandidateWithCheckQuorum for details
			r.send(pb.Message{To: m.From, Type: pb.NoOP})
		} else {
			plog.Infof("%s ignored %s with lower term (%d) from %s",
				r.describe(), m.Type, m.Term, ReplicaID(m.From))
		}
		return true
	}
	return false
}

func (r *raft) inconsistentRaftConfig(m pb.Message) bool {
	return !r.preVote && isPreVoteMessage(m.Type)
}

func (r *raft) Handle(m pb.Message) error {
	if r.inconsistentRaftConfig(m) {
		panic("received preVote message when preVote is not enabled")
	}
	if !r.onMessageTermNotMatched(m) {
		if !isPreVoteMessage(m.Type) {
			r.doubleCheckTermMatched(m.Term)
		}
		return r.handle(r, m)
	}
	plog.Infof("%s dropped %s from %s, term %d, term not matched",
		r.describe(), m.Type, ReplicaID(m.From), m.Term)
	return nil
}

func (r *raft) hasConfigChangeToApply() bool {
	// this is a hack to make it easier to port etcd raft tests
	// check those *_etcd_test.go for details
	if r.hasNotAppliedConfigChange != nil {
		return r.hasNotAppliedConfigChange()
	}
	// TODO:
	// with the current entry log implementation, the simplification below is no
	// longer required, we can now actually scan the committed but not applied
	// portion of the log as they are now all in memory.
	return r.log.committed > r.getApplied()
}

func (r *raft) canGrantVote(m pb.Message) bool {
	return r.vote == NoNode || r.vote == m.From || m.Term > r.term
}

//
// handlers for nodes in any state
//

func (r *raft) handleNodeElection(m pb.Message) error {
	if !r.isLeader() {
		// there can be multiple pending membership change entries committed but not
		// applied on this node. say with a shard of X, Y and Z, there are two
		// such entries for adding node A and B are committed but not applied
		// available on X. If X is allowed to start a new election, it can become the
		// leader with a vote from any one of the node Y or Z. Further proposals made
		// by the new leader X in the next term will require a quorum of 2 which can
		// have no overlap with the committed quorum of 3. this violates the safety
		// requirement of raft.
		// ignore the Election message when there is membership configure change
		// committed but not applied
		if r.hasConfigChangeToApply() {
			plog.Warningf("%s campaign skipped, pending config change",
				r.describe())
			if r.events != nil {
				info := server.CampaignInfo{
					ShardID:   r.shardID,
					ReplicaID: r.replicaID,
					Term:      r.term,
				}
				r.events.CampaignSkipped(info)
			}
			return nil
		}
		// prevote is enabled, but the user explicitly requested the leadership to
		// be transferred, so skip the pre-vote stage
		if r.preVote && !r.isLeaderTransferTarget {
			plog.Debugf("%s will start a preVote campaign", r.describe())
			return r.preVoteCampaign()
		}
		plog.Debugf("%s will start a campaign", r.describe())
		return r.campaign()
	}
	plog.Debugf("%s is leader, ignored Election", r.describe())
	return nil
}

func (r *raft) handleNodeRequestPreVote(m pb.Message) error {
	resp := pb.Message{
		To:   m.From,
		Type: pb.RequestPreVoteResp,
	}
	isUpToDate, err := r.log.upToDate(m.LogIndex, m.LogTerm)
	if err != nil {
		return err
	}
	if m.Term < r.term {
		panic("m.term < r.term")
	}
	if m.Term > r.term && isUpToDate {
		resp.Term = m.Term
		plog.Warningf("%s cast preVote from %s index %d term %d, log term: %d",
			r.describe(), ReplicaID(m.From), m.LogIndex, m.Term, m.LogTerm)
	} else {
		// m.Term == r.term || !isUpToDate
		plog.Warningf("%s rejected preVote %s index %d term %d,logterm %d, utd %t",
			r.describe(), ReplicaID(m.From), m.LogIndex, m.Term, m.LogTerm, isUpToDate)
		resp.Term = r.term
		resp.Reject = true
	}
	r.send(resp)
	return nil
}

func (r *raft) handleNodeRequestVote(m pb.Message) error {
	resp := pb.Message{
		To:   m.From,
		Type: pb.RequestVoteResp,
	}
	// 3rd paragraph section 5.2 of the raft paper
	canGrant := r.canGrantVote(m)
	// 2nd paragraph section 5.4 of the raft paper
	isUpToDate, err := r.log.upToDate(m.LogIndex, m.LogTerm)
	if err != nil {
		return err
	}
	if canGrant && isUpToDate {
		plog.Warningf("%s cast vote from %s index %d term %d, log term: %d",
			r.describe(), ReplicaID(m.From), m.LogIndex, m.Term, m.LogTerm)
		r.electionTick = 0
		r.vote = m.From
	} else {
		plog.Warningf("%s rejected vote %s index%d term%d,logterm%d,grant%v,utd%v",
			r.describe(), ReplicaID(m.From), m.LogIndex, m.Term,
			m.LogTerm, canGrant, isUpToDate)
		resp.Reject = true
	}
	r.send(resp)
	return nil
}

func (r *raft) handleNodeConfigChange(m pb.Message) error {
	if m.Reject {
		r.clearPendingConfigChange()
	} else {
		cctype := (pb.ConfigChangeType)(m.HintHigh)
		nodeid := m.Hint
		switch cctype {
		case pb.AddNode:
			r.addNode(nodeid)
		case pb.RemoveNode:
			r.removeNode(nodeid)
		case pb.AddNonVoting:
			r.addNonVoting(nodeid)
		case pb.AddWitness:
			r.addWitness(nodeid)
		default:
			panic("unexpected config change type")
		}
	}

	return nil
}

func (r *raft) handleLogQuery(m pb.Message) error {
	if r.logQueryResult == nil {
		entries, err := r.log.getCommittedEntries(m.From, m.To, m.Hint)
		r.logQueryResult = &pb.LogQueryResult{
			FirstIndex: r.log.firstIndex(),
			LastIndex:  r.log.committed + 1,
			Error:      err,
			Entries:    entries,
		}
	} else {
		panic("log query result is not nil")
	}
	return nil
}

func (r *raft) handleLocalTick(m pb.Message) error {
	if m.Reject {
		r.quiescedTick()
		return nil
	}
	return r.tick()
}

func (r *raft) handleRestoreRemote(m pb.Message) error {
	r.restoreRemotes(m.Snapshot)
	return nil
}

//
// message handler functions used by leader
//

func (r *raft) handleLeaderHeartbeat(m pb.Message) error {
	r.broadcastHeartbeatMessage()
	return nil
}

// p69 of the raft thesis
func (r *raft) handleLeaderCheckQuorum(m pb.Message) error {
	r.mustBeLeader()
	if !r.leaderHasQuorum() {
		plog.Warningf("%s has lost quorum", r.describe())
		r.becomeFollower(r.term, NoLeader)
	}
	return nil
}

func (r *raft) handleLeaderPropose(m pb.Message) error {
	r.mustBeLeader()
	if r.leaderTransfering() {
		plog.Warningf("%s dropped proposal, leader transferring", r.describe())
		r.reportDroppedProposal(m)
		return nil
	}
	for i, e := range m.Entries {
		if e.Type == pb.ConfigChangeEntry {
			if r.hasPendingConfigChange() {
				plog.Warningf("%s dropped config change, pending change", r.describe())
				r.reportDroppedConfigChange(m.Entries[i])
				m.Entries[i] = pb.Entry{Type: pb.ApplicationEntry}
			}
			r.setPendingConfigChange()
		}
	}
	r.appendEntries(m.Entries)
	r.broadcastReplicateMessage()

	return nil
}

// p72 of the raft thesis
func (r *raft) hasCommittedEntryAtCurrentTerm() bool {
	if r.term == 0 {
		panic("not suppose to reach here")
	}
	lastCommittedTerm, err := r.log.term(r.log.committed)
	if err != nil && !errors.Is(err, ErrCompacted) {
		plog.Panicf("%s failed to get term, %v", r.describe(), err)
	}
	return lastCommittedTerm == r.term
}

func (r *raft) clearReadyToRead() {
	r.readyToRead = r.readyToRead[:0]
}

func (r *raft) addReadyToRead(index uint64, ctx pb.SystemCtx) {
	r.readyToRead = append(r.readyToRead,
		pb.ReadyToRead{
			Index:     index,
			SystemCtx: ctx,
		})
}

// section 6.4 of the raft thesis
func (r *raft) handleLeaderReadIndex(m pb.Message) error {
	r.mustBeLeader()
	ctx := pb.SystemCtx{
		High: m.HintHigh,
		Low:  m.Hint,
	}
	if _, wok := r.witnesses[m.From]; wok {
		plog.Errorf("%s dropped ReadIndex, witness node %d", r.describe(), m.From)
	} else if !r.isSingleNodeQuorum() {
		if !r.hasCommittedEntryAtCurrentTerm() {
			// leader doesn't know the commit value of the shard
			// see raft thesis section 6.4, this is the first step of the ReadIndex
			// protocol.
			plog.Warningf("%s dropped ReadIndex, not ready", r.describe())
			r.reportDroppedReadIndex(m)
			return nil
		}
		r.readIndex.addRequest(r.log.committed, ctx, m.From)
		r.broadcastHeartbeatMessageWithHint(ctx)
	} else {
		r.addReadyToRead(r.log.committed, ctx)
		_, ook := r.nonVotings[m.From]
		if m.From != r.replicaID && ook {
			r.send(pb.Message{
				To:       m.From,
				Type:     pb.ReadIndexResp,
				LogIndex: r.log.committed,
				Hint:     m.Hint,
				HintHigh: m.HintHigh,
				Commit:   m.Commit,
			})
		}
	}
	return nil
}

func (r *raft) handleLeaderReplicateResp(m pb.Message, rp *remote) error {
	r.mustBeLeader()
	rp.setActive()
	if !m.Reject {
		paused := rp.isPaused()
		if rp.tryUpdate(m.LogIndex) {
			rp.respondedTo()
			if ok := r.tryCommit(); ok {
				r.broadcastReplicateMessage()
			} else if paused {
				r.sendReplicateMessage(m.From)
			}
			// according to the leadership transfer protocol listed on the p29 of the
			// raft thesis
			if r.leaderTransfering() && m.From == r.leaderTransferTarget &&
				r.log.lastIndex() == rp.match {
				r.sendTimeoutNowMessage(r.leaderTransferTarget)
			}
		}
	} else {
		// the replication flow control code is derived from etcd raft, it resets
		// nextIndex to match + 1. it is thus even more conservative than the raft
		// thesis's approach of nextIndex = nextIndex - 1 mentioned on the p21 of
		// the thesis.
		if rp.decreaseTo(m.LogIndex, m.Hint) {
			r.enterRetryState(rp)
			r.sendReplicateMessage(m.From)
		}
	}
	return nil
}

func (r *raft) handleLeaderHeartbeatResp(m pb.Message, rp *remote) error {
	r.mustBeLeader()
	rp.setActive()
	rp.waitToRetry()
	if rp.match < r.log.lastIndex() {
		r.sendReplicateMessage(m.From)
	}
	// heartbeat response contains leadership confirmation requested as part of
	// the ReadIndex protocol.
	if m.Hint != 0 {
		r.handleReadIndexLeaderConfirmation(m)
	}
	return nil
}

func (r *raft) handleLeaderTransfer(m pb.Message) error {
	r.mustBeLeader()
	target := m.Hint
	plog.Debugf("%s called handleLeaderTransfer, target %d", r.describe(), target)
	if target == NoNode {
		plog.Panicf("%s leader transfer target not set", r.describe())
	}
	if r.leaderTransfering() {
		plog.Warningf("LeaderTransfer ignored, leader transfer is ongoing")
		return nil
	}
	if r.replicaID == target {
		plog.Warningf("received LeaderTransfer with target pointing to itself")
		return nil
	}
	rp, ok := r.remotes[target]
	if !ok {
		plog.Warningf("unknown LeaderTransfer target")
		return nil
	}
	r.leaderTransferTarget = target
	r.electionTick = 0
	// fast path below
	// or wait for the target node to catch up, see p29 of the raft thesis
	if rp.match == r.log.lastIndex() {
		r.sendTimeoutNowMessage(target)
	}
	return nil
}

func (r *raft) handleReadIndexLeaderConfirmation(m pb.Message) {
	ctx := pb.SystemCtx{
		Low:  m.Hint,
		High: m.HintHigh,
	}
	ris := r.readIndex.confirm(ctx, m.From, r.quorum())
	for _, s := range ris {
		if s.from == NoNode || s.from == r.replicaID {
			r.addReadyToRead(s.index, s.ctx)
		} else {
			r.send(pb.Message{
				To:       s.from,
				Type:     pb.ReadIndexResp,
				LogIndex: s.index,
				Hint:     m.Hint,
				HintHigh: m.HintHigh,
			})
		}
	}
}

func (r *raft) handleLeaderSnapshotStatus(m pb.Message, rp *remote) error {
	if rp.state != remoteSnapshot {
		return nil
	}
	if m.Hint == 0 {
		if m.Reject {
			rp.clearPendingSnapshot()
			plog.Warningf("%s snapshot failed, %s is now in wait state",
				r.describe(), ReplicaID(m.From))
		} else {
			plog.Debugf("%s snapshot succeeded, %s in wait state now, next %d",
				r.describe(), ReplicaID(m.From), rp.next)
		}
		rp.becomeWait()
	} else {
		rp.setSnapshotAck(m.Hint, m.Reject)
		r.snapshotting = true
	}
	return nil
}

func (r *raft) handleLeaderUnreachable(m pb.Message, rp *remote) error {
	plog.Debugf("%s received Unreachable, %s entered retry state",
		r.describe(), ReplicaID(m.From))
	r.enterRetryState(rp)
	return nil
}

func (r *raft) handleLeaderRateLimit(m pb.Message) error {
	if r.rl.Enabled() {
		r.rl.SetFollowerState(m.From, m.Hint)
	} else {
		plog.Warningf("%s dropped rate limit msg, rl disabled", r.describe())
	}
	return nil
}

func (r *raft) enterRetryState(rp *remote) {
	if rp.state == remoteReplicate {
		rp.becomeRetry()
	}
}

func (r *raft) checkPendingSnapshotAck() error {
	if r.isLeader() && r.snapshotting {
		check := func(m map[uint64]*remote) error {
			for from, rp := range m {
				if rp.state == remoteSnapshot {
					if rp.delayed.tick() {
						if err := r.Handle(pb.Message{
							Type:   pb.SnapshotStatus,
							From:   from,
							Reject: rp.delayed.rejected,
							Hint:   0,
						}); err != nil {
							return err
						}
						rp.clearSnapshotAck()
					} else {
						r.snapshotting = true
					}
				}
			}
			return nil
		}
		r.snapshotting = false
		if err := check(r.remotes); err != nil {
			return err
		}
		if err := check(r.nonVotings); err != nil {
			return err
		}
		if err := check(r.witnesses); err != nil {
			return err
		}
	}
	return nil
}

//
// message handlers used by nonVoting, re-route them to follower handlers
//

func (r *raft) handleNonVotingReplicate(m pb.Message) error {
	return r.handleFollowerReplicate(m)
}

func (r *raft) handleNonVotingHeartbeat(m pb.Message) error {
	return r.handleFollowerHeartbeat(m)
}

func (r *raft) handleNonVotingSnapshot(m pb.Message) error {
	return r.handleFollowerInstallSnapshot(m)
}

func (r *raft) handleNonVotingPropose(m pb.Message) error {
	return r.handleFollowerPropose(m)
}

func (r *raft) handleNonVotingReadIndex(m pb.Message) error {
	return r.handleFollowerReadIndex(m)
}

func (r *raft) handleNonVotingReadIndexResp(m pb.Message) error {
	return r.handleFollowerReadIndexResp(m)
}

//
// message handlers used by witness, re-route them to follower handlers
//

func (r *raft) handleWitnessReplicate(m pb.Message) error {
	return r.handleFollowerReplicate(m)
}

func (r *raft) handleWitnessHeartbeat(m pb.Message) error {
	return r.handleFollowerHeartbeat(m)
}

func (r *raft) handleWitnessSnapshot(m pb.Message) error {
	return r.handleFollowerInstallSnapshot(m)
}

//
// message handlers used by follower
//

func (r *raft) handleFollowerPropose(m pb.Message) error {
	if r.leaderID == NoLeader {
		plog.Warningf("%s dropped proposal, no leader", r.describe())
		r.reportDroppedProposal(m)
		return nil
	}
	m.To = r.leaderID
	// the message might be queued by the transport layer, this violates the
	// requirement of the entryQueue.get() func. copy the m.Entries to its
	// own space.
	m.Entries = newEntrySlice(m.Entries)
	r.send(m)
	return nil
}

func (r *raft) leaderIsAvailable() {
	r.electionTick = 0
}

func (r *raft) handleFollowerReplicate(m pb.Message) error {
	r.leaderIsAvailable()
	r.setLeaderID(m.From)
	return r.handleReplicateMessage(m)
}

func (r *raft) handleFollowerHeartbeat(m pb.Message) error {
	r.leaderIsAvailable()
	r.setLeaderID(m.From)
	return r.handleHeartbeatMessage(m)
}

func (r *raft) handleFollowerReadIndex(m pb.Message) error {
	if r.leaderID == NoLeader {
		plog.Warningf("%s dropped ReadIndex, no leader", r.describe())
		r.reportDroppedReadIndex(m)
		return nil
	}
	m.To = r.leaderID
	r.send(m)
	return nil
}

func (r *raft) handleFollowerLeaderTransfer(m pb.Message) error {
	if r.leaderID == NoLeader {
		plog.Warningf("%s dropped LeaderTransfer, no leader", r.describe())
		return nil
	}
	m.To = r.leaderID
	r.send(m)
	return nil
}

func (r *raft) handleFollowerReadIndexResp(m pb.Message) error {
	ctx := pb.SystemCtx{
		Low:  m.Hint,
		High: m.HintHigh,
	}
	r.leaderIsAvailable()
	r.setLeaderID(m.From)
	r.addReadyToRead(m.LogIndex, ctx)
	return nil
}

func (r *raft) handleFollowerInstallSnapshot(m pb.Message) error {
	r.leaderIsAvailable()
	r.setLeaderID(m.From)
	return r.handleInstallSnapshotMessage(m)
}

func (r *raft) handleFollowerTimeoutNow(m pb.Message) error {
	// the last paragraph, p29 of the raft thesis mentions that this is nothing
	// different from the clock moving forward quickly
	plog.Debugf("%s TimeoutNow received", r.describe())
	r.electionTick = r.randomizedElectionTimeout
	r.isLeaderTransferTarget = true
	if err := r.tick(); err != nil {
		return err
	}
	if r.isLeaderTransferTarget {
		r.isLeaderTransferTarget = false
	}
	return nil
}

//
// handler functions used by candidate
//

func (r *raft) doubleCheckTermMatched(msgTerm uint64) {
	if msgTerm != 0 && r.term != msgTerm {
		plog.Panicf("%s mismatched term found", r.describe())
	}
}

func (r *raft) handleCandidatePropose(m pb.Message) error {
	plog.Warningf("%s dropped proposal, no leader", r.describe())
	r.reportDroppedProposal(m)
	return nil
}

func (r *raft) handleCandidateReadIndex(m pb.Message) error {
	plog.Warningf("%s dropped read index, no leader", r.describe())
	r.reportDroppedReadIndex(m)
	ctx := pb.SystemCtx{
		Low:  m.Hint,
		High: m.HintHigh,
	}
	r.droppedReadIndexes = append(r.droppedReadIndexes, ctx)
	return nil
}

// when any of the following three methods
// handleCandidateReplicate
// handleCandidateInstallSnapshot
// handleCandidateHeartbeat
// is called, it implies that m.Term == r.term and there is a leader
// for that term. see 4th paragraph section 5.2 of the raft paper
func (r *raft) handleCandidateReplicate(m pb.Message) error {
	r.becomeFollower(r.term, m.From)
	return r.handleReplicateMessage(m)
}

func (r *raft) handleCandidateInstallSnapshot(m pb.Message) error {
	r.becomeFollower(r.term, m.From)
	return r.handleInstallSnapshotMessage(m)
}

func (r *raft) handleCandidateHeartbeat(m pb.Message) error {
	r.becomeFollower(r.term, m.From)
	return r.handleHeartbeatMessage(m)
}

func (r *raft) handleCandidateRequestVoteResp(m pb.Message) error {
	if _, ok := r.nonVotings[m.From]; ok {
		plog.Warningf("dropped RequestVoteResp from nonVoting")
		return nil
	}
	count := r.handleVoteResp(m.From, m.Reject, false)
	plog.Warningf("%s received %d votes and %d rejections, quorum is %d",
		r.describe(), count, len(r.votes)-count, r.quorum())
	// 3rd paragraph section 5.2 of the raft paper
	if count == r.quorum() {
		r.becomeLeader()
		// get the NoOP entry committed ASAP
		r.broadcastReplicateMessage()
	} else if len(r.votes)-count == r.quorum() {
		// etcd raft does this, it is not stated in the raft paper
		r.becomeFollower(r.term, NoLeader)
	}
	return nil
}

//
// handler functions for preVote candidate
//

func (r *raft) handlePreVoteCandidateRequestPreVoteResp(m pb.Message) error {
	if _, ok := r.nonVotings[m.From]; ok {
		plog.Warningf("dropped RequestPreVoteResp from nonVoting")
		return nil
	}
	count := r.handleVoteResp(m.From, m.Reject, true)
	plog.Warningf("%s received %d preVotes and %d rejections, quorum is %d",
		r.describe(), count, len(r.votes)-count, r.quorum())
	if count == r.quorum() {
		if err := r.campaign(); err != nil {
			return err
		}
	} else if len(r.votes)-count == r.quorum() {
		// etcd raft does this, it is not stated in the raft paper
		r.becomeFollower(r.term, NoLeader)
	}
	return nil
}

func (r *raft) reportDroppedConfigChange(e pb.Entry) {
	r.droppedEntries = append(r.droppedEntries, e)
}

func (r *raft) reportDroppedProposal(m pb.Message) {
	r.droppedEntries = append(r.droppedEntries, newEntrySlice(m.Entries)...)
	if r.events != nil {
		info := server.ProposalInfo{
			ShardID:   r.shardID,
			ReplicaID: r.replicaID,
			Entries:   m.Entries,
		}
		r.events.ProposalDropped(info)
	}
}

func (r *raft) reportDroppedReadIndex(m pb.Message) {
	sysctx := pb.SystemCtx{
		Low:  m.Hint,
		High: m.HintHigh,
	}
	r.droppedReadIndexes = append(r.droppedReadIndexes, sysctx)
	if r.events != nil {
		info := server.ReadIndexInfo{
			ShardID:   r.shardID,
			ReplicaID: r.replicaID,
		}
		r.events.ReadIndexDropped(info)
	}
}

func lw(r *raft, f func(m pb.Message, rp *remote) error) handlerFunc {
	w := func(nm pb.Message) error {
		if npr, ok := r.remotes[nm.From]; ok {
			return f(nm, npr)
		} else if nob, ok := r.nonVotings[nm.From]; ok {
			return f(nm, nob)
		} else if wob, ok := r.witnesses[nm.From]; ok {
			return f(nm, wob)
		} else {
			plog.Warningf("%s no remote for %s", r.describe(), ReplicaID(nm.From))
			return nil
		}
	}
	return w
}

func defaultHandle(r *raft, m pb.Message) error {
	if f := r.handlers[r.state][m.Type]; f != nil {
		return f(m)
	}
	return nil
}

func (r *raft) initializeHandlerMap() {
	// candidate
	r.handlers[candidate][pb.Heartbeat] = r.handleCandidateHeartbeat
	r.handlers[candidate][pb.Propose] = r.handleCandidatePropose
	r.handlers[candidate][pb.ReadIndex] = r.handleCandidateReadIndex
	r.handlers[candidate][pb.Replicate] = r.handleCandidateReplicate
	r.handlers[candidate][pb.InstallSnapshot] = r.handleCandidateInstallSnapshot
	r.handlers[candidate][pb.RequestVoteResp] = r.handleCandidateRequestVoteResp
	r.handlers[candidate][pb.Election] = r.handleNodeElection
	r.handlers[candidate][pb.RequestVote] = r.handleNodeRequestVote
	r.handlers[candidate][pb.RequestPreVote] = r.handleNodeRequestPreVote
	r.handlers[candidate][pb.ConfigChangeEvent] = r.handleNodeConfigChange
	r.handlers[candidate][pb.LocalTick] = r.handleLocalTick
	r.handlers[candidate][pb.SnapshotReceived] = r.handleRestoreRemote
	r.handlers[candidate][pb.LogQuery] = r.handleLogQuery
	// prevote candidate
	r.handlers[preVoteCandidate][pb.Heartbeat] = r.handleCandidateHeartbeat
	r.handlers[preVoteCandidate][pb.Propose] = r.handleCandidatePropose
	r.handlers[preVoteCandidate][pb.ReadIndex] = r.handleCandidateReadIndex
	r.handlers[preVoteCandidate][pb.Replicate] = r.handleCandidateReplicate
	r.handlers[preVoteCandidate][pb.InstallSnapshot] = r.handleCandidateInstallSnapshot
	r.handlers[preVoteCandidate][pb.RequestPreVoteResp] = r.handlePreVoteCandidateRequestPreVoteResp
	r.handlers[preVoteCandidate][pb.Election] = r.handleNodeElection
	r.handlers[preVoteCandidate][pb.RequestVote] = r.handleNodeRequestVote
	r.handlers[preVoteCandidate][pb.RequestPreVote] = r.handleNodeRequestPreVote
	r.handlers[preVoteCandidate][pb.ConfigChangeEvent] = r.handleNodeConfigChange
	r.handlers[preVoteCandidate][pb.LocalTick] = r.handleLocalTick
	r.handlers[preVoteCandidate][pb.SnapshotReceived] = r.handleRestoreRemote
	r.handlers[preVoteCandidate][pb.LogQuery] = r.handleLogQuery
	// follower
	r.handlers[follower][pb.Propose] = r.handleFollowerPropose
	r.handlers[follower][pb.Replicate] = r.handleFollowerReplicate
	r.handlers[follower][pb.Heartbeat] = r.handleFollowerHeartbeat
	r.handlers[follower][pb.ReadIndex] = r.handleFollowerReadIndex
	r.handlers[follower][pb.LeaderTransfer] = r.handleFollowerLeaderTransfer
	r.handlers[follower][pb.ReadIndexResp] = r.handleFollowerReadIndexResp
	r.handlers[follower][pb.InstallSnapshot] = r.handleFollowerInstallSnapshot
	r.handlers[follower][pb.Election] = r.handleNodeElection
	r.handlers[follower][pb.RequestVote] = r.handleNodeRequestVote
	r.handlers[follower][pb.RequestPreVote] = r.handleNodeRequestPreVote
	r.handlers[follower][pb.TimeoutNow] = r.handleFollowerTimeoutNow
	r.handlers[follower][pb.ConfigChangeEvent] = r.handleNodeConfigChange
	r.handlers[follower][pb.LocalTick] = r.handleLocalTick
	r.handlers[follower][pb.SnapshotReceived] = r.handleRestoreRemote
	r.handlers[follower][pb.LogQuery] = r.handleLogQuery
	// leader
	r.handlers[leader][pb.LeaderHeartbeat] = r.handleLeaderHeartbeat
	r.handlers[leader][pb.CheckQuorum] = r.handleLeaderCheckQuorum
	r.handlers[leader][pb.Propose] = r.handleLeaderPropose
	r.handlers[leader][pb.ReadIndex] = r.handleLeaderReadIndex
	r.handlers[leader][pb.ReplicateResp] = lw(r, r.handleLeaderReplicateResp)
	r.handlers[leader][pb.HeartbeatResp] = lw(r, r.handleLeaderHeartbeatResp)
	r.handlers[leader][pb.SnapshotStatus] = lw(r, r.handleLeaderSnapshotStatus)
	r.handlers[leader][pb.Unreachable] = lw(r, r.handleLeaderUnreachable)
	r.handlers[leader][pb.LeaderTransfer] = r.handleLeaderTransfer
	r.handlers[leader][pb.Election] = r.handleNodeElection
	r.handlers[leader][pb.RequestVote] = r.handleNodeRequestVote
	r.handlers[leader][pb.RequestPreVote] = r.handleNodeRequestPreVote
	r.handlers[leader][pb.ConfigChangeEvent] = r.handleNodeConfigChange
	r.handlers[leader][pb.LocalTick] = r.handleLocalTick
	r.handlers[leader][pb.SnapshotReceived] = r.handleRestoreRemote
	r.handlers[leader][pb.RateLimit] = r.handleLeaderRateLimit
	r.handlers[leader][pb.LogQuery] = r.handleLogQuery
	// nonVoting
	r.handlers[nonVoting][pb.Heartbeat] = r.handleNonVotingHeartbeat
	r.handlers[nonVoting][pb.Replicate] = r.handleNonVotingReplicate
	r.handlers[nonVoting][pb.InstallSnapshot] = r.handleNonVotingSnapshot
	r.handlers[nonVoting][pb.RequestVote] = r.handleNodeRequestVote
	r.handlers[nonVoting][pb.RequestPreVote] = r.handleNodeRequestPreVote
	r.handlers[nonVoting][pb.Propose] = r.handleNonVotingPropose
	r.handlers[nonVoting][pb.ReadIndex] = r.handleNonVotingReadIndex
	r.handlers[nonVoting][pb.ReadIndexResp] = r.handleNonVotingReadIndexResp
	r.handlers[nonVoting][pb.ConfigChangeEvent] = r.handleNodeConfigChange
	r.handlers[nonVoting][pb.LocalTick] = r.handleLocalTick
	r.handlers[nonVoting][pb.SnapshotReceived] = r.handleRestoreRemote
	r.handlers[nonVoting][pb.LogQuery] = r.handleLogQuery
	// witness
	r.handlers[witness][pb.Heartbeat] = r.handleWitnessHeartbeat
	r.handlers[witness][pb.Replicate] = r.handleWitnessReplicate
	r.handlers[witness][pb.InstallSnapshot] = r.handleWitnessSnapshot
	r.handlers[witness][pb.RequestVote] = r.handleNodeRequestVote
	r.handlers[witness][pb.RequestPreVote] = r.handleNodeRequestPreVote
	r.handlers[witness][pb.ConfigChangeEvent] = r.handleNodeConfigChange
	r.handlers[witness][pb.LocalTick] = r.handleLocalTick
	r.handlers[witness][pb.SnapshotReceived] = r.handleRestoreRemote
}

func (r *raft) checkHandlerMap() {
	// following states/types are not supposed to have handler filled in
	checks := []struct {
		stateType State
		msgType   pb.MessageType
	}{
		{leader, pb.Heartbeat},
		{leader, pb.Replicate},
		{leader, pb.InstallSnapshot},
		{leader, pb.ReadIndexResp},
		{leader, pb.RequestPreVoteResp},
		{follower, pb.ReplicateResp},
		{follower, pb.HeartbeatResp},
		{follower, pb.SnapshotStatus},
		{follower, pb.Unreachable},
		{follower, pb.RequestPreVoteResp},
		{candidate, pb.ReplicateResp},
		{candidate, pb.HeartbeatResp},
		{candidate, pb.SnapshotStatus},
		{candidate, pb.Unreachable},
		{candidate, pb.RequestPreVoteResp},
		{preVoteCandidate, pb.ReplicateResp},
		{preVoteCandidate, pb.HeartbeatResp},
		{preVoteCandidate, pb.SnapshotStatus},
		{preVoteCandidate, pb.Unreachable},
		{nonVoting, pb.Election},
		{nonVoting, pb.RequestVoteResp},
		{nonVoting, pb.ReplicateResp},
		{nonVoting, pb.HeartbeatResp},
		{nonVoting, pb.RequestPreVoteResp},
		{witness, pb.Election},
		{witness, pb.Propose},
		{witness, pb.ReadIndex},
		{witness, pb.ReadIndexResp},
		{witness, pb.RequestVoteResp},
		{witness, pb.ReplicateResp},
		{witness, pb.HeartbeatResp},
		{witness, pb.RequestPreVoteResp},
		{witness, pb.LogQuery},
	}
	for _, tt := range checks {
		f := r.handlers[tt.stateType][tt.msgType]
		if f != nil {
			panic("unexpected msg handler")
		}
	}
}
````

## File: internal/raft/readindex_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raft

import (
	"testing"

	"github.com/lni/dragonboat/v4/raftpb"
)

func getTestSystemCtx(v uint64) raftpb.SystemCtx {
	return raftpb.SystemCtx{
		Low:  v,
		High: v + 1,
	}
}

func TestSameCtxCanNotBeAddedTwice(t *testing.T) {
	r := newReadIndex()
	r.addRequest(1, getTestSystemCtx(10001), 1)
	if len(r.pending) != 1 {
		t.Errorf("unexpected pending count %d", len(r.pending))
	}
	r.addRequest(2, getTestSystemCtx(10001), 2)
	if len(r.pending) != 1 {
		t.Errorf("added twice")
	}
}

func TestInconsistentPendingQueue(t *testing.T) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not triggered")
	}()
	r := newReadIndex()
	r.addRequest(1, getTestSystemCtx(10001), 1)
	r.queue = append(r.queue, getTestSystemCtx(10003))
	r.addRequest(2, getTestSystemCtx(10002), 2)
}

func TestReadIndexRequestCanBeAdded(t *testing.T) {
	r := newReadIndex()
	r.addRequest(1, getTestSystemCtx(10001), 1)
	r.addRequest(2, getTestSystemCtx(10002), 2)
	if !r.hasPendingRequest() {
		t.Errorf("hasPendingRequest %t, want true", r.hasPendingRequest())
	}
	if len(r.queue) != 2 || len(r.pending) != 2 {
		t.Errorf("request not recorded")
	}
	p, ok := r.pending[getTestSystemCtx(10002)]
	if !ok {
		t.Errorf("pending request 2 not recorded")
	}
	if p.index != 2 {
		t.Errorf("index %d, want 2", p.index)
	}
	if p.from != 2 {
		t.Errorf("from %d, want 2", p.from)
	}
	if p.ctx != getTestSystemCtx(10002) {
		t.Errorf("ctx %v, want 10002", p.ctx)
	}
	ctx := r.peepCtx()
	if ctx != getTestSystemCtx(10002) {
		t.Errorf("ctx %v, want 10002", ctx)
	}
}

func TestReadIndexChecksInputIndex(t *testing.T) {
	defer func() {
		checked := false
		if r := recover(); r != nil {
			checked = true
		}
		if !checked {
			t.Error("did not check input index")
		}
	}()

	r := newReadIndex()
	ctx := getTestSystemCtx(10001)
	ctx2 := getTestSystemCtx(10002)
	ctx3 := getTestSystemCtx(10003)
	r.addRequest(3, ctx, 1)
	r.addRequest(5, ctx2, 3)
	r.addRequest(4, ctx3, 2)
}

func TestAddConfirmationChecksInconsistentPendingQueue(t *testing.T) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not triggered")
	}()
	r := newReadIndex()
	ctx := getTestSystemCtx(10001)
	ctx2 := getTestSystemCtx(10002)
	ctx3 := getTestSystemCtx(10003)
	r.addRequest(3, ctx2, 1)
	r.addRequest(4, ctx, 3)
	r.addRequest(5, ctx3, 2)
	q := r.queue
	r.queue = append([]raftpb.SystemCtx{}, getTestSystemCtx(10004))
	r.queue = append(r.queue, q...)
	r.confirm(ctx, 1, 3)
	r.confirm(ctx, 3, 3)
}

func TestReadIndexLeaderCanBeConfirmed(t *testing.T) {
	r := newReadIndex()
	ctx := getTestSystemCtx(10001)
	ctx2 := getTestSystemCtx(10002)
	ctx3 := getTestSystemCtx(10003)
	r.addRequest(3, ctx2, 1)
	r.addRequest(4, ctx, 3)
	r.addRequest(5, ctx3, 2)
	ris := r.confirm(ctx, 1, 3)
	if ris != nil {
		t.Errorf("ris confirmed too early")
	}
	ris = r.confirm(ctx, 3, 3)
	if len(ris) != 2 {
		t.Fatalf("failed to confirm leader in ReadIndex")
	}
	if ris[1].index != 4 {
		t.Errorf("index %d, want 4", ris[1].index)
	}
	if ris[1].from != 3 {
		t.Errorf("from %d, want 3", ris[1].from)
	}
	if ris[1].ctx != ctx {
		t.Errorf("ctx %v, want %v", ris[1].ctx, ctx)
	}
	if ris[0].index != 4 {
		t.Errorf("index %d, want 4", ris[0].index)
	}
	if ris[0].from != 1 {
		t.Errorf("from %d, want 1", ris[0].from)
	}
	if ris[0].ctx != getTestSystemCtx(10002) {
		t.Errorf("ctx %d, want %v", ris[0].ctx, getTestSystemCtx(10002))
	}
	if len(r.pending) != 1 || len(r.queue) != 1 {
		t.Errorf("cofirmed requet not removed")
	}
}

func TestReadIndexIsResetAfterRaftStateChange(t *testing.T) {
	r := newTestRaft(1, []uint64{1, 2, 3}, 10, 1, NewTestLogDB())
	r.readIndex.addRequest(3, getTestSystemCtx(10001), 1)
	if len(r.readIndex.queue) != 1 || len(r.readIndex.pending) != 1 {
		t.Errorf("add request failed")
	}
	r.reset(2, true)
	if len(r.readIndex.queue) != 0 || len(r.readIndex.pending) != 0 {
		t.Errorf("readIndex not reset after raft reset")
	}
}
````

## File: internal/raft/readindex.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raft

import (
	"github.com/lni/dragonboat/v4/raftpb"
)

type readStatus struct {
	confirmed map[uint64]struct{}
	ctx       raftpb.SystemCtx
	index     uint64
	from      uint64
}

// readIndex is the struct that implements the ReadIndex protocol described in
// section 6.4 (with the idea in section 6.4.1 excluded) of Diego Ongaro's PhD
// thesis.
type readIndex struct {
	pending map[raftpb.SystemCtx]*readStatus
	queue   []raftpb.SystemCtx
}

func newReadIndex() *readIndex {
	return &readIndex{
		pending: make(map[raftpb.SystemCtx]*readStatus),
		queue:   make([]raftpb.SystemCtx, 0),
	}
}

func (r *readIndex) addRequest(index uint64,
	ctx raftpb.SystemCtx, from uint64) {
	if _, ok := r.pending[ctx]; ok {
		return
	}
	// index is the committed value of the shard, it should never move
	// backward, check it here
	if len(r.queue) > 0 {
		p, ok := r.pending[r.peepCtx()]
		if !ok {
			panic("inconsistent pending and queue")
		}
		if index < p.index {
			plog.Panicf("index moved backward in readIndex, %d:%d",
				index, p.index)
		}
	}
	r.queue = append(r.queue, ctx)
	r.pending[ctx] = &readStatus{
		index:     index,
		from:      from,
		ctx:       ctx,
		confirmed: make(map[uint64]struct{}),
	}
}

func (r *readIndex) hasPendingRequest() bool {
	return len(r.queue) > 0
}

func (r *readIndex) peepCtx() raftpb.SystemCtx {
	return r.queue[len(r.queue)-1]
}

func (r *readIndex) confirm(ctx raftpb.SystemCtx,
	from uint64, quorum int) []*readStatus {
	p, ok := r.pending[ctx]
	if !ok {
		return nil
	}
	p.confirmed[from] = struct{}{}
	if len(p.confirmed)+1 < quorum {
		return nil
	}
	done := 0
	cs := []*readStatus{}
	for _, pctx := range r.queue {
		done++
		s, ok := r.pending[pctx]
		if !ok {
			panic("inconsistent pending and queue content")
		}
		cs = append(cs, s)
		if pctx == ctx {
			for _, v := range cs {
				if v.index > s.index {
					panic("v.index > s.index is unexpected")
				}
				// re-write the index for extra safety.
				// we don't know what we don't know.
				v.index = s.index
			}
			r.queue = r.queue[done:]
			for _, v := range cs {
				delete(r.pending, v.ctx)
			}
			if len(r.queue) != len(r.pending) {
				panic("inconsistent length")
			}
			return cs
		}
	}
	return nil
}
````

## File: internal/raft/remote_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raft

import (
	"reflect"
	"testing"
)

func TestSnapshotAckTick(t *testing.T) {
	a := snapshotAck{ctick: 10}
	for i := 0; i < 10; i++ {
		r := a.tick()
		if (i == 9) != r {
			t.Errorf("unexpected tick result, i %d, tick %d, r %t", i, a.ctick, r)
		}
	}
}

func TestRemoteClearSnapshotAck(t *testing.T) {
	r := remote{delayed: snapshotAck{ctick: 10, rejected: true}}
	r.clearSnapshotAck()
	if r.delayed.ctick != 0 || r.delayed.rejected {
		t.Errorf("snapshot ack not cleared")
	}
}

func TestRemoteSetSnapshotAck(t *testing.T) {
	r := remote{
		state:   remoteSnapshot,
		delayed: snapshotAck{ctick: 10, rejected: true},
	}
	r.setSnapshotAck(20, false)
	if r.delayed.ctick != 20 || r.delayed.rejected {
		t.Errorf("setSnapshotAck failed to set values")
	}
}

func TestSetSnapshotAckWhenNotInSnapshotStateIsNotAllowed(t *testing.T) {
	defer func() {
		if r := recover(); r == nil {
			t.Errorf("failed to trigger panic")
		}
	}()
	r := remote{}
	r.setSnapshotAck(20, false)
}

func TestRemoteString(t *testing.T) {
	for _, tt := range []remoteStateType{remoteRetry,
		remoteReplicate, remoteSnapshot} {
		if tt.String() != remoteNames[uint64(tt)] {
			t.Errorf("unexpected string name")
		}
	}
}

func TestRemoteReset(t *testing.T) {
	r := &remote{
		state:         remoteSnapshot,
		snapshotIndex: 100,
		match:         100,
		next:          101,
	}
	exp := &remote{
		state:         remoteSnapshot,
		snapshotIndex: 0,
		match:         100,
		next:          101,
	}
	r.reset()
	if !reflect.DeepEqual(&r, &exp) {
		t.Errorf("unexpected state %+v", r)
	}
}

func TestRemoteActiveFlag(t *testing.T) {
	r := remote{}
	if r.isActive() {
		t.Errorf("unexpected active state1")
	}
	r.setActive()
	if !r.isActive() {
		t.Errorf("unexpected active state2")
	}
	r.setNotActive()
	if r.isActive() {
		t.Errorf("unexpected active state3")
	}
}

func TestRemoteBecomeRetry(t *testing.T) {
	r := remote{state: remoteReplicate}
	r.becomeRetry()
	if r.next != r.match+1 {
		t.Errorf("unexpected next")
	}
	if r.state != remoteRetry {
		t.Errorf("unexpected state %+v", r)
	}
}

func TestRemoteBecomeRetryFromSnapshot(t *testing.T) {
	r := remote{state: remoteSnapshot, snapshotIndex: 100}
	r.becomeRetry()
	if r.next != 101 {
		t.Errorf("unexpected next")
	}
	if r.state != remoteRetry {
		t.Errorf("unexpected state %+v", r)
	}
	if r.snapshotIndex != 0 {
		t.Errorf("snapshotIndex not reset")
	}
	r = remote{state: remoteSnapshot, match: 10, snapshotIndex: 0}
	r.becomeRetry()
	if r.next != 11 {
		t.Errorf("unexpected next")
	}
	if r.state != remoteRetry {
		t.Errorf("unexpected state %+v", r)
	}
	if r.snapshotIndex != 0 {
		t.Errorf("snapshotIndex not reset")
	}
}

func testRemoteBecomeSnapshot(t *testing.T, st remoteStateType) {
	r := &remote{state: st, match: 10, next: 11}
	r.becomeSnapshot(12)
	if r.state != remoteSnapshot {
		t.Errorf("unexpected state %+v", r)
	}
	if r.match != 10 || r.snapshotIndex != 12 {
		t.Errorf("unexpected state %+v", r)
	}
}

func TestRemoteBecomeSnapshot(t *testing.T) {
	testRemoteBecomeSnapshot(t, remoteReplicate)
	testRemoteBecomeSnapshot(t, remoteRetry)
	testRemoteBecomeSnapshot(t, remoteSnapshot)
}

func TestRemoteBecomeReplication(t *testing.T) {
	r := &remote{state: remoteRetry, match: 10, next: 11}
	r.becomeReplicate()
	if r.state != remoteReplicate {
		t.Errorf("unexpected state %+v", r)
	}
	if r.match != 10 || r.next != 11 {
		t.Errorf("unexpected match/next %+v", r)
	}
}

func TestRemoteProgress(t *testing.T) {
	r := &remote{state: remoteReplicate, match: 10, next: 11}
	r.progress(12)
	if r.next != 13 {
		t.Errorf("unexpected next: %d", r.next)
	}
	if r.match != 10 {
		t.Errorf("match unexpectedly moved")
	}
	r = &remote{state: remoteRetry, match: 10, next: 11}
	if r.isPaused() {
		t.Errorf("unexpectedly in paused state")
	}
	r.progress(12)
	if !r.isPaused() {
		t.Errorf("not paused")
	}
	if r.next != 11 || r.match != 10 {
		t.Errorf("unexpected state %+v", r)
	}
}

func TestRemoteProgressInSnapshotStateCausePanic(t *testing.T) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not triggered")
	}()
	r := &remote{state: remoteSnapshot, match: 10, next: 11}
	r.progress(12)
}

func TestRemotePanicWhenInInvalidState(t *testing.T) {
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not triggered")
	}()
	r := &remote{state: remoteStateType(100)}
	r.isPaused()
}

func TestRemoteIsPaused(t *testing.T) {
	tests := []struct {
		st  remoteStateType
		exp bool
	}{
		{remoteRetry, false},
		{remoteWait, true},
		{remoteReplicate, false},
		{remoteSnapshot, true},
	}

	for idx, tt := range tests {
		r := &remote{state: tt.st}
		if r.isPaused() != tt.exp {
			t.Errorf("%d, paused is true, st %s isPaused: %t", idx, tt.st, r.isPaused())
		}
	}
}

func TestRemoteRespondedTo(t *testing.T) {
	tests := []struct {
		st            remoteStateType
		match         uint64
		next          uint64
		snapshotIndex uint64
		expSt         remoteStateType
		expNext       uint64
	}{
		{remoteRetry, 10, 12, 0, remoteReplicate, 11},
		{remoteReplicate, 10, 12, 0, remoteReplicate, 12},
		{remoteSnapshot, 10, 12, 8, remoteRetry, 11},
		{remoteSnapshot, 10, 11, 12, remoteSnapshot, 11},
	}
	for idx, tt := range tests {
		r := &remote{
			state:         tt.st,
			match:         tt.match,
			next:          tt.next,
			snapshotIndex: tt.snapshotIndex,
		}
		r.respondedTo()
		if r.state != tt.expSt {
			t.Errorf("%d, state %s, exp %s", idx, r.state, tt.expSt)
		}
		if r.next != tt.expNext {
			t.Errorf("%d, next %d, exp %d", idx, r.next, tt.expNext)
		}
	}
}

func TestRemoteTryUpdate(t *testing.T) {
	match := uint64(10)
	next := uint64(20)
	tests := []struct {
		index      uint64
		paused     bool
		expMatch   uint64
		expNext    uint64
		expPaused  bool
		expUpdated bool
	}{
		{next, false, next, next + 1, false, true},
		{next, true, next, next + 1, false, true},
		{next - 2, false, next - 2, next, false, true},
		{next - 2, true, next - 2, next, false, true},
		{next - 1, false, next - 1, next, false, true},
		{next - 1, true, next - 1, next, false, true},
		{match - 1, false, match, next, false, false},
		{match - 1, true, match, next, true, false},
	}
	for idx, tt := range tests {
		r := &remote{
			match: match,
			next:  next,
		}
		if tt.paused {
			r.retryToWait()
		}
		updated := r.tryUpdate(tt.index)
		if updated != tt.expUpdated {
			t.Errorf("%d, updated %t, want %t", idx, updated, tt.expUpdated)
		}
		if r.next != tt.expNext || r.match != tt.expMatch {
			t.Errorf("%d, unexpected state %+v", idx, r)
		}
		if tt.expPaused {
			if r.state != remoteWait {
				t.Errorf("st %s, want remoteWait", r.state)
			}
		}
	}
}

func TestRemoteDecreaseToInReplicateState(t *testing.T) {
	tests := []struct {
		match     uint64
		next      uint64
		rejected  uint64
		decreased bool
		expNext   uint64
	}{
		{10, 15, 9, false, 15},
		{10, 15, 10, false, 15},
		{10, 15, 12, true, 11},
	}
	for idx, tt := range tests {
		r := &remote{match: tt.match, next: tt.next, state: remoteReplicate}
		decreased := r.decreaseTo(tt.rejected, 100)
		if decreased != tt.decreased {
			t.Errorf("%d, unexpected return value", idx)
		}
		if r.next != tt.expNext {
			t.Errorf("%d, next %d, exp %d", idx, r.next, tt.expNext)
		}
	}
}

func TestRemoteDecreaseToNotReplicateState(t *testing.T) {
	tests := []struct {
		match     uint64
		next      uint64
		rejected  uint64
		last      uint64
		decreased bool
		expNext   uint64
	}{
		{10, 15, 20, 100, false, 15},
		{10, 15, 14, 100, true, 14},
		{10, 15, 14, 10, true, 11},
	}
	for idx, tt := range tests {
		for _, st := range []remoteStateType{remoteRetry, remoteSnapshot} {
			r := &remote{match: tt.match, next: tt.next, state: st}
			r.retryToWait()
			decreased := r.decreaseTo(tt.rejected, tt.last)
			if decreased != tt.decreased {
				t.Errorf("%d, unexpected return value", idx)
			}
			if r.next != tt.expNext {
				t.Errorf("%d, next %d, exp %d", idx, r.next, tt.expNext)
			}
			if tt.decreased {
				if r.state == remoteWait {
					t.Errorf("not resumed")
				}
			}
		}
	}
}

func TestRemoteTryUpdateCauseResume(t *testing.T) {
	r := &remote{next: 5}
	r.retryToWait()
	r.decreaseTo(4, 4)
	if r.state == remoteWait {
		t.Errorf("still paused")
	}
	r.retryToWait()
	r.tryUpdate(5)
	if r.state == remoteWait {
		t.Errorf("still paused")
	}
}
````

## File: internal/raft/remote.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
//
// remote.go is used for tracking the state of remote raft node. it is derived
// from etcd raft's flow control code.
//
// Copyright 2015 The etcd Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raft

import (
	"fmt"
)

type snapshotAck struct {
	ctick    uint64
	rejected bool
}

func (a *snapshotAck) tick() bool {
	if a.ctick > 0 {
		a.ctick--
		return a.ctick == 0
	}
	return false
}

type remoteStateType uint64

const (
	remoteRetry remoteStateType = iota
	remoteWait
	remoteReplicate
	remoteSnapshot
)

var remoteNames = []string{
	"Retry",
	"Wait",
	"Replicate",
	"Snapshot",
}

func (r remoteStateType) String() string {
	return remoteNames[uint64(r)]
}

type remote struct {
	// called matchIndex/nextIndex in the etcd raft paper
	match         uint64
	next          uint64
	snapshotIndex uint64
	state         remoteStateType
	active        bool
	delayed       snapshotAck
}

func (r *remote) String() string {
	return fmt.Sprintf("match:%d,next:%d,state:%s,si:%d",
		r.match, r.next, r.state, r.snapshotIndex)
}

func (r *remote) clearSnapshotAck() {
	r.delayed = snapshotAck{}
}

func (r *remote) setSnapshotAck(tick uint64, rejected bool) {
	if r.state == remoteSnapshot {
		r.delayed.ctick, r.delayed.rejected = tick, rejected
	} else {
		panic("setting snapshot ack when not in snapshot state")
	}
}

func (r *remote) reset() {
	r.snapshotIndex = 0
}

func (r *remote) becomeRetry() {
	if r.state == remoteSnapshot {
		r.next = max(r.match+1, r.snapshotIndex+1)
	} else {
		r.next = r.match + 1
	}
	r.reset()
	r.state = remoteRetry
}

func (r *remote) retryToWait() {
	if r.state == remoteRetry {
		r.state = remoteWait
	}
}

func (r *remote) waitToRetry() {
	if r.state == remoteWait {
		r.state = remoteRetry
	}
}

func (r *remote) becomeWait() {
	r.clearSnapshotAck()
	r.becomeRetry()
	r.retryToWait()
}

func (r *remote) becomeReplicate() {
	r.next = r.match + 1
	r.reset()
	r.state = remoteReplicate
}

func (r *remote) becomeSnapshot(index uint64) {
	r.reset()
	r.snapshotIndex = index
	r.state = remoteSnapshot
}

func (r *remote) clearPendingSnapshot() {
	r.snapshotIndex = 0
}

func (r *remote) tryUpdate(index uint64) bool {
	if r.next < index+1 {
		r.next = index + 1
	}
	if r.match < index {
		r.waitToRetry()
		r.match = index
		return true
	}
	return false
}

func (r *remote) progress(lastIndex uint64) {
	switch r.state {
	case remoteReplicate:
		r.next = lastIndex + 1
	case remoteRetry:
		r.retryToWait()
	default:
		panic("unexpected remote state")
	}
}

func (r *remote) respondedTo() {
	switch r.state {
	case remoteRetry:
		r.becomeReplicate()
	case remoteSnapshot:
		if r.match >= r.snapshotIndex {
			r.becomeRetry()
		}
	default:
	}
}

func (r *remote) decreaseTo(rejected uint64, last uint64) bool {
	if r.state == remoteReplicate {
		if rejected <= r.match {
			// stale msg
			return false
		}
		r.next = r.match + 1
		return true
	}
	if r.next-1 != rejected {
		// stale
		return false
	}
	r.waitToRetry()
	r.next = max(1, min(rejected, last+1))
	return true
}

func (r *remote) isPaused() bool {
	switch r.state {
	case remoteRetry:
		return false
	case remoteWait:
		return true
	case remoteReplicate:
		return false
	case remoteSnapshot:
		return true
	default:
		panic("unexpected remote state")
	}
}

func (r *remote) isActive() bool {
	return r.active
}

func (r *remote) setActive() {
	r.active = true
}

func (r *remote) setNotActive() {
	r.active = false
}
````

## File: internal/registry/event.go
````go
// Copyright 2018-2022 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package registry

import (
	"github.com/hashicorp/memberlist"
)

// sliceEventDelegate is used to hook into memberlist to get notification
// about nodes joining and leaving.
type sliceEventDelegate struct {
	store *metaStore
}

var _ memberlist.EventDelegate = (*sliceEventDelegate)(nil)

func newSliceEventDelegate(store *metaStore) *sliceEventDelegate {
	return &sliceEventDelegate{
		store: store,
	}
}

func (e *sliceEventDelegate) put(eventType memberlist.NodeEventType,
	n *memberlist.Node) {
	switch eventType {
	case memberlist.NodeJoin:
		fallthrough
	case memberlist.NodeUpdate:
		var m meta
		if m.unmarshal(n.Meta) {
			e.store.put(n.Name, m)
		}
	case memberlist.NodeLeave:
		e.store.delete(n.Name)
	default:
		panic("unknown event type")
	}
}

func (e *sliceEventDelegate) NotifyJoin(n *memberlist.Node) {
	e.put(memberlist.NodeJoin, n)
}

func (e *sliceEventDelegate) NotifyLeave(n *memberlist.Node) {
	e.put(memberlist.NodeLeave, n)
}

func (e *sliceEventDelegate) NotifyUpdate(n *memberlist.Node) {
	e.put(memberlist.NodeUpdate, n)
}
````

## File: internal/registry/gossip_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package registry

import (
	"testing"
	"time"

	"github.com/lni/goutils/leaktest"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/id"
)

const (
	testNodeHostID1 = "123e4567-e89b-12d3-a456-426614174000"
	testNodeHostID2 = "123e4567-e89b-12d3-a456-426614174001"
)

func TestMeta(t *testing.T) {
	m := meta{
		RaftAddress: "localhost:9090",
		Data:        []byte("localhost:1080"),
	}
	data := m.marshal()
	m2 := meta{}
	assert.True(t, m2.unmarshal(data))
	assert.Equal(t, m, m2)
}

func TestMetaStore(t *testing.T) {
	m := metaStore{}
	meta := meta{RaftAddress: "localhost:9090"}
	m.put("123", meta)
	v, ok := m.get("123")
	assert.True(t, ok)
	assert.Equal(t, meta, v)
	m.delete("123")
	_, ok = m.get("123")
	assert.False(t, ok)
}

func TestGossipRegistry(t *testing.T) {
	defer leaktest.AfterTest(t)()
	nhid := testNodeHostID1
	nhConfig := config.NodeHostConfig{
		RaftAddress: "localhost:27001",
		Gossip: config.GossipConfig{
			BindAddress:      "localhost:26001",
			AdvertiseAddress: "127.0.0.1:26001",
			Seed:             []string{"127.0.0.1:26002"},
		},
	}
	r, err := NewGossipRegistry(nhid, nil, nhConfig, 1, id.IsNodeHostID)
	require.NoError(t, err)
	defer func() {
		err := r.Close()
		require.NoError(t, err)
	}()
	require.Equal(t, 1, r.NumMembers())
	r.Add(123, 456, nhid)
	addr, _, err := r.Resolve(123, 456)
	require.NoError(t, err)
	require.Equal(t, nhConfig.RaftAddress, addr)
	// remove node
	r.Remove(123, 456)
	_, _, err = r.Resolve(123, 456)
	require.Equal(t, ErrUnknownTarget, err)
	// add back
	r.Add(123, 456, nhid)
	addr, _, err = r.Resolve(123, 456)
	require.NoError(t, err)
	require.Equal(t, nhConfig.RaftAddress, addr)
	// remove shard
	r.RemoveShard(123)
	_, _, err = r.Resolve(123, 456)
	require.Equal(t, ErrUnknownTarget, err)
}

func TestGossipManagerCanBeCreatedAndStopped(t *testing.T) {
	defer leaktest.AfterTest(t)()
	nhid := testNodeHostID1
	nhConfig := config.NodeHostConfig{
		RaftAddress: "localhost:27001",
		Gossip: config.GossipConfig{
			BindAddress:      "localhost:26001",
			AdvertiseAddress: "127.0.0.1:26001",
			Seed:             []string{"127.0.0.1:26002"},
		},
	}
	m, err := newGossipManager(nhid, nil, nhConfig)
	require.NoError(t, err)
	defer func() {
		err := m.Close()
		require.NoError(t, err)
	}()
	require.Equal(t, 1, m.numMembers())
	require.Equal(t, "127.0.0.1:26001", m.advertiseAddress())
	addr, ok := m.GetRaftAddress(nhid)
	require.True(t, ok)
	require.Equal(t, nhConfig.RaftAddress, addr)
}

func TestGossipManagerCanGossip(t *testing.T) {
	defer leaktest.AfterTest(t)()
	nhid1 := testNodeHostID1
	nhConfig1 := config.NodeHostConfig{
		RaftAddress: "localhost:27001",
		Expert: config.ExpertConfig{
			TestGossipProbeInterval: 10 * time.Millisecond,
		},
		Gossip: config.GossipConfig{
			BindAddress:      "localhost:26001",
			AdvertiseAddress: "127.0.0.1:26001",
			Seed:             []string{"127.0.0.1:26002"},
		},
	}
	nhid2 := testNodeHostID2
	nhConfig2 := config.NodeHostConfig{
		RaftAddress: "localhost:27002",
		Expert: config.ExpertConfig{
			TestGossipProbeInterval: 10 * time.Millisecond,
		},
		Gossip: config.GossipConfig{
			BindAddress:      "localhost:26002",
			AdvertiseAddress: "127.0.0.1:26002",
			Seed:             []string{"127.0.0.1:26001"},
		},
	}
	m1, err := newGossipManager(nhid1, nil, nhConfig1)
	require.NoError(t, err)
	defer func() {
		err := m1.Close()
		require.NoError(t, err)
	}()
	m2, err := newGossipManager(nhid2, nil, nhConfig2)
	require.NoError(t, err)
	defer func() {
		err := m2.Close()
		require.NoError(t, err)
	}()
	retry := 0
	for retry < 1000 {
		retry++
		time.Sleep(5 * time.Millisecond)
		if m1.numMembers() != 2 || m2.numMembers() != 2 {
			continue
		}
		addr, ok := m1.GetRaftAddress(nhid2)
		if !ok || addr != nhConfig2.RaftAddress {
			continue
		}
		addr, ok = m2.GetRaftAddress(nhid1)
		if !ok || addr != nhConfig1.RaftAddress {
			continue
		}
		return
	}
	require.Fail(t, "failed to complete all queries")
}
````

## File: internal/registry/gossip.go
````go
// Copyright 2018-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package registry

import (
	"bytes"
	"encoding/binary"
	"encoding/gob"
	"net"
	"strconv"
	"sync"
	"time"

	"github.com/cockroachdb/errors"
	"github.com/hashicorp/memberlist"
	"github.com/lni/goutils/syncutil"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/logger"
)

var plog = logger.GetLogger("registry")

type getShardInfo func() []ShardInfo

// meta is the metadata of the node. The actual payload is specified by the user
// by setting the Config.GossipConfig.Meta field. meta contains node information
// that will not change during the life of a particular NodeHost process.
type meta struct {
	RaftAddress string
	Data        []byte
}

func (m *meta) marshal() []byte {
	var buf bytes.Buffer
	enc := gob.NewEncoder(&buf)
	if err := enc.Encode(m); err != nil {
		panic(err)
	}
	data := buf.Bytes()
	result := make([]byte, len(data)+2)
	binary.BigEndian.PutUint16(result, uint16(len(data)))
	copy(result[2:], data)
	return result
}

func (m *meta) unmarshal(data []byte) bool {
	if len(data) <= 2 {
		return false
	}
	sz := binary.BigEndian.Uint16(data)
	if sz > 512 {
		return false
	}
	data = data[:sz+2]
	buf := bytes.NewBuffer(data[2:])
	dec := gob.NewDecoder(buf)
	if err := dec.Decode(m); err != nil {
		return false
	}
	return true
}

// metaStore is a node name to node Meta concurrent map.
type metaStore struct {
	nodes sync.Map
}

func (m *metaStore) put(name string, md meta) {
	m.nodes.Store(name, md)
}

func (m *metaStore) get(name string) (meta, bool) {
	v, ok := m.nodes.Load(name)
	if ok {
		return v.(meta), true
	}
	return meta{}, false
}

func (m *metaStore) delete(name string) {
	m.nodes.Delete(name)
}

// GossipRegistry is a node registry backed by gossip. It is capable of
// supporting NodeHosts with dynamic RaftAddress values.
type GossipRegistry struct {
	nodes  *Registry
	gossip *gossipManager
}

// NewGossipRegistry creates a new GossipRegistry instance.
func NewGossipRegistry(nhid string, f getShardInfo,
	nhConfig config.NodeHostConfig, streamConnections uint64,
	v config.TargetValidator) (*GossipRegistry, error) {
	gossip, err := newGossipManager(nhid, f, nhConfig)
	if err != nil {
		return nil, err
	}
	r := &GossipRegistry{
		nodes:  NewNodeRegistry(streamConnections, v),
		gossip: gossip,
	}
	return r, nil
}

// GetNodeHostRegistry returns the NodeHostRegistry backed by gossip.
func (n *GossipRegistry) GetNodeHostRegistry() *NodeHostRegistry {
	return n.gossip.GetNodeHostRegistry()
}

// Close closes the GossipRegistry instance.
func (n *GossipRegistry) Close() error {
	return n.gossip.Close()
}

// AdvertiseAddress returns the advertise address of the gossip service.
func (n *GossipRegistry) AdvertiseAddress() string {
	return n.gossip.advertiseAddress()
}

// NumMembers returns the number of live nodes known by the gossip service.
func (n *GossipRegistry) NumMembers() int {
	return n.gossip.numMembers()
}

// Add adds a new node with its known NodeHostID to the registry.
func (n *GossipRegistry) Add(shardID uint64,
	replicaID uint64, target string) {
	n.nodes.Add(shardID, replicaID, target)
}

// Remove removes the specified node from the registry.
func (n *GossipRegistry) Remove(shardID uint64, replicaID uint64) {
	n.nodes.Remove(shardID, replicaID)
}

// RemoveShard removes the specified shard from the registry.
func (n *GossipRegistry) RemoveShard(shardID uint64) {
	n.nodes.RemoveShard(shardID)
}

// Resolve returns the current RaftAddress and connection key of the specified
// node. It returns ErrUnknownTarget when the RaftAddress is unknown.
func (n *GossipRegistry) Resolve(shardID uint64,
	replicaID uint64) (string, string, error) {
	target, key, err := n.nodes.Resolve(shardID, replicaID)
	if err != nil {
		return "", "", err
	}
	addr, ok := n.gossip.GetRaftAddress(target)
	if ok {
		return addr, key, nil
	}
	return "", "", ErrUnknownTarget
}

// delegate is used to hook into memberlist's gossip layer.
type delegate struct {
	getShardInfo getShardInfo
	meta         meta
	view         *view
}

var _ memberlist.Delegate = (*delegate)(nil)

func (d *delegate) NodeMeta(limit int) []byte {
	m := d.meta.marshal()
	if len(m) > limit {
		panic("meta message is too big")
	}
	return m
}

func (d *delegate) NotifyMsg(buf []byte) {
	d.view.updateFrom(buf)
}

func (d *delegate) GetBroadcasts(overhead, limit int) [][]byte {
	if d.getShardInfo != nil {
		d.view.update(toShardViewList(d.getShardInfo()))
	}
	data := d.view.getGossipData(limit - overhead)
	if data == nil {
		return nil
	}
	if len(data) > limit-overhead {
		panic("broadcast message is too big")
	}

	result := make([][]byte, 1)
	result[0] = data
	return result
}

func (d *delegate) MergeRemoteState(buf []byte, join bool) {
	d.view.updateFrom(buf)
}

func (d *delegate) LocalState(join bool) []byte {
	if d.getShardInfo != nil {
		d.view.update(toShardViewList(d.getShardInfo()))
	}
	return d.view.getFullSyncData()
}

func parseAddress(addr string) (string, int, error) {
	host, sp, err := net.SplitHostPort(addr)
	if err != nil {
		return "", 0, err
	}
	port, err := strconv.ParseUint(sp, 10, 16)
	if err != nil {
		return "", 0, err
	}
	return host, int(port), nil
}

type gossipManager struct {
	nhConfig config.NodeHostConfig
	cfg      *memberlist.Config
	list     *memberlist.Memberlist
	view     *view
	store    *metaStore
	stopper  *syncutil.Stopper
}

func newGossipManager(nhid string, f getShardInfo,
	nhConfig config.NodeHostConfig) (*gossipManager, error) {
	store := &metaStore{}
	cfg := memberlist.DefaultWANConfig()
	cfg.Logger = newGossipLogWrapper()
	cfg.Name = nhid
	cfg.PushPullInterval = 500 * time.Millisecond
	cfg.GossipInterval = 250 * time.Millisecond
	cfg.GossipNodes = 6
	cfg.UDPBufferSize = 32 * 1024
	if nhConfig.Expert.TestGossipProbeInterval > 0 {
		plog.Infof("gossip probe interval set to %s",
			nhConfig.Expert.TestGossipProbeInterval)
		cfg.ProbeInterval = nhConfig.Expert.TestGossipProbeInterval
	}
	bindAddr, bindPort, err := parseAddress(nhConfig.Gossip.BindAddress)
	if err != nil {
		return nil, err
	}
	cfg.BindAddr = bindAddr
	cfg.BindPort = bindPort
	plog.Infof("gossip bind address %s port %d", cfg.BindAddr, cfg.BindPort)
	if len(nhConfig.Gossip.AdvertiseAddress) > 0 {
		aAddr, aPort, err := parseAddress(nhConfig.Gossip.AdvertiseAddress)
		if err != nil {
			return nil, err
		}
		cfg.AdvertiseAddr = aAddr
		cfg.AdvertisePort = aPort
		plog.Infof("gossip advertise address %s port %d", aAddr, aPort)
	}
	view := newView(nhConfig.GetDeploymentID())
	meta := meta{
		RaftAddress: nhConfig.RaftAddress,
		Data:        nhConfig.Gossip.Meta,
	}
	cfg.Delegate = &delegate{
		meta:         meta,
		getShardInfo: f,
		view:         view,
	}
	// set memberlist's event delegate
	cfg.Events = newSliceEventDelegate(store)

	list, err := memberlist.Create(cfg)
	if err != nil {
		plog.Errorf("failed to create memberlist, %v", err)
		return nil, err
	}
	seed := make([]string, 0, len(nhConfig.Gossip.Seed))
	seed = append(seed, nhConfig.Gossip.Seed...)
	g := &gossipManager{
		nhConfig: nhConfig,
		cfg:      cfg,
		list:     list,
		view:     view,
		store:    store,
		stopper:  syncutil.NewStopper(),
	}
	// eventDelegate must be started first, otherwise join() could be blocked
	// on a large cluster
	g.join(seed)
	g.stopper.RunWorker(func() {
		ticker := time.NewTicker(500 * time.Millisecond)
		defer ticker.Stop()
		for {
			select {
			case <-ticker.C:
				if len(g.list.Members()) > 1 {
					continue
				}
				g.join(seed)
			case <-g.stopper.ShouldStop():
				return
			}
		}
	})
	return g, nil
}

func (g *gossipManager) join(seed []string) {
	if _, err := g.list.Join(seed); err != nil {
		plog.Errorf("failed to join the gossip group, %v", err)
	}
}

func (g *gossipManager) Close() error {
	g.stopper.Stop()
	if err := g.list.Leave(time.Second); err != nil {
		plog.Errorf("memberlist leave failed: %v", err)
	}
	if err := g.list.Shutdown(); err != nil {
		return errors.Wrapf(err, "shutdown memberlist failed")
	}
	return nil
}

func (g *gossipManager) GetNodeHostRegistry() *NodeHostRegistry {
	return &NodeHostRegistry{
		view:  g.view,
		store: g.store,
	}
}

func (g *gossipManager) GetRaftAddress(nhid string) (string, bool) {
	if g.cfg.Name == nhid {
		return g.nhConfig.RaftAddress, true
	}
	if v, ok := g.store.get(nhid); ok {
		return v.RaftAddress, true
	}
	return "", false
}

func (g *gossipManager) advertiseAddress() string {
	return g.list.LocalNode().Address()
}

func (g *gossipManager) numMembers() int {
	return g.list.NumMembers()
}
````

## File: internal/registry/logger.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package registry

import (
	"log"
	"strings"

	"github.com/lni/dragonboat/v4/logger"
)

type gossipLogWriter struct {
	logger logger.ILogger
}

func (l *gossipLogWriter) Write(p []byte) (int, error) {
	str := string(p)
	str = strings.TrimSuffix(str, "\n")

	switch {
	case strings.HasPrefix(str, "[WARN] "):
		str = strings.TrimPrefix(str, "[WARN] ")
		l.logger.Warningf(str)
	case strings.HasPrefix(str, "[DEBUG] "):
		str = strings.TrimPrefix(str, "[DEBUG] ")
		l.logger.Debugf(str)
	case strings.HasPrefix(str, "[INFO] "):
		str = strings.TrimPrefix(str, "[INFO] ")
		l.logger.Infof(str)
	case strings.HasPrefix(str, "[ERR] "):
		str = strings.TrimPrefix(str, "[ERR] ")
		l.logger.Warningf(str)
	default:
		l.logger.Warningf(str)
	}

	return len(p), nil
}

// newGossipLogWrapper prepare log wrapper for gossip.
// Inspirited by https://github.com/docker/docker-ce/blob/master/components/engine/libnetwork/networkdb/shard.go#L30
func newGossipLogWrapper() *log.Logger {
	return log.New(&gossipLogWriter{
		logger: logger.GetLogger("gossip"),
	}, "", 0)
}
````

## File: internal/registry/nodehost.go
````go
// Copyright 2018-2022 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package registry

// NodeHostRegistry is a NodeHost info registry backed by gossip.
type NodeHostRegistry struct {
	store *metaStore
	view  *view
}

// NumOfShards returns the number of shards known to the current NodeHost
// instance.
func (r *NodeHostRegistry) NumOfShards() int {
	return r.view.shardCount()
}

// GetMeta returns gossip metadata associated with the specified NodeHost
// instance.
func (r *NodeHostRegistry) GetMeta(nhID string) ([]byte, bool) {
	m, ok := r.store.get(nhID)
	if !ok {
		return nil, false
	}
	return m.Data, true
}

// GetShardInfo returns the shard info for the specified shard if it is
// available in the gossip view.
func (r *NodeHostRegistry) GetShardInfo(shardID uint64) (ShardView, bool) {
	r.view.mu.Lock()
	defer r.view.mu.Unlock()

	ci, ok := r.view.mu.shards[shardID]
	if !ok {
		return ShardView{}, false
	}
	result := ci
	result.Replicas = make(map[uint64]string)
	for shardID, target := range ci.Replicas {
		result.Replicas[shardID] = target
	}
	return result, true
}
````

## File: internal/registry/registry_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package registry

import (
	"testing"

	"github.com/lni/goutils/stringutil"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/internal/settings"
)

func TestPeerCanBeAdded(t *testing.T) {
	nodes := NewNodeRegistry(settings.Soft.StreamConnections, nil)
	_, _, err := nodes.Resolve(100, 2)
	require.Error(t, err, "error not reported")
	nodes.Add(100, 2, "a2:2")
	url, _, err := nodes.Resolve(100, 2)
	require.NoError(t, err, "failed to resolve address")
	require.Equal(t, "a2:2", url, "got %s, want %s", url, "a2:2")
}

func TestPeerAddressCanNotBeUpdated(t *testing.T) {
	nodes := NewNodeRegistry(settings.Soft.StreamConnections, nil)
	nodes.Add(100, 2, "a2:2")
	require.Panics(t, func() {
		nodes.Add(100, 2, "a2:3")
	}, "didn't panic when updating addr")
}

func TestPeerCanBeRemoved(t *testing.T) {
	nodes := NewNodeRegistry(settings.Soft.StreamConnections, nil)
	nodes.Add(100, 2, "a2:2")
	url, _, err := nodes.Resolve(100, 2)
	require.NoError(t, err, "failed to resolve address")
	require.Equal(t, "a2:2", url, "got %s, want %s", url, "a2:2")
	nodes.Remove(100, 2)
	_, _, err = nodes.Resolve(100, 2)
	require.Error(t, err, "error not reported")
}

func TestRemoveShard(t *testing.T) {
	nodes := NewNodeRegistry(settings.Soft.StreamConnections, nil)
	nodes.Add(100, 2, "a2:2")
	nodes.Add(100, 3, "a2:3")
	nodes.Add(200, 2, "a3:2")
	nodes.RemoveShard(100)
	_, _, err := nodes.Resolve(100, 2)
	require.Error(t, err, "shard not removed")
	_, _, err = nodes.Resolve(200, 2)
	require.NoError(t, err, "failed to get node")
}

func testInvalidAddressWillPanic(t *testing.T, addr string) {
	nodes := NewNodeRegistry(settings.Soft.StreamConnections,
		stringutil.IsValidAddress)
	require.Panics(t, func() {
		nodes.Add(100, 2, addr)
	}, "failed to panic on invalid address")
}

func TestInvalidAddressWillPanic(t *testing.T) {
	testInvalidAddressWillPanic(t, "a3")
	testInvalidAddressWillPanic(t, "3")
	testInvalidAddressWillPanic(t, "abc:")
	testInvalidAddressWillPanic(t, ":")
	testInvalidAddressWillPanic(t, ":1243")
	testInvalidAddressWillPanic(t, "abc")
	testInvalidAddressWillPanic(t, "abc:67890")
}
````

## File: internal/registry/registry.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package registry

import (
	"fmt"
	"sync"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/logutil"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/raftio"
)

var (
	// ErrUnknownTarget is the error returned when the target address of the node
	// is unknown.
	ErrUnknownTarget = errors.New("target address unknown")
)

// IResolver converts the (shard id, replica id) tuple to network address.
type IResolver interface {
	Resolve(uint64, uint64) (string, string, error)
	Add(uint64, uint64, string)
}

var _ raftio.INodeRegistry = (*Registry)(nil)
var _ IResolver = (*Registry)(nil)

// Registry is used to manage all known node addresses in the multi raft system.
// The transport layer uses this address registry to locate nodes.
type Registry struct {
	partitioner server.IPartitioner
	validate    config.TargetValidator
	addr        sync.Map // map of raftio.NodeInfo => string
}

// NewNodeRegistry returns a new Registry object.
func NewNodeRegistry(streamConnections uint64, v config.TargetValidator) *Registry {
	n := &Registry{validate: v}
	if streamConnections > 1 {
		n.partitioner = server.NewFixedPartitioner(streamConnections)
	}
	return n
}

// Close closes the registry.
func (n *Registry) Close() error { return nil }

// Add adds the specified replica and its target info to the registry.
func (n *Registry) Add(shardID uint64, replicaID uint64, target string) {
	if n.validate != nil && !n.validate(target) {
		plog.Panicf("invalid target %s", target)
	}
	key := raftio.GetNodeInfo(shardID, replicaID)
	v, ok := n.addr.LoadOrStore(key, target)
	if ok {
		if v.(string) != target {
			plog.Panicf("inconsistent target for %s, %s:%s",
				logutil.DescribeNode(shardID, replicaID), v, target)
		}
	}
}

func (n *Registry) getConnectionKey(addr string, shardID uint64) string {
	if n.partitioner == nil {
		return addr
	}
	return fmt.Sprintf("%s-%d", addr, n.partitioner.GetPartitionID(shardID))
}

// Remove removes a remote from the node registry.
func (n *Registry) Remove(shardID uint64, replicaID uint64) {
	n.addr.Delete(raftio.GetNodeInfo(shardID, replicaID))
}

// RemoveShard removes info associated with the specified shard.
func (n *Registry) RemoveShard(shardID uint64) {
	var toRemove []raftio.NodeInfo
	n.addr.Range(func(k, v interface{}) bool {
		ni := k.(raftio.NodeInfo)
		if ni.ShardID == shardID {
			toRemove = append(toRemove, ni)
		}
		return true
	})
	for _, v := range toRemove {
		n.addr.Delete(v)
	}
}

// Resolve looks up the address of the specified node.
func (n *Registry) Resolve(shardID uint64, replicaID uint64) (string, string, error) {
	key := raftio.GetNodeInfo(shardID, replicaID)
	addr, ok := n.addr.Load(key)
	if !ok {
		return "", "", ErrUnknownTarget
	}
	return addr.(string), n.getConnectionKey(addr.(string), shardID), nil
}
````

## File: internal/registry/view_test.go
````go
// Copyright 2017-2022 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package registry

import (
	"testing"

	"github.com/stretchr/testify/assert"

	"github.com/lni/dragonboat/v4/internal/raft"
)

func getTestShardView() []ShardView {
	cv1 := ShardView{
		ShardID:           100,
		ConfigChangeIndex: 200,
		Replicas: map[uint64]string{
			200: "address1",
			300: "address2",
			400: "address3",
		},
	}
	cv2 := ShardView{
		ShardID:           1340,
		ConfigChangeIndex: 126200,
		Replicas: map[uint64]string{
			1200: "myaddress1",
			4300: "theiraddress2",
			6400: "heraddress3",
		},
	}
	return []ShardView{cv1, cv2}
}

func TestGetFullSyncData(t *testing.T) {
	v := newView(123)
	cv := getTestShardView()
	v.update(cv)
	data := v.getFullSyncData()

	v2 := newView(123)
	v2.updateFrom(data)
	assert.Equal(t, v, v2)
}

func TestConfigChangeIndexIsChecked(t *testing.T) {
	v := newView(123)
	cv := getTestShardView()
	v.update(cv)

	update := []ShardView{
		{
			ShardID:           1340,
			ConfigChangeIndex: 300,
			Replicas: map[uint64]string{
				1200: "myaddress1",
				4300: "theiraddress2",
			},
		},
	}
	v.update(update)
	c, ok := v.mu.shards[1340]
	assert.True(t, ok)
	assert.Equal(t, uint64(126200), c.ConfigChangeIndex)
	assert.Equal(t, 3, len(c.Replicas))

	update = []ShardView{
		{
			ShardID:           1340,
			ConfigChangeIndex: 226200,
			Replicas: map[uint64]string{
				1200: "myaddress1",
				4300: "theiraddress2",
			},
		},
	}
	v.update(update)
	c, ok = v.mu.shards[1340]
	assert.True(t, ok)
	assert.Equal(t, uint64(226200), c.ConfigChangeIndex)
	assert.Equal(t, 2, len(c.Replicas))
}

func TestDeploymentIDIsChecked(t *testing.T) {
	v := newView(123)
	cv := getTestShardView()
	v.update(cv)
	data := v.getFullSyncData()

	v2 := newView(321)
	v2.updateFrom(data)
	assert.Equal(t, 0, len(v2.mu.shards))
}

func TestGetGossipData(t *testing.T) {
	v := newView(123)
	cv := getTestShardView()
	v.update(cv)
	data := v.getGossipData(340)
	assert.True(t, len(data) > 0)
}

func TestUpdateMembershipView(t *testing.T) {
	v := newView(0)
	cv := ShardView{
		ShardID:           123,
		ConfigChangeIndex: 100,
		Replicas:          make(map[uint64]string),
	}
	cv.Replicas[1] = "t1"
	cv.Replicas[2] = "t2"
	v.mu.shards[123] = cv

	ncv := ShardView{
		ShardID:           123,
		ConfigChangeIndex: 200,
		Replicas:          make(map[uint64]string),
	}
	ncv.Replicas[1] = "t1"
	ncv.Replicas[2] = "t2"
	ncv.Replicas[3] = "t3"
	updates := []ShardView{ncv}
	v.update(updates)

	result, ok := v.mu.shards[123]
	assert.True(t, ok)
	assert.Equal(t, ncv, result)
}

func TestOutOfDateMembershipInfoIsIgnored(t *testing.T) {
	v := newView(0)
	cv := ShardView{
		ShardID:           123,
		ConfigChangeIndex: 100,
		Replicas:          make(map[uint64]string),
	}
	cv.Replicas[1] = "t1"
	cv.Replicas[2] = "t2"
	v.mu.shards[123] = cv

	ncv := ShardView{
		ShardID:           123,
		ConfigChangeIndex: 10,
		Replicas:          make(map[uint64]string),
	}
	ncv.Replicas[1] = "t1"
	ncv.Replicas[2] = "t2"
	ncv.Replicas[3] = "t3"
	updates := []ShardView{ncv}
	v.update(updates)

	result, ok := v.mu.shards[123]
	assert.True(t, ok)
	assert.Equal(t, cv, result)
}

func TestUpdateLeadershipView(t *testing.T) {
	v := newView(0)
	cv := ShardView{
		ShardID:  123,
		LeaderID: 10,
		Term:     20,
	}
	v.mu.shards[123] = cv

	ncv := ShardView{
		ShardID:  123,
		LeaderID: 11,
		Term:     21,
	}
	updates := []ShardView{ncv}
	v.update(updates)

	result, ok := v.mu.shards[123]
	assert.True(t, ok)
	assert.Equal(t, ncv, result)
}

func TestInitialLeaderInfoCanBeRecorded(t *testing.T) {
	v := newView(0)
	cv := ShardView{
		ShardID: 123,
	}
	v.mu.shards[123] = cv

	ncv := ShardView{
		ShardID:  123,
		LeaderID: 11,
		Term:     21,
	}
	updates := []ShardView{ncv}
	v.update(updates)

	result, ok := v.mu.shards[123]
	assert.True(t, ok)
	assert.Equal(t, ncv, result)
}

func TestUnknownLeaderIsIgnored(t *testing.T) {
	v := newView(0)
	cv := ShardView{
		ShardID:  123,
		LeaderID: 10,
		Term:     20,
	}
	v.mu.shards[123] = cv

	ncv := ShardView{
		ShardID:  123,
		LeaderID: raft.NoLeader,
		Term:     21,
	}
	updates := []ShardView{ncv}
	v.update(updates)

	result, ok := v.mu.shards[123]
	assert.True(t, ok)
	assert.Equal(t, cv, result)
}
````

## File: internal/registry/view.go
````go
// Copyright 2017-2022 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package registry

import (
	"bytes"
	"encoding/binary"
	"encoding/gob"
	"math/rand"
	"sync"

	"github.com/pierrec/lz4/v4"

	"github.com/lni/dragonboat/v4/internal/raft"
	sm "github.com/lni/dragonboat/v4/statemachine"
)

var (
	binaryEnc = binary.BigEndian
)

// ShardInfo is a record for representing the state of a Raft shard based
// on the knowledge of the local NodeHost instance.
type ShardInfo struct {
	// Replicas is a map of member replica IDs to their Raft addresses.
	Replicas map[uint64]string
	// ShardID is the shard ID of the Raft shard.
	ShardID uint64
	// ReplicaID is the replica ID of the Raft replica.
	ReplicaID uint64
	// ConfigChangeIndex is the current config change index of the Raft node.
	// ConfigChangeIndex is Raft Log index of the last applied membership
	// change entry.
	ConfigChangeIndex uint64
	// StateMachineType is the type of the state machine.
	StateMachineType sm.Type
	// IsLeader indicates whether this is a leader node.
	// Deprecated: Use LeaderID and Term instead.
	IsLeader bool
	// LeaderID is the replica ID of the current leader
	LeaderID uint64
	// Term is the term of the current leader
	Term uint64
	// IsNonVoting indicates whether this is a non-voting nonVoting node.
	IsNonVoting bool
	// IsWitness indicates whether this is a witness node without actual log.
	IsWitness bool
	// Pending is a boolean flag indicating whether details of the shard node
	// is not available. The Pending flag is set to true usually because the node
	// has not had anything applied yet.
	Pending bool
}

// ShardView is the view of a shard from gossip's point of view at a certain
// point in time.
type ShardView struct {
	ShardID           uint64
	Replicas          map[uint64]string
	ConfigChangeIndex uint64
	LeaderID          uint64
	Term              uint64
}

func toShardViewList(input []ShardInfo) []ShardView {
	result := make([]ShardView, 0)
	for _, ci := range input {
		cv := ShardView{
			ShardID:           ci.ShardID,
			Replicas:          ci.Replicas,
			ConfigChangeIndex: ci.ConfigChangeIndex,
			LeaderID:          ci.LeaderID,
			Term:              ci.Term,
		}
		result = append(result, cv)
	}
	return result
}

type exchanged struct {
	DeploymentID uint64
	ShardInfo    []ShardView
}

// view contains dynamic information on shards, it can change after an
// election or a raft configuration change.
type view struct {
	deploymentID uint64
	// shardID -> ShardView
	mu struct {
		sync.Mutex
		shards map[uint64]ShardView
	}
}

func newView(deploymentID uint64) *view {
	v := &view{
		deploymentID: deploymentID,
	}
	v.mu.shards = make(map[uint64]ShardView)
	return v
}

func (v *view) shardCount() int {
	v.mu.Lock()
	defer v.mu.Unlock()
	return len(v.mu.shards)
}

func mergeShardView(current ShardView, update ShardView) ShardView {
	if current.ConfigChangeIndex < update.ConfigChangeIndex {
		current.Replicas = update.Replicas
		current.ConfigChangeIndex = update.ConfigChangeIndex
	}
	// we only keep which replica is the last known leader
	if update.LeaderID != raft.NoLeader {
		if current.LeaderID == raft.NoLeader || update.Term > current.Term {
			current.LeaderID = update.LeaderID
			current.Term = update.Term
		}
	}

	return current
}

func (v *view) update(updates []ShardView) {
	v.mu.Lock()
	defer v.mu.Unlock()

	for _, u := range updates {
		current, ok := v.mu.shards[u.ShardID]
		if !ok {
			current = ShardView{ShardID: u.ShardID}
		}
		v.mu.shards[u.ShardID] = mergeShardView(current, u)
	}
}

func (v *view) toShuffledList() []ShardView {
	ci := make([]ShardView, 0)
	func() {
		v.mu.Lock()
		defer v.mu.Unlock()
		for _, v := range v.mu.shards {
			ci = append(ci, v)
		}
	}()

	rand.Shuffle(len(ci), func(i, j int) { ci[i], ci[j] = ci[j], ci[i] })
	return ci
}

func getCompressedData(deploymentID uint64, l []ShardView, n int) []byte {
	if n == 0 {
		return nil
	}
	exchanged := exchanged{
		DeploymentID: deploymentID,
		ShardInfo:    l[:n],
	}
	var buf bytes.Buffer
	enc := gob.NewEncoder(&buf)
	if err := enc.Encode(exchanged); err != nil {
		panic(err)
	}
	data := buf.Bytes()
	compressed := make([]byte, lz4.CompressBlockBound(len(data))+4)
	var compressor lz4.Compressor
	n, err := compressor.CompressBlock(data, compressed[4:])
	if err != nil {
		panic(err)
	}
	binaryEnc.PutUint32(compressed, uint32(len(data)))
	return compressed[:n+4]
}

func (v *view) getFullSyncData() []byte {
	l := v.toShuffledList()
	return getCompressedData(v.deploymentID, l, len(l))
}

func (v *view) getGossipData(limit int) []byte {
	l := v.toShuffledList()
	if len(l) == 0 {
		return nil
	}
	// binary search to find the cut
	i, j := 1, len(l)
	for i < j {
		h := i + (j-i)/2
		data := getCompressedData(v.deploymentID, l, h)
		if len(data) < limit {
			i = h + 1
		} else {
			j = h
		}
	}

	for i > 0 {
		result := getCompressedData(v.deploymentID, l, i)
		if len(result) < limit {
			return result
		}
		i--
	}
	return nil
}

func (v *view) updateFrom(data []byte) {
	if len(data) <= 4 {
		panic("unexpected size")
	}
	sz := binaryEnc.Uint32(data)
	dst := make([]byte, sz)
	n, err := lz4.UncompressBlock(data[4:], dst)
	if err != nil {
		return
	}
	dst = dst[:n]
	buf := bytes.NewBuffer(dst)
	dec := gob.NewDecoder(buf)
	exchanged := exchanged{}
	if err := dec.Decode(&exchanged); err != nil {
		return
	}
	if exchanged.DeploymentID != v.deploymentID {
		return
	}
	v.update(exchanged.ShardInfo)
}
````

## File: internal/rsm/adapter_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"bytes"
	"testing"

	"github.com/lni/dragonboat/v4/internal/tests"
	"github.com/stretchr/testify/require"
)

func TestOnDiskSMCanBeOpened(t *testing.T) {
	applied := uint64(123)
	fd := tests.NewFakeDiskSM(applied)
	od := NewOnDiskStateMachine(fd)
	idx, err := od.Open(nil)
	require.NoError(t, err, "failed to open")
	require.Equal(t, applied, idx, "unexpected idx")
}

func TestOnDiskSMCanNotBeOpenedMoreThanOnce(t *testing.T) {
	applied := uint64(123)
	fd := tests.NewFakeDiskSM(applied)
	od := NewOnDiskStateMachine(fd)
	idx, err := od.Open(nil)
	require.NoError(t, err, "failed to open")
	require.Equal(t, applied, idx, "unexpected idx")

	require.Panics(t, func() {
		_, err := od.Open(nil)
		require.NoError(t, err, "open failed")
	}, "expected panic when opening twice")
}

func TestLookupCanBeCalledOnceOnDiskSMIsOpened(t *testing.T) {
	applied := uint64(123)
	fd := tests.NewFakeDiskSM(applied)
	od := NewOnDiskStateMachine(fd)
	_, err := od.Open(nil)
	require.NoError(t, err, "failed to open")
	_, err = od.Lookup(nil)
	require.NoError(t, err, "lookup failed")
}

func TestRecoverFromSnapshotCanComplete(t *testing.T) {
	applied := uint64(123)
	fd := tests.NewFakeDiskSM(applied)
	od := NewOnDiskStateMachine(fd)
	_, err := od.Open(nil)
	require.NoError(t, err, "failed to open")
	buf := make([]byte, 16)
	reader := bytes.NewBuffer(buf)
	stopc := make(chan struct{})
	err = od.Recover(reader, nil, stopc)
	require.NoError(t, err, "recover from snapshot failed")
}
````

## File: internal/rsm/adapter.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"io"

	"github.com/cockroachdb/errors"

	"github.com/lni/dragonboat/v4/config"
	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
)

// IStateMachine is an adapter interface for underlying sm.IStateMachine,
// sm.IConcurrentStateMachine and sm.IOnDIskStateMachine instances.
type IStateMachine interface {
	Open(<-chan struct{}) (uint64, error)
	Update(entries []sm.Entry) ([]sm.Entry, error)
	Lookup(query interface{}) (interface{}, error)
	NALookup(query []byte) ([]byte, error)
	Sync() error
	Prepare() (interface{}, error)
	Save(interface{},
		io.Writer, sm.ISnapshotFileCollection, <-chan struct{}) error
	Recover(io.Reader, []sm.SnapshotFile, <-chan struct{}) error
	Close() error
	GetHash() (uint64, error)
	Concurrent() bool
	OnDisk() bool
	Type() pb.StateMachineType
}

// InMemStateMachine is a regular state machine not capable of concurrent
// access from multiple goroutines.
type InMemStateMachine struct {
	sm sm.IStateMachine
	h  sm.IHash
	na sm.IExtended
}

var _ IStateMachine = (*InMemStateMachine)(nil)

// NewInMemStateMachine creates a new InMemStateMachine instance.
func NewInMemStateMachine(s sm.IStateMachine) *InMemStateMachine {
	i := &InMemStateMachine{sm: s}
	if h, ok := s.(sm.IHash); ok {
		i.h = h
	}
	if na, ok := s.(sm.IExtended); ok {
		i.na = na
	}
	return i
}

// Open opens the state machine.
func (i *InMemStateMachine) Open(stopc <-chan struct{}) (uint64, error) {
	panic("Open() called on InMemStateMachine")
}

// Update updates the state machine.
func (i *InMemStateMachine) Update(entries []sm.Entry) ([]sm.Entry, error) {
	if len(entries) != 1 {
		panic("len(entries) != 1")
	}
	var err error
	entries[0].Result, err = i.sm.Update(entries[0])
	return entries, errors.WithStack(err)
}

// Lookup queries the state machine.
func (i *InMemStateMachine) Lookup(query interface{}) (interface{}, error) {
	return i.sm.Lookup(query)
}

// NALookup queries the state machine.
func (i *InMemStateMachine) NALookup(query []byte) ([]byte, error) {
	if i.na == nil {
		return nil, sm.ErrNotImplemented
	}
	return i.na.NALookup(query)
}

// Sync synchronizes all in-core state with that on disk.
func (i *InMemStateMachine) Sync() error {
	panic("Sync not implemented in InMemStateMachine")
}

// Prepare makes preparations for taking concurrent snapshot.
func (i *InMemStateMachine) Prepare() (interface{}, error) {
	panic("Prepare not implemented in InMemStateMachine")
}

// Save saves the snapshot.
func (i *InMemStateMachine) Save(ctx interface{},
	w io.Writer, fc sm.ISnapshotFileCollection, stopc <-chan struct{}) error {
	if ctx != nil {
		panic("snapshot ctx is not nil")
	}
	return errors.WithStack(i.sm.SaveSnapshot(w, fc, stopc))
}

// Recover recovers the state machine from a snapshot.
func (i *InMemStateMachine) Recover(r io.Reader,
	fs []sm.SnapshotFile, stopc <-chan struct{}) error {
	return errors.WithStack(i.sm.RecoverFromSnapshot(r, fs, stopc))
}

// Close closes the state machine.
func (i *InMemStateMachine) Close() error {
	return errors.WithStack(i.sm.Close())
}

// GetHash returns the uint64 hash value representing the state of a state
// machine.
func (i *InMemStateMachine) GetHash() (uint64, error) {
	if i.h == nil {
		return 0, sm.ErrNotImplemented
	}
	h, err := i.h.GetHash()
	return h, errors.WithStack(err)
}

// Concurrent returns a boolean flag indicating whether the state machine is
// capable of taking concurrent snapshot.
func (i *InMemStateMachine) Concurrent() bool {
	return false
}

// OnDisk returns a boolean flag indicating whether this is an on disk state
// machine.
func (i *InMemStateMachine) OnDisk() bool {
	return false
}

// Type returns the type of the state machine.
func (i *InMemStateMachine) Type() pb.StateMachineType {
	return pb.RegularStateMachine
}

// ConcurrentStateMachine is an IStateMachine type capable of taking concurrent
// snapshots.
type ConcurrentStateMachine struct {
	sm sm.IConcurrentStateMachine
	h  sm.IHash
	na sm.IExtended
}

// NewConcurrentStateMachine creates a new ConcurrentStateMachine instance.
func NewConcurrentStateMachine(s sm.IConcurrentStateMachine) *ConcurrentStateMachine {
	v := &ConcurrentStateMachine{sm: s}
	if h, ok := s.(sm.IHash); ok {
		v.h = h
	}
	if na, ok := s.(sm.IExtended); ok {
		v.na = na
	}
	return v
}

// Open opens the state machine.
func (s *ConcurrentStateMachine) Open(stopc <-chan struct{}) (uint64, error) {
	panic("Open() not implemented ConcurrentStateMachine")
}

// Update updates the state machine.
func (s *ConcurrentStateMachine) Update(entries []sm.Entry) ([]sm.Entry, error) {
	results, err := s.sm.Update(entries)
	return results, errors.WithStack(err)
}

// Lookup queries the state machine.
func (s *ConcurrentStateMachine) Lookup(query interface{}) (interface{}, error) {
	return s.sm.Lookup(query)
}

// NALookup queries the state machine.
func (s *ConcurrentStateMachine) NALookup(query []byte) ([]byte, error) {
	if s.na == nil {
		return nil, sm.ErrNotImplemented
	}
	return s.na.NALookup(query)
}

// Sync synchronizes all in-core state with that on disk.
func (s *ConcurrentStateMachine) Sync() error {
	panic("Sync not implemented in ConcurrentStateMachine")
}

// Prepare makes preparations for taking concurrent snapshot.
func (s *ConcurrentStateMachine) Prepare() (interface{}, error) {
	results, err := s.sm.PrepareSnapshot()
	return results, errors.WithStack(err)
}

// Save saves the snapshot.
func (s *ConcurrentStateMachine) Save(ctx interface{},
	w io.Writer, fc sm.ISnapshotFileCollection, stopc <-chan struct{}) error {
	return errors.WithStack(s.sm.SaveSnapshot(ctx, w, fc, stopc))
}

// Recover recovers the state machine from a snapshot.
func (s *ConcurrentStateMachine) Recover(r io.Reader,
	fs []sm.SnapshotFile, stopc <-chan struct{}) error {
	return errors.WithStack(s.sm.RecoverFromSnapshot(r, fs, stopc))
}

// Close closes the state machine.
func (s *ConcurrentStateMachine) Close() error {
	return errors.WithStack(s.sm.Close())
}

// GetHash returns the uint64 hash value representing the state of a state
// machine.
func (s *ConcurrentStateMachine) GetHash() (uint64, error) {
	if s.h == nil {
		return 0, sm.ErrNotImplemented
	}
	h, err := s.h.GetHash()
	return h, errors.WithStack(err)
}

// Concurrent returns a boolean flag indicating whether the state machine is
// capable of taking concurrent snapshot.
func (s *ConcurrentStateMachine) Concurrent() bool {
	return true
}

// OnDisk returns a boolean flag indicating whether this is a on disk state
// machine.
func (s *ConcurrentStateMachine) OnDisk() bool {
	return false
}

// Type returns the type of the state machine.
func (s *ConcurrentStateMachine) Type() pb.StateMachineType {
	return pb.ConcurrentStateMachine
}

// ITestFS is an interface implemented by test SMs.
type ITestFS interface {
	SetTestFS(fs config.IFS)
}

// OnDiskStateMachine is the type to represent an on disk state machine.
type OnDiskStateMachine struct {
	sm     sm.IOnDiskStateMachine
	h      sm.IHash
	na     sm.IExtended
	opened bool
}

// NewOnDiskStateMachine creates and returns an on disk state machine.
func NewOnDiskStateMachine(s sm.IOnDiskStateMachine) *OnDiskStateMachine {
	r := &OnDiskStateMachine{sm: s}
	if h, ok := s.(sm.IHash); ok {
		r.h = h
	}
	if na, ok := s.(sm.IExtended); ok {
		r.na = na
	}
	return r
}

// SetTestFS injects the specified fs to the test SM.
func (s *OnDiskStateMachine) SetTestFS(fs config.IFS) {
	if tfs, ok := s.sm.(ITestFS); ok {
		plog.Infof("the underlying SM support test fs injection")
		tfs.SetTestFS(fs)
	}
}

// Open opens the state machine.
func (s *OnDiskStateMachine) Open(stopc <-chan struct{}) (uint64, error) {
	if s.opened {
		panic("Open invoked again")
	}
	s.opened = true
	applied, err := s.sm.Open(stopc)
	return applied, errors.WithStack(err)
}

// Update updates the state machine.
func (s *OnDiskStateMachine) Update(entries []sm.Entry) ([]sm.Entry, error) {
	s.ensureOpened()
	results, err := s.sm.Update(entries)
	return results, errors.WithStack(err)
}

// Lookup queries the state machine.
func (s *OnDiskStateMachine) Lookup(query interface{}) (interface{}, error) {
	s.ensureOpened()
	return s.sm.Lookup(query)
}

// NALookup queries the state machine.
func (s *OnDiskStateMachine) NALookup(query []byte) ([]byte, error) {
	s.ensureOpened()
	return s.na.NALookup(query)
}

// Sync synchronizes all in-core state with that on disk.
func (s *OnDiskStateMachine) Sync() error {
	s.ensureOpened()
	return errors.WithStack(s.sm.Sync())
}

// Prepare makes preparations for taking concurrent snapshot.
func (s *OnDiskStateMachine) Prepare() (interface{}, error) {
	s.ensureOpened()
	results, err := s.sm.PrepareSnapshot()
	return results, errors.WithStack(err)
}

// Save saves the snapshot.
func (s *OnDiskStateMachine) Save(ctx interface{},
	w io.Writer, fc sm.ISnapshotFileCollection, stopc <-chan struct{}) error {
	s.ensureOpened()
	return errors.WithStack(s.sm.SaveSnapshot(ctx, w, stopc))
}

// Recover recovers the state machine from a snapshot.
func (s *OnDiskStateMachine) Recover(r io.Reader,
	fs []sm.SnapshotFile, stopc <-chan struct{}) error {
	s.ensureOpened()
	return errors.WithStack(s.sm.RecoverFromSnapshot(r, stopc))
}

// Close closes the state machine.
func (s *OnDiskStateMachine) Close() error {
	return errors.WithStack(s.sm.Close())
}

// GetHash returns the uint64 hash value representing the state of a state
// machine.
func (s *OnDiskStateMachine) GetHash() (uint64, error) {
	s.ensureOpened()
	if s.h == nil {
		return 0, sm.ErrNotImplemented
	}
	h, err := s.h.GetHash()
	return h, errors.WithStack(err)
}

// Concurrent returns a boolean flag indicating whether the state machine is
// capable of taking concurrent snapshot.
func (s *OnDiskStateMachine) Concurrent() bool {
	return true
}

// OnDisk returns a boolean flag indicating whether this is an on disk state
// machine.
func (s *OnDiskStateMachine) OnDisk() bool {
	return true
}

// Type returns the type of the state machine.
func (s *OnDiskStateMachine) Type() pb.StateMachineType {
	return pb.OnDiskStateMachine
}

func (s *OnDiskStateMachine) ensureOpened() {
	if !s.opened {
		panic("not opened")
	}
}
````

## File: internal/rsm/chunkwriter_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"bytes"
	"encoding/binary"
	"testing"

	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
	"github.com/stretchr/testify/require"
)

type testSink struct {
	chunks     []pb.Chunk
	sendFailed bool
	stopped    bool
}

func (s *testSink) Receive(chunk pb.Chunk) (bool, bool) {
	if s.sendFailed || s.stopped {
		return !s.sendFailed, s.stopped
	}
	s.chunks = append(s.chunks, chunk)
	return true, false
}

func (s *testSink) Close() error {
	s.Receive(pb.Chunk{ChunkCount: pb.PoisonChunkCount})
	return nil
}

func (s *testSink) ShardID() uint64 {
	return 2000
}

func (s *testSink) ToReplicaID() uint64 {
	return 300
}

func getTestSSMeta() SSMeta {
	return SSMeta{
		Index: 1000,
		Term:  5,
		From:  150,
	}
}

func TestChunkWriterCanBeWritten(t *testing.T) {
	meta := getTestSSMeta()
	cw := NewChunkWriter(&testSink{}, meta)
	for i := 0; i < 10; i++ {
		data := make([]byte, ChunkSize)
		_, err := cw.Write(data)
		require.NoError(t, err, "failed to write the data")
	}
	err := cw.Close()
	require.NoError(t, err, "failed to flush")

	chunks := cw.sink.(*testSink).chunks
	require.Equal(t, 13, len(chunks), "chunks count mismatch")

	for idx, chunk := range chunks {
		switch idx {
		case 0:
			sz := binary.LittleEndian.Uint64(chunk.Data)
			headerData := chunk.Data[8 : 8+sz]
			var header pb.SnapshotHeader
			err := header.Unmarshal(headerData)
			require.NoError(t, err, "failed to unmarshal")
		case 11:
			require.Equal(t, pb.LastChunkCount, chunk.ChunkCount,
				"last chunk not marked, %d", idx)
			require.Equal(t, pb.LastChunkCount, chunk.FileChunkCount,
				"last chunk not marked, %d", idx)
		case 12:
		default:
			require.Equal(t, uint64(0), chunk.ChunkCount,
				"unexpectedly marked as last chunk, %d", idx)
		}
	}
}

func TestChunkWriterCanFailWrite(t *testing.T) {
	meta := getTestSSMeta()
	sink := &testSink{}
	cw := NewChunkWriter(sink, meta)
	for i := 0; i < 10; i++ {
		data := make([]byte, ChunkSize)
		_, err := cw.Write(data)
		require.NoError(t, err, "failed to write the data")
	}
	sink.sendFailed = true
	data := make([]byte, ChunkSize)
	_, err := cw.Write(data)
	require.Error(t, err, "writer didn't fail")
	require.Equal(t, sm.ErrSnapshotStreaming, err, "unexpected err")
}

func TestChunkWriterCanBeStopped(t *testing.T) {
	meta := getTestSSMeta()
	sink := &testSink{}
	cw := NewChunkWriter(sink, meta)
	for i := 0; i < 10; i++ {
		data := make([]byte, ChunkSize)
		_, err := cw.Write(data)
		require.NoError(t, err, "failed to write the data")
	}
	sink.stopped = true
	data := make([]byte, ChunkSize)
	_, err := cw.Write(data)
	require.Error(t, err, "writer didn't fail")
	require.Equal(t, sm.ErrSnapshotStopped, err, "unexpected err")
}

func TestGetTailChunk(t *testing.T) {
	meta := getTestSSMeta()
	sink := &testSink{}
	cw := NewChunkWriter(sink, meta)
	chunk := cw.getTailChunk()
	require.Equal(t, pb.LastChunkCount, chunk.ChunkCount,
		"chunk count mismatch")
	require.Equal(t, pb.LastChunkCount, chunk.FileChunkCount,
		"file chunk count mismatch")
}

func TestCloseChunk(t *testing.T) {
	meta := getTestSSMeta()
	sink := &testSink{}
	cw := NewChunkWriter(sink, meta)
	require.NoError(t, cw.Close())
	chunk := sink.chunks[len(sink.chunks)-1]
	require.Equal(t, pb.LastChunkCount-1, chunk.ChunkCount,
		"chunk count mismatch")
}

func TestFailedChunkWriterWillNotSendTheTailChunk(t *testing.T) {
	meta := getTestSSMeta()
	sink := &testSink{}
	cw := NewChunkWriter(sink, meta)
	cw.failed = true
	require.NoError(t, cw.Close())

	chunks := cw.sink.(*testSink).chunks
	for idx, chunk := range chunks {
		switch idx {
		case 0:
			sz := binary.LittleEndian.Uint64(chunk.Data)
			headerdata := chunk.Data[8 : 8+sz]
			crc := chunk.Data[8+sz : 12+sz]
			var header pb.SnapshotHeader
			err := header.Unmarshal(headerdata)
			require.NoError(t, err)

			expectedSize := HeaderSize + tailSize
			require.Equal(t, uint64(len(chunk.Data)), expectedSize,
				"unexpected data size")

			checksum := header.HeaderChecksum
			require.Nil(t, checksum, "not expected to set the checksum field")

			h := newCRC32Hash()
			_, err = h.Write(headerdata)
			require.NoError(t, err)
			require.True(t, bytes.Equal(h.Sum(nil), crc), "not a valid header")
		case 1:
			require.Equal(t, pb.PoisonChunkCount, chunk.ChunkCount,
				"unexpected chunk count")
		default:
			require.Fail(t, "unexpected chunk received")
		}
	}
}
````

## File: internal/rsm/chunkwriter.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"encoding/binary"
	"io"
	"time"

	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
)

const (
	// ChunkSize is the size of each snapshot chunk.
	ChunkSize = settings.SnapshotChunkSize
)

var _ io.WriteCloser = (*ChunkWriter)(nil)

// ChunkWriter is an io.WriteCloser type that streams snapshot chunks to its
// intended remote nodes.
type ChunkWriter struct {
	sink    pb.IChunkSink
	bw      IBlockWriter
	meta    SSMeta
	chunkID uint64
	failed  bool
	stopped bool
}

// NewChunkWriter creates and returns a chunk writer instance.
func NewChunkWriter(sink pb.IChunkSink, meta SSMeta) *ChunkWriter {
	cw := &ChunkWriter{
		sink: sink,
		meta: meta,
	}
	cw.bw = NewBlockWriter(ChunkSize, cw.onNewBlock, DefaultChecksumType)
	return cw
}

// Close closes the chunk writer.
func (cw *ChunkWriter) Close() error {
	if err := cw.bw.Close(); err != nil {
		return err
	}
	if !cw.failed {
		if err := cw.onNewChunk(cw.getTailChunk()); err != nil {
			return err
		}
	}
	return cw.sink.Close()
}

// Write writes the specified input data.
func (cw *ChunkWriter) Write(data []byte) (int, error) {
	if cw.stopped {
		return 0, sm.ErrSnapshotStopped
	}
	if cw.failed {
		return 0, sm.ErrSnapshotStreaming
	}
	return cw.bw.Write(data)
}

func (cw *ChunkWriter) onNewBlock(data []byte, crc []byte) error {
	defer func() {
		cw.chunkID = cw.chunkID + 1
	}()
	chunk := cw.getChunk()
	var payload []byte
	if cw.chunkID == 0 {
		payload = cw.getHeader()
	} else {
		payload = make([]byte, 0, len(data)+len(crc))
	}
	payload = append(payload, data...)
	payload = append(payload, crc...)
	chunk.Data = payload
	chunk.ChunkSize = uint64(len(payload))
	return cw.onNewChunk(chunk)
}

func (cw *ChunkWriter) onNewChunk(chunk pb.Chunk) error {
	sent, stopped := cw.sink.Receive(chunk)
	if stopped {
		cw.stopped = true
		return sm.ErrSnapshotStopped
	}
	if !sent {
		cw.failed = true
		return sm.ErrSnapshotStreaming
	}
	return nil
}

func (cw *ChunkWriter) getHeader() []byte {
	header := pb.SnapshotHeader{
		SessionSize:     0,
		DataStoreSize:   0,
		UnreliableTime:  uint64(time.Now().UnixNano()),
		PayloadChecksum: []byte{0, 0, 0, 0},
		ChecksumType:    DefaultChecksumType,
		Version:         uint64(V2),
		CompressionType: cw.meta.CompressionType,
	}
	data := pb.MustMarshal(&header)
	h := newCRC32Hash()
	fileutil.MustWrite(h, data)
	checksum := h.Sum(nil)
	result := make([]byte, HeaderSize)
	binary.LittleEndian.PutUint64(result, uint64(len(data)))
	copy(result[8:], data)
	copy(result[8+len(data):], checksum)
	return result
}

func (cw *ChunkWriter) getChunk() pb.Chunk {
	return pb.Chunk{
		ShardID:     cw.sink.ShardID(),
		ReplicaID:   cw.sink.ToReplicaID(),
		From:        cw.meta.From,
		ChunkId:     cw.chunkID,
		FileChunkId: cw.chunkID,
		Index:       cw.meta.Index,
		Term:        cw.meta.Term,
		OnDiskIndex: cw.meta.OnDiskIndex,
		Membership:  cw.meta.Membership,
		BinVer:      raftio.TransportBinVersion,
		Filepath:    server.GetSnapshotFilename(cw.meta.Index),
	}
}

func (cw *ChunkWriter) getTailChunk() pb.Chunk {
	tailChunk := cw.getChunk()
	tailChunk.ChunkCount = pb.LastChunkCount
	tailChunk.FileChunkCount = pb.LastChunkCount
	return tailChunk
}
````

## File: internal/rsm/encoded_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"crypto/rand"
	"testing"

	"github.com/lni/dragonboat/v4/internal/utils/dio"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/stretchr/testify/require"
)

func mustGetPayload(e pb.Entry) []byte {
	p, err := GetPayload(e)
	if err != nil {
		panic(err)
	}
	return p
}

func TestGetEntryPayload(t *testing.T) {
	e1 := pb.Entry{Cmd: []byte{1, 2, 3, 4, 5}}
	e2 := pb.Entry{Cmd: []byte{1, 2, 3}, Type: pb.ConfigChangeEntry}
	e3payload := []byte{1, 2, 3, 4, 5, 6, 7, 8, 9}
	e3 := pb.Entry{
		Type: pb.EncodedEntry,
		Cmd:  GetEncoded(dio.Snappy, e3payload, make([]byte, 512)),
	}
	require.Equal(t, e1.Cmd, mustGetPayload(e1), "e1 payload changed")
	require.Equal(t, e2.Cmd, mustGetPayload(e2), "e2 payload changed")
	require.Equal(t, e3payload, mustGetPayload(e3), "e3 payload changed")
}

func TestGetV0EncodedPayload(t *testing.T) {
	l1, _ := dio.MaxEncodedLen(dio.Snappy, 16)
	tests := []struct {
		ct  dio.CompressionType
		src uint64
		dst uint64
	}{
		{dio.NoCompression, 16, 0},
		{dio.NoCompression, 16, 1},
		{dio.NoCompression, 16, 16},
		{dio.NoCompression, 16, 17},
		{dio.Snappy, 16, 0},
		{dio.Snappy, 16, 1},
		{dio.Snappy, 16, 16},
		{dio.Snappy, 16, l1},
		{dio.Snappy, 16, l1 - 1},
		{dio.Snappy, 16, l1 + 1},
		{dio.Snappy, 16, 128},
	}
	for idx, tt := range tests {
		plog.Infof("idx: %d", idx)
		var src []byte
		if tt.src == 0 {
			src = nil
		} else {
			src = make([]byte, tt.src)
			_, err := rand.Read(src)
			require.NoError(t, err)
		}
		var dst []byte
		if tt.dst == 0 {
			dst = nil
		} else {
			dst = make([]byte, tt.dst)
		}
		result := GetEncoded(tt.ct, src, dst)
		ver, ct, hasSession := parseEncodedHeader(result)
		require.Equal(t, EEV0, ver, "invalid version number %d", ver)
		require.False(t, hasSession, "unexpectedly has session flag set")
		if tt.ct == dio.NoCompression {
			require.Equal(t, EENoCompression, ct, "unexpected ct")
		}
		if tt.ct == dio.Snappy {
			require.Equal(t, EESnappy, ct, "invalid ct")
		}
		decoded, err := getDecodedPayload(result, nil)
		require.NoError(t, err, "failed to get decoded payload %v", err)
		require.Equal(t, src, decoded, "%d, content changed", idx)
	}
}
````

## File: internal/rsm/encoded.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"encoding/binary"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/utils/dio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

// Entry Cmd format when Type = pb.EncodedEntry
//
// -------------------------------------
// |Version|CompressionFlag|SessionFlag|
// | 4Bits |     3Bits     |   1Bit    |
// -------------------------------------
const (
	EEHeaderSize uint8 = 1
	EEVersion    uint8 = 0 << 4
	EEV0         uint8 = 0 << 4

	// for V0 format, entries with empty payload will cause panic as such
	// entries always have their TYPE value set to ApplicationEntry
	EENoCompression uint8 = 0 << 1
	EESnappy        uint8 = 1 << 1

	EENoSession  uint8 = 0
	EEHasSession uint8 = 1
	// uncompressed size is binary.Uvarint encoded
	EEV0SizeOffset int = 1
)

// GetMaxBlockSize returns the maximum block length supported by the specified
// compression type.
func GetMaxBlockSize(ct config.CompressionType) uint64 {
	return dio.MaxBlockLen(ToDioType(ct))
}

// GetPayload returns the payload of the entry ready to be applied into the
// state machine.
func GetPayload(e pb.Entry) ([]byte, error) {
	switch e.Type {
	case pb.ApplicationEntry:
		fallthrough
	case pb.ConfigChangeEntry:
		return e.Cmd, nil
	case pb.EncodedEntry:
		return getDecodedPayload(e.Cmd, nil)
	}
	panic("unknown entry type")
}

// ToDioType converts the CompressionType type defined in the config package to
// the CompressionType value defined in the dio package.
func ToDioType(ct config.CompressionType) dio.CompressionType {
	return ct
}

// GetEncoded returns the encoded payload using the specified compression type
// and the default encoded entry version.
func GetEncoded(ct dio.CompressionType, cmd []byte, dst []byte) []byte {
	if len(cmd) == 0 {
		panic("empty payload")
	}
	return getEncoded(ct, cmd, dst)
}

// get v0 encoded payload
func getEncoded(ct dio.CompressionType, cmd []byte, dst []byte) []byte {
	switch ct {
	case dio.NoCompression:
		// output is 1 byte header, len(cmd) bytes of payload
		if len(dst) < len(cmd)+1 {
			dst = make([]byte, len(cmd)+1)
		}
		dst[0] = getEncodedHeader(EEV0, EENoCompression, false)
		copy(dst[1:], cmd)
		return dst[:len(cmd)+1]
	case dio.Snappy:
		maxSize, ok := dio.MaxEncodedLen(dio.Snappy, uint64(len(cmd)))
		if !ok {
			panic("invalid payload length")
		}
		// 1 byte header
		maxSize = maxSize + 1
		if uint64(len(dst)) < maxSize {
			dst = make([]byte, maxSize)
		}
		dst[0] = getEncodedHeader(EEV0, EESnappy, false)
		dstLen := dio.CompressSnappyBlock(cmd, dst[1:])
		result := make([]byte, dstLen+1)
		copy(result, dst[:dstLen+1])
		return result
	default:
		panic("unknown compression type")
	}
}

func getEncodedHeader(version uint8, cf uint8, session bool) uint8 {
	result := uint8(0)
	result = result | version
	result = result | cf
	if session {
		result = result | EEHasSession
	} else {
		result = result | EENoSession
	}
	return result
}

func parseEncodedHeader(cmd []byte) (uint8, uint8, bool) {
	vermask := uint8(15 << 4)
	ctmask := uint8(7 << 1)
	sesmask := uint8(1)
	header := cmd[0]
	return header & vermask, header & ctmask, header&sesmask == 1
}

func getDecodedPayload(cmd []byte, buf []byte) ([]byte, error) {
	ver, ct, hasSession := parseEncodedHeader(cmd)
	if ver == EEV0 {
		if hasSession {
			plog.Panicf("v0 cmd has session info")
		}
		switch ct {
		case EENoCompression:
			return getV0NoCompressPayload(cmd), nil
		case EESnappy:
			sz, offset := getV0PayloadUncompressedSize(cmd)
			if sz == 0 {
				plog.Panicf("empty uncompressed size found")
			}
			if offset == 0 {
				plog.Panicf("zero offset found")
			}
			compressed := cmd[EEV0SizeOffset:]
			var result []byte
			if uint64(len(buf)) >= sz {
				result = buf[:sz]
			} else {
				result = make([]byte, sz)
			}
			if err := dio.DecompressSnappyBlock(compressed, result); err != nil {
				return nil, err
			}
			return result, nil
		default:
			plog.Panicf("unknown compression type %d", ct)
		}
	}
	panic("unknown cmd encoding version")
}

func getV0NoCompressPayload(cmd []byte) []byte {
	return cmd[EEHeaderSize:]
}

func getV0PayloadUncompressedSize(cmd []byte) (uint64, int) {
	return binary.Uvarint(cmd[EEV0SizeOffset:])
}
````

## File: internal/rsm/files_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"testing"

	"github.com/lni/goutils/leaktest"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/internal/vfs"
)

const (
	rdbTestDirectory = "rdb_test_dir_safe_to_delete"
)

func TestFileCanBeAddedToFileCollection(t *testing.T) {
	defer leaktest.AfterTest(t)()
	fc := NewFileCollection()
	fc.AddFile(1, "test.data", make([]byte, 12))
	fc.AddFile(2, "test.data2", make([]byte, 16))
	require.Equal(t, 2, len(fc.files))
	require.Equal(t, 2, len(fc.idmap))
	require.Equal(t, "test.data", fc.files[0].Filepath)
	require.Equal(t, uint64(1), fc.files[0].FileId)
	require.Equal(t, 12, len(fc.files[0].Metadata))
	require.Equal(t, "test.data2", fc.files[1].Filepath)
	require.Equal(t, uint64(2), fc.files[1].FileId)
	require.Equal(t, 16, len(fc.files[1].Metadata))
}

func TestFileWithDuplicatedIDCanNotBeAdded(t *testing.T) {
	defer leaktest.AfterTest(t)()
	fc := NewFileCollection()
	fc.AddFile(1, "test.data", make([]byte, 12))
	fc.AddFile(2, "test.data", make([]byte, 12))
	require.Panics(t, func() {
		fc.AddFile(1, "test.data2", make([]byte, 16))
	})
}

func TestPrepareFiles(t *testing.T) {
	fs := vfs.GetTestFS()
	if fs != vfs.DefaultFS {
		t.Skip("this test only support the default fs")
	}
	defer leaktest.AfterTest(t)()
	err := fs.MkdirAll(rdbTestDirectory, 0755)
	require.NoError(t, err)
	defer func() {
		err := fs.RemoveAll(rdbTestDirectory)
		require.NoError(t, err)
	}()
	f1, err := fs.Create(fs.PathJoin(rdbTestDirectory, "test1.data"))
	require.NoError(t, err)
	n, err := f1.Write(make([]byte, 16))
	require.Equal(t, 16, n)
	require.NoError(t, err)
	require.NoError(t, f1.Close())
	f2, err := fs.Create(fs.PathJoin(rdbTestDirectory, "test2.data"))
	require.NoError(t, err)
	n, err = f2.Write(make([]byte, 32))
	require.Equal(t, 32, n)
	require.NoError(t, err)
	require.NoError(t, f2.Close())
	fc := NewFileCollection()
	fc.AddFile(1, fs.PathJoin(rdbTestDirectory, "test1.data"),
		make([]byte, 8))
	fc.AddFile(2, fs.PathJoin(rdbTestDirectory, "test2.data"),
		make([]byte, 2))
	require.Equal(t, uint64(2), fc.Size())
	rf := fc.GetFileAt(0)
	expectedPath := fs.PathJoin(rdbTestDirectory, "test1.data")
	require.Equal(t, expectedPath, rf.Filepath)
	files, err := fc.PrepareFiles(rdbTestDirectory, rdbTestDirectory)
	require.NoError(t, err)
	require.Equal(t, uint64(1), files[0].FileId)
	require.Equal(t, "external-file-1", files[0].Filename())
	require.Equal(t, uint64(16), files[0].FileSize)
	require.Equal(t, uint64(2), files[1].FileId)
	require.Equal(t, "external-file-2", files[1].Filename())
	require.Equal(t, uint64(32), files[1].FileSize)
	fi1, err := fs.Stat(fs.PathJoin(rdbTestDirectory, "external-file-1"))
	require.NoError(t, err)
	require.Equal(t, int64(16), fi1.Size())
	fi2, err := fs.Stat(fs.PathJoin(rdbTestDirectory, "external-file-2"))
	require.NoError(t, err)
	require.Equal(t, int64(32), fi2.Size())
}
````

## File: internal/rsm/files.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"os"
	"path/filepath"

	pb "github.com/lni/dragonboat/v4/raftpb"
)

// Files is a collection of external files specified by the SaveSnapshot
// method of the state machine type.
type Files struct {
	idmap map[uint64]struct{}
	files []*pb.SnapshotFile
}

// NewFileCollection creates and returns a Files instance.
func NewFileCollection() *Files {
	return &Files{
		files: make([]*pb.SnapshotFile, 0),
		idmap: make(map[uint64]struct{}),
	}
}

// AddFile adds the specified file to the external file collection.
func (fc *Files) AddFile(fileID uint64,
	path string, metadata []byte) {
	if _, ok := fc.idmap[fileID]; ok {
		plog.Panicf("trying to add file %d again", fileID)
	}
	f := &pb.SnapshotFile{
		Filepath: path,
		FileId:   fileID,
		Metadata: metadata,
	}
	fc.files = append(fc.files, f)
	fc.idmap[fileID] = struct{}{}
}

// Size returns the number of external files already added to the external file
// collection.
func (fc *Files) Size() uint64 {
	return uint64(len(fc.files))
}

// GetFileAt returns the specified file.
func (fc *Files) GetFileAt(idx uint64) *pb.SnapshotFile {
	return fc.files[idx]
}

// PrepareFiles finalize the external files added to the collection.
func (fc *Files) PrepareFiles(tmpdir string,
	finaldir string) ([]*pb.SnapshotFile, error) {
	for _, file := range fc.files {
		fn := file.Filename()
		fp := filepath.Join(tmpdir, fn)
		if err := os.Link(file.Filepath, fp); err != nil {
			return nil, err
		}
		fi, err := os.Stat(fp)
		if err != nil {
			return nil, err
		}
		if fi.IsDir() {
			plog.Panicf("%s is a dir", fp)
		}
		if fi.Size() == 0 {
			plog.Panicf("empty file found, id %d",
				file.FileId)
		}
		file.Filepath = filepath.Join(finaldir, fn)
		file.FileSize = uint64(fi.Size())
	}
	return fc.files, nil
}
````

## File: internal/rsm/lrusession_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"bytes"
	"math/rand"
	"reflect"
	"testing"

	sm "github.com/lni/dragonboat/v4/statemachine"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

func TestRecCanBeEvicted(t *testing.T) {
	m := newLRUSession(3)
	for i := RaftClientID(0); i < 3; i++ {
		s := &Session{ClientID: i}
		m.addSession(i, *s)
	}
	// client id 1 used here
	r, ok := m.getSession(RaftClientID(0))
	require.True(t, ok, "session object not returned")
	assert.Equal(t, RaftClientID(0), r.ClientID, "client id mismatch")
	// client id 1 is the LRU target to be evicted
	i := RaftClientID(3)
	s := &Session{ClientID: i}
	m.addSession(i, *s)
	_, ok = m.getSession(RaftClientID(1))
	assert.False(t, ok, "didn't evict the first session object")
	// client id 2 is the LRU target to be evicted
	i = RaftClientID(4)
	s = &Session{ClientID: i}
	m.addSession(i, *s)
	_, ok = m.getSession(RaftClientID(2))
	assert.False(t, ok, "didn't evict the first session object")
	_, ok = m.getSession(RaftClientID(0))
	assert.True(t, ok, "client session with id 0 is expected to stay")
}

func TestSessionIsMutable(t *testing.T) {
	m := newLRUSession(1)
	for i := RaftClientID(0); i < 1; i++ {
		s := &Session{ClientID: i, History: make(map[RaftSeriesID]sm.Result)}
		m.addSession(i, *s)
	}
	// client id 1 used here
	r, ok := m.getSession(RaftClientID(0))
	require.True(t, ok, "session object not returned")
	assert.Equal(t, RaftClientID(0), r.ClientID, "client id mismatch")
	r.History[RaftSeriesID(100)] = sm.Result{Value: 200}
	r, ok = m.getSession(RaftClientID(0))
	require.True(t, ok, "session object not returned")
	assert.Equal(t, RaftClientID(0), r.ClientID, "client id mismatch")
	assert.Equal(t, 1, len(r.History), "history size mismatch")
}

func TestOrderedDoIsLRUOrdered(t *testing.T) {
	m := newLRUSession(100)
	for i := RaftClientID(0); i < 100; i++ {
		s := newSession(i)
		m.addSession(i, *s)
	}
	for i := 0; i < 100; i++ {
		idx := rand.Int() % 100
		m.getSession(RaftClientID(idx))

		idList := make([]RaftClientID, 0)
		m.sessions.OrderedDo(func(k, v interface{}) {
			key := k.(*RaftClientID)
			idList = append(idList, *key)
		})

		assert.Equal(t, 100, len(idList), "size mismatch")
		assert.Equal(t, RaftClientID(idx), idList[99], "last element mismatch")
	}
}

func TestLRUSessionCanBeSavedAndRestoredWithLRUOrderPreserved(t *testing.T) {
	m := newLRUSession(100)
	for i := RaftClientID(0); i < 100; i++ {
		count := rand.Int() % 100
		s := newSession(i)
		for j := 0; j < count; j++ {
			s.addResponse(RaftSeriesID(j), sm.Result{Value: uint64(j)})
		}
		m.addSession(i, *s)
	}
	for i := 0; i < 100; i++ {
		idx := rand.Int() % 100
		m.getSession(RaftClientID(idx))
	}
	oldList := make([]RaftClientID, 0)
	m.sessions.OrderedDo(func(k, v interface{}) {
		key := k.(*RaftClientID)
		oldList = append(oldList, *key)
	})
	snapshot := &bytes.Buffer{}
	err := m.save(snapshot)
	require.NoError(t, err, "save failed")
	data := snapshot.Bytes()
	toRecover := bytes.NewBuffer(data)
	newLRUSession := newLRUSession(5)
	err = newLRUSession.load(toRecover, V2)
	require.NoError(t, err, "load failed")
	newList := make([]RaftClientID, 0)
	newLRUSession.sessions.OrderedDo(func(k, v interface{}) {
		key := k.(*RaftClientID)
		newList = append(newList, *key)
	})
	assert.Equal(t, len(oldList), len(newList), "size mismatch")
	for idx := range oldList {
		assert.Equal(t, oldList[idx], newList[idx], "order is different")
	}
	for i := 0; i < 1000; i++ {
		if i%2 == 0 {
			idx := rand.Int() % 100
			m.getSession(RaftClientID(idx))
			newLRUSession.getSession(RaftClientID(idx))
		} else {
			v := RaftClientID(10000 * i)
			s1 := newSession(v)
			s2 := newSession(v)
			m.addSession(v, *s1)
			newLRUSession.addSession(v, *s2)
		}
	}
	oldList = make([]RaftClientID, 0)
	m.sessions.OrderedDo(func(k, v interface{}) {
		key := k.(*RaftClientID)
		oldList = append(oldList, *key)
	})
	newList = make([]RaftClientID, 0)
	newLRUSession.sessions.OrderedDo(func(k, v interface{}) {
		key := k.(*RaftClientID)
		newList = append(newList, *key)
	})
	assert.Equal(t, len(oldList), len(newList), "size mismatch")
	for idx := range oldList {
		assert.Equal(t, oldList[idx], newList[idx], "order is different")
	}
}

func TestLRUSessionCanBeSavedAndRestored(t *testing.T) {
	m := newLRUSession(3)
	for i := RaftClientID(0); i < 3; i++ {
		s := newSession(i)
		if i == RaftClientID(1) {
			s.addResponse(100, sm.Result{Value: 200})
			s.addResponse(200, sm.Result{Value: 300})
		} else if i == RaftClientID(2) {
			s.addResponse(300, sm.Result{Value: 500})
			s.addResponse(400, sm.Result{Value: 300})
			s.addResponse(500, sm.Result{Value: 700})
		}
		m.addSession(i, *s)
	}
	snapshot := &bytes.Buffer{}
	err := m.save(snapshot)
	require.NoError(t, err, "save failed")
	data := snapshot.Bytes()
	toRecover := bytes.NewBuffer(data)
	// set to a different size value
	newLRUSession := newLRUSession(5)
	err = newLRUSession.load(toRecover, V2)
	require.NoError(t, err, "load failed")
	oldHash := m.getHash()
	newHash := newLRUSession.getHash()
	assert.Equal(t, oldHash, newHash, "hash mismatch")
	assert.Equal(t, m.sessions.Len(), newLRUSession.sessions.Len(),
		"Len mismatch")
	assert.Equal(t, m.size, newLRUSession.size, "size mismatch")
	testSession := newSession(9)
	testKey := RaftClientID(1)
	assert.True(t, newLRUSession.sessions.ShouldEvict(4, &testKey, testSession),
		"should evict function not adjusted")
	assert.False(t, newLRUSession.sessions.ShouldEvict(3, &testKey, testSession),
		"should evict function not adjusted")
	for i := RaftClientID(0); i < 3; i++ {
		s1, ok1 := m.getSession(i)
		s2, ok2 := newLRUSession.getSession(i)
		assert.Equal(t, ok1, ok2, "ok mismatch")
		assert.True(t, reflect.DeepEqual(s1, s2), "session mismatch")
	}
}

func TestGetEmptyLRUSession(t *testing.T) {
	s := newLRUSession(LRUMaxSessionCount)
	buf := bytes.NewBuffer(make([]byte, 0))
	err := s.save(buf)
	require.NoError(t, err, "failed to save")
	data := buf.Bytes()
	assert.Equal(t, EmptyClientSessionLength, uint64(len(data)),
		"unexpected length")
	assert.True(t, bytes.Equal(data, GetEmptyLRUSession()), "unexpected data")
}
````

## File: internal/rsm/lrusession.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"bytes"
	"crypto/md5"
	"encoding/binary"
	"io"
	"sync"

	"github.com/lni/goutils/cache"

	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/settings"
)

const (
	// EmptyClientSessionLength defines the length of an empty sessions instance.
	EmptyClientSessionLength uint64 = 16
)

var (
	// LRUMaxSessionCount is the largest number of client sessions that can be
	// concurrently managed by a LRUSession instance.
	LRUMaxSessionCount = settings.Hard.LRUMaxSessionCount
)

// GetEmptyLRUSession returns an marshaled empty sessions instance.
func GetEmptyLRUSession() []byte {
	v := make([]byte, 0)
	sz := make([]byte, 8)
	binary.LittleEndian.PutUint64(sz, LRUMaxSessionCount)
	total := make([]byte, 8)
	binary.LittleEndian.PutUint64(total, 0)
	v = append(v, sz...)
	v = append(v, total...)
	return v
}

// lrusession is a session manager that keeps up to size number of client
// sessions. LRU is the policy for evicting old ones.
type lrusession struct {
	sessions  *cache.OrderedCache
	size      uint64
	searchKey RaftClientID
	sync.Mutex
}

// Newlrusession returns a new lrusession instance that can hold up to size
// client sessions.
func newLRUSession(size uint64) *lrusession {
	if size == 0 {
		panic("lrusession size must be > 0")
	}
	rec := &lrusession{
		size:     size,
		sessions: cache.NewOrderedCache(cache.Config{Policy: cache.CacheLRU}),
	}
	rec.sessions.ShouldEvict = func(n int, k, v interface{}) bool {
		if uint64(n) > rec.size {
			clientID := k.(*RaftClientID)
			plog.Warningf("session with client id %d evicted, overloaded", *clientID)
			return true
		}
		return false
	}
	rec.sessions.OnEvicted = func(k, v, e interface{}) {}
	return rec
}

// GetSession returns the client session identified by the key.
func (rec *lrusession) getSession(key RaftClientID) (*Session, bool) {
	rec.Lock()
	defer rec.Unlock()
	return rec.getSessionLocked(key)
}

// Save checkpoints the state of the lrusession and save the checkpointed
// state into the writer.
func (rec *lrusession) save(writer io.Writer) error {
	rec.Lock()
	defer rec.Unlock()
	idList := make([]RaftClientID, 0)
	rec.sessions.OrderedDo(func(k, v interface{}) {
		key := k.(*RaftClientID)
		idList = append(idList, *key)
	})
	totalbuf := make([]byte, 8)
	binary.LittleEndian.PutUint64(totalbuf, rec.size)
	if _, err := writer.Write(totalbuf); err != nil {
		return err
	}
	binary.LittleEndian.PutUint64(totalbuf, uint64(len(idList)))
	if _, err := writer.Write(totalbuf); err != nil {
		return err
	}
	for _, key := range idList {
		session, ok := rec.getSessionLocked(key)
		if !ok || session == nil {
			panic("bad state")
		}
		if err := session.save(writer); err != nil {
			return err
		}
	}
	return nil
}

// Load restores the state the of lrusession from the provided reader.
// reader contains lrusession state previously checkpointed.
func (rec *lrusession) load(reader io.Reader, v SSVersion) error {
	rec.Lock()
	defer rec.Unlock()
	sessionList := make([]*Session, 0)
	sizebuf := make([]byte, 8)
	if _, err := io.ReadFull(reader, sizebuf); err != nil {
		return err
	}
	sz := binary.LittleEndian.Uint64(sizebuf)
	if _, err := io.ReadFull(reader, sizebuf); err != nil {
		return err
	}
	total := binary.LittleEndian.Uint64(sizebuf)
	for i := uint64(0); i < total; i++ {
		s := &Session{}
		err := s.recoverFromSnapshot(reader, v)
		if err != nil {
			return err
		}
		sessionList = append(sessionList, s)
	}
	newRec := newLRUSession(sz)
	rec.sessions = newRec.sessions
	rec.size = sz
	for _, s := range sessionList {
		rec.addSessionLocked(s.ClientID, *s)
	}
	return nil
}

func (rec *lrusession) makeEntry(key RaftClientID,
	value Session) *cache.Entry {
	alloc := struct {
		entry cache.Entry
		value Session
		key   RaftClientID
	}{
		key:   key,
		value: value,
	}
	alloc.entry.Key = &alloc.key
	alloc.entry.Value = &alloc.value
	return &alloc.entry
}

func (rec *lrusession) addSession(key RaftClientID, s Session) {
	rec.Lock()
	defer rec.Unlock()
	rec.addSessionLocked(key, s)
}

func (rec *lrusession) addSessionLocked(key RaftClientID, s Session) {
	entry := rec.makeEntry(key, s)
	rec.sessions.AddEntry(entry)
}

func (rec *lrusession) getSessionLocked(key RaftClientID) (*Session, bool) {
	rec.searchKey = key
	v, ok := rec.sessions.Get(&rec.searchKey)
	if ok {
		return v.(*Session), ok
	}
	return nil, ok
}

func (rec *lrusession) delSession(key RaftClientID) {
	rec.Lock()
	defer rec.Unlock()
	rec.sessions.Del(&key)
}

func (rec *lrusession) getHash() uint64 {
	snapshot := &bytes.Buffer{}
	if err := rec.save(snapshot); err != nil {
		panic(err)
	}
	data := snapshot.Bytes()
	hash := md5.New()
	fileutil.MustWrite(hash, data)
	md5sum := hash.Sum(nil)
	return binary.LittleEndian.Uint64(md5sum[:8])
}
````

## File: internal/rsm/managed_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"io"
	"math/rand"
	"testing"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/fileutil"
	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
	"github.com/stretchr/testify/require"
)

func TestCountedWriteCanReportTotalWritten(t *testing.T) {
	cw := countedWriter{
		w: fileutil.Discard,
	}
	total := uint64(0)
	for i := 0; i < 16; i++ {
		sz := rand.Uint64() % 1024
		v := make([]byte, sz)
		total += sz
		_, err := cw.Write(v)
		require.NoError(t, err)
	}
	require.Equal(t, total, cw.total)
}

type dummySM struct{}

func (d *dummySM) Open(<-chan struct{}) (uint64, error)          { return 0, nil }
func (d *dummySM) Update(entries []sm.Entry) ([]sm.Entry, error) { return nil, nil }
func (d *dummySM) Lookup(query interface{}) (interface{}, error) { return nil, nil }
func (d *dummySM) NALookup(query []byte) ([]byte, error)         { return nil, nil }
func (d *dummySM) Sync() error                                   { return nil }
func (d *dummySM) Prepare() (interface{}, error)                 { return nil, nil }
func (d *dummySM) Save(interface{},
	io.Writer, sm.ISnapshotFileCollection, <-chan struct{}) error {
	return nil
}
func (d *dummySM) Recover(io.Reader, []sm.SnapshotFile, <-chan struct{}) error {
	return nil
}
func (d *dummySM) Close() error             { return nil }
func (d *dummySM) GetHash() (uint64, error) { return 0, nil }
func (d *dummySM) Concurrent() bool         { return false }
func (d *dummySM) OnDisk() bool             { return false }
func (d *dummySM) Type() pb.StateMachineType {
	return pb.OnDiskStateMachine
}

func TestDestroyedFlagIsSetWhenDestroyed(t *testing.T) {
	sm := NewNativeSM(config.Config{}, &dummySM{}, nil)
	sm.Loaded()
	sm.Offloaded()
	require.Equal(t, uint64(0), sm.loadedCount)
	require.False(t, sm.destroyed)
	select {
	case <-sm.DestroyedC():
		require.Fail(t, "destroyedC unexpected closed")
	default:
	}

	require.NoError(t, sm.Close())
	require.True(t, sm.destroyed)
	select {
	case <-sm.DestroyedC():
	default:
		require.Fail(t, "destroyed ch not closed")
	}
}

func TestLookupWillFailOnClosedStateMachine(t *testing.T) {
	sm := NewNativeSM(config.Config{}, &dummySM{}, nil)
	sm.Loaded()
	sm.Offloaded()
	require.NoError(t, sm.Close())
	_, err := sm.Lookup(nil)
	require.Equal(t, ErrShardClosed, err)
	_, err = sm.NALookup(nil)
	require.Equal(t, ErrShardClosed, err)
}
````

## File: internal/rsm/managed.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"io"
	"sync"

	"github.com/cockroachdb/errors"

	"github.com/lni/dragonboat/v4/config"
	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
)

var (
	// ErrShardClosed indicates that the shard has been closed
	ErrShardClosed = errors.New("raft shard already closed")
)

// IStreamable is the interface for types that can be snapshot streamed.
type IStreamable interface {
	Stream(interface{}, io.Writer) error
}

// ISavable is the interface for types that can its content saved as snapshots.
type ISavable interface {
	Save(SSMeta, io.Writer, []byte, sm.ISnapshotFileCollection) (bool, error)
}

// IRecoverable is the interface for types that can have its state restored from
// snapshots.
type IRecoverable interface {
	Recover(io.Reader, []sm.SnapshotFile) error
}

// ILoadable is the interface for types that can load client session
// state from a snapshot.
type ILoadable interface {
	LoadSessions(io.Reader, SSVersion) error
}

// IManagedStateMachine is the interface used for managed state machine. A
// managed state machine contains a user state machine plus its engine state.
type IManagedStateMachine interface {
	Open() (uint64, error)
	Update(sm.Entry) (sm.Result, error)
	BatchedUpdate([]sm.Entry) ([]sm.Entry, error)
	Lookup(interface{}) (interface{}, error)
	ConcurrentLookup(interface{}) (interface{}, error)
	NALookup([]byte) ([]byte, error)
	NAConcurrentLookup([]byte) ([]byte, error)
	Sync() error
	GetHash() (uint64, error)
	Prepare() (interface{}, error)
	Save(SSMeta, io.Writer, []byte, sm.ISnapshotFileCollection) (bool, error)
	Recover(io.Reader, []sm.SnapshotFile) error
	Stream(interface{}, io.Writer) error
	Offloaded() bool
	Loaded()
	Close() error
	DestroyedC() <-chan struct{}
	Concurrent() bool
	OnDisk() bool
	Type() pb.StateMachineType
}

type countedWriter struct {
	w     io.Writer
	total uint64
}

func (cw *countedWriter) Write(data []byte) (int, error) {
	n, err := cw.w.Write(data)
	if err != nil {
		return 0, err
	}
	cw.total = cw.total + uint64(n)
	return n, nil
}

// ManagedStateMachineFactory is the factory function type for creating an
// IManagedStateMachine instance.
type ManagedStateMachineFactory func(shardID uint64,
	replicaID uint64, stopc <-chan struct{}) IManagedStateMachine

// NativeSM is the IManagedStateMachine object used to manage native
// data store in Golang.
type NativeSM struct {
	sm   IStateMachine
	done <-chan struct{}
	OffloadedStatus
	ue     []sm.Entry
	config config.Config
	mu     sync.RWMutex
}

var _ IManagedStateMachine = (*NativeSM)(nil)
var _ ISavable = (*NativeSM)(nil)
var _ IStreamable = (*NativeSM)(nil)
var _ IRecoverable = (*NativeSM)(nil)

// NewNativeSM creates and returns a new NativeSM object.
func NewNativeSM(config config.Config, ism IStateMachine,
	done <-chan struct{}) *NativeSM {
	s := &NativeSM{
		config: config,
		sm:     ism,
		done:   done,
		ue:     make([]sm.Entry, 1),
	}
	s.OffloadedStatus.DestroyedC = make(chan struct{})
	s.shardID = config.ShardID
	s.replicaID = config.ReplicaID
	return s
}

// Open opens on disk state machine.
func (ds *NativeSM) Open() (uint64, error) {
	return ds.sm.Open(ds.done)
}

// Offloaded offloads the data store from a user component.
func (ds *NativeSM) Offloaded() bool {
	ds.mu.Lock()
	defer ds.mu.Unlock()
	return ds.SetOffloaded() == 0
}

// Loaded marks the statemachine as loaded by the specified component.
func (ds *NativeSM) Loaded() {
	ds.mu.Lock()
	defer ds.mu.Unlock()
	ds.SetLoaded()
}

// Close closes the underlying user state machine and set the destroyed flag.
func (ds *NativeSM) Close() error {
	if err := ds.sm.Close(); err != nil {
		return err
	}
	ds.SetDestroyed()
	return nil
}

// DestroyedC returns a chan struct{} used to indicate whether the SM has been
// fully offloaded.
func (ds *NativeSM) DestroyedC() <-chan struct{} {
	return ds.OffloadedStatus.DestroyedC
}

// Concurrent returns a boolean flag to indicate whether the managed state
// machine instance is capable of doing concurrent snapshots.
func (ds *NativeSM) Concurrent() bool {
	return ds.sm.Concurrent()
}

// OnDisk returns a boolean flag indicating whether the state machine is an on
// disk state machine.
func (ds *NativeSM) OnDisk() bool {
	return ds.sm.OnDisk()
}

// Type returns the state machine type.
func (ds *NativeSM) Type() pb.StateMachineType {
	return ds.sm.Type()
}

// Update updates the data store.
func (ds *NativeSM) Update(e sm.Entry) (sm.Result, error) {
	ds.ue[0] = e
	results, err := ds.sm.Update(ds.ue)
	if err != nil {
		return sm.Result{}, err
	}
	if len(results) != 1 {
		panic("len(results) != 1")
	}
	v := results[0].Result
	ds.ue[0] = sm.Entry{}
	return v, nil
}

// BatchedUpdate applies committed entries in a batch to hide latency.
func (ds *NativeSM) BatchedUpdate(ents []sm.Entry) ([]sm.Entry, error) {
	inputLen := len(ents)
	results, err := ds.sm.Update(ents)
	if err != nil {
		return nil, err
	}
	if len(results) != inputLen {
		panic("unexpected result length")
	}
	return results, nil
}

// Lookup queries the data store.
func (ds *NativeSM) Lookup(query interface{}) (interface{}, error) {
	ds.mu.RLock()
	defer ds.mu.RUnlock()
	if ds.destroyed {
		return nil, ErrShardClosed
	}
	return ds.sm.Lookup(query)
}

// ConcurrentLookup queries the data store without obtaining the NativeSM.mu.
func (ds *NativeSM) ConcurrentLookup(query interface{}) (interface{}, error) {
	return ds.sm.Lookup(query)
}

// NALookup queries the data store.
func (ds *NativeSM) NALookup(query []byte) ([]byte, error) {
	ds.mu.RLock()
	defer ds.mu.RUnlock()
	if ds.destroyed {
		return nil, ErrShardClosed
	}
	return ds.sm.NALookup(query)
}

// NAConcurrentLookup queries the data store without obtaining the NativeSM.mu.
func (ds *NativeSM) NAConcurrentLookup(query []byte) ([]byte, error) {
	return ds.sm.NALookup(query)
}

// Sync synchronizes state machine's in-core state with that on disk.
func (ds *NativeSM) Sync() error {
	return ds.sm.Sync()
}

// GetHash returns an integer value representing the state of the data store.
func (ds *NativeSM) GetHash() (uint64, error) {
	return ds.sm.GetHash()
}

// Prepare makes preparation for concurrently taking snapshot.
func (ds *NativeSM) Prepare() (interface{}, error) {
	return ds.sm.Prepare()
}

// Save saves the state of the data store to the specified writer.
func (ds *NativeSM) Save(meta SSMeta,
	w io.Writer, session []byte, c sm.ISnapshotFileCollection) (bool, error) {
	if ds.config.IsWitness || (ds.sm.OnDisk() && !meta.Request.Exported()) {
		return true, ds.saveDummy(w, session)
	}
	return false, ds.save(meta.Ctx, w, session, c)
}

func (ds *NativeSM) saveDummy(w io.Writer, session []byte) error {
	if !ds.config.IsWitness && !ds.sm.OnDisk() {
		panic("saveDummySnapshot called on non OnDiskStateMachine")
	}
	if _, err := w.Write(session); err != nil {
		return err
	}
	return nil
}

func (ds *NativeSM) save(ctx interface{},
	w io.Writer, session []byte, c sm.ISnapshotFileCollection) error {
	if _, err := w.Write(session); err != nil {
		return err
	}
	if err := ds.sm.Save(ctx, w, c, ds.done); err != nil {
		return err
	}
	return nil
}

// Stream creates and streams snapshot to a remote node.
func (ds *NativeSM) Stream(ctx interface{}, w io.Writer) error {
	return ds.save(ctx, w, GetEmptyLRUSession(), nil)
}

// Recover recovers the state of the data store from the specified reader.
func (ds *NativeSM) Recover(r io.Reader, files []sm.SnapshotFile) error {
	return ds.sm.Recover(r, files, ds.done)
}
````

## File: internal/rsm/membership_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"testing"

	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

func TestAddressEqual(t *testing.T) {
	tests := []struct {
		addr1 string
		addr2 string
		equal bool
	}{
		{"v1", "v2", false},
		{"v1", "v1", true},
		{"v1", "V2", false},
		{"v1", "V1", true},
		{"v1", " v1", true},
		{"v1", "  v1", true},
		{"v1", "  v1   ", true},
		{"  v1  ", " V1 ", true},
	}
	for idx, tt := range tests {
		result := addressEqual(tt.addr1, tt.addr2)
		assert.Equal(t, tt.equal, result, "test case %d", idx)
	}
}

func TestDeepCopyMembership(t *testing.T) {
	m := pb.Membership{
		ConfigChangeId: 101,
		Addresses:      make(map[uint64]string),
		Removed:        make(map[uint64]bool),
		NonVotings:     make(map[uint64]string),
		Witnesses:      make(map[uint64]string),
	}
	copied := deepCopyMembership(m)
	m.ConfigChangeId = 102
	m.Addresses[1] = "addr1"
	m.Removed[1] = true
	m.NonVotings[1] = "addr1"
	m.Witnesses[1] = "addr1"
	assert.Equal(t, uint64(101), copied.ConfigChangeId)
	assert.Empty(t, copied.Addresses)
	assert.Empty(t, copied.Removed)
	assert.Empty(t, copied.NonVotings)
	assert.Empty(t, copied.Witnesses)

	copied2 := deepCopyMembership(m)
	assert.Equal(t, uint64(102), copied2.ConfigChangeId)
	assert.Len(t, copied2.Addresses, 1)
	assert.Len(t, copied2.Removed, 1)
	assert.Len(t, copied2.NonVotings, 1)
	assert.Len(t, copied2.Witnesses, 1)
}

func TestMembershipCanBeCreated(t *testing.T) {
	m := newMembership(1, 2, true)
	assert.True(t, m.ordered)
	assert.Empty(t, m.members.Addresses)
	assert.Empty(t, m.members.NonVotings)
	assert.Empty(t, m.members.Removed)
	assert.Empty(t, m.members.Witnesses)
}

func TestMembershipCanBeSet(t *testing.T) {
	m := pb.Membership{
		ConfigChangeId: 101,
		Addresses:      make(map[uint64]string),
		Removed:        make(map[uint64]bool),
		NonVotings:     make(map[uint64]string),
		Witnesses:      make(map[uint64]string),
	}
	m.Addresses[1] = "addr1"
	m.Removed[2] = true
	m.NonVotings[3] = "addr2"
	m.Witnesses[4] = "addr3"
	o := newMembership(1, 2, true)
	o.set(m)
	assert.Len(t, o.members.Addresses, 1)
	assert.Len(t, o.members.NonVotings, 1)
	assert.Len(t, o.members.Removed, 1)
	assert.Len(t, o.members.Witnesses, 1)
	assert.Equal(t, uint64(101), o.members.ConfigChangeId)

	m.ConfigChangeId = 200
	m.Addresses[5] = "addr4"
	assert.Equal(t, uint64(101), o.members.ConfigChangeId)
	assert.Len(t, o.members.Addresses, 1)
}

func TestMembershipIsEmpty(t *testing.T) {
	o := newMembership(1, 2, true)
	assert.True(t, o.isEmpty())

	o.members.NonVotings[1] = "addr1"
	assert.True(t, o.isEmpty())

	o.members.Addresses[1] = "addr2"
	assert.False(t, o.isEmpty())
}

func TestIsDeletingOnlyNode(t *testing.T) {
	o := newMembership(1, 2, true)
	o.members.Addresses[1] = "a1"
	cc := pb.ConfigChange{
		Type:      pb.RemoveNode,
		ReplicaID: 1,
	}
	cc2 := pb.ConfigChange{
		Type:      pb.AddNode,
		ReplicaID: 1,
	}
	assert.True(t, o.isDeleteOnlyNode(cc))
	assert.False(t, o.isDeleteOnlyNode(cc2))

	o.members.NonVotings[2] = "a2"
	assert.True(t, o.isDeleteOnlyNode(cc))

	o.members.Addresses[3] = "a3"
	assert.False(t, o.isDeleteOnlyNode(cc))
}

func TestIsAddingRemovedNode(t *testing.T) {
	o := newMembership(1, 2, true)
	cc := pb.ConfigChange{}
	cc.Type = pb.AddNode
	cc.ReplicaID = 1
	assert.False(t, o.isAddRemovedNode(cc))

	cc.Type = pb.AddNonVoting
	assert.False(t, o.isAddRemovedNode(cc))

	cc.Type = pb.RemoveNode
	assert.False(t, o.isAddRemovedNode(cc))

	cc.Type = pb.AddWitness
	assert.False(t, o.isAddRemovedNode(cc))

	o.members.Removed[1] = true
	cc.Type = pb.AddNode
	assert.True(t, o.isAddRemovedNode(cc))

	cc.Type = pb.AddWitness
	assert.True(t, o.isAddRemovedNode(cc))

	cc.Type = pb.AddNonVoting
	cc.ReplicaID = 2
	assert.False(t, o.isAddRemovedNode(cc))
}

func TestIsAddingNodeAsNonVoting(t *testing.T) {
	tests := []struct {
		t         pb.ConfigChangeType
		replicaID uint64
		addrs     []uint64
		result    bool
	}{
		{pb.AddNode, 1, []uint64{1}, false},
		{pb.AddNode, 1, []uint64{}, false},
		{pb.AddWitness, 1, []uint64{1}, false},
		{pb.AddWitness, 1, []uint64{}, false},
		{pb.RemoveNode, 1, []uint64{1}, false},
		{pb.RemoveNode, 1, []uint64{}, false},
		{pb.AddNonVoting, 1, []uint64{1}, true},
		{pb.AddNonVoting, 1, []uint64{1, 2}, true},
		{pb.AddNonVoting, 1, []uint64{2}, false},
		{pb.AddNonVoting, 1, []uint64{}, false},
	}
	for idx, tt := range tests {
		o := newMembership(1, 2, true)
		cc := pb.ConfigChange{
			Type:      tt.t,
			ReplicaID: tt.replicaID,
		}
		for _, v := range tt.addrs {
			o.members.Addresses[v] = "addr"
		}
		result := o.isAddNodeAsNonVoting(cc)
		assert.Equal(t, tt.result, result, "test case %d", idx)
	}
}

func TestIsConfChangeUpToDate(t *testing.T) {
	tests := []struct {
		ordered    bool
		initialize bool
		ccid       uint64
		iccid      uint64
		result     bool
	}{
		{true, true, 1, 1, true},
		{true, false, 1, 1, true},
		{false, false, 1, 1, true},
		{false, true, 1, 1, true},
		{true, true, 1, 2, true},
		{true, false, 1, 2, false},
		{false, false, 1, 2, true},
		{false, true, 1, 2, true},
	}
	for idx, tt := range tests {
		o := newMembership(1, 2, tt.ordered)
		o.members.ConfigChangeId = tt.ccid
		cc := pb.ConfigChange{
			Initialize:     tt.initialize,
			ConfigChangeId: tt.iccid,
		}
		result := o.isUpToDate(cc)
		assert.Equal(t, tt.result, result, "test case %d", idx)
	}
}

func TestIsAddingExistingMember(t *testing.T) {
	tests := []struct {
		t          pb.ConfigChangeType
		addrs      map[uint64]string
		nonVotings map[uint64]string
		addr       string
		replicaID  uint64
		result     bool
	}{
		{pb.AddNode, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a1", 3, true},
		{pb.AddNode, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a2", 4, true},
		{pb.AddNode, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a3", 3, false},
		{pb.AddNode, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a3", 1, true},
		{pb.AddNode, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a1", 1, true},
		{pb.AddNode, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a2", 2, false},
		{pb.AddWitness, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a1", 3, true},
		{pb.AddWitness, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a2", 4, true},
		{pb.AddWitness, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a3", 3, false},
		{pb.AddWitness, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a3", 1, false},
		{pb.AddWitness, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a1", 1, true},
		{pb.AddWitness, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a2", 2, true},
		{pb.AddNonVoting, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a1", 3, true},
		{pb.AddNonVoting, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a2", 4, true},
		{pb.AddNonVoting, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a3", 3, false},
		{pb.AddNonVoting, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a4", 2, true},
		{pb.AddNonVoting, map[uint64]string{1: "a1"},
			map[uint64]string{2: "a2"}, "a2", 2, true},
	}
	for idx, tt := range tests {
		o := newMembership(1, 2, true)
		for i, v := range tt.addrs {
			o.members.Addresses[i] = v
		}
		for i, v := range tt.nonVotings {
			o.members.NonVotings[i] = v
		}
		cc := pb.ConfigChange{
			Type:      tt.t,
			Address:   tt.addr,
			ReplicaID: tt.replicaID,
		}
		result := o.isAddExistingMember(cc)
		assert.Equal(t, tt.result, result, "test case %d", idx)
	}
}

func TestIsPromotingNonVoting(t *testing.T) {
	tests := []struct {
		t          pb.ConfigChangeType
		nonVotings map[uint64]string
		addr       string
		replicaID  uint64
		result     bool
	}{
		{pb.AddNode, map[uint64]string{1: "a1"}, "a1", 3, false},
		{pb.AddNode, map[uint64]string{1: "a1"}, "a2", 1, false},
		{pb.AddNode, map[uint64]string{1: "a1"}, "a1", 1, true},
		{pb.AddWitness, map[uint64]string{1: "a1"}, "a1", 3, false},
		{pb.AddWitness, map[uint64]string{1: "a1"}, "a2", 1, false},
		{pb.AddWitness, map[uint64]string{1: "a1"}, "a1", 1, false},
		{pb.AddNonVoting, map[uint64]string{1: "a1"}, "a1", 3, false},
		{pb.AddNonVoting, map[uint64]string{1: "a1"}, "a2", 1, false},
		{pb.AddNonVoting, map[uint64]string{1: "a1"}, "a1", 1, false},
	}
	for idx, tt := range tests {
		o := newMembership(1, 2, true)
		for i, v := range tt.nonVotings {
			o.members.NonVotings[i] = v
		}
		cc := pb.ConfigChange{
			Type:      tt.t,
			Address:   tt.addr,
			ReplicaID: tt.replicaID,
		}
		result := o.isPromoteNonVoting(cc)
		assert.Equal(t, tt.result, result, "test case %d", idx)
	}
}

func TestIsInvalidNonVotingPromotion(t *testing.T) {
	tests := []struct {
		t          pb.ConfigChangeType
		nonVotings map[uint64]string
		addr       string
		replicaID  uint64
		result     bool
	}{
		{pb.AddNode, map[uint64]string{1: "a1"}, "a1", 1, false},
		{pb.AddNode, map[uint64]string{1: "a1"}, "a1", 3, false},
		{pb.AddNode, map[uint64]string{1: "a1"}, "a2", 1, true},
		{pb.AddWitness, map[uint64]string{1: "a1"}, "a1", 3, false},
		{pb.AddWitness, map[uint64]string{1: "a1"}, "a2", 1, false},
		{pb.AddWitness, map[uint64]string{1: "a1"}, "a1", 1, false},
		{pb.AddNonVoting, map[uint64]string{1: "a1"}, "a1", 3, false},
		{pb.AddNonVoting, map[uint64]string{1: "a1"}, "a2", 1, false},
		{pb.AddNonVoting, map[uint64]string{1: "a1"}, "a1", 1, false},
	}
	for idx, tt := range tests {
		o := newMembership(1, 2, true)
		for i, v := range tt.nonVotings {
			o.members.NonVotings[i] = v
		}
		cc := pb.ConfigChange{
			Type:      tt.t,
			Address:   tt.addr,
			ReplicaID: tt.replicaID,
		}
		result := o.isInvalidNonVotingPromotion(cc)
		assert.Equal(t, tt.result, result, "test case %d", idx)
	}
}

func TestApplyAddNode(t *testing.T) {
	o := newMembership(1, 2, true)
	cc := pb.ConfigChange{
		Type:      pb.AddNode,
		Address:   "a1",
		ReplicaID: 100,
	}
	o.apply(cc, 1000)
	assert.Equal(t, uint64(1000), o.members.ConfigChangeId)
	v, ok := o.members.Addresses[100]
	assert.True(t, ok)
	assert.Equal(t, "a1", v)
	assert.Len(t, o.members.Addresses, 1)
}

func TestAddNodeCanPromoteNonVotingToNode(t *testing.T) {
	o := newMembership(1, 2, true)
	o.members.NonVotings[100] = "a2"
	cc := pb.ConfigChange{
		Type:      pb.AddNode,
		Address:   "a2",
		ReplicaID: 100,
	}
	o.apply(cc, 1000)
	v, ok := o.members.Addresses[100]
	assert.True(t, ok)
	assert.Equal(t, "a2", v)
	assert.Len(t, o.members.Addresses, 1)
	_, ok = o.members.NonVotings[100]
	assert.False(t, ok)
}

func TestApplyAddNonVoting(t *testing.T) {
	o := newMembership(1, 2, true)
	cc := pb.ConfigChange{
		Type:      pb.AddNonVoting,
		Address:   "a1",
		ReplicaID: 100,
	}
	o.apply(cc, 1000)
	assert.Equal(t, uint64(1000), o.members.ConfigChangeId)
	v, ok := o.members.NonVotings[100]
	assert.True(t, ok)
	assert.Equal(t, "a1", v)
	assert.Len(t, o.members.NonVotings, 1)
}

func TestAddingExistingNodeAsNonVotingIsNotAllowed(t *testing.T) {
	o := newMembership(1, 2, true)
	o.members.Addresses[100] = "a1"
	cc := pb.ConfigChange{
		Type:      pb.AddNonVoting,
		Address:   "a1",
		ReplicaID: 100,
	}
	assert.False(t, o.handleConfigChange(cc, 0))
}

func TestAddingExistingNodeAsNonVotingWillPanic(t *testing.T) {
	o := newMembership(1, 2, true)
	o.members.Addresses[100] = "a1"
	cc := pb.ConfigChange{
		Type:      pb.AddNonVoting,
		Address:   "a1",
		ReplicaID: 100,
	}
	require.Panics(t, func() {
		o.apply(cc, 1000)
	})
}

func TestAddingExistingNodeAsWitnessWillPanic(t *testing.T) {
	o := newMembership(1, 2, true)
	o.members.Addresses[100] = "a1"
	cc := pb.ConfigChange{
		Type:      pb.AddWitness,
		Address:   "a1",
		ReplicaID: 100,
	}
	require.Panics(t, func() {
		o.apply(cc, 1000)
	})
}

func TestAddingExistingNonVotingAsWitnessWillPanic(t *testing.T) {
	o := newMembership(1, 2, true)
	o.members.NonVotings[100] = "a1"
	cc := pb.ConfigChange{
		Type:      pb.AddWitness,
		Address:   "a1",
		ReplicaID: 100,
	}
	require.Panics(t, func() {
		o.apply(cc, 1000)
	})
}

func TestApplyRemoveNode(t *testing.T) {
	o := newMembership(1, 2, true)
	o.members.Addresses[100] = "a1"
	o.members.NonVotings[100] = "a1"
	o.members.Witnesses[100] = "a1"
	cc := pb.ConfigChange{
		Type:      pb.RemoveNode,
		ReplicaID: 100,
	}
	o.apply(cc, 1000)
	assert.Empty(t, o.members.Addresses)
	assert.Empty(t, o.members.NonVotings)
	assert.Empty(t, o.members.Witnesses)
	_, ok := o.members.Removed[100]
	assert.True(t, ok)
}
````

## File: internal/rsm/membership.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"crypto/md5"
	"encoding/binary"
	"sort"
	"strings"

	"github.com/lni/goutils/logutil"

	"github.com/lni/dragonboat/v4/internal/fileutil"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

func addressEqual(addr1 string, addr2 string) bool {
	return strings.EqualFold(strings.TrimSpace(addr1),
		strings.TrimSpace(addr2))
}

func deepCopyMembership(m pb.Membership) pb.Membership {
	c := pb.Membership{
		ConfigChangeId: m.ConfigChangeId,
		Addresses:      make(map[uint64]string),
		Removed:        make(map[uint64]bool),
		NonVotings:     make(map[uint64]string),
		Witnesses:      make(map[uint64]string),
	}
	for nid, addr := range m.Addresses {
		c.Addresses[nid] = addr
	}
	for nid := range m.Removed {
		c.Removed[nid] = true
	}
	for nid, addr := range m.NonVotings {
		c.NonVotings[nid] = addr
	}
	for nid, addr := range m.Witnesses {
		c.Witnesses[nid] = addr
	}
	return c
}

type membership struct {
	members   pb.Membership
	shardID   uint64
	replicaID uint64
	ordered   bool
}

func newMembership(shardID uint64, replicaID uint64, ordered bool) membership {
	return membership{
		shardID:   shardID,
		replicaID: replicaID,
		ordered:   ordered,
		members: pb.Membership{
			Addresses:  make(map[uint64]string),
			NonVotings: make(map[uint64]string),
			Removed:    make(map[uint64]bool),
			Witnesses:  make(map[uint64]string),
		},
	}
}

func (m *membership) id() string {
	return logutil.DescribeSM(m.shardID, m.replicaID)
}

func (m *membership) set(n pb.Membership) {
	m.members = deepCopyMembership(n)
}

func (m *membership) get() pb.Membership {
	return deepCopyMembership(m.members)
}

func (m *membership) getHash() uint64 {
	vals := make([]uint64, 0)
	for v := range m.members.Addresses {
		vals = append(vals, v)
	}
	sort.Slice(vals, func(i, j int) bool { return vals[i] < vals[j] })
	vals = append(vals, m.members.ConfigChangeId)
	data := make([]byte, 8)
	hash := md5.New()
	for _, v := range vals {
		binary.LittleEndian.PutUint64(data, v)
		fileutil.MustWrite(hash, data)
	}
	md5sum := hash.Sum(nil)
	return binary.LittleEndian.Uint64(md5sum[:8])
}

func (m *membership) isEmpty() bool {
	return len(m.members.Addresses) == 0
}

func (m *membership) isUpToDate(cc pb.ConfigChange) bool {
	if !m.ordered || cc.Initialize {
		return true
	}
	if m.members.ConfigChangeId == cc.ConfigChangeId {
		return true
	}
	return false
}

func (m *membership) isAddRemovedNode(cc pb.ConfigChange) bool {
	if cc.Type == pb.AddNode ||
		cc.Type == pb.AddNonVoting ||
		cc.Type == pb.AddWitness {
		_, ok := m.members.Removed[cc.ReplicaID]
		return ok
	}
	return false
}

func (m *membership) isPromoteNonVoting(cc pb.ConfigChange) bool {
	if cc.Type == pb.AddNode {
		oa, ok := m.members.NonVotings[cc.ReplicaID]
		return ok && addressEqual(oa, cc.Address)
	}
	return false
}

func (m *membership) isInvalidNonVotingPromotion(cc pb.ConfigChange) bool {
	if cc.Type == pb.AddNode {
		oa, ok := m.members.NonVotings[cc.ReplicaID]
		return ok && !addressEqual(oa, cc.Address)
	}
	return false
}

func (m *membership) isAddExistingMember(cc pb.ConfigChange) bool {
	// try to add again with the same node ID
	if cc.Type == pb.AddNode {
		_, ok := m.members.Addresses[cc.ReplicaID]
		if ok {
			return true
		}
	}
	if cc.Type == pb.AddNonVoting {
		_, ok := m.members.NonVotings[cc.ReplicaID]
		if ok {
			return true
		}
	}
	if cc.Type == pb.AddWitness {
		_, ok := m.members.Witnesses[cc.ReplicaID]
		if ok {
			return true
		}
	}
	if m.isPromoteNonVoting(cc) {
		return false
	}
	if cc.Type == pb.AddNode ||
		cc.Type == pb.AddNonVoting ||
		cc.Type == pb.AddWitness {
		for _, addr := range m.members.Addresses {
			if addressEqual(addr, cc.Address) {
				return true
			}
		}
		for _, addr := range m.members.NonVotings {
			if addressEqual(addr, cc.Address) {
				return true
			}
		}
		for _, addr := range m.members.Witnesses {
			if addressEqual(addr, cc.Address) {
				return true
			}
		}
	}
	return false
}

func (m *membership) isAddNodeAsNonVoting(cc pb.ConfigChange) bool {
	if cc.Type == pb.AddNonVoting {
		_, ok := m.members.Addresses[cc.ReplicaID]
		return ok
	}
	return false
}
func (m *membership) isAddNodeAsWitness(cc pb.ConfigChange) bool {
	if cc.Type == pb.AddWitness {
		_, ok := m.members.Addresses[cc.ReplicaID]
		return ok
	}
	return false
}

func (m *membership) isAddWitnessAsNonVoting(cc pb.ConfigChange) bool {
	if cc.Type == pb.AddNonVoting {
		_, ok := m.members.Witnesses[cc.ReplicaID]
		return ok
	}
	return false
}

func (m *membership) isAddWitnessAsNode(cc pb.ConfigChange) bool {
	if cc.Type == pb.AddNode {
		_, ok := m.members.Witnesses[cc.ReplicaID]
		return ok
	}
	return false
}

func (m *membership) isAddNonVotingAsWitness(cc pb.ConfigChange) bool {
	if cc.Type == pb.AddWitness {
		_, ok := m.members.NonVotings[cc.ReplicaID]
		return ok
	}
	return false
}

func (m *membership) isDeleteOnlyNode(cc pb.ConfigChange) bool {
	if cc.Type == pb.RemoveNode && len(m.members.Addresses) == 1 {
		_, ok := m.members.Addresses[cc.ReplicaID]
		return ok
	}
	return false
}

func (m *membership) apply(cc pb.ConfigChange, index uint64) {
	m.members.ConfigChangeId = index
	switch cc.Type {
	case pb.AddNode:
		nodeAddr := cc.Address
		delete(m.members.NonVotings, cc.ReplicaID)
		if _, ok := m.members.Witnesses[cc.ReplicaID]; ok {
			panic("not suppose to reach here")
		}
		m.members.Addresses[cc.ReplicaID] = nodeAddr
	case pb.AddNonVoting:
		if _, ok := m.members.Addresses[cc.ReplicaID]; ok {
			panic("not suppose to reach here")
		}
		m.members.NonVotings[cc.ReplicaID] = cc.Address
	case pb.AddWitness:
		if _, ok := m.members.Addresses[cc.ReplicaID]; ok {
			panic("not suppose to reach here")
		}
		if _, ok := m.members.NonVotings[cc.ReplicaID]; ok {
			panic("not suppose to reach here")
		}
		m.members.Witnesses[cc.ReplicaID] = cc.Address
	case pb.RemoveNode:
		delete(m.members.Addresses, cc.ReplicaID)
		delete(m.members.NonVotings, cc.ReplicaID)
		delete(m.members.Witnesses, cc.ReplicaID)
		m.members.Removed[cc.ReplicaID] = true
	default:
		panic("unknown config change type")
	}
}

var nid = logutil.ReplicaID

func (m *membership) handleConfigChange(cc pb.ConfigChange, index uint64) bool {
	// order id requested by user
	ccid := cc.ConfigChangeId
	nodeBecomingNonVoting := m.isAddNodeAsNonVoting(cc)
	nodeBecomingWitness := m.isAddNodeAsWitness(cc)
	witnessBecomingNode := m.isAddWitnessAsNode(cc)
	witnessBecomingNonVoting := m.isAddWitnessAsNonVoting(cc)
	nonVotingBecomingWitness := m.isAddNonVotingAsWitness(cc)
	alreadyMember := m.isAddExistingMember(cc)
	addRemovedNode := m.isAddRemovedNode(cc)
	upToDateCC := m.isUpToDate(cc)
	deleteOnlyNode := m.isDeleteOnlyNode(cc)
	invalidPromotion := m.isInvalidNonVotingPromotion(cc)
	accepted := upToDateCC &&
		!addRemovedNode &&
		!alreadyMember &&
		!nodeBecomingNonVoting &&
		!nodeBecomingWitness &&
		!witnessBecomingNode &&
		!witnessBecomingNonVoting &&
		!nonVotingBecomingWitness &&
		!deleteOnlyNode &&
		!invalidPromotion
	if accepted {
		// current entry index, it will be recorded as the conf change id of the members
		m.apply(cc, index)
		switch cc.Type {
		case pb.AddNode:
			plog.Infof("%s applied ADD ccid %d (%d), %s (%s)",
				m.id(), ccid, index, nid(cc.ReplicaID), cc.Address)
		case pb.RemoveNode:
			plog.Infof("%s applied REMOVE ccid %d (%d), %s",
				m.id(), ccid, index, nid(cc.ReplicaID))
		case pb.AddNonVoting:
			plog.Infof("%s applied ADD OBSERVER ccid %d (%d), %s (%s)",
				m.id(), ccid, index, nid(cc.ReplicaID), cc.Address)
		case pb.AddWitness:
			plog.Infof("%s applied ADD WITNESS ccid %d (%d), %s (%s)",
				m.id(), ccid, index, nid(cc.ReplicaID), cc.Address)
		default:
			plog.Panicf("unknown cc.Type value %d", cc.Type)
		}
	} else {
		if !upToDateCC {
			plog.Warningf("%s rej out-of-order ConfChange ccid %d (%d), type %s",
				m.id(), ccid, index, cc.Type)
		} else if addRemovedNode {
			plog.Warningf("%s rej add removed ccid %d (%d), %s",
				m.id(), ccid, index, nid(cc.ReplicaID))
		} else if alreadyMember {
			plog.Warningf("%s rej add exist ccid %d (%d) %s (%s)",
				m.id(), ccid, index, nid(cc.ReplicaID), cc.Address)
		} else if nodeBecomingNonVoting {
			plog.Warningf("%s rej add exist as nonVoting ccid %d (%d) %s (%s)",
				m.id(), ccid, index, nid(cc.ReplicaID), cc.Address)
		} else if nodeBecomingWitness {
			plog.Warningf("%s rej add exist as witness ccid %d (%d) %s (%s)",
				m.id(), ccid, index, nid(cc.ReplicaID), cc.Address)
		} else if witnessBecomingNode {
			plog.Warningf("%s rej add witness as node ccid %d (%d) %s (%s)",
				m.id(), ccid, index, nid(cc.ReplicaID), cc.Address)
		} else if witnessBecomingNonVoting {
			plog.Warningf("%s rej add witness as nonVoting ccid %d (%d) %s (%s)",
				m.id(), ccid, index, nid(cc.ReplicaID), cc.Address)
		} else if nonVotingBecomingWitness {
			plog.Warningf("%s rej add nonVoting as witness ccid %d (%d) %s (%s)",
				m.id(), ccid, index, nid(cc.ReplicaID), cc.Address)
		} else if deleteOnlyNode {
			plog.Warningf("%s rej remove the only node %s", m.id(), nid(cc.ReplicaID))
		} else if invalidPromotion {
			plog.Warningf("%s rej invalid nonVoting promotion ccid %d (%d) %s (%s)",
				m.id(), ccid, index, nid(cc.ReplicaID), cc.Address)
		} else {
			plog.Panicf("config change rejected for unknown reasons")
		}
	}
	return accepted
}
````

## File: internal/rsm/offload_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

/*
import (
	"testing"
)

func TestOffloadedStatusReadyToDestroy(t *testing.T) {
	o := OffloadedStatus{}
	o.DestroyedC = make(chan struct{})
	if o.ReadyToDestroy() {
		t.Errorf("ready to destroy, not expected")
	}
	if o.Destroyed() {
		t.Errorf("destroyed, not expected")
	}
	o.SetDestroyed()
	if !o.Destroyed() {
		t.Errorf("not destroyed, not expected")
	}
}

func TestOffloadedStatusAllOffloadedWillMakeItReadyToDestroy(t *testing.T) {
	o := OffloadedStatus{}
	o.DestroyedC = make(chan struct{})
	o.SetOffloaded(FromStepWorker)
	o.SetOffloaded(FromCommitWorker)
	o.SetOffloaded(FromApplyWorker)
	o.SetOffloaded(FromSnapshotWorker)
	if o.ReadyToDestroy() {
		t.Errorf("ready to destroy, not expected")
	}
	o.SetOffloaded(FromNodeHost)
	if !o.ReadyToDestroy() {
		t.Errorf("not ready to destroy, not expected")
	}
}

func TestOffloadedStatusOffloadedFromNodeHostIsHandled(t *testing.T) {
	o := OffloadedStatus{}
	o.DestroyedC = make(chan struct{})
	o.SetOffloaded(FromNodeHost)
	if !o.ReadyToDestroy() {
		t.Errorf("not ready to destroy, not expected")
	}
	o1 := OffloadedStatus{}
	o1.SetLoaded(FromStepWorker)
	if !o1.loadedByStepWorker {
		t.Errorf("set loaded didn't set component as loaded")
	}
	o1.SetOffloaded(FromNodeHost)
	if o1.ReadyToDestroy() {
		t.Errorf("ready to destroy, not expected")
	}
	o1.SetOffloaded(FromStepWorker)
	if !o1.ReadyToDestroy() {
		t.Errorf("not ready to destroy, not expected")
	}
}*/
````

## File: internal/rsm/offload.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"sync/atomic"
)

// OffloadedStatus is used for tracking whether the managed data store has been
// offloaded from various system components.
type OffloadedStatus struct {
	DestroyedC  chan struct{}
	shardID     uint64
	replicaID   uint64
	loadedCount uint64
	destroyed   bool
}

// Destroyed returns a boolean value indicating whether the belonging object
// has been destroyed.
func (o *OffloadedStatus) Destroyed() bool {
	select {
	case <-o.DestroyedC:
		return true
	default:
		return false
	}
}

// SetDestroyed set the destroyed flag to be true
func (o *OffloadedStatus) SetDestroyed() {
	o.destroyed = true
	close(o.DestroyedC)
}

// SetLoaded marks the managed data store as loaded by a user component.
func (o *OffloadedStatus) SetLoaded() {
	atomic.AddUint64(&o.loadedCount, 1)
}

// SetOffloaded marks the managed data store as offloaded from a user
// component.
func (o *OffloadedStatus) SetOffloaded() uint64 {
	return atomic.AddUint64(&o.loadedCount, ^uint64(0))
}
````

## File: internal/rsm/rwv_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"bytes"
	"crypto/rand"
	"encoding/binary"
	"hash/crc32"
	"io"
	"testing"

	"github.com/stretchr/testify/require"
)

func TestTooShortBlockAreRejected(t *testing.T) {
	for i := 1; i <= 4; i++ {
		v := make([]byte, i)
		require.False(t, validateBlock(v, getDefaultChecksum()))
	}
}

func TestRandomBlocksAreRejected(t *testing.T) {
	for i := 1; i < 128; i++ {
		v := make([]byte, i*128)
		_, err := rand.Read(v)
		require.NoError(t, err)
		require.False(t, validateBlock(v, getDefaultChecksum()))
	}
}

func TestCorruptedBlockIsRejected(t *testing.T) {
	v := make([]byte, 1023)
	_, err := rand.Read(v)
	require.NoError(t, err)
	h := GetDefaultChecksum()
	_, err = h.Write(v)
	require.NoError(t, err)
	v = append(v, h.Sum(nil)...)
	require.True(t, validateBlock(v, h))
	v[0] = v[0] + 1
	require.False(t, validateBlock(v, h))
}

func TestWellFormedBlocksAreAccepted(t *testing.T) {
	for i := 1; i < 128; i++ {
		v := make([]byte, i*128)
		_, err := rand.Read(v)
		require.NoError(t, err)
		h := getDefaultChecksum()
		_, err = h.Write(v)
		require.NoError(t, err)
		v = append(v, h.Sum(nil)...)
		require.True(t, validateBlock(v, h))
	}
}

func TestWellFormedDataCanPassV2Validator(t *testing.T) {
	szs := []uint64{
		1,
		blockSize,
		blockSize - 1,
		blockSize + 1,
		blockSize * 5,
		blockSize*6 - 1,
		blockSize*6 + 1,
	}
	for _, sz := range szs {
		v := make([]byte, sz)
		_, err := rand.Read(v)
		require.NoError(t, err)
		buf := bytes.NewBuffer(make([]byte, 0, 1024))
		w := newV2Writer(buf, defaultChecksumType)
		_, err = w.Write(v)
		require.NoError(t, err)
		err = w.Close()
		require.NoError(t, err)
		header := make([]byte, HeaderSize)
		data := append(header, buf.Bytes()...)
		validator := newV2Validator(getDefaultChecksum())
		require.True(t, validator.AddChunk(data, 0))
		require.True(t, validator.Validate())
		if sz > HeaderSize {
			s := sz / 2
			fh := data[:s]
			lh := data[s:]
			validator := newV2Validator(getDefaultChecksum())
			require.True(t, validator.AddChunk(fh, 0))
			require.True(t, validator.AddChunk(lh, 1))
			require.True(t, validator.Validate())
		}
	}
}

func testCorruptedDataCanBeDetectedByValidator(t *testing.T,
	corruptFn func([]byte) []byte, ok bool) {
	szs := []uint64{
		1,
		blockSize,
		blockSize - 1,
		blockSize + 1,
		blockSize * 3,
		blockSize*3 - 1,
		blockSize*3 + 1,
	}
	for _, sz := range szs {
		v := make([]byte, sz)
		_, err := rand.Read(v)
		require.NoError(t, err)
		buf := bytes.NewBuffer(make([]byte, 0, 1024))
		w := newV2Writer(buf, defaultChecksumType)
		_, err = w.Write(v)
		require.NoError(t, err)
		err = w.Close()
		require.NoError(t, err)
		payload := buf.Bytes()
		payload = corruptFn(payload)
		header := make([]byte, HeaderSize)
		data := append(header, payload...)
		vf := func(data []byte) bool {
			validator := newV2Validator(getDefaultChecksum())
			if !validator.AddChunk(data, 0) {
				return false
			}
			return validator.Validate()
		}
		result := vf(data)
		require.Equal(t, ok, result)
		if sz > HeaderSize {
			szz := sz
			vf := func(data []byte) bool {
				s := szz / 2
				fh := data[:s]
				lh := data[s:]
				validator := newV2Validator(getDefaultChecksum())
				if !validator.AddChunk(fh, 0) {
					return false
				}
				if !validator.AddChunk(lh, 1) {
					return false
				}
				return validator.Validate()
			}
			result := vf(data)
			require.Equal(t, ok, result)
		}
	}
}

func TestCorruptedDataCanBeDetectedByValidator(t *testing.T) {
	noop := func(data []byte) []byte {
		return data
	}
	testCorruptedDataCanBeDetectedByValidator(t, noop, true)
	firstByte := func(data []byte) []byte {
		data[0] = data[0] + 1
		return data
	}
	testCorruptedDataCanBeDetectedByValidator(t, firstByte, false)
	lastByte := func(data []byte) []byte {
		data[len(data)-1] = data[len(data)-1] + 1
		return data
	}
	testCorruptedDataCanBeDetectedByValidator(t, lastByte, false)
	truncateFirstByte := func(data []byte) []byte {
		return data[1:]
	}
	testCorruptedDataCanBeDetectedByValidator(t, truncateFirstByte, false)
	truncateLastByte := func(data []byte) []byte {
		return data[:len(data)-1]
	}
	testCorruptedDataCanBeDetectedByValidator(t, truncateLastByte, false)
	extraFirstByte := func(data []byte) []byte {
		return append([]byte{0}, data...)
	}
	testCorruptedDataCanBeDetectedByValidator(t, extraFirstByte, false)
	extraLastByte := func(data []byte) []byte {
		return append(data, 0)
	}
	testCorruptedDataCanBeDetectedByValidator(t, extraLastByte, false)
}

func TestBlockWriterCanWriteData(t *testing.T) {
	blockSize := uint64(128)
	testSz := []uint64{
		1,
		blockSize - 1,
		blockSize + 1,
		blockSize*2 - 1,
		blockSize * 2,
		blockSize*2 + 1,
		blockSize*2 + 5,
		blockSize*2 - 5,
		blockSize * 128,
		blockSize*128 + 4,
		blockSize*128 - 4,
	}
	for _, sz := range testSz {
		lastBlock := false
		written := make([]byte, 0)
		onBlock := func(data []byte, crc []byte) error {
			if lastBlock {
				require.NotEmpty(t, crc)
			}
			if len(crc) == 0 {
				lastBlock = true
			}
			if len(crc) != 0 {
				h := crc32.NewIEEE()
				_, err := h.Write(data)
				require.NoError(t, err)
				require.Equal(t, h.Sum(nil), crc)
			}
			written = append(written, data...)
			return nil
		}
		writer := newBlockWriter(blockSize, onBlock, defaultChecksumType)
		input := make([]byte, sz)
		for i := range input {
			input[i] = byte((sz + uint64(i)) % 256)
		}
		n, err := writer.Write(input)
		require.Equal(t, uint64(n), sz)
		require.NoError(t, err)

		require.NoError(t, writer.Close())
		result := written[:len(written)-16]
		meta := written[len(written)-16:]
		total := binary.LittleEndian.Uint64(meta[:8])
		magic := meta[8:]
		expSz := getChecksumedBlockSize(sz, blockSize)
		require.Equal(t, expSz, total)
		require.Equal(t, writerMagicNumber, magic)
		require.Equal(t, input, result)
	}
}

func TestBlockReaderCanReadData(t *testing.T) {
	blockSize := uint64(128)
	testSz := []uint64{
		1,
		blockSize - 1,
		blockSize + 1,
		blockSize*2 - 1,
		blockSize * 2,
		blockSize*2 + 1,
		blockSize*2 + 5,
		blockSize*2 - 5,
		blockSize * 128,
		blockSize*128 + 4,
		blockSize*128 - 4,
	}
	for _, sz := range testSz {
		readBufSz := []uint64{3, 1,
			blockSize - 1, blockSize, blockSize + 1,
			blockSize * 3, blockSize*3 - 1, blockSize*3 + 1,
			sz, sz - 1, sz + 1}
		for _, bufSz := range readBufSz {
			buf := bytes.NewBuffer(make([]byte, 0, 128*1024))
			onBlock := func(data []byte, crc []byte) error {
				toWrite := append(data, crc...)
				n, err := buf.Write(toWrite)
				require.Equal(t, len(toWrite), n)
				require.NoError(t, err)
				return nil
			}
			writer := newBlockWriter(blockSize, onBlock, defaultChecksumType)
			input := make([]byte, sz)
			v := 0
			for i := range input {
				input[i] = byte(v % 256)
				v++
			}
			n, err := writer.Write(input)
			require.Equal(t, uint64(n), sz)
			require.NoError(t, err)
			require.NoError(t, writer.Close())
			written := buf.Bytes()
			expSz := getChecksumedBlockSize(sz, blockSize) + 16
			require.Equal(t, expSz, uint64(len(written)))

			allRead := make([]byte, 0)
			if bufSz == 0 {
				continue
			}
			curRead := make([]byte, bufSz)
			lr := io.LimitReader(buf, int64(len(buf.Bytes())-16))
			reader := newBlockReader(lr, blockSize, defaultChecksumType)
			for {
				n, err = reader.Read(curRead)
				if err != nil && err != io.EOF {
					require.NoError(t, err)
				}
				allRead = append(allRead, curRead[:n]...)
				if err == io.EOF {
					break
				}
			}
			require.Equal(t, input, allRead)
		}
	}
}

func TestBlockReaderPanicOnCorruptedBlock(t *testing.T) {
	blockSize := uint64(128)
	sz := blockSize*5 + 4
	buf := bytes.NewBuffer(make([]byte, 0, blockSize*12))
	onBlock := func(data []byte, crc []byte) error {
		toWrite := append(data, crc...)
		n, err := buf.Write(toWrite)
		require.Equal(t, len(toWrite), n)
		require.NoError(t, err)
		return nil
	}
	writer := newBlockWriter(blockSize, onBlock, defaultChecksumType)
	input := make([]byte, sz)
	v := 0
	for i := range input {
		input[i] = byte(v % 256)
		v++
	}
	n, err := writer.Write(input)
	require.Equal(t, uint64(n), sz)
	require.NoError(t, err)
	require.NoError(t, writer.Close())
	written := append([]byte{}, buf.Bytes()...)
	for idx := 0; idx < len(written)-16; idx++ {
		func() {
			curRead := make([]byte, 4)
			written[idx] = written[idx] + 1
			lr := io.LimitReader(bytes.NewBuffer(written),
				int64(len(written)-16))
			reader := newBlockReader(lr, blockSize, defaultChecksumType)
			require.Panics(t, func() {
				for {
					n, err = reader.Read(curRead)
					if err != nil && err != io.EOF {
						require.NoError(t, err)
					}
					if err == io.EOF {
						break
					}
				}
			})
		}()
	}
}
````

## File: internal/rsm/rwv.go
````go
// Copyright 2017-2021 Reusee (https://github.com/reusee)
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// the blockWriter implementation had reference to reusee's project below
// https://github.com/reusee/hashingwriter

package rsm

import (
	"bytes"
	"encoding/binary"
	"hash"
	"io"
	"math"

	"github.com/cockroachdb/errors"

	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/vfs"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

const (
	blockSize = settings.SnapshotChunkSize
)

var (
	writerMagicNumber = settings.BlockFileMagicNumber
	tailSize          = uint64(16)
	checksumSize      = uint64(4)
)

// GetV2PayloadSize returns the actual on disk size for the input user payload
// size.
func GetV2PayloadSize(sz uint64) uint64 {
	return getV2PayloadSize(sz, blockSize)
}

func getV2PayloadSize(sz uint64, blockSize uint64) uint64 {
	return getChecksumedBlockSize(sz, blockSize) + tailSize
}

func getChecksumedBlockSize(sz uint64, blockSize uint64) uint64 {
	return uint64(math.Ceil(float64(sz)/float64(blockSize)))*checksumSize + sz
}

func validateBlock(block []byte, h hash.Hash) bool {
	if uint64(len(block)) <= checksumSize {
		return false
	}
	payload := block[:uint64(len(block))-checksumSize]
	crc := block[uint64(len(block))-checksumSize:]
	h.Reset()
	fileutil.MustWrite(h, payload)
	return bytes.Equal(crc, h.Sum(nil))
}

// BlockWriter is a writer type that writes the input data to the underlying
// storage with checksum appended at the end of each block.
type BlockWriter struct {
	h          hash.Hash
	fh         hash.Hash
	onNewBlock func(data []byte, crc []byte) error
	block      []byte
	blockSize  uint64
	nextStop   uint64
	written    uint64
	total      uint64
	flushed    bool
}

var _ IBlockWriter = (*BlockWriter)(nil)

// IBlockWriter is the interface for writing checksumed data blocks.
type IBlockWriter interface {
	io.WriteCloser
	GetPayloadChecksum() []byte
}

// NewBlockWriter creates and returns a block writer.
func NewBlockWriter(blockSize uint64,
	nb func(data []byte, crc []byte) error, t pb.ChecksumType) *BlockWriter {
	return newBlockWriter(blockSize, nb, t)
}

func newBlockWriter(blockSize uint64,
	nb func(data []byte, crc []byte) error, t pb.ChecksumType) *BlockWriter {
	return &BlockWriter{
		blockSize:  blockSize,
		block:      make([]byte, 0, blockSize+checksumSize),
		onNewBlock: nb,
		nextStop:   blockSize,
		h:          mustGetChecksum(t),
		fh:         mustGetChecksum(t),
	}
}

// Write writes the specified data using the block writer.
func (bw *BlockWriter) Write(bs []byte) (int, error) {
	if bw.flushed {
		panic("write called after flush")
	}
	var totalN uint64
	for len(bs) > 0 {
		l := bw.nextStop - bw.written
		if l > uint64(len(bs)) {
			l = uint64(len(bs))
		}
		bw.block = append(bw.block, bs[:l]...)
		fileutil.MustWrite(bw.h, bs[:l])
		bw.written += l
		if bw.written == bw.nextStop {
			bw.total += uint64(len(bw.block)) + checksumSize
			if err := bw.processNewBlock(bw.block, bw.h.Sum(nil)); err != nil {
				return int(totalN), err
			}
			bw.nextStop = bw.nextStop + bw.blockSize
			bw.h.Reset()
			bw.block = bw.block[:0]
		}
		bs = bs[l:]
		totalN += l
	}
	return int(totalN), nil
}

// Close closes the writer by passing all in memory buffered data to the
// underlying onNewBlock function.
func (bw *BlockWriter) Close() error {
	if bw.flushed {
		panic("flush called again")
	} else {
		bw.flushed = true
	}
	if len(bw.block) > 0 {
		bw.total += uint64(len(bw.block)) + checksumSize
		if err := bw.processNewBlock(bw.block, bw.h.Sum(nil)); err != nil {
			plog.Errorf("onNewBlock failed %v", err)
			return err
		}
	}
	totalbs := make([]byte, 8)
	binary.LittleEndian.PutUint64(totalbs, bw.total)
	tailBlock := append(totalbs, writerMagicNumber...)
	return bw.processNewBlock(tailBlock, nil)
}

// GetPayloadChecksum returns the checksum for the entire payload.
func (bw *BlockWriter) GetPayloadChecksum() []byte {
	if !bw.flushed {
		panic("not flushed yet")
	}
	return bw.fh.Sum(nil)
}

func (bw *BlockWriter) processNewBlock(data []byte, crc []byte) error {
	if len(crc) > 0 {
		fileutil.MustWrite(bw.fh, crc)
	}
	return bw.onNewBlock(data, crc)
}

type blockReader struct {
	r         io.Reader
	block     []byte
	blockSize uint64
	t         pb.ChecksumType
}

// the input reader should be a reader to all blocks, thus the 16 bytes length
// and magic number fields should not be included, use a io.LimitReader to
// exclude them
func newBlockReader(r io.Reader,
	blockSize uint64, t pb.ChecksumType) *blockReader {
	return &blockReader{
		r:         r,
		t:         t,
		blockSize: blockSize,
		block:     make([]byte, 0, blockSize+checksumSize),
	}
}

func (br *blockReader) Close() error {
	return nil
}

func (br *blockReader) Read(data []byte) (int, error) {
	want := len(data)
	if want <= len(br.block) {
		copy(data, br.block[:want])
		br.block = br.block[want:]
		return want, nil
	}
	read := len(br.block)
	copy(data, br.block)
	for read < want {
		if _, err := br.readBlock(); err != nil {
			return read, err
		}
		toRead := want - read
		if toRead > len(br.block) {
			toRead = len(br.block)
		}
		copy(data[read:], br.block[:toRead])
		br.block = br.block[toRead:]
		read += toRead
	}
	return read, nil
}

func (br *blockReader) readBlock() (int, error) {
	br.block = make([]byte, br.blockSize+checksumSize)
	n, err := io.ReadFull(br.r, br.block)
	if err != nil && err != io.ErrUnexpectedEOF {
		return n, err
	}
	br.block = br.block[:n]
	if !validateBlock(br.block, mustGetChecksum(br.t)) {
		panic("corrupted block")
	}
	br.block = br.block[:uint64(len(br.block))-checksumSize]
	return len(br.block), nil
}

// IVWriter is the interface for versioned snapshot writer.
type IVWriter interface {
	io.WriteCloser
	GetVersion() SSVersion
	GetPayloadSum() []byte
	GetPayloadSize(uint64) uint64
}

// IVReader is the interface for versioned snapshot reader.
type IVReader interface {
	Read(data []byte) (int, error)
	Sum() []byte
}

type v1writer struct {
	f io.Writer
	h hash.Hash
}

var _ IVWriter = (*v1writer)(nil)

func newV1Wrtier(f io.Writer) *v1writer {
	// v1 is hard coded to use pb.CRC32IEEE
	h := mustGetChecksum(pb.CRC32IEEE)
	return &v1writer{f: io.MultiWriter(f, h), h: h}
}

func (v1w *v1writer) Write(data []byte) (int, error) {
	return v1w.f.Write(data)
}

func (v1w *v1writer) GetVersion() SSVersion {
	return V1
}

func (v1w *v1writer) GetPayloadSize(sz uint64) uint64 {
	return sz
}

func (v1w *v1writer) GetPayloadSum() []byte {
	return v1w.h.Sum(nil)
}

func (v1w *v1writer) Close() error {
	return nil
}

type v1reader struct {
	r io.Reader
	h hash.Hash
}

var _ IVReader = (*v1reader)(nil)

func newV1Reader(r io.Reader) *v1reader {
	// v1 is hard coded to use pb.CRC32IEEE
	h := mustGetChecksum(pb.CRC32IEEE)
	return &v1reader{
		r: io.TeeReader(r, h),
		h: h,
	}
}

func (v1r *v1reader) Read(data []byte) (int, error) {
	return v1r.r.Read(data)
}

func (v1r *v1reader) Sum() []byte {
	return v1r.h.Sum(nil)
}

type v2writer struct {
	bw *BlockWriter
}

var _ IVWriter = (*v2writer)(nil)

func newV2Writer(fw io.Writer, t pb.ChecksumType) *v2writer {
	onBlock := func(data []byte, crc []byte) error {
		if len(crc) > 0 && uint64(len(crc)) != checksumSize {
			panic("unexpected crc length")
		}
		_, err := fw.Write(append(data, crc...))
		return err
	}
	return &v2writer{
		bw: newBlockWriter(blockSize, onBlock, t),
	}
}

func (v2w *v2writer) Write(data []byte) (int, error) {
	return v2w.bw.Write(data)
}

func (v2w *v2writer) GetVersion() SSVersion {
	return V2
}

func (v2w *v2writer) GetPayloadSize(sz uint64) uint64 {
	return getV2PayloadSize(sz, blockSize)
}

func (v2w *v2writer) GetPayloadSum() []byte {
	return v2w.bw.GetPayloadChecksum()
}

func (v2w *v2writer) Close() error {
	return v2w.bw.Close()
}

type v2reader struct {
	br *blockReader
}

var _ IVReader = (*v2reader)(nil)

func newV2Reader(fr io.Reader, t pb.ChecksumType) *v2reader {
	return &v2reader{br: newBlockReader(fr, blockSize, t)}
}

func (br *v2reader) Read(data []byte) (int, error) {
	return br.br.Read(data)
}

func (br *v2reader) Sum() []byte {
	return []byte{0, 0, 0, 0}
}

func getHeaderFromFirstChunk(data []byte) ([]byte, []byte, bool) {
	if uint64(len(data)) < HeaderSize {
		panic("first chunk is too small")
	}
	sz := binary.LittleEndian.Uint64(data)
	if sz > HeaderSize-8 {
		return nil, nil, false
	}
	return data[8 : 8+sz], data[8+sz : 12+sz], true
}

// IVValidator is the interface for versioned validator.
type IVValidator interface {
	AddChunk(data []byte, chunkID uint64) bool
	Validate() bool
}

type v1validator struct {
	h      hash.Hash
	header pb.SnapshotHeader
}

var _ IVValidator = (*v1validator)(nil)

func newV1Validator(header pb.SnapshotHeader) *v1validator {
	return &v1validator{
		header: header,
		h:      mustGetChecksum(header.ChecksumType),
	}
}

func (v *v1validator) AddChunk(data []byte, chunkID uint64) bool {
	var p []byte
	if chunkID == 0 {
		p = data[HeaderSize:]
	} else {
		p = data
	}
	if _, err := v.h.Write(p); err != nil {
		return false
	}
	return true
}

func (v *v1validator) Validate() bool {
	return bytes.Equal(v.h.Sum(nil), v.header.PayloadChecksum)
}

type v2validator struct {
	h     hash.Hash
	block []byte
	total int
}

var _ IVValidator = (*v2validator)(nil)

func newV2Validator(h hash.Hash) *v2validator {
	return &v2validator{
		block: make([]byte, 0, blockSize+checksumSize),
		h:     h,
	}
}

func (v *v2validator) AddChunk(data []byte, chunkID uint64) bool {
	var p []byte
	if chunkID == 0 {
		p = data[HeaderSize:]
	} else {
		p = data
	}
	v.total += len(p)
	v.block = append(v.block, p...)
	for uint64(len(v.block)) >= 2*(blockSize+checksumSize) {
		block := v.block[:blockSize+checksumSize]
		v.block = v.block[blockSize+checksumSize:]
		if !v.validateBlock(block) {
			return false
		}
	}
	return true
}

func (v *v2validator) Validate() bool {
	if uint64(len(v.block)) < tailSize {
		return false
	}
	tail := v.block[uint64(len(v.block))-tailSize:]
	block := v.block[:uint64(len(v.block))-tailSize]
	if !v.validateMagicSize(tail) {
		return false
	}
	for uint64(len(block)) > blockSize+checksumSize {
		c := block[:blockSize+checksumSize]
		block = block[blockSize+checksumSize:]
		if !v.validateBlock(c) {
			return false
		}
	}
	return len(block) == 0 || v.validateBlock(block)
}

func (v *v2validator) validateMagicSize(tail []byte) bool {
	if uint64(len(tail)) != tailSize {
		panic("invalid size")
	}
	if !bytes.Equal(tail[8:], writerMagicNumber) {
		return false
	}
	return binary.LittleEndian.Uint64(tail[:8]) == uint64(v.total)-tailSize
}

func (v *v2validator) validateBlock(block []byte) bool {
	return validateBlock(block, v.h)
}

// GetV2PayloadChecksum calculates the payload checksum of the specified
// snapshot file.
func GetV2PayloadChecksum(fp string, fs vfs.IFS) (crc []byte, err error) {
	offsets, err := getV2CRCOffsetList(fp, fs)
	if err != nil {
		return nil, err
	}
	t, err := getV2ChecksumType(fp, fs)
	if err != nil {
		return nil, err
	}
	h := mustGetChecksum(t)
	f, err := fs.Open(fp)
	if err != nil {
		return nil, err
	}
	defer func() {
		err = firstError(err, f.Close())
	}()
	for _, offset := range offsets {
		crc := make([]byte, checksumSize)
		if _, err := f.ReadAt(crc, int64(offset)); err != nil {
			return nil, err
		}
		if _, err = h.Write(crc); err != nil {
			return nil, err
		}
	}
	crc = h.Sum(nil)
	return
}

func getV2ChecksumType(fp string, fs vfs.IFS) (ct pb.ChecksumType, err error) {
	reader, header, err := NewSnapshotReader(fp, fs)
	if err != nil {
		return 0, err
	}
	defer func() {
		err = firstError(err, reader.Close())
	}()
	if header.Version != uint64(V2) {
		return pb.ChecksumType(0), errors.New("not a v2 snapshot file")
	}
	return header.ChecksumType, nil
}

func getV2CRCOffsetList(fp string, fs vfs.IFS) ([]uint64, error) {
	fi, err := fs.Stat(fp)
	if err != nil {
		return nil, err
	}
	return getV2CRCOffsetListFromFileSize(uint64(fi.Size()))
}

func getV2CRCOffsetListFromFileSize(sz uint64) ([]uint64, error) {
	if sz <= tailSize+HeaderSize {
		return nil, errors.New("invalid file size")
	}
	sz = sz - tailSize - HeaderSize
	result := make([]uint64, 0)
	offset := HeaderSize
	for sz > 0 {
		if sz >= blockSize+checksumSize {
			result = append(result, offset+blockSize)
			offset = offset + blockSize + checksumSize
			sz = sz - (blockSize + checksumSize)
		} else {
			result = append(result, offset+sz-checksumSize)
			break
		}
	}
	return result, nil
}
````

## File: internal/rsm/session_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"bytes"
	"crypto/rand"
	"encoding/binary"
	"encoding/json"
	"reflect"
	"testing"

	sm "github.com/lni/dragonboat/v4/statemachine"
	"github.com/stretchr/testify/require"
)

func TestResponseCanBeAdded(t *testing.T) {
	tests := []struct {
		seriesNumList  []RaftSeriesID
		valueList      []uint64
		size           int
		testSeriesNum  RaftSeriesID
		expectedValue  uint64
		expectedResult bool
	}{
		{[]RaftSeriesID{1, 2, 3}, []uint64{100, 200, 300}, 3, 1, 100, true},
		{[]RaftSeriesID{1, 2, 3}, []uint64{100, 200, 300}, 3, 3, 300, true},
		{[]RaftSeriesID{1, 2, 3}, []uint64{100, 200, 300}, 3, 4, 0, false},
	}
	for i, tt := range tests {
		s := newSession(0)
		for idx := range tt.seriesNumList {
			s.addResponse(tt.seriesNumList[idx],
				sm.Result{Value: tt.valueList[idx]})
		}
		require.Equal(t, tt.size, len(s.History),
			"i %d, size mismatch", i)
		v, ok := s.getResponse(tt.testSeriesNum)
		require.Equal(t, tt.expectedValue, v.Value,
			"i %d, value mismatch", i)
		require.Equal(t, tt.expectedResult, ok,
			"i %d, result mismatch", i)
	}
}

func TestCachedResponseDataCanBeCleared(t *testing.T) {
	tests := []struct {
		seriesNumList  []RaftSeriesID
		valueList      []uint64
		clearTo        RaftSeriesID
		sizeAfterClear int
		testSeriesNum  RaftSeriesID
		expectedResult bool
	}{
		{[]RaftSeriesID{1, 2, 3}, []uint64{100, 200, 300}, 2, 1, 2, false},
		{[]RaftSeriesID{1, 2, 3}, []uint64{100, 200, 300}, 2, 1, 3, true},
		{[]RaftSeriesID{1, 2, 3}, []uint64{100, 200, 300}, 3, 0, 3, false},
		{[]RaftSeriesID{1, 2, 3}, []uint64{100, 200, 300}, 4, 0, 3, false},
		{[]RaftSeriesID{3, 4, 5}, []uint64{100, 200, 300}, 2, 3, 3, true},
		{[]RaftSeriesID{3, 4, 5}, []uint64{100, 200, 300}, 6, 0, 5, false},
	}
	for i, tt := range tests {
		s := newSession(0)
		for idx := range tt.seriesNumList {
			s.addResponse(tt.seriesNumList[idx],
				sm.Result{Value: tt.valueList[idx]})
		}
		s.clearTo(tt.clearTo)
		require.Equal(t, tt.sizeAfterClear, len(s.History),
			"i %d, size after clear mismatch", i)
		_, ok := s.getResponse(tt.testSeriesNum)
		require.Equal(t, tt.expectedResult, ok,
			"i %d, response result mismatch", i)
	}
}

func TestWhetherResponsedCanBeReturned(t *testing.T) {
	tests := []struct {
		seriesNumList  []RaftSeriesID
		valueList      []uint64
		clearTo        RaftSeriesID
		testID         RaftSeriesID
		expectedResult bool
	}{
		{[]RaftSeriesID{1, 2, 3}, []uint64{100, 200, 300}, 2, 1, true},
		{[]RaftSeriesID{1, 2, 3}, []uint64{100, 200, 300}, 2, 2, true},
		{[]RaftSeriesID{1, 2, 3}, []uint64{100, 200, 300}, 2, 3, false},
		{[]RaftSeriesID{1, 2, 3}, []uint64{100, 200, 300}, 3, 3, true},
		{[]RaftSeriesID{3, 4, 5}, []uint64{100, 200, 300}, 2, 1, true},
		{[]RaftSeriesID{3, 4, 5}, []uint64{100, 200, 300}, 2, 2, true},
		{[]RaftSeriesID{3, 4, 5}, []uint64{100, 200, 300}, 2, 3, false},
	}
	for i, tt := range tests {
		s := newSession(0)
		for idx := range tt.seriesNumList {
			s.addResponse(tt.seriesNumList[idx],
				sm.Result{Value: tt.valueList[idx]})
		}

		s.clearTo(tt.clearTo)
		ok := s.hasResponded(tt.testID)
		require.Equal(t, tt.expectedResult, ok,
			"i %d, response check mismatch", i)
	}
}

func TestSessionCanBeSavedAndRestored(t *testing.T) {
	tests := []struct {
		seriesNumList []RaftSeriesID
		valueList     []uint64
	}{
		{[]RaftSeriesID{}, []uint64{}},
		{[]RaftSeriesID{1}, []uint64{100}},
		{[]RaftSeriesID{1, 2, 3}, []uint64{100, 200, 300}},
	}
	for i, tt := range tests {
		s := newSession(0)
		for idx := range tt.seriesNumList {
			cmd := make([]byte, 1234)
			_, err := rand.Read(cmd)
			require.NoError(t, err)
			s.addResponse(tt.seriesNumList[idx],
				sm.Result{Value: tt.valueList[idx], Data: cmd})
		}
		snapshot := &bytes.Buffer{}
		err := s.save(snapshot)
		require.NoError(t, err, "save failed")
		data := snapshot.Bytes()
		toRecover := bytes.NewBuffer(data)
		newS := &Session{}
		err = newS.recoverFromSnapshot(toRecover, V2)
		require.NoError(t, err, "failed to create session from snapshot")
		require.True(t, reflect.DeepEqual(newS, s),
			"i %d, session mismatch", i)
	}
}

func TestSessionCanBeRestoredFromV1Snapshot(t *testing.T) {
	session := &v1session{
		ClientID:      123,
		RespondedUpTo: 456789,
		History:       make(map[RaftSeriesID]uint64),
	}
	session.History[1234] = 324
	session.History[5678] = 458
	ss := &bytes.Buffer{}
	data, err := json.Marshal(session)
	require.NoError(t, err)
	sz := len(data)
	lenbuf := make([]byte, 8)
	binary.LittleEndian.PutUint64(lenbuf, uint64(sz))
	_, err = ss.Write(lenbuf)
	require.NoError(t, err, "failed to write length buffer")
	_, err = ss.Write(data)
	require.NoError(t, err, "failed to write data")
	newS := &Session{}
	data = ss.Bytes()
	toRecover := bytes.NewBuffer(data)
	err = newS.recoverFromSnapshot(toRecover, V1)
	require.NoError(t, err, "recover from snapshot failed")
	require.Equal(t, session.ClientID, newS.ClientID,
		"ClientID field changed")
	require.Equal(t, session.RespondedUpTo, newS.RespondedUpTo,
		"RespondedUpTo field changed")
	v1, ok := newS.History[1234]
	require.True(t, ok, "v1 not found")
	require.Equal(t, uint64(324), v1.Value, "unexpected v1 value")
	v2, ok := newS.History[5678]
	require.True(t, ok, "v2 not found")
	require.Equal(t, uint64(458), v2.Value, "unexpected v2 value")
}

func TestUnknownVersionCausePanicWhenRecoverSessionFromSnapshot(t *testing.T) {
	session := &v1session{
		ClientID:      123,
		RespondedUpTo: 456789,
		History:       make(map[RaftSeriesID]uint64),
	}
	ss := &bytes.Buffer{}
	data, err := json.Marshal(session)
	require.NoError(t, err)
	sz := len(data)
	lenbuf := make([]byte, 8)
	binary.LittleEndian.PutUint64(lenbuf, uint64(sz))
	_, err = ss.Write(lenbuf)
	require.NoError(t, err, "failed to write length buffer")
	_, err = ss.Write(data)
	require.NoError(t, err, "failed to write data")
	newS := &Session{}
	data = ss.Bytes()
	toRecover := bytes.NewBuffer(data)
	require.Panics(t, func() {
		err := newS.recoverFromSnapshot(toRecover, SSVersion(3))
		require.NoError(t, err, "recover from snapshot failed")
	}, "panic should be triggered for unknown version")
}
````

## File: internal/rsm/session.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"encoding/binary"
	"encoding/json"
	"io"

	"github.com/lni/goutils/cache/biogo/store/llrb"

	sm "github.com/lni/dragonboat/v4/statemachine"
)

// RaftClientID is the type used as client id in sessions.
type RaftClientID uint64

// RaftSeriesID is the type used as series id in sessions.
type RaftSeriesID uint64

// Compare implements the llrb.Comparable interface.
func (a *RaftClientID) Compare(b llrb.Comparable) int {
	bk := b.(*RaftClientID)
	aval := *a
	bval := *bk
	switch {
	case aval < bval:
		return -1
	case aval > bval:
		return 1
	default:
		return 0
	}
}

// Session is the session object maintained on the raft side.
type Session struct {
	History       map[RaftSeriesID]sm.Result
	ClientID      RaftClientID
	RespondedUpTo RaftSeriesID
}

// v1session is the session type used in v1 snapshot format.
type v1session struct {
	History       map[RaftSeriesID]uint64
	ClientID      RaftClientID
	RespondedUpTo RaftSeriesID
}

func newSession(id RaftClientID) *Session {
	return &Session{
		ClientID: id,
		History:  make(map[RaftSeriesID]sm.Result),
	}
}

// AddResponse adds a response.
func (s *Session) AddResponse(id RaftSeriesID, result sm.Result) {
	s.addResponse(id, result)
}

func (s *Session) getResponse(id RaftSeriesID) (sm.Result, bool) {
	v, ok := s.History[id]
	return v, ok
}

func (s *Session) addResponse(id RaftSeriesID, result sm.Result) {
	_, ok := s.History[id]
	if !ok {
		s.History[id] = result
	} else {
		panic("adding a duplicated response")
	}
}

func (s *Session) clearTo(to RaftSeriesID) {
	if to <= s.RespondedUpTo {
		return
	}
	if to == s.RespondedUpTo+1 {
		delete(s.History, to)
		s.RespondedUpTo = to
		return
	}
	s.RespondedUpTo = to
	for k := range s.History {
		if k <= to {
			delete(s.History, k)
		}
	}
}

func (s *Session) hasResponded(id RaftSeriesID) bool {
	return id <= s.RespondedUpTo
}

func (s *Session) save(writer io.Writer) error {
	data, err := json.Marshal(s)
	if err != nil {
		panic(err)
	}
	lenbuf := make([]byte, 8)
	binary.LittleEndian.PutUint64(lenbuf, uint64(len(data)))
	if _, err := writer.Write(lenbuf); err != nil {
		return err
	}
	if _, err = writer.Write(data); err != nil {
		return err
	}
	return nil
}

func (s *Session) recoverFromSnapshot(reader io.Reader, v SSVersion) error {
	lenbuf := make([]byte, 8)
	if _, err := io.ReadFull(reader, lenbuf); err != nil {
		return err
	}
	data := make([]byte, binary.LittleEndian.Uint64(lenbuf))
	if _, err := io.ReadFull(reader, data); err != nil {
		return err
	}
	switch v {
	case V1:
		s.recoverFromV1Snapshot(data)
	case V2:
		if err := json.Unmarshal(data, s); err != nil {
			panic(err)
		}
	default:
		plog.Panicf("unknown version number %d", v)
	}
	return nil
}

func (s *Session) recoverFromV1Snapshot(data []byte) {
	v := &v1session{}
	if err := json.Unmarshal(data, v); err != nil {
		panic(err)
	}
	s.ClientID = v.ClientID
	s.RespondedUpTo = v.RespondedUpTo
	s.History = make(map[RaftSeriesID]sm.Result)
	for key, val := range v.History {
		s.History[key] = sm.Result{Value: val}
	}
}
````

## File: internal/rsm/sessionmanager_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"bytes"
	"testing"

	"github.com/lni/goutils/cache"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	sm "github.com/lni/dragonboat/v4/statemachine"
)

func TestRegisteriAndUnregisterClient(t *testing.T) {
	sm := NewSessionManager()
	h1 := sm.GetSessionHash()
	_, ok := sm.ClientRegistered(123)
	assert.False(t, ok, "already has client with client id 123")
	sm.RegisterClientID(123)
	_, ok = sm.ClientRegistered(123)
	assert.True(t, ok, "client not registered")
	h2 := sm.GetSessionHash()
	v := sm.UnregisterClientID(123)
	assert.Equal(t, uint64(123), v.Value, "failed to unregister client")
	_, ok = sm.ClientRegistered(123)
	assert.False(t, ok, "still has client with client id 123")
	h3 := sm.GetSessionHash()
	assert.NotEqual(t, h1, h2, "hash does not change")
	assert.Equal(t, h1, h3, "hash unexpectedly changed")
}

func TestSessionSaveOrderWithEviction(t *testing.T) {
	sm1 := &SessionManager{lru: newLRUSession(4)}
	sm2 := &SessionManager{lru: newLRUSession(4)}

	for i := uint64(0); i < sm1.lru.size; i++ {
		sm1.RegisterClientID(i)
	}
	// touch the oldest session to make it the most recently accessed
	s, ok := sm1.ClientRegistered(uint64(0))
	require.True(t, ok, "failed to get client session")
	sm1.AddResponse(s, 1, sm.Result{Value: 123456})
	ss := &bytes.Buffer{}
	err := sm1.SaveSessions(ss)
	require.NoError(t, err, "failed to save snapshot")
	rs := bytes.NewBuffer(ss.Bytes())
	err = sm2.LoadSessions(rs, V2)
	require.NoError(t, err, "failed to restore snapshot")
	// client with the same client id (1 and 2 here) expected to be evicted
	sm1.RegisterClientID(sm1.lru.size)
	sm2.RegisterClientID(sm1.lru.size)
	sm1.RegisterClientID(sm1.lru.size + 1)
	sm2.RegisterClientID(sm1.lru.size + 1)
	s1 := &bytes.Buffer{}
	err = sm1.SaveSessions(s1)
	require.NoError(t, err, "failed to save snapshot")
	s2 := &bytes.Buffer{}
	err = sm2.SaveSessions(s2)
	require.NoError(t, err, "failed to save snapshot")
	assert.Equal(t, s1.Bytes(), s2.Bytes(), "different snapshot")
	check := func(c *cache.OrderedCache) {
		keys := make(map[uint64]struct{})
		c.OrderedDo(func(k, v interface{}) {
			clientID := k.(*RaftClientID)
			keys[uint64(*clientID)] = struct{}{}
		})
		_, ok := keys[0]
		assert.True(t, ok, "client 0 not in the session list")
		_, ok = keys[1]
		assert.False(t, ok, "client 1 unexpectedly still in the session list")
		_, ok = keys[2]
		assert.False(t, ok, "client 2 unexpectedly still in the session list")
	}
	check(sm1.lru.sessions)
	check(sm2.lru.sessions)
}
````

## File: internal/rsm/sessionmanager.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"io"

	sm "github.com/lni/dragonboat/v4/statemachine"
)

// SessionManager is the wrapper struct that implements client session related
// functionalities used in the IManagedStateMachine interface.
type SessionManager struct {
	lru *lrusession
}

var _ ILoadable = (*SessionManager)(nil)

// NewSessionManager returns a new SessionManager instance.
func NewSessionManager() *SessionManager {
	return &SessionManager{
		lru: newLRUSession(LRUMaxSessionCount),
	}
}

// GetSessionHash returns an uint64 integer representing the state of the
// session manager.
func (ds *SessionManager) GetSessionHash() uint64 {
	return ds.lru.getHash()
}

// UpdateRespondedTo updates the responded to value of the specified
// client session.
func (ds *SessionManager) UpdateRespondedTo(session *Session,
	respondedTo uint64) {
	session.clearTo(RaftSeriesID(respondedTo))
}

// RegisterClientID registers a new client, it returns the input client id
// if it is previously unknown, or 0 when the client has already been
// registered.
func (ds *SessionManager) RegisterClientID(clientID uint64) sm.Result {
	es, ok := ds.lru.getSession(RaftClientID(clientID))
	if ok {
		if es.ClientID != RaftClientID(clientID) {
			plog.Panicf("returned an expected session, got id %d, want %d",
				es.ClientID, clientID)
		}
		plog.Warningf("client ID %d already exist", clientID)
		return sm.Result{}
	}
	s := newSession(RaftClientID(clientID))
	ds.lru.addSession(RaftClientID(clientID), *s)
	return sm.Result{Value: clientID}
}

// UnregisterClientID removes the specified client session from the system.
// It returns the client id if the client is successfully removed, or 0
// if the client session does not exist.
func (ds *SessionManager) UnregisterClientID(clientID uint64) sm.Result {
	es, ok := ds.lru.getSession(RaftClientID(clientID))
	if !ok {
		return sm.Result{}
	}
	if es.ClientID != RaftClientID(clientID) {
		plog.Panicf("returned an expected session, got id %d, want %d",
			es.ClientID, clientID)
	}
	ds.lru.delSession(RaftClientID(clientID))
	return sm.Result{Value: clientID}
}

// ClientRegistered returns whether the specified client exists in the system.
func (ds *SessionManager) ClientRegistered(clientID uint64) (*Session, bool) {
	es, ok := ds.lru.getSession(RaftClientID(clientID))
	if ok {
		if es.ClientID != RaftClientID(clientID) {
			plog.Panicf("returned an expected session, got id %d, want %d",
				es.ClientID, clientID)
		}
	}
	return es, ok
}

// UpdateRequired return a tuple of request result, responded before,
// update required.
func (ds *SessionManager) UpdateRequired(session *Session,
	seriesID uint64) (sm.Result, bool, bool) {
	if session.hasResponded(RaftSeriesID(seriesID)) {
		return sm.Result{}, true, false
	}
	v, ok := session.getResponse(RaftSeriesID(seriesID))
	if ok {
		return v, false, false
	}
	return sm.Result{}, false, true
}

// MustHaveClientSeries checks whether the session manager contains a client
// session identified as clientID and whether it has seriesID responded.
func (ds *SessionManager) MustHaveClientSeries(session *Session,
	seriesID uint64) {
	_, ok := session.getResponse(RaftSeriesID(seriesID))
	if ok {
		panic("already has response in session")
	}
}

// AddResponse adds the specified result to the session.
func (ds *SessionManager) AddResponse(session *Session,
	seriesID uint64, result sm.Result) {
	session.addResponse(RaftSeriesID(seriesID), result)
}

// SaveSessions saves the sessions to the provided io.writer.
func (ds *SessionManager) SaveSessions(writer io.Writer) error {
	return ds.lru.save(writer)
}

// LoadSessions loads and restores sessions from io.Reader.
func (ds *SessionManager) LoadSessions(reader io.Reader, v SSVersion) error {
	return ds.lru.load(reader, v)
}
````

## File: internal/rsm/snapshotio_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"bytes"
	"crypto/rand"
	"encoding/binary"
	"io"
	"testing"

	"github.com/lni/dragonboat/v4/internal/vfs"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/stretchr/testify/require"
)

const (
	testSessionSize      uint64 = 16
	testPayloadSize      uint64 = 8
	testSnapshotFilename        = "testsnapshot_safe_to_delete.tmp"
)

func reportLeakedFD(fs vfs.IFS, t *testing.T) {
	vfs.ReportLeakedFD(fs, t)
}

func TestSnapshotWriterCanBeCreated(t *testing.T) {
	fs := vfs.GetTestFS()
	w, err := NewSnapshotWriter(testSnapshotFilename, pb.NoCompression, fs)
	require.NoError(t, err)
	defer func() {
		if err := fs.RemoveAll(testSnapshotFilename); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	defer func() {
		require.NoError(t, w.Close())
	}()
}

func TestSaveHeaderSavesTheHeader(t *testing.T) {
	fs := vfs.GetTestFS()
	defer func() {
		if err := fs.RemoveAll(testSnapshotFilename); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	func() {
		w, err := NewSnapshotWriter(testSnapshotFilename, pb.NoCompression, fs)
		require.NoError(t, err)
		sessionData := make([]byte, testSessionSize)
		storeData := make([]byte, testPayloadSize)
		_, err = rand.Read(sessionData)
		require.NoError(t, err)
		_, err = rand.Read(storeData)
		require.NoError(t, err)
		n, err := w.Write(sessionData)
		require.NoError(t, err)
		require.Equal(t, len(sessionData), n)
		m, err := w.Write(storeData)
		require.NoError(t, err)
		require.Equal(t, len(storeData), m)
		err = w.Close()
		require.NoError(t, err)
		r, header, err := NewSnapshotReader(testSnapshotFilename, fs)
		require.NoError(t, err)
		defer func() {
			require.NoError(t, r.Close())
		}()
		require.Equal(t, DefaultVersion, SSVersion(header.Version))
		require.Equal(t, DefaultChecksumType, header.ChecksumType)
		storeChecksum := w.vw.GetPayloadSum()
		require.True(t, bytes.Equal(header.PayloadChecksum, storeChecksum))
	}()
	reportLeakedFD(fs, t)
}

func makeTestSnapshotFile(t *testing.T, ssz uint64,
	psz uint64, v SSVersion, fs vfs.IFS) (*SnapshotWriter, []byte, []byte) {
	if err := fs.RemoveAll(testSnapshotFilename); err != nil {
		t.Fatalf("%v", err)
	}
	w, err := newVersionedSnapshotWriter(testSnapshotFilename, v,
		pb.NoCompression, fs)
	require.NoError(t, err)
	sessionData := make([]byte, ssz)
	storeData := make([]byte, psz)
	_, err = rand.Read(sessionData)
	require.NoError(t, err)
	_, err = rand.Read(storeData)
	require.NoError(t, err)
	n, err := w.Write(sessionData)
	require.NoError(t, err)
	require.Equal(t, len(sessionData), n)
	m, err := w.Write(storeData)
	require.NoError(t, err)
	require.Equal(t, len(storeData), m)
	if err := w.Close(); err != nil {
		t.Fatalf("%v", err)
	}
	return w, sessionData, storeData
}

func corruptSnapshotPayload(t *testing.T, fs vfs.IFS) {
	tmpFp := "testsnapshot.writing"
	func() {
		f, err := fs.ReuseForWrite(testSnapshotFilename, tmpFp)
		require.NoError(t, err)
		defer func() {
			require.NoError(t, f.Close())
		}()
		s := (testSessionSize + testPayloadSize) / 2
		data := make([]byte, 1)
		_, err = f.ReadAt(data, int64(HeaderSize+s))
		require.NoError(t, err)
		data[0] = data[0] + 1
		_, err = f.WriteAt(data, int64(HeaderSize+s))
		require.NoError(t, err)
	}()
	if err := fs.Rename(tmpFp, testSnapshotFilename); err != nil {
		t.Fatalf("%v", err)
	}
}

func createTestSnapshotFile(t *testing.T,
	v SSVersion, fs vfs.IFS) (*SnapshotWriter, []byte, []byte) {
	return makeTestSnapshotFile(t, testSessionSize, testPayloadSize, v, fs)
}

func testCorruptedPayloadWillBeDetected(t *testing.T, v SSVersion, fs vfs.IFS) {
	defer func() {
		if err := fs.RemoveAll(testSnapshotFilename); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	func() {
		createTestSnapshotFile(t, v, fs)
		corruptSnapshotPayload(t, fs)
		require.Panics(t, func() {
			r, header, err := NewSnapshotReader(testSnapshotFilename, fs)
			require.NoError(t, err)
			defer func() {
				require.NoError(t, r.Close())
			}()
			_, err = rand.Read(header.PayloadChecksum)
			require.NoError(t, err)
			s := make([]byte, testSessionSize)
			p := make([]byte, testPayloadSize)
			n, err := io.ReadFull(r, s)
			require.NoError(t, err)
			require.Equal(t, testSessionSize, uint64(n))
			n, err = io.ReadFull(r, p)
			require.NoError(t, err)
			require.Equal(t, testPayloadSize, uint64(n))
			r.validatePayload()
		})
	}()
	reportLeakedFD(fs, t)
}

func TestCorruptedPayloadWillBeDetected(t *testing.T) {
	fs := vfs.GetTestFS()
	testCorruptedPayloadWillBeDetected(t, V1, fs)
	testCorruptedPayloadWillBeDetected(t, V2, fs)
}

func testNormalSnapshotCanPassValidation(t *testing.T, v SSVersion, fs vfs.IFS) {
	defer func() {
		if err := fs.RemoveAll(testSnapshotFilename); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	func() {
		_, sessionData, storeData := createTestSnapshotFile(t, v, fs)
		r, _, err := NewSnapshotReader(testSnapshotFilename, fs)
		require.NoError(t, err)
		defer func() {
			require.NoError(t, r.Close())
		}()
		s := make([]byte, testSessionSize)
		p := make([]byte, testPayloadSize)
		n, err := io.ReadFull(r, s)
		require.NoError(t, err)
		require.Equal(t, testSessionSize, uint64(n))
		n, err = io.ReadFull(r, p)
		require.NoError(t, err)
		require.Equal(t, testPayloadSize, uint64(n))
		require.NotPanics(t, func() {
			r.validatePayload()
		})
		require.True(t, bytes.Equal(sessionData, s))
		require.True(t, bytes.Equal(storeData, p))
	}()
	reportLeakedFD(fs, t)
}

func TestNormalSnapshotCanPassValidation(t *testing.T) {
	fs := vfs.GetTestFS()
	testNormalSnapshotCanPassValidation(t, V1, fs)
	testNormalSnapshotCanPassValidation(t, V2, fs)
}

func readTestSnapshot(fn string, sz uint64, fs vfs.IFS) ([]byte, error) {
	file, err := fs.Open(fn)
	if err != nil {
		return nil, err
	}
	defer func() {
		if err := file.Close(); err != nil {
			panic(err)
		}
	}()
	data := make([]byte, sz)
	n, err := file.Read(data)
	if err != nil {
		return nil, err
	}
	return data[:n], nil
}

func testSingleBlockSnapshotValidation(t *testing.T, sv SSVersion, fs vfs.IFS) {
	createTestSnapshotFile(t, sv, fs)
	defer func() {
		if err := fs.RemoveAll(testSnapshotFilename); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	data, err := readTestSnapshot(testSnapshotFilename, 1024*1024, fs)
	require.NoError(t, err)
	v := NewSnapshotValidator()
	require.True(t, v.AddChunk(data, 0))
	require.True(t, v.Validate())
	require.False(t, v.AddChunk(data, 0))
	// intentionally corrupt the data
	data[len(data)-1] = data[len(data)-1] + 1
	v = NewSnapshotValidator()
	require.True(t, v.AddChunk(data, 0))
	require.False(t, v.Validate())
	reportLeakedFD(fs, t)
}

func TestSingleBlockSnapshotValidation(t *testing.T) {
	fs := vfs.GetTestFS()
	testSingleBlockSnapshotValidation(t, V1, fs)
	testSingleBlockSnapshotValidation(t, V2, fs)
}

func testMultiBlockSnapshotValidation(t *testing.T, sv SSVersion, fs vfs.IFS) {
	makeTestSnapshotFile(t, 1024*1024, 1024*1024*8, sv, fs)
	defer func() {
		if err := fs.RemoveAll(testSnapshotFilename); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	data, err := readTestSnapshot(testSnapshotFilename, 1024*1024*10, fs)
	require.NoError(t, err)
	v := NewSnapshotValidator()
	c1 := data[:1024*1024]
	c2 := data[1024*1024:]
	require.True(t, v.AddChunk(c1, 0))
	require.True(t, v.AddChunk(c2, 1))
	require.True(t, v.Validate())
	v = NewSnapshotValidator()
	c2[len(c2)-1] = c2[len(c2)-1] + 1
	require.True(t, v.AddChunk(c1, 0))
	require.True(t, v.AddChunk(c2, 1))
	require.False(t, v.Validate())
	reportLeakedFD(fs, t)
}

func TestMultiBlockSnapshotValidation(t *testing.T) {
	fs := vfs.GetTestFS()
	testMultiBlockSnapshotValidation(t, V1, fs)
	testMultiBlockSnapshotValidation(t, V2, fs)
}

func TestMustInSameDir(t *testing.T) {
	fs := vfs.GetTestFS()
	tests := []struct {
		path1   string
		path2   string
		sameDir bool
	}{
		{"/d1/d2/f1.data", "/d1/d2/f2.data", true},
		{"/d1/d2/f1.data", "/d1/d3/f2.data", false},
		{"/d1/d2/f1.data", "/d1/d2/d3/f2.data", false},
		{"/d1/d2/f1.data", "f2.data", false},
		{"/d1/d2/f1.data", "/d2/f2.data", false},
		{"/d1/d2/f1.data", "d2/f2.data", false},
	}
	for _, tt := range tests {
		func() {
			if tt.sameDir {
				require.NotPanics(t, func() {
					mustInSameDir(tt.path1, tt.path2, fs)
				})
			} else {
				require.Panics(t, func() {
					mustInSameDir(tt.path1, tt.path2, fs)
				})
			}
		}()
	}
}

func TestShrinkSnapshot(t *testing.T) {
	fs := vfs.GetTestFS()
	snapshotFilename := "test_snapshot_safe_to_delete.data"
	shrunkFilename := "test_snapshot_safe_to_delete.shrunk"
	writer, err := NewSnapshotWriter(snapshotFilename, pb.NoCompression, fs)
	require.NoError(t, err)
	defer func() {
		if err := fs.RemoveAll(snapshotFilename); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	sz := make([]byte, 8)
	binary.LittleEndian.PutUint64(sz, uint64(0))
	_, err = writer.Write(sz)
	require.NoError(t, err)
	for i := 0; i < 10; i++ {
		data := make([]byte, 1024*1024+i*256)
		_, err := rand.Read(data)
		require.NoError(t, err)
		_, err = writer.Write(data)
		require.NoError(t, err)
	}
	err = writer.Close()
	require.NoError(t, err)
	shrunk, err := IsShrunkSnapshotFile(snapshotFilename, fs)
	require.NoError(t, err)
	require.False(t, shrunk)
	err = ShrinkSnapshot(snapshotFilename, shrunkFilename, fs)
	require.NoError(t, err)
	defer func() {
		if err := fs.RemoveAll(shrunkFilename); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	shrunk, err = IsShrunkSnapshotFile(snapshotFilename, fs)
	require.NoError(t, err)
	require.False(t, shrunk)
	shrunk, err = IsShrunkSnapshotFile(shrunkFilename, fs)
	require.NoError(t, err)
	require.True(t, shrunk)
	fi, err := fs.Stat(shrunkFilename)
	require.NoError(t, err)
	require.Equal(t, int64(1060), fi.Size())
	reader, _, err := NewSnapshotReader(shrunkFilename, fs)
	require.NoError(t, err)
	err = reader.Close()
	require.NoError(t, err)
	reportLeakedFD(fs, t)
}

func TestReplaceSnapshotFile(t *testing.T) {
	fs := vfs.GetTestFS()
	f1name := "test_snapshot_safe_to_delete.data"
	f2name := "test_snapshot_safe_to_delete.data2"
	createFile := func(fn string, sz uint64) {
		f1, err := fs.Create(fn)
		require.NoError(t, err)
		data := make([]byte, sz)
		_, err = rand.Read(data)
		require.NoError(t, err)
		_, err = f1.Write(data)
		require.NoError(t, err)
		err = f1.Close()
		require.NoError(t, err)
	}
	createFile(f1name, 1024)
	createFile(f2name, 2048)
	defer func() {
		if err := fs.RemoveAll(f1name); err != nil {
			t.Fatalf("%v", err)
		}
		if err := fs.RemoveAll(f2name); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	err := ReplaceSnapshot(f2name, f1name, fs)
	require.NoError(t, err)
	_, err = fs.Stat(f2name)
	require.Error(t, err)
	fi, err := fs.Stat(f1name)
	require.NoError(t, err)
	require.Equal(t, int64(2048), fi.Size())
}

func testV2PayloadChecksumCanBeRead(t *testing.T, sz uint64, fs vfs.IFS) {
	defer func() {
		if err := fs.RemoveAll(testSnapshotFilename); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	func() {
		makeTestSnapshotFile(t, 0, sz, V2, fs)
		reader, header, err := NewSnapshotReader(testSnapshotFilename, fs)
		require.NoError(t, err)
		defer func() {
			require.NoError(t, reader.Close())
		}()
		crc, err := GetV2PayloadChecksum(testSnapshotFilename, fs)
		require.NoError(t, err)
		require.True(t, bytes.Equal(crc, header.PayloadChecksum))
	}()
	reportLeakedFD(fs, t)
}

func TestV2PayloadChecksumCanBeRead(t *testing.T) {
	fs := vfs.GetTestFS()
	testV2PayloadChecksumCanBeRead(t, blockSize-1, fs)
	testV2PayloadChecksumCanBeRead(t, blockSize, fs)
	testV2PayloadChecksumCanBeRead(t, blockSize+1, fs)
	testV2PayloadChecksumCanBeRead(t, blockSize*3-1, fs)
	testV2PayloadChecksumCanBeRead(t, blockSize*3, fs)
	testV2PayloadChecksumCanBeRead(t, blockSize*3+1, fs)
}

func TestV1SnapshotCanBeLoaded(t *testing.T) {
	// rsm: idList sz 2
	// rsm: client id 15771809973567514624, responded to 4, map[5:128]
	// rsm: client id 6760681031265190231, responded to 2, map[3:128]
	fs := vfs.GetTestFS()
	if fs != vfs.DefaultFS {
		t.Skip("skipped, the fs can not access the testdata")
	}
	fp := fs.PathJoin("testdata", "v1snapshot.gbsnap")
	reader, header, err := NewSnapshotReader(fp, fs)
	require.NoError(t, err)
	defer func() {
		require.NoError(t, reader.Close())
	}()
	require.Equal(t, uint64(1), header.Version)
	v := (SSVersion)(header.Version)
	sm := NewSessionManager()
	err = sm.LoadSessions(reader, v)
	require.NoError(t, err)
	sessions := sm.lru
	s1, ok := sessions.getSession(15771809973567514624)
	require.True(t, ok)
	s2, ok := sessions.getSession(6760681031265190231)
	require.True(t, ok)
	require.Equal(t, RaftClientID(15771809973567514624), s1.ClientID)
	require.Equal(t, RaftSeriesID(4), s1.RespondedUpTo)
	require.Equal(t, RaftClientID(6760681031265190231), s2.ClientID)
	require.Equal(t, RaftSeriesID(2), s2.RespondedUpTo)
	r1, ok := s1.History[5]
	require.True(t, ok)
	require.Equal(t, uint64(128), r1.Value)
	require.Equal(t, 1, len(s1.History))
	r2, ok := s2.History[3]
	require.True(t, ok)
	require.Equal(t, uint64(128), r2.Value)
	require.Equal(t, 1, len(s2.History))
	data := make([]byte, 11)
	_, err = io.ReadFull(reader, data)
	require.NoError(t, err)
	require.Equal(t, "random-data", string(data))
}

func TestValidateHeader(t *testing.T) {
	data := []byte{1, 2, 3, 4}
	require.True(t, validateHeader(data, fourZeroBytes))
	h := newCRC32Hash()
	_, err := h.Write(data)
	require.NoError(t, err)
	crc := h.Sum(nil)
	require.True(t, validateHeader(data, crc))
	crc[0] = crc[0] + 1
	require.False(t, validateHeader(data, crc))
}

func TestGetWitnessSnapshot(t *testing.T) {
	fs := vfs.GetTestFS()
	d, err := GetWitnessSnapshot(fs)
	require.NoError(t, err)
	data := GetEmptyLRUSession()
	expectedLen := HeaderSize + uint64(len(data)) + 20
	require.Equal(t, expectedLen, uint64(len(d)))
}
````

## File: internal/rsm/snapshotio.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"bytes"
	"encoding/binary"
	"hash"
	"hash/crc32"
	"io"
	"time"

	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/vfs"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

// SSVersion is the snapshot version value type.
type SSVersion uint64

const (
	// V1 is the value of snapshot version 1.
	V1 SSVersion = 1
	// V2 is the value of snapshot version 2.
	V2 SSVersion = 2
	// DefaultVersion is the snapshot binary format version.
	DefaultVersion SSVersion = V2
	// HeaderSize is the size of snapshot in number of bytes.
	HeaderSize = settings.SnapshotHeaderSize
	// which checksum type to use.
	// CRC32IEEE and google's highway hash are supported
	defaultChecksumType = pb.CRC32IEEE
	// DefaultChecksumType is the default checksum type.
	DefaultChecksumType = defaultChecksumType
)

func newCRC32Hash() hash.Hash {
	return crc32.NewIEEE()
}

func getChecksumType() pb.ChecksumType {
	return defaultChecksumType
}

// GetDefaultChecksum returns the default hash.Hash instance.
func GetDefaultChecksum() hash.Hash {
	return getDefaultChecksum()
}

func getDefaultChecksum() hash.Hash {
	return mustGetChecksum(getChecksumType())
}

func getChecksum(t pb.ChecksumType) (hash.Hash, bool) {
	if t == pb.CRC32IEEE {
		return newCRC32Hash(), true
	}
	return nil, false
}

func mustGetChecksum(t pb.ChecksumType) hash.Hash {
	c, ok := getChecksum(t)
	if !ok {
		plog.Panicf("failed to get checksum, type %d", t)
	}
	return c
}

func getVersionedWriter(w io.Writer, v SSVersion) (IVWriter, bool) {
	switch v {
	case V1:
		return newV1Wrtier(w), true
	case V2:
		return newV2Writer(w, defaultChecksumType), true
	default:
		return nil, false
	}
}

func mustGetVersionedWriter(w io.Writer, v SSVersion) IVWriter {
	vw, ok := getVersionedWriter(w, v)
	if !ok {
		plog.Panicf("failed to get version writer, v %d", v)
	}
	return vw
}

func getVersionedReader(r io.Reader,
	v SSVersion, t pb.ChecksumType) (IVReader, bool) {
	switch v {
	case V1:
		return newV1Reader(r), true
	case V2:
		return newV2Reader(r, t), true
	default:
		return nil, false
	}
}

func mustGetVersionedReader(r io.Reader,
	v SSVersion, t pb.ChecksumType) IVReader {
	vr, ok := getVersionedReader(r, v, t)
	if !ok {
		plog.Panicf("failed to get version reader, v %d", v)
	}
	return vr
}

func getVersionedValidator(header pb.SnapshotHeader) (IVValidator, bool) {
	v := (SSVersion)(header.Version)
	switch v {
	case V1:
		return newV1Validator(header), true
	case V2:
		h, ok := getChecksum(header.ChecksumType)
		if !ok {
			return nil, false
		}
		return newV2Validator(h), true
	default:
		return nil, false
	}
}

// GetWitnessSnapshot returns the content of a witness snapshot.
func GetWitnessSnapshot(fs vfs.IFS) (result []byte, err error) {
	f, path, err := fileutil.TempFile("", "dragonboat-witness-snapshot", fs)
	if err != nil {
		return nil, err
	}
	w, err := newSnapshotWriter(f, path, DefaultVersion, pb.NoCompression, fs)
	if err != nil {
		return nil, err
	}
	if _, err := w.Write(GetEmptyLRUSession()); err != nil {
		return nil, err
	}
	if err := w.Close(); err != nil {
		return nil, err
	}
	df, err := fs.Open(path)
	if err != nil {
		return nil, err
	}
	defer func() {
		err = firstError(err, df.Close())
	}()
	buf := bytes.NewBuffer(nil)
	if _, err = io.Copy(buf, df); err != nil {
		return nil, err
	}
	return buf.Bytes(), nil
}

// SnapshotWriter is an io.Writer used to write snapshot file.
type SnapshotWriter struct {
	vw     IVWriter
	file   vfs.File
	fs     vfs.IFS
	fp     string
	ct     pb.CompressionType
	closed bool
}

// NewSnapshotWriter creates a new snapshot writer instance.
func NewSnapshotWriter(fp string,
	ct pb.CompressionType, fs vfs.IFS) (*SnapshotWriter, error) {
	return newVersionedSnapshotWriter(fp, DefaultVersion, ct, fs)
}

func newVersionedSnapshotWriter(fp string,
	v SSVersion, ct pb.CompressionType, fs vfs.IFS) (*SnapshotWriter, error) {
	f, err := fs.Create(fp)
	if err != nil {
		return nil, err
	}
	return newSnapshotWriter(f, fp, v, ct, fs)
}

func newSnapshotWriter(f vfs.File, fp string,
	v SSVersion, ct pb.CompressionType, fs vfs.IFS) (*SnapshotWriter, error) {
	dummy := make([]byte, HeaderSize)
	if _, err := f.Write(dummy); err != nil {
		return nil, err
	}
	sw := &SnapshotWriter{
		vw:   mustGetVersionedWriter(f, v),
		file: f,
		fp:   fp,
		ct:   ct,
		fs:   fs,
	}
	return sw, nil
}

// Close closes the snapshot writer instance.
func (sw *SnapshotWriter) Close() (err error) {
	sw.closed = true
	err = firstError(err, sw.flush())
	err = firstError(err, sw.saveHeader())
	err = firstError(err, sw.file.Sync())
	err = firstError(err, sw.file.Close())
	return firstError(err, fileutil.SyncDir(sw.fs.PathDir(sw.fp), sw.fs))
}

// Write writes the specified data to the snapshot.
func (sw *SnapshotWriter) Write(data []byte) (int, error) {
	return sw.vw.Write(data)
}

// GetPayloadSize returns the payload size.
func (sw *SnapshotWriter) GetPayloadSize(sz uint64) uint64 {
	if !sw.closed {
		panic("not closed")
	}
	return sw.vw.GetPayloadSize(sz)
}

// GetPayloadChecksum returns the payload checksum.
func (sw *SnapshotWriter) GetPayloadChecksum() []byte {
	if !sw.closed {
		panic("not closed")
	}
	return sw.vw.GetPayloadSum()
}

func (sw *SnapshotWriter) flush() error {
	return sw.vw.Close()
}

func (sw *SnapshotWriter) saveHeader() error {
	// for v2, the PayloadChecksu field is really the checksum of all block
	// checksums
	sh := pb.SnapshotHeader{
		UnreliableTime:  uint64(time.Now().UnixNano()),
		PayloadChecksum: sw.GetPayloadChecksum(),
		ChecksumType:    getChecksumType(),
		Version:         uint64(sw.vw.GetVersion()),
		CompressionType: sw.ct,
	}
	data := pb.MustMarshal(&sh)
	headerHash := getDefaultChecksum()
	if _, err := headerHash.Write(data); err != nil {
		return err
	}
	sh.HeaderChecksum = headerHash.Sum(nil)
	data = pb.MustMarshal(&sh)
	if uint64(len(data)) > HeaderSize-8 {
		panic("snapshot header is too large")
	}
	lenbuf := make([]byte, 8)
	binary.LittleEndian.PutUint64(lenbuf, uint64(len(data)))
	if _, err := sw.file.WriteAt(lenbuf, 0); err != nil {
		return err
	}
	if _, err := sw.file.WriteAt(data, 8); err != nil {
		return err
	}
	return nil
}

// SnapshotReader is an io.Reader for reading from snapshot files.
type SnapshotReader struct {
	r      IVReader
	file   vfs.File
	header pb.SnapshotHeader
}

// NewSnapshotReader creates a new snapshot reader instance.
func NewSnapshotReader(fp string,
	fs vfs.IFS) (*SnapshotReader, pb.SnapshotHeader, error) {
	f, err := fs.Open(fp)
	if err != nil {
		return nil, pb.SnapshotHeader{}, err
	}
	r := &SnapshotReader{file: f}
	header, err := r.getHeader()
	if err != nil {
		return nil, pb.SnapshotHeader{}, err
	}
	return r, header, nil
}

// Close closes the snapshot reader instance.
func (sr *SnapshotReader) Close() error {
	// defer is used here to make sure file is always closed, otherwise tests that
	// panic in validatePayload() will complain about not closed file.
	defer sr.validatePayload()
	return sr.file.Close()
}

// Read reads up to len(data) bytes from the snapshot file.
func (sr *SnapshotReader) Read(data []byte) (int, error) {
	return sr.r.Read(data)
}

func (sr *SnapshotReader) validatePayload() {
	if sr.header.Version == uint64(V1) {
		if !bytes.Equal(sr.r.Sum(), sr.header.PayloadChecksum) {
			panic("corrupted snapshot payload")
		}
	}
}

func (sr *SnapshotReader) getHeader() (pb.SnapshotHeader, error) {
	if sr.r != nil {
		panic("getHeader called again?")
	}
	empty := pb.SnapshotHeader{}
	lenbuf := make([]byte, 8)
	if _, err := io.ReadFull(sr.file, lenbuf); err != nil {
		return empty, err
	}
	sz := binary.LittleEndian.Uint64(lenbuf)
	if sz > HeaderSize-8 {
		panic("invalid snapshot header size")
	}
	data := make([]byte, sz)
	if _, err := io.ReadFull(sr.file, data); err != nil {
		return empty, err
	}
	pb.MustUnmarshal(&sr.header, data)
	crcdata := make([]byte, 4)
	if _, err := io.ReadFull(sr.file, crcdata); err != nil {
		return empty, err
	}
	if !validateHeader(data, crcdata) {
		panic("corrupted header")
	}
	blank := make([]byte, HeaderSize-8-sz-4)
	if _, err := io.ReadFull(sr.file, blank); err != nil {
		return empty, err
	}
	var reader io.Reader = sr.file
	v := SSVersion(sr.header.Version)
	if v == V2 {
		st, err := sr.file.Stat()
		if err != nil {
			return empty, err
		}
		payloadSz := st.Size() - int64(HeaderSize) - int64(tailSize)
		reader = io.LimitReader(reader, payloadSz)
	}
	sr.r = mustGetVersionedReader(reader, v, sr.header.ChecksumType)
	return sr.header, nil
}

var fourZeroBytes = []byte{0, 0, 0, 0}

func validateHeader(header []byte, crc32 []byte) bool {
	if len(crc32) != 4 {
		plog.Panicf("invalid crc32 len: %d", len(crc32))
	}
	if !bytes.Equal(crc32, fourZeroBytes) {
		h := newCRC32Hash()
		fileutil.MustWrite(h, header)
		return bytes.Equal(h.Sum(nil), crc32)
	}
	return true
}

// SnapshotValidator is the validator used to check incoming snapshot chunks.
type SnapshotValidator struct {
	v IVValidator
}

// NewSnapshotValidator creates and returns a new SnapshotValidator instance.
func NewSnapshotValidator() *SnapshotValidator {
	return &SnapshotValidator{}
}

// AddChunk adds a new snapshot chunk to the validator.
func (v *SnapshotValidator) AddChunk(data []byte, chunkID uint64) bool {
	if chunkID == 0 {
		header, crc, ok := getHeaderFromFirstChunk(data)
		if !ok {
			plog.Errorf("failed to get header from first chunk")
			return false
		}
		if v.v != nil {
			return false
		}
		if !validateHeader(header, crc) {
			plog.Errorf("validate header failed")
			return false
		}
		var headerRec pb.SnapshotHeader
		pb.MustUnmarshal(&headerRec, header)
		v.v, ok = getVersionedValidator(headerRec)
		if !ok {
			return false
		}
	} else {
		if v.v == nil {
			return false
		}
	}
	return v.v.AddChunk(data, chunkID)
}

// Validate validates the added chunks and return a boolean flag indicating
// whether the snapshot chunks are valid.
func (v *SnapshotValidator) Validate() bool {
	if v.v == nil {
		return false
	}
	return v.v.Validate()
}

// IsShrunkSnapshotFile returns a boolean flag indicating whether the
// specified snapshot file is already shrunk.
func IsShrunkSnapshotFile(fp string, fs vfs.IFS) (shrunk bool, err error) {
	reader, _, err := NewSnapshotReader(fp, fs)
	if err != nil {
		return false, err
	}
	defer func() {
		err = firstError(err, reader.Close())
	}()
	sz := make([]byte, 8)
	if _, err := io.ReadFull(reader, sz); err != nil {
		return false, err
	}
	if _, err = io.ReadFull(reader, sz); err != nil {
		return false, err
	}
	if size := binary.LittleEndian.Uint64(sz); size != 0 {
		return false, nil
	}
	oneByte := make([]byte, 1)
	if _, err := io.ReadFull(reader, oneByte); err != nil {
		if err != io.EOF && err != io.ErrUnexpectedEOF {
			return false, err
		}
		return true, nil
	}
	return false, nil
}

func mustInSameDir(fp string, newFp string, fs vfs.IFS) {
	if fs.PathDir(fp) != fs.PathDir(newFp) {
		plog.Panicf("not in the same dir, dir 1: %s, dir 2: %s",
			fs.PathDir(fp), fs.PathDir(newFp))
	}
}

// ShrinkSnapshot shrinks the specified snapshot file and save the generated
// shrunk version to the path specified by newFp.
func ShrinkSnapshot(fp string, newFp string, fs vfs.IFS) (err error) {
	mustInSameDir(fp, newFp, fs)
	reader, _, err := NewSnapshotReader(fp, fs)
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, reader.Close())
	}()
	writer, err := NewSnapshotWriter(newFp, pb.NoCompression, fs)
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, writer.Close())
	}()
	if _, err := writer.Write(GetEmptyLRUSession()); err != nil {
		return err
	}
	return nil
}

// ReplaceSnapshot replace the specified snapshot file with the shrunk
// version atomically.
func ReplaceSnapshot(newFp string, fp string, fs vfs.IFS) error {
	mustInSameDir(fp, newFp, fs)
	if err := fs.Rename(newFp, fp); err != nil {
		return err
	}
	return fileutil.SyncDir(fs.PathDir(fp), fs)
}
````

## File: internal/rsm/statemachine_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"bytes"
	"crypto/rand"
	"fmt"
	"io"
	"testing"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/leaktest"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/client"
	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/raft"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/tests"
	"github.com/lni/dragonboat/v4/internal/tests/kvpb"
	"github.com/lni/dragonboat/v4/internal/utils/dio"
	"github.com/lni/dragonboat/v4/internal/vfs"
	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
)

const (
	testSnapshotterDir = "rsm_test_data_safe_to_delete"
	snapshotFileSuffix = "gbsnap"
)

func removeTestDir(fs vfs.IFS) {
	if err := fs.RemoveAll(testSnapshotterDir); err != nil {
		panic(err)
	}
}

func createTestDir(fs vfs.IFS) {
	removeTestDir(fs)
	if err := fs.MkdirAll(testSnapshotterDir, 0755); err != nil {
		panic(err)
	}
}

type testNodeProxy struct {
	applyConfChange    bool
	addPeer            bool
	removePeer         bool
	reject             bool
	accept             bool
	smResult           sm.Result
	index              uint64
	rejected           bool
	ignored            bool
	applyUpdateInvoked bool
	notifyReadClient   bool
	addPeerCount       uint64
	addNonVoting       bool
	addNonVotingCount  uint64
	nodeReady          uint64
	applyUpdateCalled  bool
	firstIndex         uint64
}

func newTestNodeProxy() *testNodeProxy {
	return &testNodeProxy{}
}

func (p *testNodeProxy) StepReady() { p.nodeReady++ }

func (p *testNodeProxy) ShouldStop() <-chan struct{} {
	return nil
}

func (p *testNodeProxy) ApplyUpdate(entry pb.Entry,
	result sm.Result, rejected bool, ignored bool, notifyReadClient bool) {
	if !p.applyUpdateCalled {
		p.applyUpdateCalled = true
		p.firstIndex = entry.Index
	}
	p.smResult = result
	p.index = entry.Index
	p.rejected = rejected
	p.ignored = ignored
	p.applyUpdateInvoked = true
	p.notifyReadClient = notifyReadClient
}

func (p *testNodeProxy) SetLastApplied(v uint64) {}

func (p *testNodeProxy) RestoreRemotes(s pb.Snapshot) error {
	for k := range s.Membership.Addresses {
		_ = k
		p.addPeer = true
		p.addPeerCount++
	}
	return nil
}

func (p *testNodeProxy) ApplyConfigChange(cc pb.ConfigChange,
	key uint64, rejected bool) error {
	if !rejected {
		p.applyConfChange = true
		switch cc.Type {
		case pb.AddNode:
			p.addPeer = true
			p.addPeerCount++
		case pb.AddNonVoting:
			p.addNonVoting = true
			p.addNonVotingCount++
		case pb.RemoveNode:
			p.removePeer = true
		}
	}
	p.configChangeProcessed(key, rejected)
	return nil
}

func (p *testNodeProxy) configChangeProcessed(index uint64, rejected bool) {
	if rejected {
		p.reject = true
	} else {
		p.accept = true
	}
}

func (p *testNodeProxy) ReplicaID() uint64 { return 1 }
func (p *testNodeProxy) ShardID() uint64   { return 1 }

type noopCompactor struct{}

func (noopCompactor) Compact(uint64) error { return nil }

var testCompactor = &noopCompactor{}

type testSnapshotter struct {
	index    uint64
	dataSize uint64
	fs       vfs.IFS
}

func newTestSnapshotter(fs vfs.IFS) *testSnapshotter {
	return &testSnapshotter{fs: fs}
}

func (s *testSnapshotter) GetSnapshot() (pb.Snapshot, error) {
	fn := fmt.Sprintf("snapshot-test.%s", snapshotFileSuffix)
	fp := s.fs.PathJoin(testSnapshotterDir, fn)
	address := make(map[uint64]string)
	address[1] = "localhost:1"
	address[2] = "localhost:2"
	ss := pb.Snapshot{
		Filepath: fp,
		FileSize: s.dataSize,
		Index:    s.index,
		Term:     2,
		Membership: pb.Membership{
			Addresses: address,
		},
	}
	ss.Load(testCompactor)
	return ss, nil
}

func (s *testSnapshotter) Shrunk(ss pb.Snapshot) (bool, error) {
	return IsShrunkSnapshotFile(s.getFilePath(ss.Index), s.fs)
}

func (s *testSnapshotter) getFilePath(index uint64) string {
	filename := fmt.Sprintf("snapshot-test.%s", snapshotFileSuffix)
	return s.fs.PathJoin(testSnapshotterDir, filename)
}

func (s *testSnapshotter) GetSnapshotFromLogDB() (pb.Snapshot, error) {
	fn := fmt.Sprintf("snapshot-test.%s", snapshotFileSuffix)
	fp := s.fs.PathJoin(testSnapshotterDir, fn)
	snap := pb.Snapshot{
		Filepath: fp,
		FileSize: s.dataSize,
		Index:    s.index,
		Term:     2,
	}
	return snap, nil
}

func (s *testSnapshotter) IsNoSnapshotError(err error) bool {
	if err == nil {
		return false
	}
	return err.Error() == "no snapshot available"
}

func (s *testSnapshotter) Stream(streamable IStreamable,
	meta SSMeta, sink pb.IChunkSink) error {
	writer := NewChunkWriter(sink, meta)
	defer func() {
		if err := writer.Close(); err != nil {
			panic(err)
		}
	}()
	return streamable.Stream(meta.Ctx, writer)
}

func (s *testSnapshotter) Save(savable ISavable,
	meta SSMeta) (ss pb.Snapshot, env SSEnv, err error) {
	s.index = meta.Index
	f := func(cid uint64, nid uint64) string {
		return testSnapshotterDir
	}
	env = server.NewSSEnv(f, 1, 1, s.index, 1, server.SnapshotMode, s.fs)
	fn := fmt.Sprintf("snapshot-test.%s", snapshotFileSuffix)
	fp := s.fs.PathJoin(testSnapshotterDir, fn)
	writer, err := NewSnapshotWriter(fp, pb.NoCompression, s.fs)
	if err != nil {
		return pb.Snapshot{}, env, err
	}
	cw := dio.NewCountedWriter(writer)
	defer func() {
		err = firstError(err, cw.Close())
		if ss.Index > 0 {
			ss.FileSize = cw.BytesWritten() + HeaderSize
		}
	}()
	session := meta.Session.Bytes()
	if _, err := savable.Save(SSMeta{},
		cw, session, nil); err != nil {
		return pb.Snapshot{}, env, err
	}
	ss = pb.Snapshot{
		Filepath:   env.GetFilepath(),
		Membership: meta.Membership,
		Index:      meta.Index,
		Term:       meta.Term,
	}
	return ss, env, nil
}

func (s *testSnapshotter) Load(ss pb.Snapshot,
	loadable ILoadable, recoverable IRecoverable) error {
	fp := s.getFilePath(ss.Index)
	fs := make([]sm.SnapshotFile, 0)
	for _, f := range ss.Files {
		fs = append(fs, sm.SnapshotFile{
			FileID:   f.FileId,
			Filepath: f.Filepath,
			Metadata: f.Metadata,
		})
	}
	reader, header, err := NewSnapshotReader(fp, s.fs)
	if err != nil {
		return err
	}
	defer func() {
		err = reader.Close()
	}()
	v := SSVersion(header.Version)
	if err := loadable.LoadSessions(reader, v); err != nil {
		return err
	}
	if err := recoverable.Recover(reader, fs); err != nil {
		return err
	}
	return nil
}

func runSMTest(t *testing.T,
	tf func(t *testing.T, sm *StateMachine), fs vfs.IFS) {
	defer leaktest.AfterTest(t)()
	store := tests.NewKVTest(1, 1)
	config := config.Config{ShardID: 1, ReplicaID: 1}
	store.(*tests.KVTest).DisableLargeDelay()
	ds := NewNativeSM(config, NewInMemStateMachine(store), make(chan struct{}))
	nodeProxy := newTestNodeProxy()
	snapshotter := newTestSnapshotter(fs)
	sm := NewStateMachine(ds, snapshotter, config, nodeProxy, fs)
	tf(t, sm)
	reportLeakedFD(fs, t)
}

func runSMTest2(t *testing.T,
	tf func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine), fs vfs.IFS) {
	defer leaktest.AfterTest(t)()
	createTestDir(fs)
	defer removeTestDir(fs)
	store := tests.NewKVTest(1, 1)
	config := config.Config{ShardID: 1, ReplicaID: 1}
	store.(*tests.KVTest).DisableLargeDelay()
	ds := NewNativeSM(config, NewInMemStateMachine(store), make(chan struct{}))
	nodeProxy := newTestNodeProxy()
	snapshotter := newTestSnapshotter(fs)
	sm := NewStateMachine(ds, snapshotter, config, nodeProxy, fs)
	tf(t, sm, ds, nodeProxy, snapshotter, store)
	reportLeakedFD(fs, t)
}

func TestDefaultTaskIsNotSnapshotTask(t *testing.T) {
	task := Task{}
	require.False(t, task.IsSnapshotTask())
}

func TestUpdatesCanBeBatched(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	createTestDir(fs)
	defer removeTestDir(fs)
	store := &tests.ConcurrentUpdate{}
	config := config.Config{ShardID: 1, ReplicaID: 1}
	ds := NewNativeSM(config,
		NewConcurrentStateMachine(store), make(chan struct{}))
	nodeProxy := newTestNodeProxy()
	snapshotter := newTestSnapshotter(fs)
	sm := NewStateMachine(ds, snapshotter, config, nodeProxy, fs)
	e1 := pb.Entry{
		ClientID: 123,
		SeriesID: client.NoOPSeriesID,
		Index:    235,
		Term:     1,
	}
	e2 := pb.Entry{
		ClientID: 123,
		SeriesID: client.NoOPSeriesID,
		Index:    236,
		Term:     1,
	}
	e3 := pb.Entry{
		ClientID: 123,
		SeriesID: client.NoOPSeriesID,
		Index:    237,
		Term:     1,
	}
	commit := Task{
		Entries: []pb.Entry{e1, e2, e3},
	}
	sm.lastApplied.index = 234
	sm.index = 234
	sm.taskQ.Add(commit)
	batch := make([]Task, 0, 8)
	_, err := sm.Handle(batch, nil)
	require.NoError(t, err)
	require.Equal(t, uint64(237), sm.GetLastApplied())
	count := store.UpdateCount
	require.Equal(t, 3, count,
		"not batched as expected, batched update count %d, want 3", count)
	reportLeakedFD(fs, t)
}

func TestMetadataEntryCanBeHandled(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	store := &tests.ConcurrentUpdate{}
	config := config.Config{ShardID: 1, ReplicaID: 1}
	ds := NewNativeSM(config,
		NewConcurrentStateMachine(store), make(chan struct{}))
	nodeProxy := newTestNodeProxy()
	sm := NewStateMachine(ds, nil, config, nodeProxy, fs)
	e1 := pb.Entry{
		Type:  pb.MetadataEntry,
		Index: 235,
		Term:  1,
	}
	e2 := pb.Entry{
		Type:  pb.MetadataEntry,
		Index: 236,
		Term:  1,
	}
	e3 := pb.Entry{
		Type:  pb.MetadataEntry,
		Index: 237,
		Term:  1,
	}
	commit := Task{
		Entries: []pb.Entry{e1, e2, e3},
	}
	sm.lastApplied.index = 234
	sm.index = 234
	sm.taskQ.Add(commit)
	batch := make([]Task, 0, 8)
	_, err := sm.Handle(batch, nil)
	require.NoError(t, err)
	require.Equal(t, uint64(237), sm.GetLastApplied())
	require.Equal(t, 0, store.UpdateCount,
		"Update() not suppose to be called")
	reportLeakedFD(fs, t)
}

func testHandleBatchedSnappyEncodedEntry(t *testing.T,
	ct dio.CompressionType, fs vfs.IFS) {
	defer leaktest.AfterTest(t)()
	createTestDir(fs)
	defer removeTestDir(fs)
	store := tests.NewConcurrentKVTest(1, 1)
	config := config.Config{ShardID: 1, ReplicaID: 1}
	ds := NewNativeSM(config,
		NewConcurrentStateMachine(store), make(chan struct{}))
	nodeProxy := newTestNodeProxy()
	snapshotter := newTestSnapshotter(fs)
	tsm := NewStateMachine(ds, snapshotter, config, nodeProxy, fs)
	data1 := getTestKVData()
	data2 := getTestKVData2()
	encoded1 := GetEncoded(ct, data1, make([]byte, 512))
	encoded2 := GetEncoded(ct, data2, make([]byte, 512))
	e1 := pb.Entry{
		Type:     pb.EncodedEntry,
		ClientID: 123,
		SeriesID: 0,
		Cmd:      encoded1,
		Index:    236,
		Term:     1,
	}
	e2 := pb.Entry{
		Type:     pb.EncodedEntry,
		ClientID: 123,
		SeriesID: 0,
		Cmd:      encoded2,
		Index:    237,
		Term:     1,
	}
	tsm.lastApplied.index = 235
	tsm.index = 235
	entries := []pb.Entry{e1, e2}
	batch := make([]sm.Entry, 0, 8)
	require.NoError(t, tsm.handleBatch(entries, batch))
	v, err := store.Lookup([]byte("test-key"))
	require.NoError(t, err)
	require.Equal(t, "test-value", string(v.([]byte)))
	v, err = store.Lookup([]byte("test-key-2"))
	require.NoError(t, err)
	require.Equal(t, "test-value-2", string(v.([]byte)))
	reportLeakedFD(fs, t)
}

func TestHandleBatchedSnappyEncodedEntry(t *testing.T) {
	fs := vfs.GetTestFS()
	testHandleBatchedSnappyEncodedEntry(t, dio.Snappy, fs)
	testHandleBatchedSnappyEncodedEntry(t, dio.NoCompression, fs)
}

func TestHandleAllocationCount(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	store := &tests.NoOP{NoAlloc: true}
	config := config.Config{ShardID: 1, ReplicaID: 1}
	ds := NewNativeSM(config, NewInMemStateMachine(store), make(chan struct{}))
	nodeProxy := newTestNodeProxy()
	snapshotter := newTestSnapshotter(fs)
	sm := NewStateMachine(ds, snapshotter, config, nodeProxy, fs)
	sm.lastApplied.index = 1
	sm.index = 1
	idx := uint64(1)
	batch := make([]Task, 0, 8)
	entries := make([]pb.Entry, 1)
	ac := testing.AllocsPerRun(1000, func() {
		idx++
		e1 := pb.Entry{
			ClientID: 123,
			SeriesID: client.NoOPSeriesID,
			Index:    idx,
			Term:     1,
		}
		entries[0] = e1
		commit := Task{
			Entries: entries,
		}
		sm.taskQ.Add(commit)
		// not using testify here to avoid any allocation by such 3rd party lib
		if _, err := sm.Handle(batch, nil); err != nil {
			t.Fatalf("handle failed %v", err)
		}
		if sm.GetLastApplied() != idx {
			t.Errorf("last applied %d, want %d", sm.GetLastApplied(), idx)
		}
	})
	require.Equal(t, float64(0), ac)
}

func TestUpdatesNotBatchedWhenNotAllNoOPUpdates(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	createTestDir(fs)
	defer removeTestDir(fs)
	store := &tests.ConcurrentUpdate{}
	config := config.Config{ShardID: 1, ReplicaID: 1}
	ds := NewNativeSM(config,
		&ConcurrentStateMachine{sm: store}, make(chan struct{}))
	nodeProxy := newTestNodeProxy()
	snapshotter := newTestSnapshotter(fs)
	sm := NewStateMachine(ds, snapshotter, config, nodeProxy, fs)
	e1 := pb.Entry{
		ClientID: 123,
		SeriesID: client.NoOPSeriesID,
		Index:    235,
		Term:     1,
	}
	e2 := pb.Entry{
		ClientID: 123,
		SeriesID: client.SeriesIDForRegister,
		Index:    236,
		Term:     1,
	}
	e3 := pb.Entry{
		ClientID: 123,
		SeriesID: client.NoOPSeriesID,
		Index:    237,
		Term:     1,
	}
	commit := Task{
		Entries: []pb.Entry{e1, e2, e3},
	}
	sm.lastApplied.index = 234
	sm.index = 234
	sm.taskQ.Add(commit)
	batch := make([]Task, 0, 8)
	_, err := sm.Handle(batch, nil)
	require.NoError(t, err)
	require.Equal(t, uint64(237), sm.GetLastApplied())
	reportLeakedFD(fs, t)
}

func TestStateMachineCanBeCreated(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine) {
		require.False(t, sm.TaskChanBusy())
	}
	fs := vfs.GetTestFS()
	runSMTest(t, tf, fs)
}

func applyConfigChangeEntry(sm *StateMachine,
	configChangeID uint64, configType pb.ConfigChangeType, ReplicaID uint64,
	addressToAdd string, index uint64) {
	cc := pb.ConfigChange{
		ConfigChangeId: configChangeID,
		Type:           configType,
		ReplicaID:      ReplicaID,
	}
	if addressToAdd != "" {
		cc.Address = addressToAdd
	}
	data, err := cc.Marshal()
	if err != nil {
		panic(err)
	}
	e := pb.Entry{
		Cmd:   data,
		Type:  pb.ConfigChangeEntry,
		Index: index,
		Term:  1,
	}
	commit := Task{
		Entries: []pb.Entry{e},
	}
	sm.lastApplied.index = index - 1
	sm.index = index - 1
	sm.taskQ.Add(commit)
}

func TestLookupNotAllowedOnClosedShard(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine) {
		sm.aborted = true
		v, err := sm.Lookup(make([]byte, 10))
		require.Equal(t, ErrShardClosed, err)
		require.Nil(t, v)
	}
	fs := vfs.GetTestFS()
	runSMTest(t, tf, fs)
}

func TestGetMembership(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine) {
		sm.members.members = pb.Membership{
			Addresses: map[uint64]string{
				100: "a100",
				234: "a234",
			},
			NonVotings: map[uint64]string{
				200: "a200",
				300: "a300",
			},
			Removed: map[uint64]bool{
				400: true,
				500: true,
				600: true,
			},
			Witnesses: map[uint64]string{
				700: "a700",
			},
			ConfigChangeId: 12345,
		}
		m := sm.GetMembership()
		require.Equal(t, uint64(12345), m.ConfigChangeId)
		require.Len(t, m.Addresses, 2)
		require.Len(t, m.NonVotings, 2)
		require.Len(t, m.Removed, 3)
		require.Len(t, m.Witnesses, 1)
	}
	fs := vfs.GetTestFS()
	runSMTest(t, tf, fs)
}

func TestGetMembershipNodes(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine) {
		sm.members.members = pb.Membership{
			Addresses: map[uint64]string{
				100: "a100",
				234: "a234",
			},
			NonVotings: map[uint64]string{
				200: "a200",
				300: "a300",
			},
			Removed: map[uint64]bool{
				400: true,
				500: true,
				600: true,
			},
			ConfigChangeId: 12345,
		}
		m := sm.GetMembership()
		n := m.Addresses
		require.Len(t, n, 2)
		_, ok1 := n[100]
		_, ok2 := n[234]
		require.True(t, ok1)
		require.True(t, ok2)
	}
	fs := vfs.GetTestFS()
	runSMTest(t, tf, fs)
}

func TestGetMembershipHash(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine) {
		sm.members.members = pb.Membership{
			Addresses: map[uint64]string{
				100: "a100",
				234: "a234",
			},
			ConfigChangeId: 12345,
		}
		h1 := sm.GetMembershipHash()
		sm.members.members.Addresses[200] = "a200"
		h2 := sm.GetMembershipHash()
		require.NotEqual(t, h1, h2)
	}
	fs := vfs.GetTestFS()
	runSMTest(t, tf, fs)
}

func TestGetSSMetaPanicWhenThereIsNoMember(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine) {
		require.Panics(t, func() {
			_, err := sm.getSSMeta(nil, SSRequest{})
			require.NoError(t, err)
		})
	}
	fs := vfs.GetTestFS()
	runSMTest(t, tf, fs)
}

func TestGetSSMeta(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine) {
		sm.members.members = pb.Membership{
			Addresses: map[uint64]string{
				100: "a100",
				234: "a234",
			},
			ConfigChangeId: 12345,
		}
		applySessionRegisterEntry(sm, 12345, 789)
		sm.lastApplied.index = 100
		sm.index = 100
		sm.term = 101
		sm.lastApplied.term = 101
		meta, err := sm.getSSMeta(make([]byte, 123), SSRequest{})
		require.NoError(t, err)
		require.Equal(t, uint64(100), meta.Index)
		require.Equal(t, uint64(101), meta.Term)
		v := meta.Ctx.([]byte)
		require.Len(t, v, 123)
		require.Len(t, meta.Membership.Addresses, 2)
		v = meta.Session.Bytes()
		require.NotEmpty(t, v)
	}
	fs := vfs.GetTestFS()
	runSMTest(t, tf, fs)
}

func TestHandleConfChangeAddNode(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		applyConfigChangeEntry(sm, 1, pb.AddNode, 4, "localhost:1010", 123)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, uint64(123), sm.GetLastApplied())
		require.True(t, nodeProxy.accept)
		require.True(t, nodeProxy.addPeer)
		v, ok := sm.members.members.Addresses[4]
		require.True(t, ok)
		require.Equal(t, "localhost:1010", v)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestAddNodeAsNonVotingWillBeRejected(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		applyConfigChangeEntry(sm, 1, pb.AddNode, 4, "localhost:1010", 123)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, uint64(123), sm.GetLastApplied())
		applyConfigChangeEntry(sm,
			123,
			pb.AddNonVoting,
			4,
			"localhost:1010",
			124)
		_, err = sm.Handle(batch, nil)
		require.NoError(t, err)
		require.True(t, nodeProxy.reject)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestNonVotingCanBeAdded(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		applyConfigChangeEntry(sm,
			1,
			pb.AddNonVoting,
			4,
			"localhost:1010",
			123)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, uint64(123), sm.GetLastApplied())
		require.True(t, nodeProxy.accept)
		require.False(t, nodeProxy.addPeer)
		require.True(t, nodeProxy.addNonVoting)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestNonVotingPromotion(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		applyConfigChangeEntry(sm, 1, pb.AddNonVoting, 4, "localhost:1010", 123)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, uint64(123), sm.GetLastApplied())
		require.True(t, nodeProxy.accept)
		require.False(t, nodeProxy.addPeer)
		require.True(t, nodeProxy.addNonVoting)
		applyConfigChangeEntry(sm, 123, pb.AddNode, 4, "localhost:1010", 124)
		_, err = sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, uint64(124), sm.GetLastApplied())
		require.True(t, nodeProxy.addPeer)
		require.Len(t, sm.members.members.Addresses, 1)
		require.Empty(t, sm.members.members.NonVotings)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestInvalidNonVotingPromotionIsRejected(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		applyConfigChangeEntry(sm, 1, pb.AddNonVoting, 4, "localhost:1010", 123)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, uint64(123), sm.GetLastApplied())
		require.True(t, nodeProxy.accept)
		require.False(t, nodeProxy.addPeer)
		require.True(t, nodeProxy.addNonVoting)
		nodeProxy.accept = false
		applyConfigChangeEntry(sm, 123, pb.AddNode, 4, "localhost:1011", 124)
		_, err = sm.Handle(batch, nil)
		require.NoError(t, err)
		_, ok := sm.members.members.Addresses[4]
		require.False(t, ok)
		require.False(t, nodeProxy.accept)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func testAddExistingMemberIsRejected(t *testing.T,
	tt pb.ConfigChangeType, fs vfs.IFS) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		sm.members.members.Addresses[5] = "localhost:1010"
		applyConfigChangeEntry(sm, 1, tt, 4, "localhost:1010", 123)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, uint64(123), sm.GetLastApplied())
		require.False(t, nodeProxy.accept)
		require.False(t, nodeProxy.addPeer)
	}
	runSMTest2(t, tf, fs)
}

func TestAddExistingMemberIsRejected(t *testing.T) {
	fs := vfs.GetTestFS()
	testAddExistingMemberIsRejected(t, pb.AddNode, fs)
	testAddExistingMemberIsRejected(t, pb.AddNonVoting, fs)
}

func testAddExistingMemberWithSameReplicaIDIsRejected(t *testing.T,
	tt pb.ConfigChangeType, fs vfs.IFS) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		switch tt {
		case pb.AddNode:
			sm.members.members.Addresses[5] = "localhost:1010"
		case pb.AddNonVoting:
			sm.members.members.NonVotings[5] = "localhost:1010"
		default:
			require.Fail(t, "unknown tt")
		}
		applyConfigChangeEntry(sm, 1, tt, 5, "localhost:1011", 123)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, uint64(123), sm.GetLastApplied())
		require.False(t, nodeProxy.accept)
		require.False(t, nodeProxy.addPeer)
	}
	runSMTest2(t, tf, fs)
}

func TestAddExistingMemberWithSameReplicaIDIsRejected(t *testing.T) {
	fs := vfs.GetTestFS()
	testAddExistingMemberWithSameReplicaIDIsRejected(t, pb.AddNode, fs)
	testAddExistingMemberWithSameReplicaIDIsRejected(t, pb.AddNonVoting, fs)
}

func TestHandleConfChangeRemoveNode(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		sm.members.members.Addresses[1] = "localhost:1010"
		sm.members.members.Addresses[2] = "localhost:1011"
		applyConfigChangeEntry(sm, 1, pb.RemoveNode, 1, "", 123)
		_, ok := sm.members.members.Addresses[1]
		require.True(t, ok)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, uint64(123), sm.GetLastApplied())
		require.True(t, nodeProxy.accept)
		require.True(t, nodeProxy.removePeer)
		_, ok = sm.members.members.Addresses[1]
		require.False(t, ok)
		_, ok = sm.members.members.Removed[1]
		require.True(t, ok)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestOrderedConfChangeIsAccepted(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		sm.members.members.ConfigChangeId = 6
		applyConfigChangeEntry(sm, 6, pb.RemoveNode, 1, "", 123)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.False(t, nodeProxy.reject)
		require.True(t, nodeProxy.accept)
		require.True(t, nodeProxy.removePeer)
		require.Equal(t, uint64(123), sm.members.members.ConfigChangeId)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestRemoveOnlyNodeIsRejected(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		sm.members.members.Addresses[1] = "localhost:1010"
		applyConfigChangeEntry(sm, 1, pb.RemoveNode, 1, "", 123)
		_, ok := sm.members.members.Addresses[1]
		require.True(t, ok)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, uint64(123), sm.GetLastApplied())
		require.False(t, nodeProxy.accept)
		require.False(t, nodeProxy.removePeer)
		_, ok = sm.members.members.Addresses[1]
		require.True(t, ok)
		_, ok = sm.members.members.Removed[1]
		require.False(t, ok)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestAddingNodeOnTheSameNodeHostWillBeRejected(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		sm.members.members.ConfigChangeId = 6
		sm.members.members.Addresses[100] = "test.nodehost"
		applyConfigChangeEntry(sm, 7, pb.AddNode, 2, "test.nodehost", 123)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.True(t, nodeProxy.reject)
		require.False(t, nodeProxy.addPeer)
		require.NotEqual(t, uint64(123), sm.members.members.ConfigChangeId)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestAddingRemovedNodeWillBeRejected(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		sm.members.members.ConfigChangeId = 6
		sm.members.members.Removed[2] = true
		applyConfigChangeEntry(sm, 7, pb.AddNode, 2, "a1", 123)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.True(t, nodeProxy.reject)
		require.False(t, nodeProxy.addPeer)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestOutOfOrderConfChangeIsRejected(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		sm.members.ordered = true
		sm.members.members.ConfigChangeId = 6
		applyConfigChangeEntry(sm, 1, pb.RemoveNode, 1, "", 123)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.True(t, nodeProxy.reject)
		require.False(t, nodeProxy.removePeer)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestHandleSyncTask(t *testing.T) {
	tf := func(t *testing.T, tsm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		task1 := Task{
			PeriodicSync: true,
		}
		task2 := Task{
			PeriodicSync: true,
		}
		tsm.lastApplied.index = 100
		tsm.taskQ.Add(task1)
		tsm.taskQ.Add(task2)
		_, err := tsm.Handle(make([]Task, 0), make([]sm.Entry, 0))
		require.NoError(t, err)
		require.Equal(t, uint64(100), tsm.lastApplied.index)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestHandleEmptyEvent(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		e := pb.Entry{
			Type:  pb.ApplicationEntry,
			Index: 234,
			Term:  1,
		}
		commit := Task{
			Entries: []pb.Entry{e},
		}
		sm.lastApplied.index = 233
		sm.index = 233
		sm.taskQ.Add(commit)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, uint64(234), sm.GetLastApplied())
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func getTestKVData() []byte {
	return genTestKVData("test-key", "test-value")
}

func getTestKVData2() []byte {
	return genTestKVData("test-key-2", "test-value-2")
}

func genTestKVData(k, d string) []byte {
	u := kvpb.PBKV{
		Key: k,
		Val: d,
	}
	data, err := u.Marshal()
	if err != nil {
		panic(err)
	}
	return data
}

func testHandleSnappyEncodedEntry(t *testing.T,
	ct dio.CompressionType, fs vfs.IFS) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		data := getTestKVData()
		encoded := GetEncoded(ct, data, make([]byte, 512))
		e1 := pb.Entry{
			Type:     pb.EncodedEntry,
			ClientID: 123,
			SeriesID: 0,
			Cmd:      encoded,
			Index:    236,
			Term:     1,
		}
		commit := Task{
			Entries: []pb.Entry{e1},
		}
		sm.lastApplied.index = 235
		sm.index = 235
		sm.taskQ.Add(commit)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, uint64(236), sm.GetLastApplied())
		v, ok := store.(*tests.KVTest).KVStore["test-key"]
		require.True(t, ok)
		require.Equal(t, "test-value", v)
	}
	runSMTest2(t, tf, fs)
}

func TestHandleSnappyEncodedEntry(t *testing.T) {
	fs := vfs.GetTestFS()
	testHandleSnappyEncodedEntry(t, dio.Snappy, fs)
	testHandleSnappyEncodedEntry(t, dio.NoCompression, fs)
}

func TestHandleUpate(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		data := getTestKVData()
		e1 := pb.Entry{
			ClientID: 123,
			SeriesID: client.SeriesIDForRegister,
			Index:    235,
			Term:     1,
		}
		e2 := pb.Entry{
			ClientID: 123,
			SeriesID: 2,
			Cmd:      data,
			Index:    236,
			Term:     1,
		}
		commit := Task{
			Entries: []pb.Entry{e1, e2},
		}
		sm.lastApplied.index = 234
		sm.index = 234
		sm.taskQ.Add(commit)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, uint64(236), sm.GetLastApplied())
		v, ok := store.(*tests.KVTest).KVStore["test-key"]
		require.True(t, ok)
		require.Equal(t, "test-value", v)
		result, err := sm.Lookup([]byte("test-key"))
		require.NoError(t, err)
		require.Equal(t, "test-value", string(result.([]byte)))
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestSnapshotCanBeApplied(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		sm.members.members.Addresses[1] = "localhost:1234"
		store.(*tests.KVTest).KVStore["test-key1"] = "test-value1"
		store.(*tests.KVTest).KVStore["test-key2"] = "test-value2"
		sm.lastApplied.index = 3
		sm.index = 3
		hash1, err := sm.GetHash()
		require.NoError(t, err)
		ss, _, err := sm.Save(SSRequest{})
		require.NoError(t, err)
		index := ss.Index
		commit := Task{
			Index: index,
		}
		store2 := tests.NewKVTest(1, 1)
		config := config.Config{ShardID: 1, ReplicaID: 1}
		store2.(*tests.KVTest).DisableLargeDelay()
		ds2 := NewNativeSM(config, NewInMemStateMachine(store2), make(chan struct{}))
		nodeProxy2 := newTestNodeProxy()
		snapshotter2 := newTestSnapshotter(fs)
		snapshotter2.index = commit.Index
		sm2 := NewStateMachine(ds2, snapshotter2, config, nodeProxy2, fs)
		require.Empty(t, sm2.members.members.Addresses)
		ss2, err := sm2.Recover(commit)
		require.NoError(t, err)
		require.Equal(t, index, ss2.Index)
		hash2, err := sm2.GetHash()
		require.NoError(t, err)
		require.Equal(t, hash1, hash2)
		require.Len(t, sm2.members.members.Addresses, 2)
		v1, ok1 := sm2.members.members.Addresses[1]
		v2, ok2 := sm2.members.members.Addresses[2]
		require.True(t, ok1)
		require.True(t, ok2)
		require.Equal(t, "localhost:1", v1)
		require.Equal(t, "localhost:2", v2)
		require.Equal(t, uint64(2), nodeProxy2.addPeerCount)
	}
	runSMTest2(t, tf, fs)
}

func TestMembersAreSavedWhenMakingSnapshot(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		sm.members.members.Addresses[1] = "localhost:1"
		sm.members.members.Addresses[2] = "localhost:2"
		ss, _, err := sm.Save(SSRequest{})
		require.NoError(t, err)
		cs := ss.Membership
		require.Len(t, cs.Addresses, 2)
		v1, ok1 := cs.Addresses[1]
		v2, ok2 := cs.Addresses[2]
		require.True(t, ok1)
		require.True(t, ok2)
		require.Equal(t, "localhost:1", v1)
		require.Equal(t, "localhost:2", v2)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestSnapshotTwiceIsHandled(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		sm.members.members.Addresses[1] = "localhost:1"
		sm.members.members.Addresses[2] = "localhost:2"
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		data := getTestKVData()
		e := applyTestEntry(sm, 12345, client.NoOPSeriesID, 1, 0, data)
		_, err = sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, e.Index, sm.GetLastApplied())
		_, _, err = sm.Save(SSRequest{})
		require.NoError(t, err)
		_, _, err = sm.Save(SSRequest{})
		require.Equal(t, raft.ErrSnapshotOutOfDate, err)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func applySessionRegisterEntry(sm *StateMachine,
	clientID uint64, index uint64) pb.Entry {
	e := pb.Entry{
		ClientID: clientID,
		SeriesID: client.SeriesIDForRegister,
		Index:    index,
		Term:     1,
	}
	commit := Task{
		Entries: []pb.Entry{e},
	}
	sm.lastApplied.index = index - 1
	sm.index = index - 1
	sm.taskQ.Add(commit)
	return e
}

func applySessionUnregisterEntry(sm *StateMachine,
	clientID uint64, index uint64) pb.Entry {
	e := pb.Entry{
		ClientID: clientID,
		SeriesID: client.SeriesIDForUnregister,
		Index:    index,
		Term:     1,
	}
	commit := Task{
		Entries: []pb.Entry{e},
	}
	sm.taskQ.Add(commit)
	return e
}

func applyTestEntry(sm *StateMachine,
	clientID uint64, seriesID uint64, index uint64,
	respondedTo uint64, data []byte) pb.Entry {
	e := pb.Entry{
		ClientID:    clientID,
		SeriesID:    seriesID,
		Index:       index,
		Term:        1,
		Cmd:         data,
		RespondedTo: respondedTo,
	}
	commit := Task{
		Entries: []pb.Entry{e},
	}
	sm.taskQ.Add(commit)
	return e
}

func TestSessionCanBeCreatedAndRemoved(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		clientID := uint64(12345)
		applySessionRegisterEntry(sm, clientID, 789)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, uint64(789), sm.GetLastApplied())
		sessionManager := sm.sessions.lru
		_, ok := sessionManager.getSession(RaftClientID(clientID))
		require.True(t, ok)
		require.Equal(t, clientID, nodeProxy.smResult.Value)
		index := uint64(790)
		applySessionUnregisterEntry(sm, 12345, index)
		_, err = sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, index, sm.GetLastApplied())
		_, ok = sessionManager.getSession(RaftClientID(clientID))
		require.False(t, ok)
		require.Equal(t, clientID, nodeProxy.smResult.Value)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestDuplicatedSessionWillBeReported(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		e := applySessionRegisterEntry(sm, 12345, 789)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, e.Index, sm.GetLastApplied())
		sessionManager := sm.sessions.lru
		_, ok := sessionManager.getSession(RaftClientID(e.ClientID))
		require.True(t, ok)
		require.Equal(t, e.ClientID, nodeProxy.smResult.Value)
		e.Index = 790
		commit := Task{
			Entries: []pb.Entry{e},
		}
		sm.taskQ.Add(commit)
		require.False(t, nodeProxy.rejected)
		_, err = sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, e.Index, sm.GetLastApplied())
		require.Equal(t, uint64(0), nodeProxy.smResult.Value)
		require.True(t, nodeProxy.rejected)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestRemovingUnregisteredSessionWillBeReported(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		sm.lastApplied.index = 789
		sm.index = 789
		e := applySessionUnregisterEntry(sm, 12345, 790)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, e.Index, sm.GetLastApplied())
		sessionManager := sm.sessions.lru
		_, ok := sessionManager.getSession(RaftClientID(e.ClientID))
		require.False(t, ok)
		require.Equal(t, uint64(0), nodeProxy.smResult.Value)
		require.True(t, nodeProxy.rejected)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestUpdateFromUnregisteredClientWillBeReported(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		data := getTestKVData()
		e := applyTestEntry(sm, 12345, 1, 790, 0, data)
		sm.lastApplied.index = 789
		sm.index = 789
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, e.Index, sm.GetLastApplied())
		require.False(t, nodeProxy.ignored)
		require.Equal(t, uint64(0), nodeProxy.smResult.Value)
		require.True(t, nodeProxy.rejected)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestDuplicatedUpdateWillNotBeAppliedTwice(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		applySessionRegisterEntry(sm, 12345, 789)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		data := getTestKVData()
		e := applyTestEntry(sm, 12345, 1, 790, 0, data)
		_, err = sm.Handle(batch, nil)
		require.NoError(t, err)
		plog.Infof("Handle returned")
		require.Equal(t, e.Index, sm.GetLastApplied())
		require.True(t, nodeProxy.applyUpdateInvoked)
		require.Equal(t, uint64(len(data)), nodeProxy.smResult.Value)
		nodeProxy.applyUpdateInvoked = false
		storeCount := store.(*tests.KVTest).Count
		v := store.(*tests.KVTest).KVStore["test-key"]
		require.Equal(t, "test-value", v)
		e = applyTestEntry(sm, 12345, 1, 791, 0, data)
		plog.Infof("going to handle the second commit rec")
		_, err = sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, e.Index, sm.GetLastApplied())
		require.True(t, nodeProxy.applyUpdateInvoked)
		require.Equal(t, storeCount, store.(*tests.KVTest).Count)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestRespondedUpdateWillNotBeAppliedTwice(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		applySessionRegisterEntry(sm, 12345, 789)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		data := getTestKVData()
		e := applyTestEntry(sm, 12345, 1, 790, 0, data)
		_, err = sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, e.Index, sm.GetLastApplied())
		storeCount := store.(*tests.KVTest).Count
		e = applyTestEntry(sm, 12345, 1, 791, 1, data)
		_, err = sm.Handle(batch, nil)
		require.NoError(t, err)
		sessionManager := sm.sessions.lru
		session, _ := sessionManager.getSession(RaftClientID(12345))
		require.Equal(t, RaftSeriesID(1), session.RespondedUpTo)
		nodeProxy.applyUpdateInvoked = false
		e = applyTestEntry(sm, 12345, 1, 792, 1, data)
		_, err = sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, e.Index, sm.GetLastApplied())
		require.False(t, nodeProxy.applyUpdateInvoked)
		require.Equal(t, storeCount, store.(*tests.KVTest).Count)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestNoOPSessionAllowEntryToBeAppliedTwice(t *testing.T) {
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		applySessionRegisterEntry(sm, 12345, 789)
		batch := make([]Task, 0, 8)
		_, err := sm.Handle(batch, nil)
		require.NoError(t, err)
		data := getTestKVData()
		e := applyTestEntry(sm, 12345, client.NoOPSeriesID, 790, 0, data)
		_, err = sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, e.Index, sm.GetLastApplied())
		storeCount := store.(*tests.KVTest).Count
		e = applyTestEntry(sm, 12345, client.NoOPSeriesID, 791, 0, data)
		_, err = sm.Handle(batch, nil)
		require.NoError(t, err)
		require.Equal(t, e.Index, sm.GetLastApplied())
		require.NotEqual(t, storeCount, store.(*tests.KVTest).Count)
	}
	fs := vfs.GetTestFS()
	runSMTest2(t, tf, fs)
}

func TestNoOPEntryIsNotBatched(t *testing.T) {
	updates, _ := getEntryTypes([]pb.Entry{{}})
	require.False(t, updates)
}

func TestRegularSessionedEntryIsNotBatched(t *testing.T) {
	e := pb.Entry{
		ClientID: client.NotSessionManagedClientID + 1,
		SeriesID: client.NoOPSeriesID + 1,
	}
	_, allNoOP := getEntryTypes([]pb.Entry{{}, e})
	require.False(t, allNoOP)
}

func TestNonUpdateEntryIsNotBatched(t *testing.T) {
	cce := pb.Entry{Type: pb.ConfigChangeEntry}
	notSessionManaged := pb.Entry{ClientID: client.NotSessionManagedClientID}
	newSessionEntry := pb.Entry{SeriesID: client.SeriesIDForRegister}
	unSessionEntry := pb.Entry{SeriesID: client.SeriesIDForUnregister}
	entries := []pb.Entry{cce, notSessionManaged, newSessionEntry, unSessionEntry}
	for _, e := range entries {
		require.False(t, e.IsUpdateEntry())
	}
	for _, e := range entries {
		updates, _ := getEntryTypes([]pb.Entry{e})
		require.False(t, updates)
	}
}

func TestEntryAppliedInDiskSM(t *testing.T) {
	tests := []struct {
		onDiskSM        bool
		onDiskInitIndex uint64
		index           uint64
		result          bool
	}{
		{true, 100, 50, true},
		{true, 100, 100, true},
		{true, 100, 200, false},
		{false, 100, 50, false},
		{false, 100, 100, false},
		{false, 100, 200, false},
	}
	for _, tt := range tests {
		sm := StateMachine{onDiskSM: tt.onDiskSM,
			onDiskInitIndex: tt.onDiskInitIndex}
		result := sm.entryInInitDiskSM(tt.index)
		require.Equal(t, tt.result, result)
	}
}

func TestRecoverSMRequired2(t *testing.T) {
	tests := []struct {
		init          bool
		ssOnDiskIndex uint64
		onDiskIndex   uint64
		shouldPanic   bool
	}{
		{true, 100, 100, false},
		{true, 200, 100, true},
		{true, 300, 100, true},

		{true, 100, 200, false},
		{true, 200, 200, false},
		{true, 300, 200, true},

		{true, 100, 300, false},
		{true, 200, 300, false},
		{true, 300, 300, false},

		{false, 100, 100, false},
		{false, 200, 100, true},
		{false, 300, 100, true},

		{false, 100, 200, false},
		{false, 200, 200, false},
		{false, 300, 200, true},

		{false, 100, 300, false},
		{false, 200, 300, false},
		{false, 300, 300, false},
	}
	ssIndex := uint64(200)
	node := newTestNodeProxy()
	for _, tt := range tests {
		sm := &StateMachine{
			onDiskSM: true,
			node:     node,
		}
		if tt.init {
			sm.onDiskInitIndex = tt.onDiskIndex
		} else {
			sm.onDiskIndex = tt.onDiskIndex
		}
		ss := pb.Snapshot{
			Index:       ssIndex,
			OnDiskIndex: tt.ssOnDiskIndex,
		}
		f := func() {
			sm.checkPartialSnapshotApplyOnDiskSM(ss, tt.init)
		}
		if tt.shouldPanic {
			require.Panics(t, f)
		} else {
			require.NotPanics(t, f)
		}
	}
}

func TestRecoverSMRequired(t *testing.T) {
	tests := []struct {
		witness         bool
		dummy           bool
		init            bool
		onDiskIndex     uint64
		onDiskInitIndex uint64
		required        bool
	}{
		{false, false, true, 100, 100, false},
		{false, false, true, 200, 100, true},
		{false, false, true, 100, 200, false},
		{false, false, false, 100, 100, false},
		{false, false, false, 200, 100, true},
		{false, false, false, 100, 200, false},

		{false, true, true, 100, 100, false},
		{false, true, true, 200, 100, false},
		{false, true, true, 100, 200, false},
		{false, true, false, 100, 100, false},
		{false, true, false, 200, 100, false},
		{false, true, false, 100, 200, false},

		{true, false, true, 100, 100, false},
		{true, false, true, 200, 100, false},
		{true, false, true, 100, 200, false},
		{true, false, false, 100, 100, false},
		{true, false, false, 200, 100, false},
		{true, false, false, 100, 200, false},

		{true, true, true, 100, 100, false},
		{true, true, true, 200, 100, false},
		{true, true, true, 100, 200, false},
		{true, true, false, 100, 100, false},
		{true, true, false, 200, 100, false},
		{true, true, false, 100, 200, false},
	}
	ssIndex := uint64(200)
	for _, tt := range tests {
		sm := &StateMachine{
			onDiskSM:        true,
			onDiskInitIndex: tt.onDiskInitIndex,
			onDiskIndex:     tt.onDiskInitIndex,
		}
		ss := pb.Snapshot{
			Index:       ssIndex,
			OnDiskIndex: tt.onDiskIndex,
			Witness:     tt.witness,
			Dummy:       tt.dummy,
		}
		f := func() {
			require.Equal(t, tt.required, sm.recoverRequired(ss, tt.init))
		}
		if tt.dummy || tt.witness {
			require.Panics(t, f)
		} else {
			require.NotPanics(t, f)
		}
	}
}

func TestIsShrunkSnapshot(t *testing.T) {
	tests := []struct {
		shrunk   bool
		init     bool
		isShrunk bool
	}{
		{false, false, false},
		{false, true, false},

		{true, false, false},
		{true, true, true},
	}
	ssIndex := uint64(200)
	for _, tt := range tests {
		fs := vfs.GetTestFS()
		defer func() {
			require.NoError(t, fs.RemoveAll(testSnapshotterDir))
		}()
		func() {
			require.NoError(t, fs.RemoveAll(testSnapshotterDir))
			require.NoError(t, fs.MkdirAll(testSnapshotterDir, 0755))
			snapshotter := newTestSnapshotter(fs)
			sm := &StateMachine{
				snapshotter: snapshotter,
				onDiskSM:    true,
				fs:          fs,
			}
			fp := snapshotter.getFilePath(ssIndex)
			if tt.shrunk {
				fp = fp + ".tmp"
			}
			w, err := NewSnapshotWriter(fp, pb.NoCompression, fs)
			require.NoError(t, err)
			sessionData := make([]byte, testSessionSize)
			storeData := make([]byte, testPayloadSize)
			_, err = rand.Read(sessionData)
			require.NoError(t, err)
			_, err = rand.Read(storeData)
			require.NoError(t, err)
			n, err := w.Write(sessionData)
			require.NoError(t, err)
			require.Equal(t, len(sessionData), n)
			m, err := w.Write(storeData)
			require.NoError(t, err)
			require.Equal(t, len(storeData), m)
			require.NoError(t, w.Close())
			if tt.shrunk {
				err := ShrinkSnapshot(fp, snapshotter.getFilePath(ssIndex), fs)
				require.NoError(t, err)
			}
			ss := pb.Snapshot{
				Index: ssIndex,
			}
			f := func() {
				res, err := sm.isShrunkSnapshot(ss, tt.init)
				require.NoError(t, err)
				require.Equal(t, tt.isShrunk, res)
			}
			if !tt.init && tt.shrunk {
				require.Panics(t, f)
			} else {
				require.NotPanics(t, f)
			}
		}()
		reportLeakedFD(fs, t)
	}
}

func TestReadyToStreamSnapshot(t *testing.T) {
	tests := []struct {
		onDisk          bool
		index           uint64
		onDiskInitIndex uint64
		ready           bool
	}{
		{true, 100, 100, true},
		{true, 200, 100, true},
		{true, 100, 200, false},
		{false, 100, 100, true},
		{false, 200, 100, true},
		{false, 100, 200, true},
	}
	for _, tt := range tests {
		sm := &StateMachine{
			onDiskSM:        tt.onDisk,
			onDiskInitIndex: tt.onDiskInitIndex,
		}
		sm.lastApplied.index = tt.index
		require.Equal(t, tt.ready, sm.ReadyToStream())
	}
}

func TestAlreadyAppliedInOnDiskSMEntryTreatedAsNoOP(t *testing.T) {
	sm := &StateMachine{
		onDiskSM:        true,
		onDiskInitIndex: 100,
		index:           90,
	}
	sm.lastApplied.index = 90
	sm.lastApplied.term = 5
	ent := pb.Entry{
		ClientID: 100,
		Index:    91,
		Term:     6,
	}
	require.NoError(t, sm.handleEntry(ent, false))
}

type testManagedStateMachine struct {
	first          uint64
	last           uint64
	synced         bool
	nalookup       bool
	corruptIndex   bool
	concurrent     bool
	onDisk         bool
	smType         pb.StateMachineType
	prepareInvoked bool
}

func (t *testManagedStateMachine) Open() (uint64, error) { return 10, nil }
func (t *testManagedStateMachine) Update(sm.Entry) (sm.Result, error) {
	return sm.Result{}, nil
}
func (t *testManagedStateMachine) Lookup(interface{}) (interface{}, error) {
	return nil, nil
}
func (t *testManagedStateMachine) NALookup(input []byte) ([]byte, error) {
	t.nalookup = true
	return input, nil
}
func (t *testManagedStateMachine) ConcurrentLookup(interface{}) (interface{},
	error) {
	return nil, nil
}
func (t *testManagedStateMachine) NAConcurrentLookup(input []byte) ([]byte,
	error) {
	t.nalookup = true
	return input, nil
}
func (t *testManagedStateMachine) Sync() error {
	t.synced = true
	return nil
}
func (t *testManagedStateMachine) GetHash() (uint64, error) { return 0, nil }
func (t *testManagedStateMachine) Prepare() (interface{}, error) {
	t.prepareInvoked = true
	return nil, nil
}
func (t *testManagedStateMachine) Save(SSMeta,
	io.Writer, []byte, sm.ISnapshotFileCollection) (bool, error) {
	return false, nil
}
func (t *testManagedStateMachine) Recover(io.Reader,
	[]sm.SnapshotFile) error {
	return nil
}
func (t *testManagedStateMachine) Stream(interface{}, io.Writer) error {
	return nil
}
func (t *testManagedStateMachine) Offloaded() bool             { return false }
func (t *testManagedStateMachine) Loaded()                     {}
func (t *testManagedStateMachine) Close() error                { return nil }
func (t *testManagedStateMachine) DestroyedC() <-chan struct{} { return nil }
func (t *testManagedStateMachine) Concurrent() bool {
	return t.concurrent
}
func (t *testManagedStateMachine) OnDisk() bool { return t.onDisk }
func (t *testManagedStateMachine) Type() pb.StateMachineType {
	return t.smType
}
func (t *testManagedStateMachine) BatchedUpdate(ents []sm.Entry) ([]sm.Entry,
	error) {
	if !t.corruptIndex {
		t.first = ents[0].Index
		t.last = ents[len(ents)-1].Index
	} else {
		for idx := range ents {
			ents[idx].Index = ents[idx].Index + 1
		}
	}
	return ents, nil
}

func TestOnDiskStateMachineCanBeOpened(t *testing.T) {
	msm := &testManagedStateMachine{}
	np := newTestNodeProxy()
	sm := &StateMachine{
		onDiskSM: true,
		sm:       msm,
		node:     np,
	}
	index, err := sm.OpenOnDiskStateMachine()
	require.NoError(t, err)
	require.Equal(t, uint64(10), index)
	require.Equal(t, uint64(10), sm.onDiskInitIndex)
}

func TestSaveConcurrentSnapshot(t *testing.T) {
	fs := vfs.GetTestFS()
	msm := &testManagedStateMachine{}
	np := newTestNodeProxy()
	createTestDir(fs)
	defer removeTestDir(fs)
	sm := &StateMachine{
		onDiskSM:    true,
		sm:          msm,
		node:        np,
		index:       100,
		snapshotter: newTestSnapshotter(fs),
		sessions:    NewSessionManager(),
		members:     newMembership(1, 1, false),
		fs:          fs,
	}
	sm.lastApplied.index = 100
	sm.lastApplied.term = 5
	sm.members.members.Addresses[1] = "a1"
	ss, _, err := sm.concurrentSave(SSRequest{})
	require.NoError(t, err)
	require.Equal(t, uint64(100), ss.Index)
	require.True(t, msm.synced)
	reportLeakedFD(fs, t)
}

func TestStreamSnapshot(t *testing.T) {
	msm := &testManagedStateMachine{}
	np := newTestNodeProxy()
	fs := vfs.GetTestFS()
	sm := &StateMachine{
		onDiskSM:    true,
		sm:          msm,
		node:        np,
		snapshotter: newTestSnapshotter(fs),
		sessions:    NewSessionManager(),
		members:     newMembership(1, 1, false),
		fs:          fs,
	}
	sm.lastApplied.index = 100
	sm.lastApplied.term = 5
	sm.members.members.Addresses[1] = "a1"
	ts := &testSink{
		chunks: make([]pb.Chunk, 0),
	}
	require.NoError(t, sm.Stream(ts))
	require.Len(t, ts.chunks, 3)
	require.True(t, ts.chunks[1].IsLastChunk())
	require.True(t, ts.chunks[2].IsPoisonChunk())
	reportLeakedFD(fs, t)
}

func TestHandleBatchedEntriesForOnDiskSM(t *testing.T) {
	tests := []struct {
		onDiskInitIndex uint64
		index           uint64
		first           uint64
		last            uint64
		firstApplied    uint64
		lastApplied     uint64
	}{
		{100, 50, 51, 60, 0, 0},
		{100, 50, 51, 100, 0, 0},
		{100, 50, 51, 110, 101, 110},
		{100, 100, 101, 120, 101, 120},
		{100, 110, 111, 125, 111, 125},
	}
	for idx, tt := range tests {
		input := make([]pb.Entry, 0)
		for i := tt.first; i <= tt.last; i++ {
			input = append(input, pb.Entry{Index: i, Term: 100})
		}
		ents := make([]sm.Entry, 0)
		msm := &testManagedStateMachine{}
		np := newTestNodeProxy()
		sm := &StateMachine{
			onDiskSM:        true,
			onDiskInitIndex: tt.onDiskInitIndex,
			index:           tt.index,
			sm:              msm,
			node:            np,
		}
		sm.lastApplied.index = tt.index
		sm.lastApplied.term = 100
		require.NoError(t, sm.handleBatch(input, ents))
		require.Equal(t, tt.firstApplied, msm.first, "idx %d", idx)
		require.Equal(t, tt.lastApplied, msm.last, "idx %d", idx)
		require.Equal(t, tt.firstApplied, np.firstIndex)
		require.Equal(t, tt.lastApplied, np.index, "idx %d", idx)
		require.Equal(t, tt.index, sm.GetLastApplied(), "idx %d", idx)
	}
}

func TestCorruptedIndexValueWillBeDetected(t *testing.T) {
	ents := make([]sm.Entry, 0)
	msm := &testManagedStateMachine{corruptIndex: true}
	np := newTestNodeProxy()
	sm := &StateMachine{
		onDiskSM:        true,
		onDiskInitIndex: 0,
		sm:              msm,
		node:            np,
	}
	sm.lastApplied.index = 0
	sm.lastApplied.term = 100
	input := make([]pb.Entry, 0)
	for i := uint64(1); i <= 10; i++ {
		input = append(input, pb.Entry{Index: i, Term: 100})
	}
	require.Panics(t, func() {
		_ = sm.handleBatch(input, ents)
	})
}

func TestNodeReadyIsSetWhenAnythingFromTaskQIsProcessed(t *testing.T) {
	ents := make([]sm.Entry, 0)
	batch := make([]Task, 0)
	msm := &testManagedStateMachine{}
	np := newTestNodeProxy()
	sm := &StateMachine{
		onDiskSM:        true,
		onDiskInitIndex: 0,
		sm:              msm,
		taskQ:           NewTaskQueue(),
		node:            np,
	}
	rec, err := sm.Handle(batch, ents)
	require.False(t, rec.IsSnapshotTask())
	require.NoError(t, err)
	require.Equal(t, uint64(0), np.nodeReady)
	sm.taskQ.Add(Task{})
	rec, err = sm.Handle(batch, ents)
	require.False(t, rec.IsSnapshotTask())
	require.NoError(t, err)
	require.Equal(t, uint64(1), np.nodeReady)
}

func TestSyncedIndex(t *testing.T) {
	sm := &StateMachine{}
	sm.setSyncedIndex(100)
	require.Equal(t, uint64(100), sm.GetSyncedIndex())
	sm.setSyncedIndex(100)
	require.Equal(t, uint64(100), sm.GetSyncedIndex())
	require.Panics(t, func() {
		sm.setSyncedIndex(99)
	})
}

func TestNALookup(t *testing.T) {
	msm := &testManagedStateMachine{}
	sm := &StateMachine{
		sm: msm,
	}
	input := make([]byte, 128)
	_, err := rand.Read(input)
	require.NoError(t, err)
	result, err := sm.NALookup(input)
	require.NoError(t, err)
	require.True(t, bytes.Equal(input, result))
	require.True(t, msm.nalookup)
}

func TestIsDummySnapshot(t *testing.T) {
	tests := []struct {
		onDisk    bool
		exported  bool
		streaming bool
		dummy     bool
	}{
		{false, false, false, false},
		{false, true, false, false},
		{false, false, true, false},
		{true, true, false, false},
		{true, false, true, false},
		{true, false, false, true},
	}
	for idx, tt := range tests {
		s := &StateMachine{onDiskSM: tt.onDisk}
		r := SSRequest{Type: Periodic}
		if tt.exported {
			r.Type = Exported
		}
		if tt.streaming {
			r.Type = Streaming
		}
		require.Equal(t, tt.dummy, s.isDummySnapshot(r), "idx %d", idx)
	}
}

func TestWitnessNodePanicWhenSavingSnapshot(t *testing.T) {
	sm := &StateMachine{isWitness: true}
	require.Panics(t, func() {
		_, _, err := sm.Save(SSRequest{})
		require.NoError(t, err)
	})
}

func TestSetLastApplied(t *testing.T) {
	tests := []struct {
		index   uint64
		term    uint64
		entries []pb.Entry
		crash   bool
	}{
		{100, 5, []pb.Entry{{Index: 0, Term: 1}}, true},
		{100, 5, []pb.Entry{{Index: 1, Term: 0}}, true},
		{100, 5, []pb.Entry{{Index: 102, Term: 5}}, true},
		{100, 5, []pb.Entry{{Index: 101, Term: 4}}, true},
		{100, 5, []pb.Entry{{Index: 101, Term: 5}, {Index: 103, Term: 5}}, true},
		{100, 5, []pb.Entry{{Index: 101, Term: 6}, {Index: 102, Term: 5}}, true},
		{100, 5, []pb.Entry{{Index: 101, Term: 5}, {Index: 102, Term: 6}}, false},
	}
	for idx, tt := range tests {
		sm := StateMachine{}
		sm.lastApplied.index = tt.index
		sm.lastApplied.term = tt.term
		f := func() {
			sm.setLastApplied(tt.entries)
			require.Equal(t, tt.entries[len(tt.entries)-1].Index, sm.lastApplied.index)
			require.Equal(t, tt.entries[len(tt.entries)-1].Term, sm.lastApplied.term)
		}
		if tt.crash {
			require.Panics(t, f, "idx %d", idx)
		} else {
			require.NotPanics(t, f, "idx %d", idx)
		}
	}
}

func TestSavingDummySnapshot(t *testing.T) {
	tests := []struct {
		smType    pb.StateMachineType
		streaming bool
		export    bool
		result    bool
	}{
		{pb.RegularStateMachine, true, false, false},
		{pb.RegularStateMachine, false, true, false},
		{pb.RegularStateMachine, false, false, false},
		{pb.ConcurrentStateMachine, true, false, false},
		{pb.ConcurrentStateMachine, false, true, false},
		{pb.ConcurrentStateMachine, false, false, false},
		{pb.OnDiskStateMachine, true, false, false},
		{pb.OnDiskStateMachine, false, true, false},
		{pb.OnDiskStateMachine, false, false, true},
	}
	for idx, tt := range tests {
		sm := StateMachine{
			onDiskSM: tt.smType == pb.OnDiskStateMachine,
		}
		var rt SSReqType
		require.False(t, tt.export && tt.streaming)
		if tt.export {
			rt = Exported
		} else if tt.streaming {
			rt = Streaming
		}
		r := sm.savingDummySnapshot(SSRequest{Type: rt})
		require.Equal(t, tt.result, r, "idx %d", idx)
	}
}

func TestPrepareIsNotCalledWhenSavingDummySnapshot(t *testing.T) {
	tests := []struct {
		onDiskSM       bool
		streaming      bool
		export         bool
		prepareInvoked bool
	}{
		{true, false, false, false},
		{true, true, false, true},
		{true, false, true, true},
		{false, false, false, true},
		{false, false, true, true},
	}

	for idx, tt := range tests {
		msm := &testManagedStateMachine{
			concurrent: true,
			onDisk:     tt.onDiskSM,
			smType:     pb.ConcurrentStateMachine,
		}
		if tt.onDiskSM {
			msm.smType = pb.OnDiskStateMachine
		}
		m := membership{
			members: pb.Membership{
				Addresses: map[uint64]string{1: "localhost:1234"},
			},
		}
		sm := StateMachine{
			index:    100,
			onDiskSM: tt.onDiskSM,
			sm:       msm,
			members:  m,
			node:     &testNodeProxy{},
			sessions: NewSessionManager(),
		}
		var rt SSReqType
		require.False(t, tt.export && tt.streaming)
		if tt.export {
			rt = Exported
		} else if tt.streaming {
			rt = Streaming
		}
		meta, err := sm.prepare(SSRequest{Type: rt})
		require.NoError(t, err)
		require.Equal(t, uint64(100), meta.Index)
		require.Equal(t, tt.prepareInvoked, msm.prepareInvoked, "idx %d", idx)
	}
}

var errReturnedError = errors.New("test error")

func expectedError(err error) bool {
	return errors.Is(err, errReturnedError) && tests.HasStack(err)
}

type errorUpdateSM struct{}

func (e *errorUpdateSM) Update(i sm.Entry) (sm.Result, error) {
	return sm.Result{}, errReturnedError
}
func (e *errorUpdateSM) Lookup(q interface{}) (interface{}, error) {
	return nil, nil
}
func (e *errorUpdateSM) SaveSnapshot(io.Writer,
	sm.ISnapshotFileCollection, <-chan struct{}) error {
	return errReturnedError
}
func (e *errorUpdateSM) RecoverFromSnapshot(io.Reader,
	[]sm.SnapshotFile, <-chan struct{}) error {
	return errReturnedError
}
func (e *errorUpdateSM) Close() error { return nil }

func TestUpdateErrorIsReturned(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	createTestDir(fs)
	defer removeTestDir(fs)
	defer reportLeakedFD(fs, t)
	store := &errorUpdateSM{}
	config := config.Config{ShardID: 1, ReplicaID: 1}
	ds := NewNativeSM(config, NewInMemStateMachine(store), make(chan struct{}))
	nodeProxy := newTestNodeProxy()
	snapshotter := newTestSnapshotter(fs)
	sm := NewStateMachine(ds, snapshotter, config, nodeProxy, fs)
	e1 := pb.Entry{
		ClientID: 123,
		SeriesID: client.NoOPSeriesID,
		Index:    235,
		Term:     1,
	}
	commit := Task{
		Entries: []pb.Entry{e1},
	}
	sm.lastApplied.index = 234
	sm.index = 234
	sm.taskQ.Add(commit)
	batch := make([]Task, 0, 8)
	_, err := sm.Handle(batch, nil)
	require.True(t, expectedError(err))
}

type errorNodeProxy struct{}

func (e *errorNodeProxy) StepReady() {}
func (e *errorNodeProxy) RestoreRemotes(pb.Snapshot) error {
	return errReturnedError
}
func (e *errorNodeProxy) ApplyUpdate(pb.Entry, sm.Result, bool, bool, bool) {}
func (e *errorNodeProxy) ApplyConfigChange(pb.ConfigChange, uint64,
	bool) error {
	return errReturnedError
}
func (e *errorNodeProxy) ReplicaID() uint64           { return 1 }
func (e *errorNodeProxy) ShardID() uint64             { return 1 }
func (e *errorNodeProxy) ShouldStop() <-chan struct{} { return make(chan struct{}) }

func TestConfigChangeErrorIsReturned(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	createTestDir(fs)
	defer removeTestDir(fs)
	defer reportLeakedFD(fs, t)
	store := &errorUpdateSM{}
	config := config.Config{ShardID: 1, ReplicaID: 1}
	ds := NewNativeSM(config, NewInMemStateMachine(store), make(chan struct{}))
	nodeProxy := &errorNodeProxy{}
	snapshotter := newTestSnapshotter(fs)
	sm := NewStateMachine(ds, snapshotter, config, nodeProxy, fs)
	cc := pb.ConfigChange{
		ConfigChangeId: 1,
		Type:           pb.AddNode,
		ReplicaID:      2,
		Address:        "localhost:1222",
	}
	data, err := cc.Marshal()
	require.NoError(t, err)
	e := pb.Entry{
		Cmd:   data,
		Type:  pb.ConfigChangeEntry,
		Index: 235,
		Term:  1,
	}
	commit := Task{
		Entries: []pb.Entry{e},
	}
	sm.lastApplied.index = 234
	sm.index = 234
	sm.taskQ.Add(commit)
	batch := make([]Task, 0, 8)
	_, err = sm.Handle(batch, nil)
	require.True(t, expectedError(err))
}

func TestSaveErrorIsReturned(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	createTestDir(fs)
	defer removeTestDir(fs)
	defer reportLeakedFD(fs, t)
	store := &errorUpdateSM{}
	config := config.Config{ShardID: 1, ReplicaID: 1}
	ds := NewNativeSM(config, NewInMemStateMachine(store), make(chan struct{}))
	nodeProxy := newTestNodeProxy()
	snapshotter := newTestSnapshotter(fs)
	sm := NewStateMachine(ds, snapshotter, config, nodeProxy, fs)
	sm.members.members.Addresses[1] = "localhost:1234"
	sm.lastApplied.index = 234
	sm.index = 234
	_, _, err := sm.Save(SSRequest{})
	require.True(t, expectedError(err))
}

func TestRecoverErrorIsReturned(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		sm.members.members.Addresses[1] = "localhost:1234"
		sm.lastApplied.index = 3
		sm.index = 3
		ss, _, err := sm.Save(SSRequest{})
		require.NoError(t, err)
		index := ss.Index
		commit := Task{
			Index: index,
		}
		store2 := &errorUpdateSM{}
		config := config.Config{ShardID: 1, ReplicaID: 1}
		ds2 := NewNativeSM(config, NewInMemStateMachine(store2), make(chan struct{}))
		nodeProxy2 := newTestNodeProxy()
		snapshotter2 := newTestSnapshotter(fs)
		snapshotter2.index = commit.Index
		sm2 := NewStateMachine(ds2, snapshotter2, config, nodeProxy2, fs)
		_, err = sm2.Recover(commit)
		require.True(t, expectedError(err))
	}
	runSMTest2(t, tf, fs)
}

func TestRestoreRemoteErrorIsReturned(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(t *testing.T, sm *StateMachine, ds IManagedStateMachine,
		nodeProxy *testNodeProxy, snapshotter *testSnapshotter,
		store sm.IStateMachine) {
		sm.members.members.Addresses[1] = "localhost:1234"
		sm.lastApplied.index = 3
		sm.index = 3
		ss, _, err := sm.Save(SSRequest{})
		require.NoError(t, err)
		index := ss.Index
		commit := Task{
			Index: index,
		}
		store2 := tests.NewKVTest(1, 1)
		store2.(*tests.KVTest).DisableLargeDelay()
		config := config.Config{ShardID: 1, ReplicaID: 1}
		ds2 := NewNativeSM(config, NewInMemStateMachine(store2), make(chan struct{}))
		nodeProxy2 := &errorNodeProxy{}
		snapshotter2 := newTestSnapshotter(fs)
		snapshotter2.index = commit.Index
		sm2 := NewStateMachine(ds2, snapshotter2, config, nodeProxy2, fs)
		_, err = sm2.Recover(commit)
		require.True(t, expectedError(err))
	}
	runSMTest2(t, tf, fs)
}
````

## File: internal/rsm/statemachine.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

/*
Package rsm implements State Machines used in Dragonboat.

This package is internally used by Dragonboat, applications are not expected to
import this package.
*/
package rsm

import (
	"bytes"
	"sync"
	"sync/atomic"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/logutil"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/raft"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/utils"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/logger"
	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
)

var (
	plog = logger.GetLogger("rsm")
)

var (
	// ErrRestoreSnapshot indicates there is error when trying to restore
	// from a snapshot
	ErrRestoreSnapshot             = errors.New("failed to restore snapshot")
	batchedEntryApply              = settings.Soft.BatchedEntryApply
	sessionBufferInitialCap uint64 = 128 * 1024
)

// SSReqType is the type of a snapshot request.
type SSReqType uint64

const (
	// Periodic is the value to indicate periodic snapshot.
	Periodic SSReqType = iota
	// UserRequested is the value to indicate user requested snapshot.
	UserRequested
	// Exported is the value to indicate exported snapshot.
	Exported
	// Streaming is the value to indicate snapshot streaming.
	Streaming
)

// SSEnv is the snapshot environment type.
type SSEnv = server.SSEnv

// DefaultSSRequest is the default SSRequest.
var DefaultSSRequest = SSRequest{}

// SSRequest contains details of a snapshot request.
type SSRequest struct {
	Path               string
	Type               SSReqType
	Key                uint64
	CompactionOverhead uint64
	CompactionIndex    uint64
	OverrideCompaction bool
}

// Exported returns a boolean value indicating whether the snapshot request
// is to create an exported snapshot.
func (r *SSRequest) Exported() bool {
	return r.Type == Exported
}

// Streaming returns a boolean value indicating whether the snapshot request
// is to stream snapshot.
func (r *SSRequest) Streaming() bool {
	return r.Type == Streaming
}

// SSMeta is the metadata of a snapshot.
type SSMeta struct {
	Membership      pb.Membership
	Ctx             interface{}
	Session         *bytes.Buffer
	Request         SSRequest
	From            uint64
	OnDiskIndex     uint64
	Index           uint64
	Term            uint64
	Type            pb.StateMachineType
	CompressionType config.CompressionType
}

// Task describes a task that need to be handled by StateMachine.
type Task struct {
	Entries      []pb.Entry
	SSRequest    SSRequest
	ShardID      uint64
	ReplicaID    uint64
	Index        uint64
	Save         bool
	Stream       bool
	PeriodicSync bool
	NewNode      bool
	Recover      bool
	Initial      bool
}

// IsSnapshotTask returns a boolean flag indicating whether it is a snapshot
// task.
func (t *Task) IsSnapshotTask() bool {
	return t.Recover || t.Save || t.Stream
}

func (t *Task) isSyncTask() bool {
	if t.PeriodicSync && t.IsSnapshotTask() {
		panic("invalid task")
	}
	return t.PeriodicSync
}

// SMFactoryFunc is the function type for creating an IStateMachine instance
type SMFactoryFunc func(shardID uint64,
	replicaID uint64, done <-chan struct{}) IManagedStateMachine

// INode is the interface of a dragonboat node.
type INode interface {
	StepReady()
	RestoreRemotes(pb.Snapshot) error
	ApplyUpdate(pb.Entry, sm.Result, bool, bool, bool)
	ApplyConfigChange(pb.ConfigChange, uint64, bool) error
	ReplicaID() uint64
	ShardID() uint64
	ShouldStop() <-chan struct{}
}

// ISnapshotter is the interface for the snapshotter object.
type ISnapshotter interface {
	GetSnapshot() (pb.Snapshot, error)
	Stream(IStreamable, SSMeta, pb.IChunkSink) error
	Shrunk(ss pb.Snapshot) (bool, error)
	Save(ISavable, SSMeta) (pb.Snapshot, SSEnv, error)
	Load(pb.Snapshot, ILoadable, IRecoverable) error
	IsNoSnapshotError(error) bool
}

// StateMachine is the state machine component in the replicated state machine
// scheme.
type StateMachine struct {
	node        INode
	fs          vfs.IFS
	sm          IManagedStateMachine
	snapshotter ISnapshotter
	taskQ       *TaskQueue
	sessions    *SessionManager
	members     membership
	// lastApplied is the last applied index visibile to other modules in the
	// system. it is updated by only setting the last index and term values of
	// the update batch. it is protected by its own mutex to minimize contention
	// with the update thread.
	lastApplied struct {
		sync.Mutex
		index uint64
		term  uint64
	}
	// index and term values updated for each applied entry
	index           uint64
	term            uint64
	snapshotIndex   uint64
	onDiskInitIndex uint64
	onDiskIndex     uint64
	syncedIndex     uint64
	mu              sync.RWMutex
	sct             config.CompressionType
	onDiskSM        bool
	aborted         bool
	isWitness       bool
}

var firstError = utils.FirstError

// NewStateMachine creates a new application state machine object.
func NewStateMachine(sm IManagedStateMachine,
	snapshotter ISnapshotter,
	cfg config.Config, node INode, fs vfs.IFS) *StateMachine {
	ordered := cfg.OrderedConfigChange
	return &StateMachine{
		snapshotter: snapshotter,
		sm:          sm,
		onDiskSM:    sm.OnDisk(),
		taskQ:       NewTaskQueue(),
		node:        node,
		sessions:    NewSessionManager(),
		members:     newMembership(node.ShardID(), node.ReplicaID(), ordered),
		isWitness:   cfg.IsWitness,
		sct:         cfg.SnapshotCompressionType,
		fs:          fs,
	}
}

// Type returns the state machine type.
func (s *StateMachine) Type() pb.StateMachineType {
	return s.sm.Type()
}

// TaskQ returns the task queue.
func (s *StateMachine) TaskQ() *TaskQueue {
	return s.taskQ
}

// TaskChanBusy returns whether the TaskC chan is busy. Busy is defined as
// having more than half of its buffer occupied.
func (s *StateMachine) TaskChanBusy() bool {
	sz := s.taskQ.Size()
	return sz*2 > taskQueueBusyCap
}

// Close closes the state machine.
func (s *StateMachine) Close() error {
	return s.sm.Close()
}

// DestroyedC return a chan struct{} used to indicate whether the SM has been
// fully unloaded.
func (s *StateMachine) DestroyedC() <-chan struct{} {
	return s.sm.DestroyedC()
}

// Recover applies the snapshot.
func (s *StateMachine) Recover(t Task) (_ pb.Snapshot, err error) {
	ss, err := s.getSnapshot(t)
	if err != nil {
		return pb.Snapshot{}, err
	}
	if pb.IsEmptySnapshot(ss) {
		return pb.Snapshot{}, nil
	}
	plog.Debugf("%s called Recover, %s, on disk idx %d",
		s.id(), s.ssid(ss.Index), ss.OnDiskIndex)
	if err := s.recover(ss, t.Initial); err != nil {
		return pb.Snapshot{}, err
	}
	if err := s.node.RestoreRemotes(ss); err != nil {
		return pb.Snapshot{}, err
	}
	plog.Debugf("%s restored %s", s.id(), s.ssid(ss.Index))
	return ss, nil
}

func (s *StateMachine) getSnapshot(t Task) (pb.Snapshot, error) {
	ss, err := s.snapshotter.GetSnapshot()
	if !t.Initial {
		if err != nil && !s.snapshotter.IsNoSnapshotError(err) {
			return pb.Snapshot{}, ErrRestoreSnapshot
		}
		if s.snapshotter.IsNoSnapshotError(err) {
			return pb.Snapshot{}, err
		}
		if ss.Index < t.Index {
			plog.Panicf("%s out of date snapshot, %d, %d", s.id(), ss.Index, t.Index)
		}
		return ss, nil
	}
	if s.snapshotter.IsNoSnapshotError(err) {
		plog.Infof("%s no snapshot available during launch", s.id())
		return pb.Snapshot{}, nil
	}
	return ss, nil
}

func (s *StateMachine) mustBeOnDiskSM() {
	if !s.OnDiskStateMachine() {
		panic("not an IOnDiskStateMachine")
	}
}

// isShrunkSnapshot determines if the snapshot is shrunk. This check involves
// disk I/O
func (s *StateMachine) isShrunkSnapshot(ss pb.Snapshot, init bool) (bool, error) {
	if !s.OnDiskStateMachine() {
		return false, nil
	}
	if ss.Witness || ss.Dummy {
		return false, nil
	}
	shrunk, err := s.snapshotter.Shrunk(ss)
	if err != nil {
		return false, err
	}
	if !init && shrunk {
		panic("not initial recovery but snapshot shrunk")
	}
	return shrunk, nil
}

// recoverRequired determines if the snapshot must be recovered or not. This method
// mustn't be called on a non-disk-sm or on a partial snapshot (shrunk, witness or dummy).
// The check that this method is not called on a shrunk snapshot is not enforced, because
// this would involve a disk I/O. The caller is responsible for not calling this method
// on a shrunk snapshot and can optimize for only determining once if it has been shrunk
// or not.
func (s *StateMachine) recoverRequired(ss pb.Snapshot, init bool) bool {
	s.mustBeOnDiskSM()
	if ss.Witness || ss.Dummy {
		panic("called on a partial snapshot")
	}
	if init {
		if ss.Imported {
			return true
		}
		return ss.OnDiskIndex > s.onDiskInitIndex
	}
	return ss.OnDiskIndex > s.onDiskIndex
}

func (s *StateMachine) checkRecoverOnDiskSM(ss pb.Snapshot, init bool) {
	s.mustBeOnDiskSM()
	if ss.Imported && init {
		return
	}
	if ss.OnDiskIndex <= s.onDiskInitIndex {
		plog.Panicf("%s, ss.OnDiskIndex (%d) <= s.onDiskInitIndex (%d)",
			s.id(), ss.OnDiskIndex, s.onDiskInitIndex)
	}
	if ss.OnDiskIndex <= s.onDiskIndex {
		plog.Panicf("%s, ss.OnDiskInit (%d) <= s.onDiskIndex (%d)",
			s.id(), ss.OnDiskIndex, s.onDiskIndex)
	}
}

func (s *StateMachine) checkPartialSnapshotApplyOnDiskSM(ss pb.Snapshot, init bool) {
	s.mustBeOnDiskSM()
	// For OnDisk StateMachines check for corruption: A partial snapshot can only be
	// applied if we have at least the OnDiskIndex of the snapshot reached.
	if init {
		if ss.OnDiskIndex > s.onDiskInitIndex {
			plog.Panicf("%s, ss.OnDiskIndex (%d) > s.onDiskInitIndex (%d)",
				s.id(), ss.OnDiskIndex, s.onDiskInitIndex)
		}
	} else if ss.OnDiskIndex > s.onDiskIndex {
		plog.Panicf("%s, ss.OnDiskInit (%d) > s.onDiskIndex (%d)",
			s.id(), ss.OnDiskIndex, s.onDiskIndex)
	}
}

func (s *StateMachine) recover(ss pb.Snapshot, init bool) error {
	if err := s.doRecover(ss, init); err != nil {
		return err
	}
	s.apply(ss, init)
	return nil
}

func (s *StateMachine) doRecover(ss pb.Snapshot, init bool) error {
	s.mu.Lock()
	defer s.mu.Unlock()
	if s.GetLastApplied() >= ss.Index {
		return raft.ErrSnapshotOutOfDate
	}
	if s.aborted {
		return sm.ErrSnapshotStopped
	}
	onDisk := s.OnDiskStateMachine()
	shrunk, err := s.isShrunkSnapshot(ss, init)
	if err != nil {
		return err
	}
	if ss.Witness || ss.Dummy || shrunk {
		if onDisk {
			s.checkPartialSnapshotApplyOnDiskSM(ss, init)
		}
		return nil
	}
	if !onDisk {
		if err := s.load(ss, init); err != nil {
			return err
		}
		return nil
	}
	if s.recoverRequired(ss, init) {
		s.checkRecoverOnDiskSM(ss, init)
		if err := s.load(ss, init); err != nil {
			return err
		}
		s.applyOnDisk(ss, init)
	}
	return nil
}

func (s *StateMachine) load(ss pb.Snapshot, init bool) error {
	if err := s.snapshotter.Load(ss, s.sessions, s.sm); err != nil {
		plog.Errorf("%s failed to load %s, %v", s.id(), s.ssid(ss.Index), err)
		if err == sm.ErrSnapshotStopped {
			s.aborted = true
		}
		return err
	}
	return nil
}

func (s *StateMachine) apply(ss pb.Snapshot, init bool) {
	index := ss.Index
	plog.Debugf("%s recovering from %s, init %t", s.id(), s.ssid(index), init)
	s.logMembership("members", index, ss.Membership.Addresses)
	s.logMembership("nonVotings", index, ss.Membership.NonVotings)
	s.logMembership("witnesses", index, ss.Membership.Witnesses)
	s.members.set(ss.Membership)
	s.lastApplied.Lock()
	defer s.lastApplied.Unlock()
	s.lastApplied.index, s.lastApplied.term = ss.Index, ss.Term
	s.index, s.term = ss.Index, ss.Term
}

func (s *StateMachine) applyOnDisk(ss pb.Snapshot, init bool) {
	s.mustBeOnDiskSM()
	s.onDiskIndex = ss.OnDiskIndex
	if ss.Imported && init {
		s.onDiskInitIndex = ss.OnDiskIndex
	}
}

//TODO: add test to cover the case when ReadyToStreamSnapshot returns false

// ReadyToStream returns a boolean flag to indicate whether the state machine
// is ready to stream snapshot. It can not stream a full snapshot when
// membership state is catching up with the all disk SM state. Meta only
// snapshot can be taken at any time.
func (s *StateMachine) ReadyToStream() bool {
	if !s.OnDiskStateMachine() {
		return true
	}
	return s.GetLastApplied() >= s.onDiskInitIndex
}

func (s *StateMachine) tryInjectTestFS() {
	if nsm, ok := s.sm.(*NativeSM); ok {
		if odsm, ok := nsm.sm.(*OnDiskStateMachine); ok {
			odsm.SetTestFS(s.fs)
		}
	}
}

// OpenOnDiskStateMachine opens the on disk state machine.
func (s *StateMachine) OpenOnDiskStateMachine() (uint64, error) {
	s.mustBeOnDiskSM()
	s.mu.Lock()
	defer s.mu.Unlock()
	s.tryInjectTestFS()
	index, err := s.sm.Open()
	if err != nil {
		plog.Errorf("%s failed to open on disk SM, %v", s.id(), err)
		if err == sm.ErrOpenStopped {
			s.aborted = true
		}
		return 0, err
	}
	plog.Infof("%s opened disk SM, index %d", s.id(), index)
	s.onDiskInitIndex = index
	s.onDiskIndex = index
	return index, nil
}

// Offloaded marks the state machine as offloaded from the specified compone.
// It returns a boolean value indicating whether the node has been fully
// unloaded after unloading from the specified compone.
func (s *StateMachine) Offloaded() bool {
	return s.sm.Offloaded()
}

// Loaded marks the state machine as loaded from the specified compone.
func (s *StateMachine) Loaded() {
	s.sm.Loaded()
}

// Lookup queries the local state machine.
func (s *StateMachine) Lookup(query interface{}) (interface{}, error) {
	if s.Concurrent() {
		return s.concurrentLookup(query)
	}
	return s.lookup(query)
}

func (s *StateMachine) lookup(query interface{}) (interface{}, error) {
	s.mu.RLock()
	defer s.mu.RUnlock()
	if s.aborted {
		return nil, ErrShardClosed
	}
	return s.sm.Lookup(query)
}

func (s *StateMachine) concurrentLookup(query interface{}) (interface{}, error) {
	return s.sm.ConcurrentLookup(query)
}

// NALookup queries the local state machine.
func (s *StateMachine) NALookup(query []byte) ([]byte, error) {
	if s.Concurrent() {
		return s.naConcurrentLookup(query)
	}
	return s.nalookup(query)
}

func (s *StateMachine) nalookup(query []byte) ([]byte, error) {
	s.mu.RLock()
	defer s.mu.RUnlock()
	if s.aborted {
		return nil, ErrShardClosed
	}
	return s.sm.NALookup(query)
}

func (s *StateMachine) naConcurrentLookup(query []byte) ([]byte, error) {
	return s.sm.NAConcurrentLookup(query)
}

// GetMembership returns the membership info maintained by the state machine.
func (s *StateMachine) GetMembership() pb.Membership {
	s.mu.RLock()
	defer s.mu.RUnlock()
	return s.members.get()
}

// Concurrent returns a boolean flag indicating whether the state machine is
// capable of taking concurrent snapshot.
func (s *StateMachine) Concurrent() bool {
	return s.sm.Concurrent()
}

// OnDiskStateMachine returns a boolean flag indicating whether it is an on
// disk state machine.
func (s *StateMachine) OnDiskStateMachine() bool {
	return s.onDiskSM
}

// Save creates a snapshot.
func (s *StateMachine) Save(req SSRequest) (pb.Snapshot, SSEnv, error) {
	if req.Streaming() {
		panic("invalid snapshot request")
	}
	if s.isWitness {
		plog.Panicf("witness %s saving snapshot", s.id())
	}
	if s.Concurrent() {
		return s.concurrentSave(req)
	}
	return s.save(req)
}

// Stream starts to stream snapshot from the current SM to a remote node
// targeted by the provided sink.
func (s *StateMachine) Stream(sink pb.IChunkSink) error {
	return s.stream(sink)
}

// Sync synchronizes state machine's in-core state with that on disk.
func (s *StateMachine) Sync() error {
	return s.sync()
}

// GetHash returns the state machine hash.
func (s *StateMachine) GetHash() (uint64, error) {
	s.mu.RLock()
	defer s.mu.RUnlock()
	return s.sm.GetHash()
}

// GetSessionHash returns the session hash.
func (s *StateMachine) GetSessionHash() uint64 {
	s.mu.RLock()
	defer s.mu.RUnlock()
	return s.sessions.GetSessionHash()
}

// GetMembershipHash returns the hash of the membership instance.
func (s *StateMachine) GetMembershipHash() uint64 {
	s.mu.RLock()
	defer s.mu.RUnlock()
	return s.members.getHash()
}

// Handle pulls the committed record and apply it if there is any available.
func (s *StateMachine) Handle(batch []Task, apply []sm.Entry) (Task, error) {
	batch = batch[:0]
	apply = apply[:0]
	processed := false
	defer func() {
		// give the node worker a chance to run when
		//  - the visible index value has been updated
		//  - taskC has been popped
		if processed {
			s.node.StepReady()
		}
	}()
	if rec, ok := s.taskQ.Get(); ok {
		processed = true
		if rec.IsSnapshotTask() {
			return rec, nil
		}
		if !rec.isSyncTask() {
			batch = append(batch, rec)
		} else {
			if err := s.sync(); err != nil {
				return Task{}, err
			}
		}
		done := false
		for !done {
			if rec, ok := s.taskQ.Get(); ok {
				if rec.IsSnapshotTask() {
					if err := s.handle(batch, apply); err != nil {
						return Task{}, err
					}
					return rec, nil
				}
				if !rec.isSyncTask() {
					batch = append(batch, rec)
				} else {
					if err := s.sync(); err != nil {
						return Task{}, err
					}
				}
			} else {
				done = true
			}
		}
	}
	return Task{}, s.handle(batch, apply)
}

func (s *StateMachine) isDummySnapshot(r SSRequest) bool {
	return s.OnDiskStateMachine() && !r.Exported() && !r.Streaming()
}

func (s *StateMachine) logMembership(name string,
	index uint64, members map[uint64]string) {
	plog.Debugf("%d %s included in %s", len(members), name, s.ssid(index))
	for nid, addr := range members {
		plog.Debugf("\t%s : %s", logutil.ReplicaID(nid), addr)
	}
}

func (s *StateMachine) getSSMeta(c interface{}, r SSRequest) (SSMeta, error) {
	if s.members.isEmpty() {
		plog.Panicf("%s, empty membership", s.id())
	}
	ct := s.sct
	// never compress dummy snapshot file
	if s.isDummySnapshot(r) {
		ct = config.NoCompression
	}
	buf := bytes.NewBuffer(make([]byte, 0, sessionBufferInitialCap))
	meta := SSMeta{
		From:            s.node.ReplicaID(),
		Ctx:             c,
		Index:           s.index,
		Term:            s.term,
		OnDiskIndex:     s.onDiskIndex,
		Request:         r,
		Session:         buf,
		Membership:      s.members.get(),
		Type:            s.sm.Type(),
		CompressionType: ct,
	}
	s.logMembership("members", meta.Index, meta.Membership.Addresses)
	if err := s.sessions.SaveSessions(meta.Session); err != nil {
		return SSMeta{}, err
	}
	return meta, nil
}

// GetLastApplied returns the last applied value.
func (s *StateMachine) GetLastApplied() uint64 {
	s.lastApplied.Lock()
	defer s.lastApplied.Unlock()
	return s.lastApplied.index
}

// SetLastApplied sets the last applied index to the specified value. This
// method is only used in tests.
func (s *StateMachine) SetLastApplied(index uint64) {
	s.lastApplied.Lock()
	defer s.lastApplied.Unlock()
	s.lastApplied.index = index
}

// GetSyncedIndex returns the index value that is known to have been
// synchronized.
func (s *StateMachine) GetSyncedIndex() uint64 {
	return atomic.LoadUint64(&s.syncedIndex)
}

func (s *StateMachine) setSyncedIndex(index uint64) {
	if s.GetSyncedIndex() > index {
		panic("synced index moving backward")
	}
	atomic.StoreUint64(&s.syncedIndex, index)
}

func (s *StateMachine) setApplied(index uint64, term uint64) {
	if s.index+1 != index {
		plog.Panicf("%s, applied index %d, new index %d",
			s.id(), s.index, index)
	}
	if s.term > term {
		plog.Panicf("%s, applied term %d, new term %d", s.id(), s.term, term)
	}
	s.index, s.term = index, term
}

func (s *StateMachine) setLastApplied(entries []pb.Entry) {
	if len(entries) > 0 {
		index := uint64(0)
		term := uint64(0)
		for idx, e := range entries {
			if e.Index == 0 || e.Term == 0 {
				plog.Panicf("invalid entry, %v", e)
			}
			if idx == 0 {
				index = e.Index
				term = e.Term
			} else {
				if e.Index != index+1 {
					plog.Panicf("%s, index gap found, %d,%d", s.id(), index, e.Index)
				}
				if e.Term < term {
					plog.Panicf("%s, term moving backward, %d, %d", s.id(), term, e.Term)
				}
				index = e.Index
				term = e.Term
			}
		}
		s.lastApplied.Lock()
		defer s.lastApplied.Unlock()
		if s.lastApplied.index+1 != entries[0].Index {
			plog.Panicf("%s, gap between batches, %d, %d", s.id())
		}
		if s.lastApplied.term > entries[0].Term {
			plog.Panicf("%s, invalid term", s.id())
		}
		s.lastApplied.index = entries[len(entries)-1].Index
		s.lastApplied.term = entries[len(entries)-1].Term
	}
}

func (s *StateMachine) savingDummySnapshot(r SSRequest) bool {
	return s.OnDiskStateMachine() && !r.Streaming() && !r.Exported()
}

func (s *StateMachine) checkSnapshotStatus(r SSRequest) error {
	if s.aborted {
		return sm.ErrSnapshotStopped
	}
	index := s.GetLastApplied()
	if index < s.snapshotIndex {
		panic("s.index < s.snapshotIndex")
	}
	if !s.OnDiskStateMachine() {
		if !r.Exported() && index > 0 && index == s.snapshotIndex {
			return raft.ErrSnapshotOutOfDate
		}
	}
	return nil
}

func (s *StateMachine) stream(sink pb.IChunkSink) error {
	var err error
	var meta SSMeta
	if err := func() error {
		s.mu.RLock()
		defer s.mu.RUnlock()
		meta, err = s.prepare(SSRequest{Type: Streaming})
		return err
	}(); err != nil {
		return err
	}
	return s.snapshotter.Stream(s.sm, meta, sink)
}

func (s *StateMachine) concurrentSave(r SSRequest) (pb.Snapshot, SSEnv, error) {
	var err error
	var meta SSMeta
	if err := func() error {
		s.mu.RLock()
		defer s.mu.RUnlock()
		meta, err = s.prepare(r)
		return err
	}(); err != nil {
		return pb.Snapshot{}, SSEnv{}, err
	}
	if err := s.sync(); err != nil {
		return pb.Snapshot{}, SSEnv{}, err
	}
	return s.doSave(meta)
}

func (s *StateMachine) save(r SSRequest) (pb.Snapshot, SSEnv, error) {
	s.mu.RLock()
	defer s.mu.RUnlock()
	meta, err := s.prepare(r)
	if err != nil {
		plog.Errorf("prepare snapshot failed %v", err)
		return pb.Snapshot{}, SSEnv{}, err
	}
	return s.doSave(meta)
}

func (s *StateMachine) prepare(r SSRequest) (SSMeta, error) {
	if err := s.checkSnapshotStatus(r); err != nil {
		return SSMeta{}, err
	}
	var err error
	var ctx interface{}
	if s.Concurrent() && !s.savingDummySnapshot(r) {
		ctx, err = s.sm.Prepare()
		if err != nil {
			return SSMeta{}, err
		}
	}
	return s.getSSMeta(ctx, r)
}

func (s *StateMachine) sync() error {
	if !s.OnDiskStateMachine() {
		return nil
	}
	s.mu.Lock()
	defer s.mu.Unlock()
	index := s.GetLastApplied()
	if err := s.sm.Sync(); err != nil {
		return err
	}
	s.setSyncedIndex(index)
	return nil
}

func (s *StateMachine) doSave(meta SSMeta) (pb.Snapshot, SSEnv, error) {
	ss, env, err := s.snapshotter.Save(s.sm, meta)
	if err != nil {
		plog.Errorf("%s snapshotter.Save failed %v", s.id(), err)
		return pb.Snapshot{}, env, err
	}
	s.snapshotIndex = meta.Index
	return ss, env, nil
}

func getEntryTypes(entries []pb.Entry) (bool, bool) {
	allUpdate := true
	allNoOP := true
	for _, v := range entries {
		if allUpdate && !v.IsUpdateEntry() {
			allUpdate = false
		}
		if allNoOP && !v.IsNoOPSession() {
			allNoOP = false
		}
	}
	return allUpdate, allNoOP
}

func (s *StateMachine) handle(t []Task, a []sm.Entry) error {
	batch := batchedEntryApply && s.Concurrent()
	for idx := range t {
		if t[idx].IsSnapshotTask() || t[idx].isSyncTask() {
			plog.Panicf("%s trying to handle a snapshot/sync request", s.id())
		}
		// TODO: add a test for this
		var entries []pb.Entry
		func() {
			s.mu.Lock()
			defer s.mu.Unlock()
			entries = pb.EntriesToApply(t[idx].Entries, s.index, false)
		}()
		update, noop := getEntryTypes(entries)
		if batch && update && noop {
			if err := s.handleBatch(entries, a); err != nil {
				return err
			}
		} else {
			for i := range entries {
				last := idx == len(t)-1 && i == len(entries)-1
				if err := s.handleEntry(entries[i], last); err != nil {
					return err
				}
			}
		}
		s.setLastApplied(entries)
	}
	return nil
}

func isEmptyResult(result sm.Result) bool {
	return result.Data == nil && result.Value == 0
}

func (s *StateMachine) entryInInitDiskSM(index uint64) bool {
	if !s.OnDiskStateMachine() {
		return false
	}
	return index <= s.onDiskInitIndex
}

func (s *StateMachine) setOnDiskIndex(first uint64, last uint64) {
	if !s.OnDiskStateMachine() {
		return
	}
	if first > last {
		panic("first > last")
	}
	if first <= s.onDiskInitIndex {
		plog.Panicf("first index %d, init on disk %d", first, s.onDiskInitIndex)
	}
	if first <= s.onDiskIndex {
		plog.Panicf("first index %d, on disk index %d", first, s.onDiskIndex)
	}
	s.onDiskIndex = last
}

func (s *StateMachine) handleEntry(e pb.Entry, last bool) error {
	if e.IsConfigChange() {
		return s.configChange(e)
	}
	if !e.IsSessionManaged() {
		if e.IsEmpty() {
			s.noop(e)
			s.node.ApplyUpdate(e, sm.Result{}, false, true, last)
		} else {
			panic("not session managed, not empty")
		}
	} else {
		if e.IsNewSessionRequest() {
			r := s.registerSession(e)
			s.node.ApplyUpdate(e, r, isEmptyResult(r), false, last)
		} else if e.IsEndOfSessionRequest() {
			r := s.unregisterSession(e)
			s.node.ApplyUpdate(e, r, isEmptyResult(r), false, last)
		} else {
			if !s.entryInInitDiskSM(e.Index) {
				r, ignored, rejected, err := s.update(e)
				if err != nil {
					return err
				}
				if !ignored {
					s.node.ApplyUpdate(e, r, rejected, ignored, last)
				}
			} else {
				// treat it as a NoOP entry
				s.noop(pb.Entry{Index: e.Index, Term: e.Term})
			}
		}
	}
	return nil
}

func (s *StateMachine) onApplied(e pb.Entry,
	result sm.Result, ignored bool, rejected bool, last bool) {
	if !ignored {
		s.node.ApplyUpdate(e, result, rejected, ignored, last)
	}
}

func (s *StateMachine) handleBatch(input []pb.Entry, ents []sm.Entry) error {
	if len(ents) != 0 {
		panic("ents is not empty")
	}
	s.mu.Lock()
	defer s.mu.Unlock()
	skipped := 0
	for _, e := range input {
		if !s.entryInInitDiskSM(e.Index) {
			payload, err := GetPayload(e)
			if err != nil {
				return err
			}
			ents = append(ents, sm.Entry{Index: e.Index, Cmd: payload})
		} else {
			skipped++
			s.setApplied(e.Index, e.Term)
		}
	}
	if len(ents) > 0 {
		results, err := s.sm.BatchedUpdate(ents)
		if err != nil {
			return err
		}
		for idx, e := range results {
			ce := input[skipped+idx]
			if ce.Index != e.Index {
				// probably because user modified the Index value in results
				plog.Panicf("%s alignment error, %d, %d, %d",
					s.id(), ce.Index, e.Index, skipped)
			}
			last := ce.Index == input[len(input)-1].Index
			s.onApplied(ce, e.Result, false, false, last)
			s.setApplied(ce.Index, ce.Term)
		}
		s.setOnDiskIndex(ents[0].Index, ents[len(ents)-1].Index)
	}
	return nil
}

func (s *StateMachine) configChange(e pb.Entry) error {
	var cc pb.ConfigChange
	pb.MustUnmarshal(&cc, e.Cmd)
	rejected := true
	func() {
		s.mu.Lock()
		defer s.mu.Unlock()
		defer s.setApplied(e.Index, e.Term)
		if s.members.handleConfigChange(cc, e.Index) {
			rejected = false
		}
	}()
	return s.node.ApplyConfigChange(cc, e.Key, rejected)
}

func (s *StateMachine) registerSession(e pb.Entry) sm.Result {
	s.mu.Lock()
	defer s.mu.Unlock()
	defer s.setApplied(e.Index, e.Term)
	return s.sessions.RegisterClientID(e.ClientID)
}

func (s *StateMachine) unregisterSession(e pb.Entry) sm.Result {
	s.mu.Lock()
	defer s.mu.Unlock()
	defer s.setApplied(e.Index, e.Term)
	return s.sessions.UnregisterClientID(e.ClientID)
}

func (s *StateMachine) noop(e pb.Entry) {
	s.mu.Lock()
	defer s.mu.Unlock()
	defer s.setApplied(e.Index, e.Term)
	if !e.IsEmpty() || e.IsSessionManaged() {
		panic("handle empty event called on non-empty event")
	}
}

// result is a tuple of (result, should ignore, rejected, error)
func (s *StateMachine) update(e pb.Entry) (sm.Result, bool, bool, error) {
	s.mu.Lock()
	defer s.mu.Unlock()
	defer s.setApplied(e.Index, e.Term)
	var ok bool
	var session *Session
	if !e.IsNoOPSession() {
		session, ok = s.sessions.ClientRegistered(e.ClientID)
		if !ok {
			// client is expected to crash
			return sm.Result{}, false, true, nil
		}
		s.sessions.UpdateRespondedTo(session, e.RespondedTo)
		v, responded, toUpdate := s.sessions.UpdateRequired(session, e.SeriesID)
		if responded {
			// should ignore. client is expected to timeout
			return sm.Result{}, true, false, nil
		}
		if !toUpdate {
			// server responded, client never confirmed
			// return the result again but not update the sm again
			// this implements the no-more-than-once update of the SM
			return v, false, false, nil
		}
	}
	if !e.IsNoOPSession() && session == nil {
		panic("session not found")
	}
	if session != nil {
		if _, ok := session.getResponse(RaftSeriesID(e.SeriesID)); ok {
			panic("already has response in session")
		}
	}
	payload, err := GetPayload(e)
	if err != nil {
		return sm.Result{}, false, false, err
	}
	r, err := s.sm.Update(sm.Entry{Index: e.Index, Cmd: payload})
	if err != nil {
		return sm.Result{}, false, false, err
	}
	s.setOnDiskIndex(e.Index, e.Index)
	if session != nil {
		session.addResponse(RaftSeriesID(e.SeriesID), r)
	}
	return r, false, false, nil
}

func (s *StateMachine) id() string {
	return logutil.DescribeSM(s.node.ShardID(), s.node.ReplicaID())
}

func (s *StateMachine) ssid(index uint64) string {
	return logutil.DescribeSS(s.node.ShardID(), s.node.ReplicaID(), index)
}
````

## File: internal/rsm/taskqueue_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"reflect"
	"testing"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

func TestTaskQueueCanBeCreated(t *testing.T) {
	tq := NewTaskQueue()
	sz := tq.Size()
	assert.Equal(t, uint64(0), sz, "unexpected len")
	_, ok := tq.Get()
	assert.False(t, ok, "unexpectedly returned item from tq")
}

func TestAddToTaskQueue(t *testing.T) {
	t1 := Task{ShardID: 1}
	t2 := Task{ShardID: 2}
	t3 := Task{ShardID: 3}
	tq := NewTaskQueue()
	tq.Add(t1)
	tq.Add(t2)
	tq.Add(t3)
	sz := tq.Size()
	assert.Equal(t, uint64(3), sz, "unexpected len")
	assert.True(t, reflect.DeepEqual(&t1, &(tq.tasks[0])),
		"value changed")
	assert.True(t, reflect.DeepEqual(&t2, &(tq.tasks[1])),
		"value changed")
	assert.True(t, reflect.DeepEqual(&t3, &(tq.tasks[2])),
		"value changed")
}

func TestGetCanReturnAddedTaskFromTaskQueue(t *testing.T) {
	t1 := Task{ShardID: 1}
	t2 := Task{ShardID: 2}
	t3 := Task{ShardID: 3}
	tq := NewTaskQueue()
	tq.Add(t1)
	tq.Add(t2)
	tq.Add(t3)
	v1, ok := tq.Get()
	require.True(t, ok, "unexpected result")
	assert.True(t, reflect.DeepEqual(&t1, &v1), "unexpected result")
	v2, ok := tq.Get()
	require.True(t, ok, "unexpected result")
	assert.True(t, reflect.DeepEqual(&t2, &v2), "unexpected result")
	sz := tq.Size()
	assert.Equal(t, uint64(1), sz, "unexpected size")
	v3, ok := tq.Get()
	require.True(t, ok, "unexpected result")
	assert.True(t, reflect.DeepEqual(&t3, &v3), "unexpected result")
}

func TestTaskQueueResize(t *testing.T) {
	tq := NewTaskQueue()
	for i := uint64(0); i < initialTaskQueueCap*3; i++ {
		task := Task{ShardID: i}
		tq.Add(task)
	}
	initCap := uint64(cap(tq.tasks))
	assert.GreaterOrEqual(t, initCap, initialTaskQueueCap*3,
		"unexpected init cap")
	for {
		if _, ok := tq.Get(); !ok {
			break
		}
	}
	sz := tq.Size()
	assert.Equal(t, uint64(0), sz, "unexpected size")
	curCap := uint64(cap(tq.tasks))
	assert.LessOrEqual(t, curCap, initialTaskQueueCap, "not resized")
}

func TestMoreEntryToApply(t *testing.T) {
	tq := NewTaskQueue()
	for i := uint64(0); i < taskQueueBusyCap-1; i++ {
		task := Task{ShardID: i}
		tq.Add(task)
		more := tq.MoreEntryToApply()
		assert.True(t, more, "unexpectedly reported no more entry to apply")
	}
	task := Task{ShardID: 0}
	tq.Add(task)
	more := tq.MoreEntryToApply()
	assert.False(t, more, "entry not limited")
}

func TestTaskQueueGetAll(t *testing.T) {
	t1 := Task{ShardID: 1}
	t2 := Task{ShardID: 2}
	t3 := Task{ShardID: 3}
	tq := NewTaskQueue()
	all := tq.GetAll()
	assert.Len(t, all, 0, "unexpected results")
	tq.Add(t1)
	all = tq.GetAll()
	require.Len(t, all, 1, "unexpected results")
	assert.True(t, reflect.DeepEqual(&t1, &all[0]), "unexpected task")
	tq.Add(t1)
	tq.Add(t2)
	tq.Add(t3)
	all = tq.GetAll()
	require.Len(t, all, 3, "unexpected results")
	assert.True(t, reflect.DeepEqual(&t1, &all[0]), "unexpected results")
	assert.True(t, reflect.DeepEqual(&t2, &all[1]), "unexpected results")
	assert.True(t, reflect.DeepEqual(&t3, &all[2]), "unexpected results")
}
````

## File: internal/rsm/taskqueue.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package rsm

import (
	"sync"

	"github.com/lni/dragonboat/v4/internal/settings"
)

var (
	initialTaskQueueCap = settings.Soft.TaskQueueInitialCap
	taskQueueBusyCap    = settings.Soft.TaskQueueTargetLength
	emptyTask           = Task{}
)

// TaskQueue is a queue of tasks to be processed by the state machine.
type TaskQueue struct {
	tasks []Task
	next  uint64
	mu    sync.Mutex
}

// NewTaskQueue creates and returns a new task queue.
func NewTaskQueue() *TaskQueue {
	return &TaskQueue{
		tasks: make([]Task, 0, initialTaskQueueCap),
	}
}

// MoreEntryToApply returns a boolean value indicating whether it is ok to
// queue more entries to apply.
func (tq *TaskQueue) MoreEntryToApply() bool {
	tq.mu.Lock()
	defer tq.mu.Unlock()
	return tq.size() < taskQueueBusyCap
}

// Add adds a new task to the queue.
func (tq *TaskQueue) Add(task Task) {
	tq.mu.Lock()
	defer tq.mu.Unlock()
	tq.tasks = append(tq.tasks, task)
}

// GetAll returns all tasks currently in the queue.
func (tq *TaskQueue) GetAll() []Task {
	tq.mu.Lock()
	defer tq.mu.Unlock()
	result := tq.tasks
	tq.tasks = make([]Task, 0, initialTaskQueueCap)
	tq.next = 0
	return result
}

// Get returns a task from the queue if there is any.
func (tq *TaskQueue) Get() (Task, bool) {
	tq.mu.Lock()
	defer tq.mu.Unlock()
	if tq.next < uint64(len(tq.tasks)) {
		task := tq.tasks[tq.next]
		tq.tasks[tq.next] = emptyTask
		tq.next++
		tq.resize()
		return task, true
	}
	tq.resize()
	return emptyTask, false
}

// Size returns the number of queued tasks.
func (tq *TaskQueue) Size() uint64 {
	tq.mu.Lock()
	defer tq.mu.Unlock()
	return tq.size()
}

func (tq *TaskQueue) size() uint64 {
	return uint64(len(tq.tasks)) - tq.next
}

func (tq *TaskQueue) resize() {
	if uint64(cap(tq.tasks)) > initialTaskQueueCap*2 {
		if tq.size() < initialTaskQueueCap {
			tasks := make([]Task, tq.size())
			copy(tasks, tq.tasks[tq.next:])
			tq.tasks = tasks
			tq.next = 0
		}
	}
}
````

## File: internal/server/environment_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package server

import (
	"fmt"
	"testing"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/id"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/raftio"
	"github.com/lni/dragonboat/v4/raftpb"
)

const (
	singleNodeHostTestDir = "test_nodehost_dir_safe_to_delete"
	testLogDBName         = "test-name"
	testBinVer            = raftio.LogDBBinVersion
	testAddress           = "localhost:1111"
	testDeploymentID      = 100
)

func getTestNodeHostConfig() config.NodeHostConfig {
	return config.NodeHostConfig{
		WALDir:         singleNodeHostTestDir,
		NodeHostDir:    singleNodeHostTestDir,
		RTTMillisecond: 50,
		RaftAddress:    testAddress,
	}
}

func TestCheckNodeHostDirWorksWhenEverythingMatches(t *testing.T) {
	fs := vfs.GetTestFS()
	defer func() {
		if err := fs.RemoveAll(singleNodeHostTestDir); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	func() {
		c := getTestNodeHostConfig()
		defer func() {
			require.NotPanics(t, func() {
				if r := recover(); r != nil {
					t.Fatalf("panic not expected")
				}
			})
		}()
		env, err := NewEnv(c, fs)
		require.NoError(t, err, "failed to new environment")
		_, _, err = env.CreateNodeHostDir(testDeploymentID)
		require.NoError(t, err)
		dir, _ := env.getDataDirs()
		testName := "test-name"
		cfg := config.NodeHostConfig{
			Expert:       config.GetDefaultExpertConfig(),
			RaftAddress:  testAddress,
			DeploymentID: testDeploymentID,
		}
		status := raftpb.RaftDataStatus{
			Address: testAddress,
			BinVer:  raftio.LogDBBinVersion,
			HardHash: settings.HardHash(cfg.Expert.Engine.ExecShards,
				cfg.Expert.LogDB.Shards, settings.Hard.LRUMaxSessionCount,
				settings.Hard.LogDBEntryBatchSize),
			LogdbType:    testName,
			Hostname:     env.hostname,
			DeploymentId: testDeploymentID,
		}
		err = fileutil.CreateFlagFile(dir, flagFilename, &status, fs)
		require.NoError(t, err, "failed to create flag file")
		err = env.CheckNodeHostDir(cfg,
			raftio.LogDBBinVersion, testName)
		require.NoError(t, err, "check node host dir failed")
	}()
	reportLeakedFD(fs, t)
}

func TestRaftAddressIsAllowedToChangeWhenRequested(t *testing.T) {
	fs := vfs.GetTestFS()
	c := getTestNodeHostConfig()
	defer func() {
		if err := fs.RemoveAll(singleNodeHostTestDir); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	binVer := uint32(100)
	testLogDBName := "test-name"
	hostname := ""
	env, err := NewEnv(c, fs)
	require.NoError(t, err, "failed to new environment")
	_, _, err = env.CreateNodeHostDir(testDeploymentID)
	require.NoError(t, err)
	dir, _ := env.getDataDirs()
	cfg := config.NodeHostConfig{
		Expert:       config.GetDefaultExpertConfig(),
		DeploymentID: testDeploymentID,
		RaftAddress:  "addr1:12345",
	}
	status := raftpb.RaftDataStatus{
		Address: "addr2:54321",
		BinVer:  binVer,
		HardHash: settings.HardHash(cfg.Expert.Engine.ExecShards,
			cfg.Expert.LogDB.Shards, settings.Hard.LRUMaxSessionCount,
			settings.Hard.LogDBEntryBatchSize),
		LogdbType: testLogDBName,
		Hostname:  hostname,
	}
	err = fileutil.CreateFlagFile(dir, flagFilename, &status, fs)
	require.NoError(t, err, "failed to create flag file")
	err = env.CheckNodeHostDir(cfg, binVer, testLogDBName)
	require.Error(t, err, "changed raft address not detected")
	cfg.DefaultNodeRegistryEnabled = true
	status.AddressByNodeHostId = true
	err = fileutil.CreateFlagFile(dir, flagFilename, &status, fs)
	require.NoError(t, err, "failed to create flag file")
	err = env.CheckNodeHostDir(cfg, binVer, testLogDBName)
	require.NoError(t, err, "changed raft address not allowed")
}

func testNodeHostDirectoryDetectsMismatches(t *testing.T,
	addr string, hostname string, binVer uint32, name string,
	hardHashMismatch bool, addressByNodeHostID bool, expErr error,
	fs vfs.IFS) {
	c := getTestNodeHostConfig()
	defer func() {
		if err := fs.RemoveAll(singleNodeHostTestDir); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	env, err := NewEnv(c, fs)
	require.NoError(t, err, "failed to new environment")
	_, _, err = env.CreateNodeHostDir(testDeploymentID)
	require.NoError(t, err)
	dir, _ := env.getDataDirs()
	cfg := config.NodeHostConfig{
		Expert:                     config.GetDefaultExpertConfig(),
		DeploymentID:               testDeploymentID,
		RaftAddress:                testAddress,
		DefaultNodeRegistryEnabled: addressByNodeHostID,
	}

	status := raftpb.RaftDataStatus{
		Address: addr,
		BinVer:  binVer,
		HardHash: settings.HardHash(cfg.Expert.Engine.ExecShards,
			cfg.Expert.LogDB.Shards, settings.Hard.LRUMaxSessionCount,
			settings.Hard.LogDBEntryBatchSize),
		LogdbType:           name,
		Hostname:            hostname,
		AddressByNodeHostId: false,
	}
	if hardHashMismatch {
		status.HardHash = 1
	}
	err = fileutil.CreateFlagFile(dir, flagFilename, &status, fs)
	require.NoError(t, err, "failed to create flag file")
	err = env.CheckNodeHostDir(cfg, testBinVer, testLogDBName)
	assert.Equal(t, expErr, err, "expect err %v, got %v", expErr, err)
	reportLeakedFD(fs, t)
}

func TestCanDetectMismatchedHostname(t *testing.T) {
	fs := vfs.GetTestFS()
	testNodeHostDirectoryDetectsMismatches(t,
		testAddress, "incorrect-hostname", raftio.LogDBBinVersion,
		testLogDBName, false, false, ErrHostnameChanged, fs)
}

func TestCanDetectMismatchedLogDBName(t *testing.T) {
	fs := vfs.GetTestFS()
	testNodeHostDirectoryDetectsMismatches(t,
		testAddress, "", raftio.LogDBBinVersion,
		"incorrect name", false, false, ErrLogDBType, fs)
}

func TestCanDetectMismatchedBinVer(t *testing.T) {
	fs := vfs.GetTestFS()
	testNodeHostDirectoryDetectsMismatches(t,
		testAddress, "", raftio.LogDBBinVersion+1,
		testLogDBName, false, false, ErrIncompatibleData, fs)
}

func TestCanDetectMismatchedAddress(t *testing.T) {
	fs := vfs.GetTestFS()
	testNodeHostDirectoryDetectsMismatches(t,
		"invalid:12345", "", raftio.LogDBBinVersion,
		testLogDBName, false, false, ErrNotOwner, fs)
}

func TestCanDetectMismatchedHardHash(t *testing.T) {
	fs := vfs.GetTestFS()
	testNodeHostDirectoryDetectsMismatches(t,
		testAddress, "", raftio.LogDBBinVersion,
		testLogDBName, true, false, ErrHardSettingsChanged, fs)
}

func TestCanDetectMismatchedDefaultNodeRegistryEnabled(t *testing.T) {
	fs := vfs.GetTestFS()
	testNodeHostDirectoryDetectsMismatches(t,
		testAddress, "", raftio.LogDBBinVersion,
		testLogDBName, false, true, ErrDefaultNodeRegistryEnabledChanged, fs)
}

func TestLockFileCanBeLockedAndUnlocked(t *testing.T) {
	fs := vfs.GetTestFS()
	c := getTestNodeHostConfig()
	defer func() {
		if err := fs.RemoveAll(singleNodeHostTestDir); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	env, err := NewEnv(c, fs)
	require.NoError(t, err, "failed to new environment")
	_, _, err = env.CreateNodeHostDir(c.DeploymentID)
	require.NoError(t, err)
	err = env.LockNodeHostDir()
	require.NoError(t, err, "failed to lock the directory")
	err = env.Close()
	require.NoError(t, err, "failed to stop env")
	reportLeakedFD(fs, t)
}

func TestNodeHostIDCanBeGenerated(t *testing.T) {
	fs := vfs.GetTestFS()
	err := fs.RemoveAll(singleNodeHostTestDir)
	require.NoError(t, err)
	err = fs.MkdirAll(singleNodeHostTestDir, 0755)
	require.NoError(t, err)
	defer func() {
		if err := fs.RemoveAll(singleNodeHostTestDir); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	c := getTestNodeHostConfig()
	env, err := NewEnv(c, fs)
	require.NoError(t, err, "failed to create env")
	v, err := env.PrepareNodeHostID("")
	require.NoError(t, err, "failed to prepare nodehost id")
	require.NotEmpty(t, v.String(), "failed to generate UUID")
}

func TestPrepareNodeHostIDWillReportNodeHostIDChange(t *testing.T) {
	fs := vfs.GetTestFS()
	err := fs.RemoveAll(singleNodeHostTestDir)
	require.NoError(t, err)
	err = fs.MkdirAll(singleNodeHostTestDir, 0755)
	require.NoError(t, err)
	defer func() {
		if err := fs.RemoveAll(singleNodeHostTestDir); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	c := getTestNodeHostConfig()
	env, err := NewEnv(c, fs)
	require.NoError(t, err, "failed to create env")
	v, err := env.PrepareNodeHostID("")
	require.NoError(t, err, "failed to prepare nodehost id")
	// using the same uuid is okay
	v2, err := env.PrepareNodeHostID(v.String())
	require.NoError(t, err, "failed to prepare nodehost id")
	assert.Equal(t, v.String(), v2.String(), "returned UUID is unexpected")
	// change it is not allowed
	v3 := id.New()
	_, err = env.PrepareNodeHostID(v3.String())
	require.ErrorIs(t, err, ErrNodeHostIDChanged,
		"failed to report ErrNodeHostIDChanged")
}

func TestRemoveSavedSnapshots(t *testing.T) {
	fs := vfs.GetTestFS()
	err := fs.RemoveAll(singleNodeHostTestDir)
	require.NoError(t, err)
	err = fs.MkdirAll(singleNodeHostTestDir, 0755)
	require.NoError(t, err)
	defer func() {
		if err := fs.RemoveAll(singleNodeHostTestDir); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	for i := 0; i < 16; i++ {
		ssdir := fs.PathJoin(singleNodeHostTestDir, fmt.Sprintf("snapshot-%X", i))
		err := fs.MkdirAll(ssdir, 0755)
		require.NoError(t, err, "failed to mkdir")
	}
	for i := 1; i <= 2; i++ {
		ssdir := fs.PathJoin(singleNodeHostTestDir, fmt.Sprintf("mydata-%X", i))
		err := fs.MkdirAll(ssdir, 0755)
		require.NoError(t, err, "failed to mkdir")
	}
	err = removeSavedSnapshots(singleNodeHostTestDir, fs)
	require.NoError(t, err, "failed to remove saved snapshots")
	files, err := fs.List(singleNodeHostTestDir)
	require.NoError(t, err, "failed to read dir")
	for _, fn := range files {
		fi, err := fs.Stat(fs.PathJoin(singleNodeHostTestDir, fn))
		require.NoError(t, err, "failed to get stat")
		assert.True(t, fi.IsDir(), "found unexpected file %v", fi)
		assert.Contains(t, []string{"mydata-1", "mydata-2"}, fi.Name(),
			"unexpected dir found %s", fi.Name())
	}
	reportLeakedFD(fs, t)
}

func TestWALDirCanBeSet(t *testing.T) {
	walDir := "d2-wal-dir-name"
	nhConfig := config.NodeHostConfig{
		NodeHostDir: "d1",
		WALDir:      walDir,
	}
	fs := vfs.GetTestFS()
	c, err := NewEnv(nhConfig, fs)
	require.NoError(t, err, "failed to get environment")
	defer func() {
		if err := c.Close(); err != nil {
			t.Fatalf("failed to stop the env %v", err)
		}
	}()
	dir, lldir := c.GetLogDBDirs(12345)
	assert.NotEqual(t, dir, lldir, "wal dir not considered")
	assert.Contains(t, lldir, walDir, "wal dir not used, %s", lldir)
	assert.NotContains(t, dir, walDir, "wal dir appeared in node host dir, %s", dir)
}

func TestCompatibleLogDBType(t *testing.T) {
	tests := []struct {
		saved      string
		name       string
		compatible bool
	}{
		{"sharded-rocksdb", "sharded-pebble", true},
		{"sharded-pebble", "sharded-rocksdb", true},
		{"pebble", "rocksdb", true},
		{"rocksdb", "pebble", true},
		{"pebble", "tee", false},
		{"tee", "pebble", false},
		{"rocksdb", "tee", false},
		{"tee", "rocksdb", false},
		{"tee", "tee", true},
		{"", "tee", false},
		{"tee", "", false},
		{"tee", "inmem", false},
	}
	for idx, tt := range tests {
		r := compatibleLogDBType(tt.saved, tt.name)
		assert.Equal(t, tt.compatible, r,
			"%d, compatibleLogDBType failed, want %t, got %t",
			idx, tt.compatible, r)
	}
}
````

## File: internal/server/environment.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package server

import (
	"fmt"
	"io"
	"os"
	"strings"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/random"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/id"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/utils"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/logger"
	"github.com/lni/dragonboat/v4/raftio"
	"github.com/lni/dragonboat/v4/raftpb"
)

var (
	plog = logger.GetLogger("server")
	// ErrNodeHostIDChanged indicates that NodeHostID changed.
	ErrNodeHostIDChanged = errors.New("NodeHostID changed")
	// ErrHardSettingChanged indicates that one or more of the hard settings
	// changed.
	ErrHardSettingChanged = errors.New("hard setting changed")
	// ErrDirMarkedAsDeleted is the error used to indicate that the directory has
	// been marked as deleted and can not be used again.
	ErrDirMarkedAsDeleted = errors.New("trying to use a dir marked as deleted")
	// ErrHostnameChanged is the error used to indicate that the hostname changed.
	ErrHostnameChanged = errors.New("hostname changed")
	// ErrDeploymentIDChanged is the error used to indicate that the deployment
	// ID changed.
	ErrDeploymentIDChanged = errors.New("deployment ID changed")
	// ErrDefaultNodeRegistryEnabledChanged is the error used to indicate that the
	// DefaultNodeRegistryEnabled setting has changed.
	ErrDefaultNodeRegistryEnabledChanged = errors.New("DefaultNodeRegistryEnabled changed")
	// ErrLogDBType is the error used to indicate that the LogDB type changed.
	ErrLogDBType = errors.New("logdb type changed")
	// ErrNotOwner indicates that the data directory belong to another NodeHost
	// instance.
	ErrNotOwner = errors.New("not the owner of the data directory")
	// ErrLockDirectory indicates that obtaining exclusive lock to the data
	// directory failed.
	ErrLockDirectory = errors.New("failed to lock data directory")
	// ErrHardSettingsChanged indicates that hard settings changed.
	ErrHardSettingsChanged = errors.New("internal/settings/hard.go settings changed")
	// ErrIncompatibleData indicates that the specified data directory contains
	// incompatible data.
	ErrIncompatibleData = errors.New("incompatible LogDB data format")
	// ErrLogDBBrokenChange indicates that your NodeHost failed to be created as
	// your code is hit by the LogDB breaking change introduced in v3.0. Set your
	// NodeHostConfig.LogDBFactory to rocksdb.OpenBatchedLogDB to continue.
	ErrLogDBBrokenChange = errors.New("using new LogDB on existing Raft Log")
)

const (
	flagFilename = "dragonboat.ds"
	lockFilename = "LOCK"
	idFilename   = "NODEHOST.ID"
)

var firstError = utils.FirstError

// Env is the server environment for NodeHost.
type Env struct {
	fs           vfs.IFS
	randomSource random.Source
	partitioner  IPartitioner
	nhid         *id.UUID
	flocks       map[string]io.Closer
	hostname     string
	nhConfig     config.NodeHostConfig
}

// NewEnv creates and returns a new server Env object.
func NewEnv(nhConfig config.NodeHostConfig, fs vfs.IFS) (*Env, error) {
	s := &Env{
		randomSource: random.NewLockedRand(),
		nhConfig:     nhConfig,
		partitioner:  NewFixedPartitioner(defaultShardIDMod),
		flocks:       make(map[string]io.Closer),
		fs:           fs,
	}
	hostname, err := os.Hostname()
	if err != nil {
		return nil, err
	}
	if len(hostname) == 0 {
		panic("failed to get hostname")
	}
	s.hostname = hostname
	return s, nil
}

// Close closes the environment.
func (env *Env) Close() (err error) {
	for _, fl := range env.flocks {
		err = firstError(err, fl.Close())
	}
	return err
}

// GetRandomSource returns the random source associated with the Nodehost.
func (env *Env) GetRandomSource() random.Source {
	return env.randomSource
}

// GetSnapshotDir returns the snapshot directory name.
func (env *Env) GetSnapshotDir(did uint64, shardID uint64,
	replicaID uint64) string {
	parts, _, _ := env.getSnapshotDirParts(did, shardID, replicaID)
	return env.fs.PathJoin(parts...)
}

func (env *Env) getSnapshotDirParts(did uint64,
	shardID uint64, replicaID uint64) ([]string, string, []string) {
	dd := env.getDeploymentIDSubDirName(did)
	pd := fmt.Sprintf("snapshot-part-%d", env.partitioner.GetPartitionID(shardID))
	sd := fmt.Sprintf("snapshot-%d-%d", shardID, replicaID)
	dir := env.nhConfig.NodeHostDir
	parts := make([]string, 0)
	toBeCreated := make([]string, 0)
	return append(parts, dir, env.hostname, dd, pd, sd),
		env.fs.PathJoin(dir, env.hostname, dd), append(toBeCreated, pd, sd)
}

// GetLogDBDirs returns the directory names for LogDB
func (env *Env) GetLogDBDirs(did uint64) (string, string) {
	dir, lldir := env.getDataDirs()
	didStr := env.getDeploymentIDSubDirName(did)
	dir = env.fs.PathJoin(dir, env.hostname, didStr)
	if len(env.nhConfig.WALDir) > 0 {
		lldir = env.fs.PathJoin(lldir, env.hostname, didStr)
		return dir, lldir
	}
	return dir, dir
}

func (env *Env) getDataDirs() (string, string) {
	lldir := env.nhConfig.WALDir
	dir := env.nhConfig.NodeHostDir
	if len(env.nhConfig.WALDir) > 0 {
		return dir, lldir
	}
	return dir, dir
}

// CreateNodeHostDir creates the top level dirs used by nodehost.
func (env *Env) CreateNodeHostDir(did uint64) (string, string, error) {
	dir, lldir := env.GetLogDBDirs(did)
	if err := fileutil.MkdirAll(dir, env.fs); err != nil {
		return "", "", err
	}
	if err := fileutil.MkdirAll(lldir, env.fs); err != nil {
		return "", "", err
	}
	return dir, lldir, nil
}

// CreateSnapshotDir creates the snapshot directory for the specified node.
func (env *Env) CreateSnapshotDir(did uint64,
	shardID uint64, replicaID uint64) error {
	_, path, parts := env.getSnapshotDirParts(did, shardID, replicaID)
	for _, part := range parts {
		path = env.fs.PathJoin(path, part)
		exist, err := fileutil.Exist(path, env.fs)
		if err != nil {
			return err
		}
		if !exist {
			if err := fileutil.Mkdir(path, env.fs); err != nil {
				return err
			}
		} else {
			deleted, err := fileutil.IsDirMarkedAsDeleted(path, env.fs)
			if err != nil {
				return err
			}
			if deleted {
				return ErrDirMarkedAsDeleted
			}
		}
	}
	return nil
}

// NodeHostID returns the string representation of the NodeHost ID value.
func (env *Env) NodeHostID() string {
	return env.nhid.String()
}

// PrepareNodeHostID prepares NodeHostID and stores it in the expected data
// file.
func (env *Env) PrepareNodeHostID(nhID string) (*id.UUID, error) {
	v, err := env.loadNodeHostID()
	if err != nil {
		return nil, err
	}
	if v != nil {
		// we already have NodeHostID registered
		if len(nhID) > 0 {
			n, err := id.NewUUID(nhID)
			if err != nil {
				return nil, err
			}
			if v.String() != n.String() {
				return nil, errors.Wrapf(ErrNodeHostIDChanged, "existing %s, new %s",
					v.String(), n.String())
			}
		}
		env.nhid = v
		return v, nil
	}
	// we don't have NodeHostID registered
	v = id.New()
	if len(nhID) > 0 {
		v, err = id.NewUUID(nhID)
		if err != nil {
			return nil, err
		}
	}
	if err := env.storeNodeHostID(v); err != nil {
		return nil, err
	}
	env.nhid = v
	return v, nil
}

func (env *Env) storeNodeHostID(nhID *id.UUID) error {
	dir, _ := env.getDataDirs()
	if fileutil.HasFlagFile(dir, idFilename, env.fs) {
		panic("trying to overwrite NodeHostID file")
	}
	return fileutil.CreateFlagFile(dir, idFilename, nhID, env.fs)
}

func (env *Env) loadNodeHostID() (*id.UUID, error) {
	dir, _ := env.getDataDirs()
	var storedUUID id.UUID
	if fileutil.HasFlagFile(dir, idFilename, env.fs) {
		if err := fileutil.GetFlagFileContent(dir,
			idFilename, &storedUUID, env.fs); err != nil {
			return nil, err
		}
		return &storedUUID, nil
	}
	return nil, nil
}

// SetNodeHostID sets the NodeHostID value recorded in Env. This is typically
// invoked by tests.
func (env *Env) SetNodeHostID(nhid *id.UUID) {
	if env.nhid != nil {
		panic("trying to change NodeHostID")
	}
	env.nhid = nhid
}

// CheckNodeHostDir checks whether NodeHost dir is owned by the
// current nodehost.
func (env *Env) CheckNodeHostDir(cfg config.NodeHostConfig,
	binVer uint32, dbType string) error {
	return env.checkNodeHostDir(cfg, binVer, dbType, false)
}

// CheckLogDBType checks whether LogDB type is compatible.
func (env *Env) CheckLogDBType(cfg config.NodeHostConfig,
	dbType string) error {
	return env.checkNodeHostDir(cfg, 0, dbType, true)
}

// LockNodeHostDir tries to lock the NodeHost data directories.
func (env *Env) LockNodeHostDir() error {
	dir, lldir := env.getDataDirs()
	if err := env.tryLockNodeHostDir(dir); err != nil {
		return err
	}
	if err := env.tryLockNodeHostDir(lldir); err != nil {
		return err
	}
	return nil
}

// RemoveSnapshotDir marks the node snapshot directory as removed and have all
// existing snapshots deleted.
func (env *Env) RemoveSnapshotDir(did uint64,
	shardID uint64, replicaID uint64) error {
	dir := env.GetSnapshotDir(did, shardID, replicaID)
	exist, err := fileutil.Exist(dir, env.fs)
	if err != nil {
		return err
	}
	if exist {
		if err := env.markSnapshotDirRemoved(did, shardID, replicaID); err != nil {
			return err
		}
		if err := removeSavedSnapshots(dir, env.fs); err != nil {
			return err
		}
	}
	return nil
}

func (env *Env) markSnapshotDirRemoved(did uint64, shardID uint64,
	replicaID uint64) error {
	dir := env.GetSnapshotDir(did, shardID, replicaID)
	s := &raftpb.RaftDataStatus{}
	return fileutil.MarkDirAsDeleted(dir, s, env.fs)
}

func removeSavedSnapshots(dir string, fs vfs.IFS) error {
	files, err := fs.List(dir)
	if err != nil {
		return err
	}
	for _, fn := range files {
		fi, err := fs.Stat(fs.PathJoin(dir, fn))
		if err != nil {
			return err
		}
		if !fi.IsDir() {
			continue
		}
		if SnapshotDirNameRe.Match([]byte(fi.Name())) {
			ssdir := fs.PathJoin(dir, fi.Name())
			if err := fs.RemoveAll(ssdir); err != nil {
				return err
			}
		}
	}
	return fileutil.SyncDir(dir, fs)
}

func (env *Env) checkNodeHostDir(cfg config.NodeHostConfig,
	binVer uint32, name string, dbto bool) error {
	dir, lldir := env.getDataDirs()
	if err := env.check(cfg, dir, binVer, name, dbto); err != nil {
		return err
	}
	if err := env.check(cfg, lldir, binVer, name, dbto); err != nil {
		return err
	}
	return nil
}

func (env *Env) tryLockNodeHostDir(dir string) error {
	fp := env.fs.PathJoin(dir, lockFilename)
	if _, ok := env.flocks[fp]; !ok {
		c, err := env.fs.Lock(fp)
		if err != nil {
			return ErrLockDirectory
		}
		env.flocks[fp] = c
	}
	return nil
}

func (env *Env) getDeploymentIDSubDirName(did uint64) string {
	return fmt.Sprintf("%020d", did)
}

func compatibleLogDBType(saved string, name string) bool {
	if saved == name {
		return true
	}
	return (saved == "rocksdb" && name == "pebble") ||
		(saved == "pebble" && name == "rocksdb") ||
		(saved == "sharded-pebble" && name == "sharded-rocksdb") ||
		(saved == "sharded-rocksdb" && name == "sharded-pebble")
}

func (env *Env) check(cfg config.NodeHostConfig,
	dir string, binVer uint32, name string, dbto bool) error {
	fn := flagFilename
	fp := env.fs.PathJoin(dir, fn)
	se := func(s1 string, s2 string) bool {
		return strings.EqualFold(strings.TrimSpace(s1), strings.TrimSpace(s2))
	}
	if _, err := env.fs.Stat(fp); vfs.IsNotExist(err) {
		if dbto {
			return nil
		}
		return env.createFlagFile(cfg, dir, binVer, name)
	}
	s := raftpb.RaftDataStatus{}
	if err := fileutil.GetFlagFileContent(dir, fn, &s, env.fs); err != nil {
		return err
	}
	if !compatibleLogDBType(s.LogdbType, name) {
		return ErrLogDBType
	}
	if !dbto {
		if !cfg.NodeRegistryEnabled() && !se(s.Address, cfg.RaftAddress) {
			return ErrNotOwner
		}
		if len(s.Hostname) > 0 && !se(s.Hostname, env.hostname) {
			return ErrHostnameChanged
		}
		if s.DeploymentId != 0 && s.DeploymentId != cfg.GetDeploymentID() {
			return ErrDeploymentIDChanged
		}
		if s.AddressByNodeHostId != cfg.DefaultNodeRegistryEnabled {
			return ErrDefaultNodeRegistryEnabledChanged
		}
		if s.BinVer != binVer {
			if s.BinVer == raftio.LogDBBinVersion &&
				binVer == raftio.PlainLogDBBinVersion {
				return ErrLogDBBrokenChange
			}
			plog.Errorf("logdb binary ver changed, %d vs %d", s.BinVer, binVer)
			return ErrIncompatibleData
		}
		if s.HardHash != 0 {
			if s.HardHash != settings.HardHash(cfg.Expert.Engine.ExecShards,
				cfg.Expert.LogDB.Shards, settings.Hard.LRUMaxSessionCount,
				settings.Hard.LogDBEntryBatchSize) {
				return ErrHardSettingsChanged
			}
		} else {
			if s.StepWorkerCount != cfg.Expert.Engine.ExecShards ||
				s.LogdbShardCount != cfg.Expert.LogDB.Shards ||
				s.MaxSessionCount != settings.Hard.LRUMaxSessionCount ||
				s.EntryBatchSize != settings.Hard.LogDBEntryBatchSize {
				return ErrHardSettingChanged
			}
		}
	}
	return nil
}

func (env *Env) createFlagFile(cfg config.NodeHostConfig,
	dir string, ver uint32, name string) error {
	s := raftpb.RaftDataStatus{
		Address:             cfg.RaftAddress,
		BinVer:              ver,
		HardHash:            0,
		LogdbType:           name,
		Hostname:            env.hostname,
		DeploymentId:        cfg.GetDeploymentID(),
		StepWorkerCount:     cfg.Expert.Engine.ExecShards,
		LogdbShardCount:     cfg.Expert.LogDB.Shards,
		MaxSessionCount:     settings.Hard.LRUMaxSessionCount,
		EntryBatchSize:      settings.Hard.LogDBEntryBatchSize,
		AddressByNodeHostId: cfg.DefaultNodeRegistryEnabled,
	}
	return fileutil.CreateFlagFile(dir, flagFilename, &s, env.fs)
}
````

## File: internal/server/event.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package server

import (
	pb "github.com/lni/dragonboat/v4/raftpb"
)

const (
	// NoLeader is the flag used to indcate that there is no leader or the leader
	// is unknown.
	NoLeader uint64 = 0
)

// LeaderInfo contains leader info.
type LeaderInfo struct {
	ShardID   uint64
	ReplicaID uint64
	Term      uint64
	LeaderID  uint64
}

// CampaignInfo contains campaign info.
type CampaignInfo struct {
	ShardID   uint64
	ReplicaID uint64
	PreVote   bool
	Term      uint64
}

// SnapshotInfo contains info of a snapshot.
type SnapshotInfo struct {
	ShardID   uint64
	ReplicaID uint64
	From      uint64
	Index     uint64
	Term      uint64
}

// ReplicationInfo contains info of a replication message.
type ReplicationInfo struct {
	ShardID   uint64
	ReplicaID uint64
	From      uint64
	Index     uint64
	Term      uint64
}

// ProposalInfo contains info on proposals.
type ProposalInfo struct {
	Entries   []pb.Entry
	ShardID   uint64
	ReplicaID uint64
}

// ReadIndexInfo contains info on read index requests.
type ReadIndexInfo struct {
	ShardID   uint64
	ReplicaID uint64
}

// IRaftEventListener is the event listener used by the Raft implementation.
type IRaftEventListener interface {
	LeaderUpdated(info LeaderInfo)
	CampaignLaunched(info CampaignInfo)
	CampaignSkipped(info CampaignInfo)
	SnapshotRejected(info SnapshotInfo)
	ReplicationRejected(info ReplicationInfo)
	ProposalDropped(info ProposalInfo)
	ReadIndexDropped(info ReadIndexInfo)
}

// SystemEventType is the type of system events.
type SystemEventType uint64

const (
	// NodeHostShuttingDown ...
	NodeHostShuttingDown SystemEventType = iota
	// NodeReady ...
	NodeReady
	// NodeUnloaded ...
	NodeUnloaded
	// NodeDeleted
	NodeDeleted
	// MembershipChanged ...
	MembershipChanged
	// ConnectionEstablished ...
	ConnectionEstablished
	// ConnectionFailed ...
	ConnectionFailed
	// SendSnapshotStarted ...
	SendSnapshotStarted
	// SendSnapshotCompleted ...
	SendSnapshotCompleted
	// SendSnapshotAborted ...
	SendSnapshotAborted
	// SnapshotReceived ...
	SnapshotReceived
	// SnapshotRecovered ...
	SnapshotRecovered
	// SnapshotCreated ...
	SnapshotCreated
	// SnapshotCompacted ...
	SnapshotCompacted
	// LogCompacted ...
	LogCompacted
	// LogDBCompacted ...
	LogDBCompacted
)

// SystemEvent is an system event record published by the system that can be
// handled by a raftio.ISystemEventListener.
type SystemEvent struct {
	Address            string
	Type               SystemEventType
	ShardID            uint64
	ReplicaID          uint64
	From               uint64
	Index              uint64
	SnapshotConnection bool
}
````

## File: internal/server/message_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package server

import (
	"testing"

	"github.com/stretchr/testify/require"

	pb "github.com/lni/dragonboat/v4/raftpb"
)

func TestMessageQueueCanBeCreated(t *testing.T) {
	q := NewMessageQueue(8, false, 0, 0)
	require.Equal(t, 8, len(q.left))
	require.Equal(t, 8, len(q.right))
}

func TestMessageCanBeAddedAndGet(t *testing.T) {
	q := NewMessageQueue(8, false, 0, 0)
	for i := 0; i < 8; i++ {
		added, stopped := q.Add(pb.Message{})
		require.True(t, added)
		require.False(t, stopped)
	}
	add, stopped := q.Add(pb.Message{})
	add2, stopped2 := q.Add(pb.Message{})
	require.False(t, add)
	require.False(t, add2)
	require.False(t, stopped)
	require.False(t, stopped2)
	require.Equal(t, uint64(8), q.idx)
	lr := q.leftInWrite
	q.Get()
	require.Equal(t, uint64(0), q.idx)
	require.NotEqual(t, lr, q.leftInWrite)
	add, stopped = q.Add(pb.Message{})
	add2, stopped2 = q.Add(pb.Message{})
	require.True(t, add)
	require.True(t, add2)
	require.False(t, stopped)
	require.False(t, stopped2)
}

func TestNonSnapshotMsgByCallingMustAddWillPanic(t *testing.T) {
	q := NewMessageQueue(8, false, 0, 0)
	require.Panics(t, func() {
		q.MustAdd(pb.Message{})
	})
}

func TestSnapshotCanAlwaysBeAdded(t *testing.T) {
	q := NewMessageQueue(8, false, 0, 0)
	for i := 0; i < 1024; i++ {
		n := len(q.nodrop)
		added := q.MustAdd(pb.Message{Type: pb.InstallSnapshot})
		require.True(t, added)
		require.Equal(t, n+1, len(q.nodrop))
	}
}

func TestUnreachableMsgCanAlwaysBeAdded(t *testing.T) {
	q := NewMessageQueue(8, false, 0, 0)
	for i := 0; i < 1024; i++ {
		n := len(q.nodrop)
		added := q.MustAdd(pb.Message{Type: pb.Unreachable})
		require.True(t, added)
		require.Equal(t, n+1, len(q.nodrop))
	}
}

func TestAddedSnapshotWillBeReturned(t *testing.T) {
	q := NewMessageQueue(8, false, 0, 0)
	added := q.MustAdd(pb.Message{Type: pb.InstallSnapshot})
	require.True(t, added)
	for i := 0; i < 4; i++ {
		added, stopped := q.Add(pb.Message{})
		require.True(t, added)
		require.False(t, stopped)
	}
	added = q.MustAdd(pb.Message{Type: pb.InstallSnapshot})
	require.True(t, added)
	for i := 0; i < 4; i++ {
		added, stopped := q.Add(pb.Message{})
		require.True(t, added)
		require.False(t, stopped)
	}
	added = q.MustAdd(pb.Message{Type: pb.InstallSnapshot})
	require.True(t, added)
	msgs := q.Get()
	require.Equal(t, 11, len(msgs))
	count := 0
	for _, msg := range msgs {
		if msg.Type == pb.InstallSnapshot {
			count++
		}
	}
	require.Equal(t, 3, count)
	require.Equal(t, 0, len(q.nodrop))
}

func TestMessageQueueCanBeStopped(t *testing.T) {
	q := NewMessageQueue(8, false, 0, 0)
	q.Close()
	for i := 0; i < 4; i++ {
		added, stopped := q.Add(pb.Message{})
		require.False(t, added)
		require.True(t, stopped)
	}
	added := q.MustAdd(pb.Message{Type: pb.InstallSnapshot})
	require.False(t, added)
}

func TestRateLimiterCanBeEnabledInMessageQueue(t *testing.T) {
	q := NewMessageQueue(8, false, 0, 0)
	require.False(t, q.rl.Enabled())
	q = NewMessageQueue(8, false, 0, 1024)
	require.True(t, q.rl.Enabled())
}

func TestSingleMessageCanAlwaysBeAdded(t *testing.T) {
	q := NewMessageQueue(10000, false, 0, 1024)
	e := pb.Entry{Index: 1, Cmd: make([]byte, 2048)}
	m := pb.Message{
		Type:    pb.Replicate,
		Entries: []pb.Entry{e},
	}
	added, stopped := q.Add(m)
	require.True(t, added)
	require.False(t, stopped)
	require.True(t, q.rl.RateLimited())
}

func TestAddMessageIsRateLimited(t *testing.T) {
	q := NewMessageQueue(10000, false, 0, 1024)
	for i := 0; i < 10000; i++ {
		e := pb.Entry{Index: uint64(i + 1)}
		m := pb.Message{
			Type:    pb.Replicate,
			Entries: []pb.Entry{e},
		}
		if q.rl.RateLimited() {
			added, stopped := q.Add(m)
			if !added && !stopped {
				return
			}
		} else {
			sz := q.rl.Get()
			added, stopped := q.Add(m)
			if added {
				expected := sz + pb.GetEntrySliceInMemSize([]pb.Entry{e})
				require.Equal(t, expected, q.rl.Get())
			}
			require.True(t, added)
			require.False(t, stopped)
		}
	}
	require.Fail(t, "failed to observe any rate limited message")
}

func TestGetWillResetTheRateLimiterSize(t *testing.T) {
	q := NewMessageQueue(10000, false, 0, 1024)
	for i := 0; i < 8; i++ {
		e := pb.Entry{Index: uint64(i + 1)}
		m := pb.Message{
			Type:    pb.Replicate,
			Entries: []pb.Entry{e},
		}
		added, stopped := q.Add(m)
		require.True(t, added || stopped)
	}
	require.NotEqual(t, 0, q.rl.Get())
	q.Get()
	require.Equal(t, uint64(0), q.rl.Get())
}

func TestGetDelayed(t *testing.T) {
	q := NewMessageQueue(10, false, 0, 1024)
	q.AddDelayed(pb.Message{From: 1, Type: pb.SnapshotStatus}, 2)
	q.AddDelayed(pb.Message{From: 2, Type: pb.SnapshotStatus}, 10)
	require.Equal(t, 2, len(q.delayed))
	result := q.getDelayed()
	require.Equal(t, 0, len(result))
	q.Tick()
	q.Tick()
	q.Tick()
	result = q.getDelayed()
	require.Equal(t, 1, len(result))
	require.Equal(t, 1, len(q.delayed))
	require.Equal(t, uint64(1), result[0].From)
	require.Equal(t, uint64(2), q.delayed[0].m.From)
}

func TestDelayedMessage(t *testing.T) {
	q := NewMessageQueue(10, false, 0, 1024)
	rm := pb.Message{}
	mm := pb.Message{Type: pb.InstallSnapshot}
	q.Add(rm)
	q.MustAdd(mm)
	dm1 := pb.Message{From: 1, Type: pb.SnapshotStatus}
	dm2 := pb.Message{From: 2, Type: pb.SnapshotStatus}
	q.AddDelayed(dm1, 2)
	q.AddDelayed(dm2, 10)
	q.Tick()
	q.Tick()
	q.Tick()
	result := q.Get()
	require.Equal(t, 1, len(q.delayed))
	require.Equal(t, dm2, q.delayed[0].m)
	require.Equal(t, 3, len(result))
	require.Equal(t, mm, result[0])
	require.Equal(t, dm1, result[1])
	require.Equal(t, rm, result[2])
}
````

## File: internal/server/message.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package server

import (
	"sync"
	"sync/atomic"

	pb "github.com/lni/dragonboat/v4/raftpb"
)

type delayed struct {
	m    pb.Message
	tick uint64
}

// MessageQueue is the queue used to hold Raft messages.
type MessageQueue struct {
	ch            chan struct{}
	rl            *RateLimiter
	left          []pb.Message
	right         []pb.Message
	nodrop        []pb.Message
	delayed       []delayed
	tick          uint64
	cycle         uint64
	size          uint64
	lazyFreeCycle uint64
	idx           uint64
	oldIdx        uint64
	mu            sync.Mutex
	stopped       bool
	leftInWrite   bool
}

// NewMessageQueue creates a new MessageQueue instance.
func NewMessageQueue(size uint64,
	ch bool, lazyFreeCycle uint64, maxMemorySize uint64) *MessageQueue {
	q := &MessageQueue{
		rl:            NewRateLimiter(maxMemorySize),
		size:          size,
		lazyFreeCycle: lazyFreeCycle,
		left:          make([]pb.Message, size),
		right:         make([]pb.Message, size),
		nodrop:        make([]pb.Message, 0),
		delayed:       make([]delayed, 0),
	}
	if ch {
		q.ch = make(chan struct{}, 1)
	}
	return q
}

// Tick increases the internal tick value.
func (q *MessageQueue) Tick() {
	atomic.AddUint64(&q.tick, 1)
}

// Close closes the queue so no further messages can be added.
func (q *MessageQueue) Close() {
	q.mu.Lock()
	defer q.mu.Unlock()
	q.stopped = true
}

// Notify notifies the notification channel listener that a new message is now
// available in the queue.
func (q *MessageQueue) Notify() {
	if q.ch != nil {
		select {
		case q.ch <- struct{}{}:
		default:
		}
	}
}

// Ch returns the notification channel.
func (q *MessageQueue) Ch() <-chan struct{} {
	return q.ch
}

func (q *MessageQueue) targetQueue() []pb.Message {
	var t []pb.Message
	if q.leftInWrite {
		t = q.left
	} else {
		t = q.right
	}
	return t
}

// Add adds the specified message to the queue.
func (q *MessageQueue) Add(msg pb.Message) (bool, bool) {
	q.mu.Lock()
	defer q.mu.Unlock()
	if q.idx >= q.size {
		return false, q.stopped
	}
	if q.stopped {
		return false, true
	}
	if !q.tryAdd(msg) {
		return false, false
	}
	w := q.targetQueue()
	w[q.idx] = msg
	q.idx++
	return true, false
}

// AddDelayed adds the specified message to the queue and makes sure that the
// message will stay in the queue for at least delay ticks.
func (q *MessageQueue) AddDelayed(msg pb.Message, delay uint64) bool {
	if msg.Type != pb.SnapshotStatus {
		panic("not a snapshot status message")
	}
	tick := atomic.LoadUint64(&q.tick)
	q.mu.Lock()
	defer q.mu.Unlock()
	if q.stopped {
		return false
	}
	q.delayed = append(q.delayed, delayed{msg, delay + tick})
	return true
}

// MustAdd adds the specified message to the queue.
func (q *MessageQueue) MustAdd(msg pb.Message) bool {
	if msg.CanDrop() {
		panic("not a snapshot or unreachable message")
	}
	q.mu.Lock()
	defer q.mu.Unlock()
	if q.stopped {
		return false
	}
	q.nodrop = append(q.nodrop, msg)
	return true
}

func (q *MessageQueue) tryAdd(msg pb.Message) bool {
	if !q.rl.Enabled() || msg.Type != pb.Replicate {
		return true
	}
	if q.rl.RateLimited() {
		plog.Warningf("rate limited dropped a Replicate msg from %d", msg.ShardID)
		return false
	}
	q.rl.Increase(pb.GetEntrySliceInMemSize(msg.Entries))
	return true
}

func (q *MessageQueue) gc() {
	if q.lazyFreeCycle > 0 {
		oldq := q.targetQueue()
		if q.lazyFreeCycle == 1 {
			for i := uint64(0); i < q.oldIdx; i++ {
				oldq[i].Entries = nil
			}
		} else if q.cycle%q.lazyFreeCycle == 0 {
			for i := uint64(0); i < q.size; i++ {
				oldq[i].Entries = nil
			}
		}
	}
}

func (q *MessageQueue) getDelayed() []pb.Message {
	if len(q.delayed) == 0 {
		return nil
	}
	sz := len(q.delayed)
	var result []pb.Message
	tick := atomic.LoadUint64(&q.tick)
	for idx, rec := range q.delayed {
		if rec.tick < tick {
			result = append(result, rec.m)
		} else {
			q.delayed[idx-len(result)] = rec
		}
	}
	if len(result) > 0 {
		q.delayed = q.delayed[:sz-len(result)]
	}
	return result
}

// Get returns everything current in the queue.
func (q *MessageQueue) Get() []pb.Message {
	q.mu.Lock()
	defer q.mu.Unlock()
	q.cycle++
	sz := q.idx
	q.idx = 0
	t := q.targetQueue()
	q.leftInWrite = !q.leftInWrite
	q.gc()
	q.oldIdx = sz
	if q.rl.Enabled() {
		q.rl.Set(0)
	}
	if len(q.nodrop) == 0 && len(q.delayed) == 0 {
		return t[:sz]
	}

	var result []pb.Message
	if len(q.nodrop) > 0 {
		ssm := q.nodrop
		q.nodrop = make([]pb.Message, 0)
		result = append(result, ssm...)
	}
	delayed := q.getDelayed()
	if len(delayed) > 0 {
		result = append(result, delayed...)
	}
	return append(result, t[:sz]...)
}
````

## File: internal/server/partition.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package server

var (
	defaultShardIDMod uint64 = 512
)

// IPartitioner is the interface for partitioning shards.
type IPartitioner interface {
	GetPartitionID(shardID uint64) uint64
}

// FixedPartitioner is the IPartitioner with fixed capacity and naive
// partitioning strategy.
type FixedPartitioner struct {
	capacity uint64
}

// NewFixedPartitioner creates a new FixedPartitioner instance.
func NewFixedPartitioner(capacity uint64) *FixedPartitioner {
	return &FixedPartitioner{capacity: capacity}
}

// GetPartitionID returns the partition ID for the specified raft shard.
func (p *FixedPartitioner) GetPartitionID(shardID uint64) uint64 {
	return shardID % p.capacity
}

// DoubleFixedPartitioner is the IPartitioner with two fixed capacity and naive
// partitioning strategy.
type DoubleFixedPartitioner struct {
	capacity    uint64
	workerCount uint64
}

// NewDoubleFixedPartitioner creates a new DoubleFixedPartitioner instance.
func NewDoubleFixedPartitioner(capacity uint64,
	workerCount uint64) *DoubleFixedPartitioner {
	return &DoubleFixedPartitioner{
		capacity:    capacity,
		workerCount: workerCount,
	}
}

// GetPartitionID returns the partition ID for the specified raft shard.
func (p *DoubleFixedPartitioner) GetPartitionID(shardID uint64) uint64 {
	return (shardID % p.workerCount) % p.capacity
}
````

## File: internal/server/rate_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package server

import (
	"math"
	"testing"

	"github.com/stretchr/testify/require"
)

func TestRateLimiterCanBeEnabled(t *testing.T) {
	tests := []struct {
		maxSize uint64
		enabled bool
	}{
		{0, false},
		{math.MaxUint64, false},
		{1, true},
		{math.MaxUint64 - 1, true},
	}
	for idx, tt := range tests {
		r := NewInMemRateLimiter(tt.maxSize)
		require.Equal(t, tt.enabled, r.Enabled(),
			"%d, enabled %t, want %t", idx, r.Enabled(), tt.enabled)
	}
}

func TestInMemLogSizeIsAccessible(t *testing.T) {
	r := NewInMemRateLimiter(100)
	require.Equal(t, uint64(0), r.Get(), "sz %d, want 0", r.Get())
	r.Increase(100)
	require.Equal(t, uint64(100), r.Get(), "sz %d, want 100", r.Get())
	r.Decrease(10)
	require.Equal(t, uint64(90), r.Get(), "sz %d, want 90", r.Get())
	r.Set(243)
	require.Equal(t, uint64(243), r.Get(), "sz %d, want 243", r.Get())
}

func TestRateLimiterTick(t *testing.T) {
	r := NewInMemRateLimiter(100)
	for i := 0; i < 100; i++ {
		r.Tick()
		require.Equal(t, uint64(i+2), r.tick,
			"tick %d, want %d", r.tick, i+2)
	}
}

func TestFollowerStateCanBeSet(t *testing.T) {
	r := NewInMemRateLimiter(100)
	r.SetFollowerState(100, 1)
	r.SetFollowerState(101, 2)
	r.Tick()
	r.Tick()
	r.SetFollowerState(101, 4)
	r.SetFollowerState(102, 200)
	require.Equal(t, 3, len(r.followerSizes), "not all state recorded")
	tests := []struct {
		replicaID uint64
		v         uint64
		tick      uint64
	}{
		{100, 1, 0},
		{101, 4, 2},
		{102, 200, 2},
	}
	for idx, tt := range tests {
		rec, ok := r.followerSizes[tt.replicaID]
		require.True(t, ok, "%d, state not found", idx)
		require.Equal(t, tt.v, rec.inMemLogSize,
			"%d, v %d, want %d", idx, rec.inMemLogSize, tt.v)
		require.Equal(t, tt.tick+1, rec.tick,
			"%d, tick %d, want %d", idx, rec.tick, tt.tick+1)
	}
}

func TestGCRemoveOutOfDateFollowerState(t *testing.T) {
	r := NewInMemRateLimiter(100)
	r.SetFollowerState(101, 1)
	r.Tick()
	r.SetFollowerState(102, 2)
	r.SetFollowerState(103, 3)
	r.gc()
	require.Equal(t, 3, len(r.followerSizes),
		"count %d, want 3", len(r.followerSizes))
	for i := uint64(0); i < gcTick; i++ {
		r.Tick()
	}
	r.gc()
	require.Equal(t, 2, len(r.followerSizes),
		"count %d, want 2", len(r.followerSizes))
	_, ok := r.followerSizes[101]
	require.False(t, ok, "old follower state not removed")
	r.Tick()
	r.gc()
	require.Equal(t, 0, len(r.followerSizes),
		"count %d, want 0", len(r.followerSizes))
}

func TestRateLimited(t *testing.T) {
	r := NewInMemRateLimiter(100)
	r.Increase(100)
	require.False(t, r.RateLimited(), "unexpectedly rate limited")
	r.Increase(1)
	require.True(t, r.RateLimited(), "not rate limited")
}

func TestRateLimitedWhenFollowerIsRateLimited(t *testing.T) {
	r := NewInMemRateLimiter(100)
	r.Increase(100)
	require.False(t, r.RateLimited(), "unexpectedly rate limited")
	r.SetFollowerState(1, 100)
	r.SetFollowerState(2, 101)
	require.True(t, r.RateLimited(), "not rate limited")
}

func TestRateNotLimitedWhenOutOfDateFollowerStateIsLimited(t *testing.T) {
	r := NewInMemRateLimiter(100)
	r.Increase(100)
	require.False(t, r.RateLimited(), "unexpectedly rate limited")
	r.SetFollowerState(1, 100)
	r.SetFollowerState(2, 101)
	r.Tick()
	r.Tick()
	r.Tick()
	r.Tick()
	require.False(t, r.RateLimited(), "unexpectedly rate limited")
	require.Equal(t, 0, len(r.followerSizes),
		"out of date follower state not GCed")
}

func TestNotEnabledRateLimitNeverLimitRates(t *testing.T) {
	r := NewInMemRateLimiter(0)
	for i := 0; i < 10000; i++ {
		r.Increase(math.MaxUint64 / 2)
		require.False(t, r.RateLimited(), "unexpectedly rate limited")
	}
}

func TestResetFollowerState(t *testing.T) {
	rl := NewInMemRateLimiter(1024)
	rl.SetFollowerState(1, 1025)
	require.True(t, rl.RateLimited(), "not rate limited as expected")
	rl.Reset()
	for i := uint64(0); i <= ChangeTickThreashold; i++ {
		rl.Tick()
	}
	require.False(t, rl.RateLimited(), "unexpectedly rate limited")
}

func TestUnlimitedThreshild(t *testing.T) {
	r := NewInMemRateLimiter(100)
	r.Increase(101)
	require.True(t, r.RateLimited(), "unexpectedly not rate limited")
	for i := uint64(0); i <= ChangeTickThreashold; i++ {
		r.Tick()
	}
	r.Set(99)
	require.True(t, r.RateLimited(), "unexpectedly not rate limited")
	r.Set(70)
	require.True(t, r.RateLimited(), "unexpectedly not rate limited")
	r.Set(69)
	require.False(t, r.RateLimited(), "unexpectedly rate limited")
}

func TestRateLimitChangeCantChangeVeryOften(t *testing.T) {
	r := NewInMemRateLimiter(100)
	r.Increase(101)
	for i := uint64(0); i <= ChangeTickThreashold; i++ {
		r.Tick()
	}
	require.True(t, r.RateLimited(), "unexpectedly not rate limited")
	r.Set(69)
	require.True(t, r.RateLimited(), "unexpectedly not rate limited")
	for i := uint64(0); i <= ChangeTickThreashold; i++ {
		r.Tick()
	}
	require.False(t, r.RateLimited(), "unexpectedly rate limited")
}
````

## File: internal/server/rate.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package server

import (
	"math"
	"sync/atomic"
)

const (
	gcTick uint64 = 3
	// ChangeTickThreashold is the minimum number of ticks required to update
	// the state of the rate limiter.
	ChangeTickThreashold uint64 = 10
)

type followerState struct {
	tick         uint64
	inMemLogSize uint64
}

// RateLimiter is the struct used to keep tracking consumed memory size.
type RateLimiter struct {
	size    uint64
	maxSize uint64
}

// NewRateLimiter creates and returns a rate limiter instance.
func NewRateLimiter(max uint64) *RateLimiter {
	return &RateLimiter{
		maxSize: max,
	}
}

// Enabled returns a boolean flag indicating whether the rate limiter is
// enabled.
func (r *RateLimiter) Enabled() bool {
	return r.maxSize > 0 && r.maxSize != math.MaxUint64
}

// Increase increases the recorded in memory log size by sz bytes.
func (r *RateLimiter) Increase(sz uint64) {
	atomic.AddUint64(&r.size, sz)
}

// Decrease decreases the recorded in memory log size by sz bytes.
func (r *RateLimiter) Decrease(sz uint64) {
	atomic.AddUint64(&r.size, ^(sz - 1))
}

// Set sets the recorded in memory log size to sz bytes.
func (r *RateLimiter) Set(sz uint64) {
	atomic.StoreUint64(&r.size, sz)
}

// Get returns the recorded in memory log size.
func (r *RateLimiter) Get() uint64 {
	return atomic.LoadUint64(&r.size)
}

// RateLimited returns a boolean flag indicating whether the node is rate
// limited.
func (r *RateLimiter) RateLimited() bool {
	if !r.Enabled() {
		return false
	}
	v := r.Get()
	if v > r.maxSize {
		plog.Infof("rate limited, v: %d, maxSize %d", v, r.maxSize)
		return true
	}
	return false
}

// InMemRateLimiter is the struct used to keep tracking the in memory rate log size.
type InMemRateLimiter struct {
	followerSizes map[uint64]followerState
	rl            RateLimiter
	tick          uint64
	tickLimited   uint64
	limited       bool
}

// NewInMemRateLimiter creates and returns a rate limiter instance.
func NewInMemRateLimiter(maxSize uint64) *InMemRateLimiter {
	return &InMemRateLimiter{
		// so tickLimited won't be 0
		tick:          1,
		rl:            RateLimiter{maxSize: maxSize},
		followerSizes: make(map[uint64]followerState),
	}
}

// Enabled returns a boolean flag indicating whether the rate limiter is
// enabled.
func (r *InMemRateLimiter) Enabled() bool {
	return r.rl.Enabled()
}

// Tick advances the internal logical clock.
func (r *InMemRateLimiter) Tick() {
	r.tick++
}

// GetTick returns the internal logical clock value.
func (r *InMemRateLimiter) GetTick() uint64 {
	return r.tick
}

// Increase increases the recorded in memory log size by sz bytes.
func (r *InMemRateLimiter) Increase(sz uint64) {
	r.rl.Increase(sz)
}

// Decrease decreases the recorded in memory log size by sz bytes.
func (r *InMemRateLimiter) Decrease(sz uint64) {
	r.rl.Decrease(sz)
}

// Set sets the recorded in memory log size to sz bytes.
func (r *InMemRateLimiter) Set(sz uint64) {
	r.rl.Set(sz)
}

// Get returns the recorded in memory log size.
func (r *InMemRateLimiter) Get() uint64 {
	return r.rl.Get()
}

// Reset clears all recorded follower states.
func (r *InMemRateLimiter) Reset() {
	r.followerSizes = make(map[uint64]followerState)
}

// SetFollowerState sets the follower rate identiified by replicaID to sz bytes.
func (r *InMemRateLimiter) SetFollowerState(replicaID uint64, sz uint64) {
	r.followerSizes[replicaID] = followerState{
		tick:         r.tick,
		inMemLogSize: sz,
	}
}

// RateLimited returns a boolean flag indicating whether the node is rate
// limited.
func (r *InMemRateLimiter) RateLimited() bool {
	limited := r.limitedByInMemSize()
	if limited != r.limited {
		if r.tickLimited == 0 || r.tick-r.tickLimited > ChangeTickThreashold {
			r.limited = limited
			r.tickLimited = r.tick
		}
	}
	return r.limited
}

func (r *InMemRateLimiter) limitedByInMemSize() bool {
	if !r.Enabled() {
		return false
	}
	maxInMemSize := uint64(0)
	gc := false
	for _, v := range r.followerSizes {
		if r.tick-v.tick > gcTick {
			gc = true
			continue
		}
		if v.inMemLogSize > maxInMemSize {
			maxInMemSize = v.inMemLogSize
		}
	}
	sz := r.Get()
	if sz > maxInMemSize {
		maxInMemSize = sz
	}
	if gc {
		r.gc()
	}
	if !r.limited {
		return maxInMemSize > r.rl.maxSize
	}
	return maxInMemSize >= (r.rl.maxSize * 7 / 10)
}

func (r *InMemRateLimiter) gc() {
	followerStates := make(map[uint64]followerState)
	for nid, v := range r.followerSizes {
		if r.tick-v.tick > gcTick {
			continue
		}
		followerStates[nid] = v
	}
	r.followerSizes = followerStates
}
````

## File: internal/server/snapshotenv_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package server

import (
	"testing"

	"github.com/lni/dragonboat/v4/internal/vfs"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

func reportLeakedFD(fs vfs.IFS, t *testing.T) {
	vfs.ReportLeakedFD(fs, t)
}

func TestGetSnapshotDirName(t *testing.T) {
	v := GetSnapshotDirName(1)
	assert.Equal(t, "snapshot-0000000000000001", v)
	v = GetSnapshotDirName(255)
	assert.Equal(t, "snapshot-00000000000000FF", v)
}

func TestMustBeChild(t *testing.T) {
	tests := []struct {
		parent string
		child  string
		ok     bool
	}{
		{"/home/test", "/home", false},
		{"/home/test", "/home/test", false},
		{"/home/test", "/home/data", false},
		{"/home/test", "/home/test1", false},
		{"/home/test", "/home/test/data", true},
		{"/home/test", "", false},
	}
	for _, tt := range tests {
		parent := tt.parent
		child := tt.child
		expectedOk := tt.ok

		if expectedOk {
			assert.NotPanics(t, func() {
				mustBeChild(parent, child)
			})
		} else {
			require.Panics(t, func() {
				mustBeChild(parent, child)
			})
		}
	}
}

func TestTempSuffix(t *testing.T) {
	f := func(cid uint64, nid uint64) string {
		return "/data"
	}
	fs := vfs.GetTestFS()
	env := NewSSEnv(f, 1, 1, 1, 2, SnapshotMode, fs)
	dir := env.GetTempDir()
	assert.Contains(t, dir, ".generating")
	env = NewSSEnv(f, 1, 1, 1, 2, ReceivingMode, fs)
	dir = env.GetTempDir()
	assert.Contains(t, dir, ".receiving")
	reportLeakedFD(fs, t)
}

func TestFinalSnapshotDirDoesNotContainTempSuffix(t *testing.T) {
	f := func(cid uint64, nid uint64) string {
		return "/data"
	}
	fs := vfs.GetTestFS()
	env := NewSSEnv(f, 1, 1, 1, 2, SnapshotMode, fs)
	dir := env.GetFinalDir()
	assert.NotContains(t, dir, ".generating")
}

func TestRootDirIsTheParentOfTempFinalDirs(t *testing.T) {
	f := func(cid uint64, nid uint64) string {
		return "/data"
	}
	fs := vfs.GetTestFS()
	env := NewSSEnv(f, 1, 1, 1, 2, SnapshotMode, fs)
	tmpDir := env.GetTempDir()
	finalDir := env.GetFinalDir()
	rootDir := env.GetRootDir()
	assert.NotPanics(t, func() {
		mustBeChild(rootDir, tmpDir)
	})
	assert.NotPanics(t, func() {
		mustBeChild(rootDir, finalDir)
	})
	reportLeakedFD(fs, t)
}

func runEnvTest(t *testing.T, f func(t *testing.T, env SSEnv),
	fs vfs.IFS) {
	rd := "server-pkg-test-data-safe-to-delete"
	defer func() {
		err := fs.RemoveAll(rd)
		require.NoError(t, err)
	}()
	func() {
		ff := func(cid uint64, nid uint64) string {
			return rd
		}
		env := NewSSEnv(ff, 1, 1, 1, 2, SnapshotMode, fs)
		tmpDir := env.GetTempDir()
		err := fs.MkdirAll(tmpDir, 0755)
		require.NoError(t, err)
		f(t, env)
	}()
	reportLeakedFD(fs, t)
}

func TestRenameTempDirToFinalDir(t *testing.T) {
	tf := func(t *testing.T, env SSEnv) {
		err := env.renameToFinalDir()
		require.NoError(t, err)
	}
	fs := vfs.GetTestFS()
	runEnvTest(t, tf, fs)
}

func TestRenameTempDirToFinalDirCanComplete(t *testing.T) {
	tf := func(t *testing.T, env SSEnv) {
		assert.False(t, env.finalDirExists())
		err := env.renameToFinalDir()
		require.NoError(t, err)
		assert.True(t, env.finalDirExists())
		assert.False(t, env.HasFlagFile())
	}
	fs := vfs.GetTestFS()
	runEnvTest(t, tf, fs)
}

func TestFlagFileExists(t *testing.T) {
	tf := func(t *testing.T, env SSEnv) {
		assert.False(t, env.finalDirExists())
		msg := &pb.Message{}
		err := env.createFlagFile(msg)
		require.NoError(t, err)
		err = env.renameToFinalDir()
		require.NoError(t, err)
		assert.True(t, env.finalDirExists())
		assert.True(t, env.HasFlagFile())
	}
	fs := vfs.GetTestFS()
	runEnvTest(t, tf, fs)
}

func TestFinalizeSnapshotCanComplete(t *testing.T) {
	tf := func(t *testing.T, env SSEnv) {
		m := &pb.Message{}
		err := env.FinalizeSnapshot(m)
		require.NoError(t, err)
		assert.True(t, env.HasFlagFile())
		assert.True(t, env.finalDirExists())
	}
	fs := vfs.GetTestFS()
	runEnvTest(t, tf, fs)
}

func TestFinalizeSnapshotReturnOutOfDateWhenFinalDirExist(t *testing.T) {
	tf := func(t *testing.T, env SSEnv) {
		finalDir := env.GetFinalDir()
		err := env.fs.MkdirAll(finalDir, 0755)
		require.NoError(t, err)
		m := &pb.Message{}
		err = env.FinalizeSnapshot(m)
		assert.Equal(t, ErrSnapshotOutOfDate, err)
		assert.False(t, env.HasFlagFile())
	}
	fs := vfs.GetTestFS()
	runEnvTest(t, tf, fs)
}
````

## File: internal/server/snapshotenv.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package server

import (
	"fmt"
	"path/filepath"
	"regexp"
	"strings"
	"sync"

	"github.com/cockroachdb/errors"

	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/vfs"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

var (
	// ErrSnapshotOutOfDate is the error to indicate that snapshot is out of date.
	ErrSnapshotOutOfDate = errors.New("snapshot out of date")
	// MetadataFilename is the filename of a snapshot's metadata file.
	MetadataFilename = "snapshot.metadata"
	// SnapshotFileSuffix is the filename suffix of a snapshot file.
	SnapshotFileSuffix = "gbsnap"
	// SnapshotDirNameRe is the regex of snapshot names.
	SnapshotDirNameRe = regexp.MustCompile(`^snapshot-[0-9A-F]+$`)
	// SnapshotDirNamePartsRe is used to find the index value from snapshot folder name.
	SnapshotDirNamePartsRe = regexp.MustCompile(`^snapshot-([0-9A-F]+)$`)
	// GenSnapshotDirNameRe is the regex of temp snapshot directory name used when
	// generating snapshots.
	GenSnapshotDirNameRe = regexp.MustCompile(`^snapshot-[0-9A-F]+-[0-9A-F]+\.generating$`)
	// RecvSnapshotDirNameRe is the regex of temp snapshot directory name used when
	// receiving snapshots from remote NodeHosts.
	RecvSnapshotDirNameRe = regexp.MustCompile(`^snapshot-[0-9A-F]+-[0-9A-F]+\.receiving$`)
)

var (
	genTmpDirSuffix  = "generating"
	recvTmpDirSuffix = "receiving"
	shrunkSuffix     = "shrunk"
)

var finalizeLock sync.Mutex

// SnapshotDirFunc is the function type that returns the snapshot dir
// for the specified raft node.
type SnapshotDirFunc func(shardID uint64, replicaID uint64) string

// Mode is the snapshot env mode.
type Mode uint64

const (
	// SnapshotMode is the mode used when taking snapshotting.
	SnapshotMode Mode = iota
	// ReceivingMode is the mode used when receiving snapshots from remote nodes.
	ReceivingMode
)

// GetSnapshotDirName returns the snapshot dir name for the snapshot captured
// at the specified index.
func GetSnapshotDirName(index uint64) string {
	return getDirName(index)
}

// GetSnapshotFilename returns the filename of the snapshot file.
func GetSnapshotFilename(index uint64) string {
	return getFilename(index)
}

func mustBeChild(parent string, child string) {
	if v, err := filepath.Rel(parent, child); err != nil {
		plog.Panicf("%+v", err)
	} else {
		if len(v) == 0 || strings.Contains(v, string(filepath.Separator)) ||
			strings.HasPrefix(v, ".") {
			plog.Panicf("not a direct child, %s", v)
		}
	}
}

func getDirName(index uint64) string {
	return fmt.Sprintf("snapshot-%016X", index)
}

func getFilename(index uint64) string {
	return fmt.Sprintf("snapshot-%016X.%s", index, SnapshotFileSuffix)
}

func getShrinkedFilename(index uint64) string {
	return fmt.Sprintf("snapshot-%016X.%s", index, shrunkSuffix)
}

func getTempDirName(rootDir string,
	suffix string, index uint64, from uint64) string {
	dir := fmt.Sprintf("%s-%d.%s", getDirName(index), from, suffix)
	return filepath.Join(rootDir, dir)
}

func getFinalDirName(rootDir string, index uint64) string {
	return filepath.Join(rootDir, getDirName(index))
}

// SSEnv is the struct used to manage involved directories for taking or
// receiving snapshots.
type SSEnv struct {
	fs vfs.IFS
	// rootDir is the parent of all snapshot tmp/final dirs for a specified
	// raft node
	rootDir  string
	tmpDir   string
	finalDir string
	filepath string
	index    uint64
}

// NewSSEnv creates and returns a new SSEnv instance.
func NewSSEnv(f SnapshotDirFunc,
	shardID uint64, replicaID uint64, index uint64,
	from uint64, mode Mode, fs vfs.IFS) SSEnv {
	var tmpSuffix string
	if mode == SnapshotMode {
		tmpSuffix = genTmpDirSuffix
	} else {
		tmpSuffix = recvTmpDirSuffix
	}
	rootDir := f(shardID, replicaID)
	fp := fs.PathJoin(getFinalDirName(rootDir, index), getFilename(index))
	return SSEnv{
		index:    index,
		rootDir:  rootDir,
		tmpDir:   getTempDirName(rootDir, tmpSuffix, index, from),
		finalDir: getFinalDirName(rootDir, index),
		filepath: fp,
		fs:       fs,
	}
}

// GetTempDir returns the temp snapshot directory.
func (se *SSEnv) GetTempDir() string {
	return se.tmpDir
}

// GetFinalDir returns the final snapshot directory.
func (se *SSEnv) GetFinalDir() string {
	return se.finalDir
}

// GetRootDir returns the root directory. The temp and final snapshot
// directories are children of the root directory.
func (se *SSEnv) GetRootDir() string {
	return se.rootDir
}

// RemoveTempDir removes the temp snapshot directory.
func (se *SSEnv) RemoveTempDir() error {
	return se.removeDir(se.tmpDir)
}

// MustRemoveTempDir removes the temp snapshot directory and panic if there
// is any error.
func (se *SSEnv) MustRemoveTempDir() {
	if err := se.removeDir(se.tmpDir); err != nil {
		exist, cerr := fileutil.DirExist(se.tmpDir, se.fs)
		if cerr != nil || exist {
			panic(err)
		}
	}
}

// FinalizeSnapshot finalizes the snapshot.
func (se *SSEnv) FinalizeSnapshot(msg pb.Marshaler) error {
	finalizeLock.Lock()
	defer finalizeLock.Unlock()
	if err := se.createFlagFile(msg); err != nil {
		return err
	}
	if se.finalDirExists() {
		return ErrSnapshotOutOfDate
	}
	return se.renameToFinalDir()
}

// CreateTempDir creates the temp snapshot directory.
func (se *SSEnv) CreateTempDir() error {
	return se.createDir(se.tmpDir)
}

// RemoveFinalDir removes the final snapshot directory.
func (se *SSEnv) RemoveFinalDir() error {
	return se.removeDir(se.finalDir)
}

// SaveSSMetadata saves the metadata of the snapshot file.
func (se *SSEnv) SaveSSMetadata(msg pb.Marshaler) error {
	return fileutil.CreateFlagFile(se.tmpDir, MetadataFilename, msg, se.fs)
}

// HasFlagFile returns a boolean flag indicating whether the flag file is
// available in the final directory.
func (se *SSEnv) HasFlagFile() bool {
	fp := se.fs.PathJoin(se.finalDir, fileutil.SnapshotFlagFilename)
	_, err := se.fs.Stat(fp)
	return !vfs.IsNotExist(err)
}

// RemoveFlagFile removes the flag file from the final directory.
func (se *SSEnv) RemoveFlagFile() error {
	return fileutil.RemoveFlagFile(se.finalDir,
		fileutil.SnapshotFlagFilename, se.fs)
}

// GetFilename returns the snapshot filename.
func (se *SSEnv) GetFilename() string {
	return getFilename(se.index)
}

// GetFilepath returns the snapshot file path.
func (se *SSEnv) GetFilepath() string {
	return se.fs.PathJoin(se.finalDir, getFilename(se.index))
}

// GetShrinkedFilepath returns the file path of the shrunk snapshot.
func (se *SSEnv) GetShrinkedFilepath() string {
	return se.fs.PathJoin(se.finalDir, getShrinkedFilename(se.index))
}

// GetTempFilepath returns the temp snapshot file path.
func (se *SSEnv) GetTempFilepath() string {
	return se.fs.PathJoin(se.tmpDir, getFilename(se.index))
}

func (se *SSEnv) createDir(dir string) error {
	mustBeChild(se.rootDir, dir)
	return fileutil.Mkdir(dir, se.fs)
}

func (se *SSEnv) removeDir(dir string) error {
	mustBeChild(se.rootDir, dir)
	if err := se.fs.RemoveAll(dir); err != nil {
		return err
	}
	return fileutil.SyncDir(se.rootDir, se.fs)
}

func (se *SSEnv) finalDirExists() bool {
	_, err := se.fs.Stat(se.finalDir)
	return !vfs.IsNotExist(err)
}

func (se *SSEnv) renameToFinalDir() error {
	if err := se.fs.Rename(se.tmpDir, se.finalDir); err != nil {
		return err
	}
	return fileutil.SyncDir(se.rootDir, se.fs)
}

func (se *SSEnv) createFlagFile(msg pb.Marshaler) error {
	return fileutil.CreateFlagFile(se.tmpDir,
		fileutil.SnapshotFlagFilename, msg, se.fs)
}
````

## File: internal/settings/hard.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

/*
Package settings is used for managing internal parameters that can be set at
compile time by expert level users. Most of those parameters can also be
overwritten by using the json mechanism described below.
*/
package settings

import (
	"crypto/md5"
	"encoding/binary"
	"fmt"
	"io"

	"github.com/lni/dragonboat/v4/logger"
)

var (
	plog = logger.GetLogger("settings")
)

//
// Parameters in both hard.go and soft.go are _NOT_ a part of the public API.
// There is no guarantee that any of these parameters are going to be available
// in future releases. Change them only when you know what you are doing.
//
// This file contain hard configuration values that should _NEVER_ be changed
// after your system has been deployed. Changing any value here will CORRUPT
// the data in your existing deployment.
//
// We do have an mechanism to overwrite the default values for the hard struct.
// To tune these parameters, place a json file named
// dragonboat-hard-settings.json in the current working directory of your
// dragonboat application, all fields in the json file will be applied to
// overwrite the default setting values. e.g. for a json file with the
// following content -
//
// {
//   "LRUMaxSessionCount": 32,
// }
//
// hard.LRUMaxSessionCount will be set to 32
//
// The application need to be restarted to apply such configuration changes.
// Again - tuning these hard parameters using the above described json file
// will cause your existing data to be corrupted. Decide them in your dev/test
// phase, once your system is deployed in production, _NEVER_ change them.
//

// Hard is the hard settings that can not be changed after the system has been
// deployed.
var Hard = getHardSettings()

type hard struct {
	// LRUMaxSessionCount is the max number of client sessions that can be
	// concurrently held and managed by each raft shard.
	LRUMaxSessionCount uint64
	// LogDBEntryBatchSize is the max size of each entry batch.
	LogDBEntryBatchSize uint64
}

// BlockFileMagicNumber is the magic number used in block based snapshot files.
var BlockFileMagicNumber = []byte{0x3F, 0x5B, 0xCB, 0xF1, 0xFA, 0xBA, 0x81, 0x9F}

const (
	//
	// RSM
	//

	// SnapshotHeaderSize defines the snapshot header size in number of bytes.
	SnapshotHeaderSize uint64 = 1024

	//
	// transport
	//

	// UnmanagedDeploymentID is the special deployment ID value used when no user
	// deployment ID is specified.
	UnmanagedDeploymentID uint64 = 1
	// MaxMessageBatchSize is the max size for a single message batch sent between
	// nodehosts.
	MaxMessageBatchSize uint64 = LargeEntitySize
	// SnapshotChunkSize is the snapshot chunk size.
	SnapshotChunkSize uint64 = 2 * 1024 * 1024
)

// HardHash returns the hash value of the Hard setting.
func HardHash(execShards uint64,
	logDBShards uint64, sessionCount uint64, batchSize uint64) uint64 {
	hashstr := fmt.Sprintf("%d-%d-%t-%d-%d",
		execShards,
		logDBShards,
		false, // was the UseRangeDelete option
		sessionCount,
		batchSize)
	mh := md5.New()
	if _, err := io.WriteString(mh, hashstr); err != nil {
		panic(err)
	}
	return binary.LittleEndian.Uint64(mh.Sum(nil))
}

func getHardSettings() hard {
	org := getDefaultHardSettings()
	overwriteHardSettings(&org)
	return org
}

func getDefaultHardSettings() hard {
	return hard{
		LRUMaxSessionCount:  4096,
		LogDBEntryBatchSize: 48,
	}
}
````

## File: internal/settings/overwrite.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package settings

import (
	"encoding/json"
	"os"
	"path/filepath"
	"reflect"
)

func getParsedConfig(fn string) map[string]interface{} {
	if _, err := os.Stat(fn); os.IsNotExist(err) {
		return nil
	}
	m := map[string]interface{}{}
	b, err := os.ReadFile(filepath.Clean(fn))
	if err != nil {
		panic(err)
	}
	if err := json.Unmarshal(b, &m); err != nil {
		panic(err)
	}
	return m
}

func overwriteHardSettings(org *hard) {
	cfg := getParsedConfig("dragonboat-hard-settings.json")
	rd := reflect.Indirect(reflect.ValueOf(org))
	overwriteSettings(cfg, rd)
}

func overwriteSoftSettings(org *soft) {
	cfg := getParsedConfig("dragonboat-soft-settings.json")
	rd := reflect.Indirect(reflect.ValueOf(org))
	overwriteSettings(cfg, rd)
}

func overwriteSettings(cfg map[string]interface{}, rd reflect.Value) {
	for key, val := range cfg {
		field := rd.FieldByName(key)
		if field.IsValid() {
			switch field.Type().String() {
			case "uint64":
				nv := uint64(val.(float64))
				plog.Infof("Setting %s to uint64 value %d", key, nv)
				field.SetUint(nv)
			case "bool":
				plog.Infof("Setting %s to bool value %t", key, val.(bool))
				field.SetBool(val.(bool))
			case "string":
				plog.Infof("Setting %s to string value %s", key, val.(string))
				field.SetString(val.(string))
			default:
			}
		}
	}
}
````

## File: internal/settings/soft.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package settings

const (
	// EntryNonCmdFieldsSize defines the upper limit of the non-cmd field
	// length in pb.Entry.
	EntryNonCmdFieldsSize = 16 * 8
	// LargeEntitySize defines what is considered as a large entity for per node
	// entities.
	LargeEntitySize uint64 = 64 * 1024 * 1024
)

//
// Tuning configuration parameters here will impact the performance of your
// system. It will not corrupt your data. Only tune these parameters when
// you know what you are doing.
//
// To tune these parameters, place a json file named
// dragonboat-soft-settings.json in the current working directory of your
// dragonboat application, all fields in the json file will be applied to
// overwrite the default setting values. e.g. for a json file with the
// following content -
//
// {
//   "GetConnectedTimeoutSecond": 15,
// }
//
// soft.GetConnectedTimeoutSecond will be 15,
//
// The application need to be restarted to apply such configuration changes.
//

// Soft is the soft settings that can be changed after the deployment of a
// system.
var Soft = getSoftSettings()

type soft struct {
	//
	// Raft
	//

	// MaxEntrySize defines the max total entry size that can be included in
	// the Replicate message.
	MaxEntrySize uint64
	// InMemEntrySliceSize defines the maximum length of the in memory entry
	// slice.
	InMemEntrySliceSize uint64
	// MinEntrySliceFreeSize defines the minimum length of the free in memory
	// entry slice. A new entry slice of length InMemEntrySliceSize will be
	// allocated once the free entry size in the current slice is less than
	// MinEntrySliceFreeSize.
	MinEntrySliceFreeSize uint64
	// InMemGCTimeout defines how often dragonboat collects partial object.
	// It is defined in terms of number of ticks.
	InMemGCTimeout uint64
	// MaxApplyEntrySize defines the max size of entries to apply.
	MaxApplyEntrySize uint64

	//
	// Multiraft
	//

	// PendingProposalShards defines the number of shards for the pending
	// proposal data structure.
	PendingProposalShards uint64
	// SyncTaskInterval defines the interval in millisecond of periodic sync
	// state machine task.
	SyncTaskInterval uint64
	// IncomingReadIndexQueueLength defines the number of pending read index
	// requests allowed for each raft group.
	IncomingReadIndexQueueLength uint64
	// IncomingProposalQueueLength defines the number of pending proposals
	// allowed for each raft group.
	IncomingProposalQueueLength uint64
	// ReceiveQueueLength is the length of the receive queue on each node.
	ReceiveQueueLength uint64
	// SnapshotStatusPushDelayMS is the number of millisecond delays we impose
	// before pushing the snapshot results to raft node.
	SnapshotStatusPushDelayMS uint64
	// TaskQueueTargetLength defined the target length of each node's taskQ.
	// Dragonboat tries to make sure the queue is no longer than this target
	// length.
	TaskQueueTargetLength uint64
	// TaskQueueInitialCap defines the initial capcity of a task queue.
	TaskQueueInitialCap uint64
	// NodeHostRequestStatePoolShards defines the number of sync pools.
	NodeHostRequestStatePoolShards uint64
	// LazyFreeCycle defines how often should entry queue and message queue
	// to be freed.
	LazyFreeCycle uint64
	// PanicOnSizeMismatch defines whether dragonboat should panic when snapshot
	// file size doesn't match the size recorded in snapshot metadata.
	PanicOnSizeMismatch bool

	//
	// RSM
	//
	BatchedEntryApply bool

	//
	// step engine
	//

	// TaskBatchSize defines the length of the committed batch slice.
	TaskBatchSize uint64
	// NodeReloadMillisecond defines how often step engine should reload
	// nodes, it is defined in number of millisecond.
	NodeReloadMillisecond uint64
	// CloseWorkerTimedWaitSecond is the number of seconds allowed for the
	// close worker to run cleanups before exit.
	CloseWorkerTimedWaitSecond uint64

	//
	// transport
	//

	// GetConnectedTimeoutSecond is the default timeout value in second when
	// trying to connect to a gRPC based server.
	GetConnectedTimeoutSecond uint64
	// MaxSnapshotConnections defines the max number of concurrent outgoing
	// snapshot connections.
	MaxSnapshotConnections uint64
	// MaxConcurrentStreamingSnapshot defines the max number of concurrent
	// incoming snapshot streams.
	MaxConcurrentStreamingSnapshot uint64
	// SendQueueLength is the length of the send queue used to hold messages
	// exchanged between nodehosts. You may need to increase this value when
	// you want to host large number nodes per nodehost.
	SendQueueLength uint64
	// StreamConnections defines how many connections to use for each remote
	// nodehost whene exchanging raft messages
	StreamConnections uint64
	// PerConnBufSize is the size of the per connection buffer used for
	// receiving incoming messages.
	PerConnectionSendBufSize uint64
	// PerConnectionRecvBufSize is the size of the recv buffer size.
	PerConnectionRecvBufSize uint64
	// SnapshotGCTick defines the number of ticks between two snapshot GC
	// operations.
	SnapshotGCTick uint64
	// SnapshotChunkTimeoutTick defines the max time allowed to receive
	// a snapshot.
	SnapshotChunkTimeoutTick uint64

	//
	// LogDB
	//
	KVTolerateCorruptedTailRecords bool
	// KVUseUniversalCompaction defines whether to use universal compaction to
	// reduce write amplification. This setting is default to false, change it to
	// true for existing system might cause unexpected consequences, please check
	// the documentation of your KV store for more details.
	//
	// KVUseUniversalCompaction support is experimental - it is not fully tested.
	KVUseUniversalCompaction bool
}

func getSoftSettings() soft {
	org := getDefaultSoftSettings()
	overwriteSoftSettings(&org)
	return org
}

func getDefaultSoftSettings() soft {
	return soft{
		MaxConcurrentStreamingSnapshot: 128,
		MaxSnapshotConnections:         64,
		SyncTaskInterval:               180000,
		PanicOnSizeMismatch:            true,
		LazyFreeCycle:                  1,
		BatchedEntryApply:              true,
		GetConnectedTimeoutSecond:      5,
		MaxEntrySize:                   MaxMessageBatchSize,
		InMemGCTimeout:                 100,
		InMemEntrySliceSize:            512,
		MaxApplyEntrySize:              64 * 1024 * 1024,
		MinEntrySliceFreeSize:          96,
		IncomingReadIndexQueueLength:   4096,
		IncomingProposalQueueLength:    2048,
		SnapshotStatusPushDelayMS:      1000,
		PendingProposalShards:          16,
		TaskQueueInitialCap:            24,
		TaskQueueTargetLength:          64,
		NodeHostRequestStatePoolShards: 8,
		TaskBatchSize:                  512,
		NodeReloadMillisecond:          200,
		CloseWorkerTimedWaitSecond:     5,
		SendQueueLength:                1024 * 2,
		ReceiveQueueLength:             1024,
		StreamConnections:              4,
		PerConnectionSendBufSize:       2 * 1024 * 1024,
		PerConnectionRecvBufSize:       2 * 1024 * 1024,
		SnapshotGCTick:                 30,
		SnapshotChunkTimeoutTick:       900,
		KVTolerateCorruptedTailRecords: true,
		KVUseUniversalCompaction:       false,
	}
}
````

## File: internal/tan/benchmark_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"testing"

	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/logger"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/stretchr/testify/require"
)

var benchmarkTestDirname = "tan_benchmark_dir"

func benchmarkWrite(b *testing.B, sz int) {
	logger.GetLogger("tan").SetLevel(logger.WARNING)
	b.ReportAllocs()
	fs := vfs.GetTestFS()
	opts := &Options{
		FS: fs,
	}
	if err := fs.MkdirAll(benchmarkTestDirname, 0766); err != nil {
		b.Fatalf("failed to create dir %v", err)
	}
	defer func() {
		if err := fs.RemoveAll(benchmarkTestDirname); err != nil {
			b.Fatalf("failed to remove dir %v", err)
		}
	}()
	db, err := open("test-db", benchmarkTestDirname, opts)
	if err != nil {
		b.Fatalf("failed to open db %v", err)
	}
	defer func() {
		require.NoError(b, db.close())
	}()

	u := pb.Update{
		ShardID: 100,
		EntriesToSave: []pb.Entry{
			{Cmd: make([]byte, sz)},
		},
	}
	buf := make([]byte, sz*2)
	for i := 0; i < b.N; i++ {
		sync, err := db.write(u, buf)
		if err != nil {
			b.Fatalf("failed to write %v", err)
		}
		if sync {
			if err := db.sync(); err != nil {
				b.Fatalf("failed to sync %v", err)
			}
		}
		b.SetBytes(int64(sz))
	}
}

func BenchmarkWrite16K(b *testing.B) {
	benchmarkWrite(b, 16*1024)
}

func BenchmarkWrite128K(b *testing.B) {
	benchmarkWrite(b, 128*1024)
}

func BenchmarkWrite512K(b *testing.B) {
	benchmarkWrite(b, 512*1024)
}

func BenchmarkWrite1024K(b *testing.B) {
	benchmarkWrite(b, 1024*1024)
}
````

## File: internal/tan/bootstrap.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"bytes"
	"io"

	"github.com/cockroachdb/errors/oserror"

	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/lni/vfs"
)

// getBootstrap returns saved bootstrap record. Bootstrap records are saved
// side by side with the tan db itself, so strictly speaking, they are not
// a part of the tan db.
func getBootstrap(fs vfs.FS, dirname string,
	shardID uint64, replicaID uint64) (rec pb.Bootstrap, err error) {
	filename := makeBootstrapFilename(fs, dirname, shardID, replicaID, false)
	f, err := fs.Open(filename)
	if err != nil {
		if oserror.IsNotExist(err) {
			return pb.Bootstrap{}, raftio.ErrNoBootstrapInfo
		}
		return pb.Bootstrap{}, err
	}
	defer func() {
		err = firstError(err, f.Close())
	}()
	r := newReader(f, fileNum(0))
	rr, err := r.next()
	if err != nil {
		return pb.Bootstrap{}, err
	}
	var buf bytes.Buffer
	if _, err := io.Copy(&buf, rr); err != nil {
		return pb.Bootstrap{}, err
	}
	pb.MustUnmarshal(&rec, buf.Bytes())
	return rec, nil
}

func saveBootstrap(fs vfs.FS,
	dirname string, dataDir vfs.File,
	shardID uint64, replicaID uint64, rec pb.Bootstrap) (err error) {
	buf := pb.MustMarshal(&rec)
	fn := makeBootstrapFilename(fs, dirname, shardID, replicaID, true)
	ffn := makeBootstrapFilename(fs, dirname, shardID, replicaID, false)
	f, err := fs.Create(fn)
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, f.Sync())
		err = firstError(err, f.Close())
		if err == nil {
			err = fs.Rename(fn, ffn)
		}
		err = firstError(err, dataDir.Sync())
	}()
	w := newWriter(f)
	defer func() {
		err = firstError(err, w.close())
	}()
	if _, err = w.writeRecord(buf); err != nil {
		return err
	}
	return nil
}

func removeBootstrap(fs vfs.FS, dirname string, dataDir vfs.File,
	shardID uint64, replicaID uint64) error {
	fn := makeBootstrapFilename(fs, dirname, shardID, replicaID, false)
	if _, err := fs.Stat(fn); oserror.IsNotExist(err) {
		return nil
	}
	if err := fs.RemoveAll(fn); err != nil {
		return err
	}
	return dataDir.Sync()
}
````

## File: internal/tan/compaction_test.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"math"
	"testing"
	"time"

	"github.com/cockroachdb/errors/oserror"

	"github.com/lni/dragonboat/v4/config"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/lni/goutils/leaktest"
	"github.com/lni/vfs"
	"github.com/stretchr/testify/require"
)

func TestRemoveEntries(t *testing.T) {
	fs := vfs.NewMem()
	opts := &Options{
		MaxLogFileSize:      1,
		MaxManifestFileSize: MaxManifestFileSize,
		FS:                  fs,
	}
	tf := func(t *testing.T, db *db) {
		buf := make([]byte, 1024)
		for i := uint64(1); i < uint64(100); i++ {
			u := pb.Update{
				ShardID:       2,
				ReplicaID:     3,
				State:         pb.State{Commit: i},
				EntriesToSave: []pb.Entry{{Index: i, Term: 1}},
			}
			_, err := db.write(u, buf)
			require.NoError(t, err)
		}
		require.NoError(t, db.removeEntries(2, 3, uint64(99)))
		// FIXME: this is a race
		/*db.mu.Lock()
		require.Equal(t, 98, len(db.mu.versions.obsoleteTables))
		db.mu.Unlock()*/
	}
	runTanTest(t, opts, tf, fs)

	tf = func(t *testing.T, db *db) {
		require.Equal(t, 3, len(db.mu.versions.currentVersion().files))
		var entries []pb.Entry
		entries, _, err := db.getEntries(2, 3, entries, 0, 99, 100, math.MaxUint64)
		require.NoError(t, err)
		require.Equal(t, 0, len(entries))
		var entries1 []pb.Entry
		entries1, _, err = db.getEntries(2, 3, entries1, 0, 98, 98, math.MaxUint64)
		require.NoError(t, err)
		require.Equal(t, 0, len(entries1))

		for j := 0; j < 3000; j++ {
			ls, err := db.opts.FS.List(db.dirname)
			require.NoError(t, err)
			noObsolete := true
			for _, filename := range ls {
				fileType, fileNum, ok := parseFilename(db.opts.FS, filename)
				if !ok {
					continue
				}
				if fileType == fileTypeLog {
					_, ok := db.mu.versions.currentVersion().files[fileNum]
					if !ok {
						noObsolete = false
					}
				}
			}
			if noObsolete {
				return
			}
			time.Sleep(time.Millisecond)
		}
		t.Fatalf("failed to remove all obsolete files")
	}
	runTanTest(t, opts, tf, fs)
}

func TestRemovedEntriesMultiplexedLogSetup(t *testing.T) {
	defer leaktest.AfterTest(t)()
	cfg := config.NodeHostConfig{
		Expert: config.ExpertConfig{FS: vfs.NewMem()},
	}
	require.NoError(t, cfg.Prepare())
	dirs := []string{"db-dir"}
	ldb, err := CreateLogMultiplexedTan(cfg, nil, dirs, []string{})
	require.NoError(t, err)
	defer func() {
		require.NoError(t, ldb.Close())
	}()
	for i := uint64(0); i < 16; i++ {
		updates := []pb.Update{
			{
				ShardID:   1,
				ReplicaID: 1,
				Snapshot:  pb.Snapshot{Index: i * uint64(100), Term: 10},
				State:     pb.State{Commit: i * uint64(100), Term: 10},
				EntriesToSave: []pb.Entry{
					{Index: i*2 + 1, Term: 10},
					{Index: i*2 + 2, Term: 10},
				},
			},
			{
				ShardID:   17,
				ReplicaID: 1,
				Snapshot:  pb.Snapshot{Index: i * uint64(200), Term: 20},
				State:     pb.State{Commit: i * uint64(200), Term: 20},
				EntriesToSave: []pb.Entry{
					{Index: i*3 + 1, Term: 20},
					{Index: i*3 + 2, Term: 20},
					{Index: i*3 + 3, Term: 20},
				},
			},
		}
		require.NoError(t, ldb.SaveRaftState(updates, 1))
		db, err := ldb.collection.getDB(1, 1)
		require.NoError(t, err)
		// switchToNewLog() should only be called when db.mu is locked
		db.mu.Lock()
		require.NoError(t, db.switchToNewLog())
		db.mu.Unlock()
	}
	db, err := ldb.collection.getDB(1, 1)
	require.NoError(t, err)
	current := db.mu.versions.currentVersion()
	fileCount := len(current.files)
	// not suppose tp have any log file removed
	require.NoError(t, ldb.RemoveEntriesTo(1, 1, 32))
	current = db.mu.versions.currentVersion()
	require.Equal(t, fileCount, len(current.files))
	// this should trigger log compaction
	require.NoError(t, ldb.RemoveEntriesTo(17, 1, 48))
	current = db.mu.versions.currentVersion()
	// a log file and an empty log file just created by the last switchToNewLog
	require.Equal(t, 2, len(current.files))
}

func TestRemoveAll(t *testing.T) {
	fs := vfs.NewMem()
	opts := &Options{
		MaxLogFileSize:      1024,
		MaxManifestFileSize: MaxManifestFileSize,
		FS:                  fs,
	}
	tf := func(t *testing.T, db *db) {
		u := pb.Update{
			ShardID:   2,
			ReplicaID: 3,
			State: pb.State{
				Commit: 100,
				Term:   5,
				Vote:   3,
			},
			Snapshot: pb.Snapshot{
				Index: 100,
				Term:  5,
			},
			EntriesToSave: []pb.Entry{
				{Index: 0, Term: 5},
			},
		}
		buf := make([]byte, 1024)
		for i := uint64(1); i <= uint64(100); i++ {
			u.EntriesToSave[0].Index = i
			_, err := db.write(u, buf)
			require.NoError(t, err)
		}
		require.NoError(t, db.removeAll(2, 3))
		for i := 0; i < 3000; i++ {
			ls, err := db.opts.FS.List(db.dirname)
			require.NoError(t, err)
			unexpectedFile := false
			for _, file := range ls {
				fileType, fileNum, ok := parseFilename(db.opts.FS, file)
				if !ok {
					continue
				}
				if fileType == fileTypeLog {
					if fileNum != db.mu.logNum {
						unexpectedFile = true
					}
				}
				if fileType == fileTypeIndex {
					unexpectedFile = true
				}
			}
			if unexpectedFile {
				time.Sleep(time.Millisecond)
			} else {
				break
			}
		}
	}
	runTanTest(t, opts, tf, fs)
}

func TestInstallSnapshot(t *testing.T) {
	fs := vfs.NewMem()
	opts := &Options{
		MaxLogFileSize:      1024,
		MaxManifestFileSize: MaxManifestFileSize,
		FS:                  fs,
	}
	tf := func(t *testing.T, db *db) {
		u := pb.Update{
			ShardID:   2,
			ReplicaID: 3,
			State: pb.State{
				Commit: 100,
				Term:   5,
				Vote:   3,
			},
			Snapshot: pb.Snapshot{
				Index: 101,
				Term:  5,
			},
			EntriesToSave: []pb.Entry{
				{Index: 0, Term: 5},
			},
		}
		buf := make([]byte, 1024)
		for i := uint64(1); i <= uint64(100); i++ {
			u.EntriesToSave[0].Index = i
			_, err := db.write(u, buf)
			require.NoError(t, err)
		}
		ss := pb.Snapshot{
			ShardID: 2,
			Index:   50,
			Term:    3,
		}
		require.NoError(t, db.importSnapshot(2, 3, ss))
		for i := uint64(1); i <= uint64(100); i++ {
			var result []pb.Entry
			entries, _, err := db.getEntries(2, 3, result, 0, i, i, 1024)
			require.NoError(t, err)
			require.Equal(t, 0, len(entries))
		}

		rs, err := db.getRaftState(2, 3, 50)
		require.NoError(t, err)
		require.Equal(t, ss.Index, rs.State.Commit)
		require.Equal(t, ss.Term, rs.State.Term)
		snapshot, err := db.getSnapshot(2, 3)
		require.NoError(t, err)
		require.Equal(t, ss, snapshot)
	}
	runTanTest(t, opts, tf, fs)
}

func TestScanObsoleteFiles(t *testing.T) {
	fs := vfs.NewMem()
	var dbdir string
	tf := func(t *testing.T, db *db) { dbdir = db.dirname }
	runTanTest(t, nil, tf, fs)
	manifestFn := "MANIFEST-1000"
	logFn := "10001.log"
	f, err := fs.Create(fs.PathJoin(dbdir, manifestFn))
	require.NoError(t, err)
	require.NoError(t, f.Close())
	f, err = fs.Create(fs.PathJoin(dbdir, logFn))
	require.NoError(t, err)
	require.NoError(t, f.Close())
	runTanTest(t, nil, tf, fs)
	_, err = fs.Stat(manifestFn)
	require.True(t, oserror.IsNotExist(err))
	_, err = fs.Stat(logFn)
	require.True(t, oserror.IsNotExist(err))
}

func TestNodeIndexCompaction(t *testing.T) {
	nodeIndex := nodeIndex{
		entries: index{
			compactedTo: 9,
			entries: []indexEntry{
				{1, 2, 1, 5, 10},
				{3, 4, 2, 5, 10},
				{5, 6, 3, 5, 10},
				{7, 7, 4, 5, 10},
				{8, 10, 5, 5, 10},
				{11, 12, 6, 5, 10},
			},
		},
		state:    indexEntry{7, 0, 4, 5, 100},
		snapshot: indexEntry{5, 0, 3, 5, 100},
	}
	n0 := nodeIndex
	n0.state = indexEntry{}
	n0.snapshot = indexEntry{}
	require.Equal(t, []fileNum{1, 2, 3, 4}, n0.compaction())
	n1 := nodeIndex
	require.Equal(t, []fileNum{1, 2}, n1.compaction())
	n2 := nodeIndex
	n2.entries = index{}
	require.Nil(t, n2.compaction())
	n3 := nodeIndex
	n3.state = indexEntry{}
	require.Equal(t, []fileNum{1, 2}, n3.compaction())
	n4 := nodeIndex
	n4.snapshot = indexEntry{}
	require.Equal(t, []fileNum{1, 2, 3}, n4.compaction())
}
````

## File: internal/tan/compaction.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"sort"

	pb "github.com/lni/dragonboat/v4/raftpb"
)

// when compacting entries, a compaction update is written to the log to record
// the op. the compactedTo field of the index.entries and index.currEntries are
// set.
func (d *db) removeEntries(shardID uint64, replicaID uint64, index uint64) error {
	return d.remove(shardID, replicaID, index)
}

func (d *db) remove(shardID uint64, replicaID uint64, index uint64) error {
	d.mu.Lock()
	defer d.mu.Unlock()
	update := getCompactionUpdate(shardID, replicaID, index)
	buf := make([]byte, update.SizeUpperLimit())
	data := pb.MustMarshalTo(&update, buf)
	if err := d.doWriteLocked(update, data); err != nil {
		return err
	}
	nodeIndex := d.mu.nodeStates.getIndex(shardID, replicaID)
	nodeIndex.currEntries.setCompactedTo(index)
	nodeIndex.entries.setCompactedTo(index)
	return d.compactionLocked(nodeIndex)
}

func (d *db) compactionLocked(index *nodeIndex) error {
	if obsolete := index.compaction(); len(obsolete) > 0 {
		obsolete = d.mu.nodeStates.getObsolete(obsolete)
		if len(obsolete) > 0 {
			ve := versionEdit{
				deletedFiles: make(map[deletedFileEntry]*fileMetadata),
			}
			for _, fn := range obsolete {
				ve.deletedFiles[deletedFileEntry{fn}] = &fileMetadata{fileNum: fn}
			}
			d.mu.versions.logLock()
			if err := d.mu.versions.logAndApply(&ve, d.dataDir); err != nil {
				return err
			}
		}
	}
	d.updateReadStateLocked(nil)
	d.notifyDeleteObsoleteWorker()
	return nil
}

func merge(a, b []fileNum) []fileNum {
	if len(b) == 0 {
		return a
	}

	a = append(a, b...)
	sort.Slice(a, func(i, j int) bool {
		return a[i] < a[j]
	})

	n := 0
	for i := 0; i < len(a); i++ {
		if n == 0 || a[i] != a[n-1] {
			a[n] = a[i]
			n++
		}
	}
	return a[:n]
}

func mergeFileMetas(a, b []*fileMetadata) []*fileMetadata {
	if len(b) == 0 {
		return a
	}

	a = append(a, b...)
	sort.Slice(a, func(i, j int) bool {
		return a[i].fileNum < a[j].fileNum
	})

	n := 0
	for i := 0; i < len(a); i++ {
		if n == 0 || a[i].fileNum != a[n-1].fileNum {
			a[n] = a[i]
			n++
		}
	}
	return a[:n]
}

func (d *db) scanObsoleteFiles(list []string) {
	manifestFileNum := d.mu.versions.manifestFileNum
	liveFiles := d.mu.versions.currentVersion().files

	var obsoleteTables []*fileMetadata
	var obsoleteManifests []fileNum

	for _, filename := range list {
		fileType, fileNum, ok := parseFilename(d.opts.FS, filename)
		if !ok {
			continue
		}
		switch fileType {
		case fileTypeManifest:
			if fileNum == manifestFileNum {
				continue
			}
			obsoleteManifests = append(obsoleteManifests, fileNum)
		case fileTypeLog:
			if _, ok := liveFiles[fileNum]; ok {
				continue
			}
			fileMeta := &fileMetadata{
				fileNum: fileNum,
			}
			obsoleteTables = append(obsoleteTables, fileMeta)
		default:
			// Don't delete files we don't know about.
			continue
		}
	}
	d.mu.versions.obsoleteTables = mergeFileMetas(d.mu.versions.obsoleteTables, obsoleteTables)
	d.mu.versions.obsoleteManifests = merge(d.mu.versions.obsoleteManifests, obsoleteManifests)
}

func (d *db) notifyDeleteObsoleteWorker() {
	select {
	case d.deleteObsoleteCh <- struct{}{}:
	default:
	}
}

func (d *db) deleteObsoleteWorkerMain() {
	for {
		select {
		case <-d.deleteobsoleteWorker.ShouldStop():
			return
		case <-d.deleteObsoleteCh:
			if err := d.deleteObsoleteFiles(); err != nil {
				panicNow(err)
			}
		}
	}
}

func (d *db) deleteObsoleteFiles() error {
	d.mu.Lock()
	obsoleteManifests := d.mu.versions.obsoleteManifests
	d.mu.versions.obsoleteManifests = nil
	obsoleteTables := d.mu.versions.obsoleteTables
	d.mu.versions.obsoleteTables = nil
	d.mu.Unlock()

	for _, fn := range obsoleteManifests {
		filename := makeFilename(d.opts.FS, d.dirname, fileTypeManifest, fn)
		if err := d.opts.FS.RemoveAll(filename); err != nil {
			return err
		}
	}
	for _, meta := range obsoleteTables {
		filename := makeFilename(d.opts.FS, d.dirname, fileTypeLog, meta.fileNum)
		indexFilename := makeFilename(d.opts.FS, d.dirname, fileTypeIndex, meta.fileNum)
		if err := d.opts.FS.RemoveAll(filename); err != nil {
			return err
		}
		if err := d.opts.FS.RemoveAll(indexFilename); err != nil {
			return err
		}
	}
	return nil
}

func (d *db) removeAll(shardID uint64, replicaID uint64) error {
	d.mu.Lock()
	defer d.mu.Unlock()
	return d.removeAllLocked(shardID, replicaID, false)
}

func (d *db) importSnapshot(shardID uint64,
	replicaID uint64, ss pb.Snapshot) error {
	// TODO: need to remove the bootstrap record first
	return d.installSnapshot(shardID, replicaID, ss)
}

func (d *db) installSnapshot(shardID uint64,
	replicaID uint64, ss pb.Snapshot) error {
	d.mu.Lock()
	defer d.mu.Unlock()
	if err := d.removeAllLocked(shardID, replicaID, true); err != nil {
		return err
	}
	update := pb.Update{
		ShardID:   shardID,
		ReplicaID: replicaID,
		State: pb.State{
			Commit: ss.Index,
			Term:   ss.Term,
		},
		Snapshot: ss,
	}
	buf := make([]byte, update.SizeUpperLimit())
	data := pb.MustMarshalTo(&update, buf)
	return d.doWriteLocked(update, data)
}

func (d *db) removeAllLocked(shardID uint64, replicaID uint64, newLog bool) error {
	if newLog {
		if err := d.createNewLog(); err != nil {
			return err
		}
	}
	index := d.mu.nodeStates.getIndex(shardID, replicaID)
	index.removeAll()
	v := d.mu.versions.currentVersion()
	ve := versionEdit{
		deletedFiles: make(map[deletedFileEntry]*fileMetadata),
	}
	for fn := range v.files {
		if fn != d.mu.versions.manifestFileNum && fn != d.mu.logNum {
			ve.deletedFiles[deletedFileEntry{fn}] = &fileMetadata{fileNum: fn}
		}
	}
	d.mu.versions.logLock()
	if err := d.mu.versions.logAndApply(&ve, d.dataDir); err != nil {
		return err
	}
	d.updateReadStateLocked(nil)
	d.notifyDeleteObsoleteWorker()
	return nil
}

// when compacting entries, a compaction update is written to the log to record
// the op.
func isCompactionUpdate(update pb.Update) (uint64, bool) {
	isCompaction := update.Term == compactionFlag
	if len(update.EntriesToSave) == 0 && pb.IsEmptySnapshot(update.Snapshot) &&
		isCompaction {
		return update.Commit, true
	}
	return 0, false
}

func getCompactionUpdate(shardID uint64, replicaID uint64, index uint64) pb.Update {
	return pb.Update{
		ShardID:   shardID,
		ReplicaID: replicaID,
		State:     pb.State{Commit: index, Term: compactionFlag},
	}
}
````

## File: internal/tan/crc.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"github.com/cespare/xxhash/v2"
)

func getCRC(data []byte) uint32 {
	return uint32(xxhash.Sum64(data))
}
````

## File: internal/tan/db_keeper.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"fmt"

	"github.com/cockroachdb/errors/oserror"

	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/raftio"
	"github.com/lni/vfs"
)

// dbKeeper keeps all tan db instances managed by a tan LogDB.
type dbKeeper interface {
	multiplexedLog() bool
	name(shardID uint64, replicaID uint64) string
	key(shardID uint64) uint64
	get(shardID uint64, replicaID uint64) (*db, bool)
	set(shardID uint64, replicaID uint64, db *db)
	iterate(f func(*db) error) error
}

var _ dbKeeper = (*regularKeeper)(nil)

// regularKeeper assigns a unique tan db instance to each raft node.
type regularKeeper struct {
	dbs map[raftio.NodeInfo]*db
}

func newRegularDBKeeper() *regularKeeper {
	return &regularKeeper{
		dbs: make(map[raftio.NodeInfo]*db),
	}
}

func (k *regularKeeper) multiplexedLog() bool {
	return false
}

func (k *regularKeeper) name(shardID uint64, replicaID uint64) string {
	return fmt.Sprintf("node-%d-%d", shardID, replicaID)
}

func (k *regularKeeper) key(shardID uint64) uint64 {
	panic("not suppose to be called")
}

func (k *regularKeeper) get(shardID uint64, replicaID uint64) (*db, bool) {
	ni := raftio.NodeInfo{ShardID: shardID, ReplicaID: replicaID}
	v, ok := k.dbs[ni]
	return v, ok
}

func (k *regularKeeper) set(shardID uint64, replicaID uint64, db *db) {
	ni := raftio.NodeInfo{ShardID: shardID, ReplicaID: replicaID}
	k.dbs[ni] = db
}

func (k *regularKeeper) iterate(f func(*db) error) error {
	for _, db := range k.dbs {
		if err := f(db); err != nil {
			return err
		}
	}
	return nil
}

var _ dbKeeper = (*multiplexedKeeper)(nil)

// multiplexedKeeper divide all raft nodes into groups and assign nodes within
// the same group to a unique tan db instance. Each raft node is assigned to
// such a group by a so called key value.
type multiplexedKeeper struct {
	dbs map[uint64]*db
}

func newMultiplexedDBKeeper() *multiplexedKeeper {
	return &multiplexedKeeper{dbs: make(map[uint64]*db)}
}

func (k *multiplexedKeeper) multiplexedLog() bool {
	return true
}

func (k *multiplexedKeeper) name(shardID uint64, replicaID uint64) string {
	return fmt.Sprintf("shard-%d", k.key(shardID))
}

func (k *multiplexedKeeper) key(shardID uint64) uint64 {
	return shardID % 16
}

func (k *multiplexedKeeper) get(shardID uint64, replicaID uint64) (*db, bool) {
	v, ok := k.dbs[k.key(shardID)]
	return v, ok
}

func (k *multiplexedKeeper) set(shardID uint64, replicaID uint64, db *db) {
	k.dbs[k.key(shardID)] = db
}

func (k *multiplexedKeeper) iterate(f func(*db) error) error {
	for _, db := range k.dbs {
		if err := f(db); err != nil {
			return err
		}
	}
	return nil
}

// collection owns a collection of tan db instances.
type collection struct {
	fs      vfs.FS
	dirname string
	keeper  dbKeeper
}

func newCollection(dirname string, fs vfs.FS, regular bool) collection {
	var k dbKeeper
	if regular {
		k = newRegularDBKeeper()
	} else {
		k = newMultiplexedDBKeeper()
	}
	return collection{
		fs:      fs,
		dirname: dirname,
		keeper:  k,
	}
}

func (c *collection) multiplexedLog() bool {
	return c.keeper.multiplexedLog()
}

func (c *collection) key(shardID uint64) uint64 {
	return c.keeper.key(shardID)
}

func (c *collection) getDB(shardID uint64, replicaID uint64) (*db, error) {
	db, ok := c.keeper.get(shardID, replicaID)
	if ok {
		return db, nil
	}
	name := c.keeper.name(shardID, replicaID)
	dbdir := c.fs.PathJoin(c.dirname, name)
	if err := c.prepareDir(dbdir); err != nil {
		return nil, err
	}
	db, err := open(dbdir, dbdir, &Options{FS: c.fs})
	if err != nil {
		return nil, err
	}
	c.keeper.set(shardID, replicaID, db)
	return db, nil
}

func (c *collection) prepareDir(dbdir string) error {
	if _, err := c.fs.Stat(dbdir); oserror.IsNotExist(err) {
		if err := fileutil.MkdirAll(dbdir, c.fs); err != nil {
			return err
		}
	}
	return nil
}

func (c *collection) iterate(f func(*db) error) error {
	return c.keeper.iterate(f)
}
````

## File: internal/tan/db_test.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"math"
	"os"
	"sync"
	"testing"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/leaktest"
	"github.com/lni/vfs"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

func runTanTest(t *testing.T, opts *Options, tf func(t *testing.T, d *db), fs vfs.FS) {
	defer leaktest.AfterTest(t)()
	if opts == nil {
		opts = &Options{
			MaxManifestFileSize: MaxManifestFileSize,
			MaxLogFileSize:      MaxLogFileSize,
			FS:                  fs,
		}
	} else if opts.FS == nil {
		panic("fs not specified")
	}
	defer vfs.ReportLeakedFD(opts.FS, t)
	dirname := "/Users/lni/db-dir"
	require.NoError(t, fileutil.MkdirAll(dirname, opts.FS))
	db, err := open(dirname, dirname, opts)
	require.NoError(t, err)
	defer func() {
		require.NoError(t, db.close())
	}()
	tf(t, db)
}

func TestOpenNewDB(t *testing.T) {
	fs := vfs.NewMem()
	tf := func(t *testing.T, db *db) {
		rs, err := db.getRaftState(2, 3, 200)
		require.Equal(t, raftio.ErrNoSavedLog, err)
		require.Equal(t, raftio.RaftState{}, rs)
		require.Equal(t, uint64(0), rs.EntryCount)
		require.Equal(t, uint64(0), rs.FirstIndex)
		ss, err := db.getSnapshot(2, 3)
		require.NoError(t, err)
		require.True(t, pb.IsEmptySnapshot(ss))
		var entries []pb.Entry
		entries, size, err := db.getEntries(2, 3, entries, 0, 0, 100, math.MaxUint64)
		require.NoError(t, err)
		require.Equal(t, 0, len(entries))
		require.Equal(t, uint64(0), size)
	}
	runTanTest(t, nil, tf, fs)
}

func TestBasicDBReadWrite(t *testing.T) {
	for _, testSize := range []uint64{0, 1, 1024, 16 * 1024, blockSize, blockSize * 3} {
		size := testSize
		fs := vfs.NewMem()
		tf := func(t *testing.T, db *db) {
			var cmd []byte
			if size > 0 {
				cmd = make([]byte, size)
			}
			u1 := pb.Update{
				ShardID:   2,
				ReplicaID: 3,
				State: pb.State{
					Commit: 100,
					Term:   5,
					Vote:   3,
				},
				Snapshot: pb.Snapshot{
					Index: 100,
					Term:  5,
				},
				EntriesToSave: []pb.Entry{
					{Index: 101, Term: 5, Cmd: cmd},
					{Index: 102, Term: 5, Cmd: cmd},
					{Index: 103, Term: 5, Cmd: cmd},
				},
			}
			u2 := pb.Update{
				ShardID:   2,
				ReplicaID: 3,
				State: pb.State{
					Commit: 200,
					Term:   10,
					Vote:   6,
				},
				Snapshot: pb.Snapshot{
					Index: 200,
					Term:  10,
				},
				EntriesToSave: []pb.Entry{
					{Index: 201, Term: 10, Cmd: cmd},
					{Index: 202, Term: 10, Cmd: cmd},
					{Index: 203, Term: 10, Cmd: cmd},
				},
			}
			buf := make([]byte, 1024)
			_, err := db.write(u1, buf)
			require.NoError(t, err)
			_, err = db.write(u2, buf)
			require.NoError(t, err)

			require.Equal(t, u2.State, db.mu.nodeStates.getState(2, 3))
			rs, err := db.getRaftState(2, 3, 200)
			require.NoError(t, err)
			require.Equal(t, u2.State, rs.State)
			require.Equal(t, uint64(201), rs.FirstIndex)
			require.Equal(t, uint64(3), rs.EntryCount)

			snapshot, err := db.getSnapshot(2, 3)
			require.NoError(t, err)
			require.Equal(t, u2.Snapshot, snapshot)

			var result []pb.Entry
			entries, _, err := db.getEntries(2, 3, result, 0, 201, 203, math.MaxUint64)
			require.NoError(t, err)
			require.Equal(t, 2, len(entries))
			require.Equal(t, size, uint64(len(entries[0].Cmd)))
		}
		runTanTest(t, nil, tf, fs)
	}
}

func TestEntryOverwrite(t *testing.T) {
	type entry struct {
		index uint64
		term  uint64
	}
	type entryRange struct {
		start uint64
		end   uint64
		term  uint64
	}

	tests := []struct {
		input    []entryRange
		low      uint64
		high     uint64
		expected []entry
	}{
		{
			[]entryRange{{101, 105, 5}, {102, 104, 10}},
			101, 102,
			[]entry{{101, 5}},
		},
		{
			[]entryRange{{101, 105, 5}, {102, 104, 10}},
			102, 105,
			[]entry{{102, 10}, {103, 10}, {104, 10}},
		},
		{
			[]entryRange{{101, 105, 5}, {102, 104, 10}},
			103, 105,
			[]entry{{103, 10}, {104, 10}},
		},
		{
			[]entryRange{{101, 105, 5}, {102, 104, 10}},
			101, 105,
			[]entry{{101, 5}, {102, 10}, {103, 10}, {104, 10}},
		},
		{
			[]entryRange{{101, 105, 5}, {102, 104, 10}, {100, 105, 15}},
			100, 105,
			[]entry{{100, 15}, {101, 15}, {102, 15}, {103, 15}, {104, 15}},
		},
		{
			[]entryRange{{101, 105, 5}, {102, 104, 10}, {103, 105, 15}},
			101, 105,
			[]entry{{101, 5}, {102, 10}, {103, 15}, {104, 15}},
		},
	}

	buf := make([]byte, 1024)
	for testIdx, tt := range tests {
		idx := testIdx
		input := tt.input
		expected := tt.expected
		low := tt.low
		high := tt.high
		func() {
			fs := vfs.NewMem()
			tf := func(t *testing.T, db *db) {
				for _, ir := range input {
					u := pb.Update{ShardID: 2, ReplicaID: 3}
					for j := ir.start; j <= ir.end; j++ {
						u.EntriesToSave = append(u.EntriesToSave, pb.Entry{Index: j, Term: ir.term})
					}
					_, err := db.write(u, buf)
					require.NoError(t, err)
				}
			}
			runTanTest(t, nil, tf, fs)

			tf = func(t *testing.T, db *db) {
				var result []pb.Entry
				entries, _, err := db.getEntries(2, 3, result, 0, low, high, 1024)
				require.NoError(t, err)
				require.Equal(t, len(expected), len(entries))
				for j, r := range expected {
					require.Equalf(t, r.index, entries[j].Index, "idx: %d, j: %d", idx, j)
					require.Equalf(t, r.term, entries[j].Term, "idx: %d, j: %d", idx, j)
				}
			}
			runTanTest(t, nil, tf, fs)
		}()
	}
}

func TestEmptyEntryUpdate(t *testing.T) {
	fs := vfs.NewMem()
	tf := func(t *testing.T, db *db) {
		u1 := pb.Update{
			ShardID:   2,
			ReplicaID: 3,
			EntriesToSave: []pb.Entry{
				{Index: 1, Term: 5},
			},
		}
		u2 := pb.Update{
			ShardID:   2,
			ReplicaID: 3,
			Snapshot: pb.Snapshot{
				Index: 1,
				Term:  5,
			},
		}
		u3 := pb.Update{
			ShardID:   2,
			ReplicaID: 3,
			State: pb.State{
				Commit: 1,
				Term:   5,
			},
		}
		u4 := pb.Update{
			ShardID:   2,
			ReplicaID: 3,
			EntriesToSave: []pb.Entry{
				{Index: 2, Term: 5},
			},
		}
		buf := make([]byte, 1024)
		_, err1 := db.write(u1, buf)
		require.NoError(t, err1)
		_, err2 := db.write(u2, buf)
		require.NoError(t, err2)
		_, err3 := db.write(u3, buf)
		require.NoError(t, err3)
		_, err4 := db.write(u4, buf)
		require.NoError(t, err4)
		var result []pb.Entry
		entries, _, err := db.getEntries(2, 3, result, 0, 1, 3, 10240)
		require.NoError(t, err)
		require.Equal(t, 2, len(entries))
	}
	runTanTest(t, nil, tf, fs)
}

func TestSnapshotUpdate(t *testing.T) {
	fs := vfs.NewMem()
	tf := func(t *testing.T, db *db) {
		u1 := pb.Update{
			ShardID:   2,
			ReplicaID: 3,
			Snapshot: pb.Snapshot{
				Index: 100,
				Term:  5,
			},
		}
		u2 := pb.Update{
			ShardID:   2,
			ReplicaID: 3,
			Snapshot: pb.Snapshot{
				Index: 90,
				Term:  5,
			},
		}
		u3 := pb.Update{
			ShardID:   2,
			ReplicaID: 3,
			Snapshot: pb.Snapshot{
				Index: 80,
				Term:  5,
			},
		}
		buf := make([]byte, 1024)
		_, err1 := db.write(u1, buf)
		require.NoError(t, err1)
		_, err2 := db.write(u2, buf)
		require.NoError(t, err2)
		_, err3 := db.write(u3, buf)
		require.NoError(t, err3)
		snapshot, err := db.getSnapshot(2, 3)
		require.NoError(t, err)
		require.Equal(t, uint64(100), snapshot.Index)
	}
	runTanTest(t, nil, tf, fs)
}

func TestLogRotation(t *testing.T) {
	fs := vfs.NewMem()
	opts := &Options{
		MaxLogFileSize:      1024,
		MaxManifestFileSize: MaxManifestFileSize,
		FS:                  fs,
	}
	tf := func(t *testing.T, db *db) {
		logNum := db.mu.logNum
		fn := makeFilename(opts.FS, db.dirname, fileTypeLog, logNum)
		_, err := opts.FS.Stat(fn)
		require.NoError(t, err)
		u := pb.Update{
			ShardID:   2,
			ReplicaID: 3,
			State: pb.State{
				Commit: 100,
				Term:   5,
				Vote:   3,
			},
			Snapshot: pb.Snapshot{
				Index: 100,
				Term:  5,
			},
			EntriesToSave: []pb.Entry{
				{Index: 0, Term: 5},
			},
		}
		buf := make([]byte, 1024)
		for i := uint64(1); i <= uint64(100); i++ {
			u.EntriesToSave[0].Index = i
			_, err := db.write(u, buf)
			require.NoError(t, err)
		}
		require.NotEqual(t, logNum, db.mu.logNum)
		fn = makeFilename(opts.FS, db.dirname, fileTypeLog, db.mu.logNum)
		_, err = opts.FS.Stat(fn)
		require.NoError(t, err)
		// check we can query across multiple logs
		var result []pb.Entry
		entries, _, err := db.getEntries(2, 3, result, 0, 1, 100, math.MaxUint64)
		require.NoError(t, err)
		require.Equal(t, 99, len(entries))
		for i := uint64(1); i < uint64(100); i++ {
			require.Equal(t, i, entries[i-1].Index)
		}
		// don't assume when the rotation happened, just check again to make sure the
		// db is accessible by writes
		_, err = db.write(u, buf)
		require.NoError(t, err)
	}
	runTanTest(t, opts, tf, fs)
}

func TestDBRestart(t *testing.T) {
	fs := vfs.NewMem()
	opts := &Options{
		MaxLogFileSize:      1024,
		MaxManifestFileSize: MaxManifestFileSize,
		FS:                  fs,
	}
	var logNum fileNum
	tf := func(t *testing.T, db *db) {
		u := pb.Update{
			ShardID:   2,
			ReplicaID: 3,
			State: pb.State{
				Commit: 100,
				Term:   5,
				Vote:   3,
			},
			EntriesToSave: []pb.Entry{
				{Index: 0, Term: 5},
			},
		}
		buf := make([]byte, 1024)
		for i := uint64(1); i <= uint64(100); i++ {
			u.EntriesToSave[0].Index = i
			_, err := db.write(u, buf)
			require.NoError(t, err)
		}
		logNum = db.mu.logNum
	}
	runTanTest(t, opts, tf, fs)

	tf = func(t *testing.T, db *db) {
		require.NotEqual(t, logNum, db.mu.logNum)
		// this will write an entry with index 100 term 6
		// query should return this entry
		u := pb.Update{
			ShardID:   2,
			ReplicaID: 3,
			State: pb.State{
				Commit: 100,
				Term:   5,
				Vote:   3,
			},
			Snapshot: pb.Snapshot{
				Index: 100,
				Term:  5,
			},
			EntriesToSave: []pb.Entry{
				{Index: 100, Term: 6},
			},
		}
		buf := make([]byte, 1024)
		_, err := db.write(u, buf)
		require.NoError(t, err)
		var result []pb.Entry
		entries, _, err := db.getEntries(2, 3, result, 0, 1, 100, math.MaxUint64)
		require.NoError(t, err)
		require.Equal(t, 99, len(entries))
		for i := uint64(1); i < uint64(100); i++ {
			require.Equal(t, i, entries[i-1].Index)
		}
		require.Equal(t, uint64(5), entries[len(entries)-1].Term)
		snapshot, err := db.getSnapshot(2, 3)
		require.NoError(t, err)
		require.Equal(t, uint64(100), snapshot.Index)

		rs, err := db.getRaftState(2, 3, 1)
		require.NoError(t, err)
		require.Equal(t, u.State, rs.State)
		require.Equal(t, uint64(2), rs.FirstIndex)
		require.Equal(t, uint64(99), rs.EntryCount)
	}
	runTanTest(t, opts, tf, fs)
}

func TestDBConcurrentAccess(t *testing.T) {
	defer leaktest.AfterTest(t)()
	fs := vfs.NewMem()
	defer vfs.ReportLeakedFD(fs, t)
	opts := &Options{
		MaxLogFileSize:      1,
		MaxManifestFileSize: MaxManifestFileSize,
		FS:                  fs,
	}
	dirname := "db-dir"
	require.NoError(t, fs.MkdirAll(dirname, 0700))
	db, err := open(dirname, dirname, opts)
	require.NoError(t, err)
	defer func() {
		require.NoError(t, db.close())
	}()
	var wg sync.WaitGroup
	wg.Add(1)
	go func() {
		defer wg.Done()
		buf := make([]byte, 1024)
		for i := uint64(1); i <= uint64(1000); i++ {
			u := pb.Update{
				ShardID:   2,
				ReplicaID: 3,
				State: pb.State{
					Commit: i,
					Term:   6,
				},
				Snapshot: pb.Snapshot{
					Index: i,
					Term:  6,
				},
				EntriesToSave: []pb.Entry{
					{Index: i, Term: 6},
				},
			}
			_, err := db.write(u, buf)
			require.NoError(t, err)
		}
	}()
	wg.Add(1)
	go func() {
		defer wg.Done()
		buf := make([]byte, 1024)
		for i := uint64(1); i <= uint64(1000); i++ {
			if i%uint64(10) == 0 {
				require.NoError(t, db.removeAll(2, 3))
			} else if i%uint64(11) == 0 {
				if err := db.removeEntries(2, 3, i); err != nil {
					if err != ErrNoState {
						t.Errorf("failed to remove entries %v", err)
					}
				}
			} else {
				u := pb.Update{
					ShardID:   2,
					ReplicaID: 3,
					State: pb.State{
						Commit: i,
						Term:   5,
					},
					Snapshot: pb.Snapshot{
						Index: i,
						Term:  5,
					},
					EntriesToSave: []pb.Entry{
						{Index: i, Term: 5},
					},
				}
				_, err := db.write(u, buf)
				require.NoError(t, err)
			}
		}
	}()
	wg.Add(1)
	go func() {
		defer wg.Done()
		for i := 0; i < 1000; i++ {
			var result []pb.Entry
			_, _, err := db.getEntries(2, 3, result, 0, 1, 100, math.MaxUint64)
			require.NoError(t, err)
			_, err = db.getSnapshot(2, 3)
			require.NoError(t, err)
			_, err = db.getRaftState(2, 3, 1)
			require.True(t, err == nil || errors.Is(err, raftio.ErrNoSavedLog))
		}
	}()
	wg.Wait()
}

func TestDBIndexIsSavedOnClose(t *testing.T) {
	fs := vfs.NewMem()
	var logNum fileNum
	var index []indexEntry
	var dirname string
	tf := func(t *testing.T, db *db) {
		buf := make([]byte, 1024)
		for i := uint64(1); i <= uint64(10); i++ {
			u := pb.Update{
				ShardID:   2,
				ReplicaID: 3,
				EntriesToSave: []pb.Entry{
					{Index: i, Term: 6},
				},
			}
			_, err := db.write(u, buf)
			require.NoError(t, err)
		}
		dirname = db.dirname
		logNum = db.mu.logNum
		index = db.mu.nodeStates.getIndex(2, 3).currEntries.entries
	}
	runTanTest(t, nil, tf, fs)
	fn := makeFilename(fs, dirname, fileTypeIndex, logNum)
	_, err := fs.Stat(fn)
	require.NoError(t, err)
	require.True(t, len(index) > 0)
}

func TestRebuildIndex(t *testing.T) {
	fs := vfs.NewMem()
	var logNum fileNum
	var savedIndex index
	var snapshot indexEntry
	var dirname string
	tf := func(t *testing.T, db *db) {
		dirname = db.dirname
		buf := make([]byte, 1024)
		for i := uint64(1); i <= uint64(1000); i++ {
			u := pb.Update{
				ShardID:   2,
				ReplicaID: 3,
				EntriesToSave: []pb.Entry{
					{Index: i, Term: 5},
				},
				Snapshot: pb.Snapshot{Index: 300, Term: 10},
			}
			_, err := db.write(u, buf)
			require.NoError(t, err)
		}
		require.NoError(t, db.removeEntries(2, 3, 500))
		logNum = db.mu.logNum
		savedIndex = db.mu.nodeStates.getIndex(2, 3).entries
		snapshot = db.mu.nodeStates.getIndex(2, 3).snapshot
	}
	runTanTest(t, nil, tf, fs)

	fn := makeFilename(fs, dirname, fileTypeIndex, logNum)
	require.NoError(t, fs.RemoveAll(fn))

	tf = func(t *testing.T, db *db) {
		require.Equal(t, savedIndex, db.mu.nodeStates.getIndex(2, 3).entries)
		require.Equal(t, snapshot, db.mu.nodeStates.getIndex(2, 3).snapshot)
		require.Equal(t, uint64(500), db.mu.nodeStates.compactedTo(2, 3))
	}
	runTanTest(t, nil, tf, fs)
}

func TestRebuildLog(t *testing.T) {
	defer leaktest.AfterTest(t)()
	fs := vfs.Default
	defer vfs.ReportLeakedFD(fs, t)
	opts := &Options{
		MaxManifestFileSize: MaxManifestFileSize,
		FS:                  fs,
	}
	dirname := "db-dir"
	require.NoError(t, fs.RemoveAll(dirname))
	require.NoError(t, fs.MkdirAll(dirname, 0700))
	defer func() {
		require.NoError(t, fs.RemoveAll(dirname))
	}()
	db, err := open(dirname, dirname, opts)
	require.NoError(t, err)
	buf := make([]byte, 1024)
	for i := uint64(1); i <= uint64(20); i++ {
		u := pb.Update{
			ShardID:   2,
			ReplicaID: 3,
			EntriesToSave: []pb.Entry{
				{Index: i, Term: 5, Cmd: make([]byte, 32)},
			},
		}
		_, err := db.write(u, buf)
		require.NoError(t, err)
	}
	logNum := db.mu.logNum
	require.NoError(t, db.close())
	logFn := makeFilename(fs, dirname, fileTypeLog, logNum)
	idxFn := makeFilename(fs, dirname, fileTypeIndex, logNum)
	lf, err := os.OpenFile(logFn, os.O_RDWR, 0755)
	require.NoError(t, err)
	fi, err := lf.Stat()
	require.NoError(t, err)
	// truncate the file, remove the index
	require.NoError(t, lf.Truncate(fi.Size()-16))
	require.NoError(t, lf.Close())
	require.NoError(t, fs.RemoveAll(idxFn))
	db, err = open(dirname, dirname, opts)
	require.NoError(t, err)
	var result []pb.Entry
	result, _, err = db.getEntries(2, 3, result, 0, 1, 21, math.MaxUint64)
	require.NoError(t, err)
	require.Equal(t, 19, len(result))
	require.Equal(t, uint64(19), result[len(result)-1].Index)
	require.NoError(t, db.close())
	lf, err = os.Open(logFn)
	require.NoError(t, err)
	fi2, err := lf.Stat()
	require.NoError(t, err)
	require.Equal(t, fi.Size()*19/20, fi2.Size())
}

func TestGetEntriesWithMaxSize(t *testing.T) {
	fs := vfs.NewMem()
	opts := &Options{
		MaxManifestFileSize: MaxManifestFileSize,
		MaxLogFileSize:      1024,
		FS:                  fs,
	}
	tf := func(t *testing.T, db *db) {
		cmd := make([]byte, 128)
		buf := make([]byte, 1024)
		for i := 0; i < 128; i++ {
			u := pb.Update{
				ShardID:   2,
				ReplicaID: 3,
				EntriesToSave: []pb.Entry{
					{Index: 1 + uint64(i), Term: 5, Cmd: cmd},
				},
			}
			_, err := db.write(u, buf)
			require.NoError(t, err)
		}
		entries, _, err := db.getEntries(2, 3, nil, 0, 1, 128, 128)
		require.NoError(t, err)
		require.Equal(t, 1, len(entries))
	}
	runTanTest(t, opts, tf, fs)
}
````

## File: internal/tan/db.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"bytes"
	"io"
	"math"
	"sync"
	"sync/atomic"

	"github.com/cockroachdb/errors"
	"github.com/lni/dragonboat/v4/logger"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/lni/goutils/syncutil"
	"github.com/lni/vfs"
)

var (
	plog = logger.GetLogger("tan")
)

const (
	stateFlag      uint64 = math.MaxUint64
	snapshotFlag   uint64 = math.MaxUint64 - 1
	compactionFlag uint64 = math.MaxUint64 - 2
)

var (
	// ErrClosed is the error used to indicate that the db has already been closed
	ErrClosed = errors.New("db closed")
	// ErrNoBootstrap is the error used to indicate that there is no saved
	// bootstrap record
	ErrNoBootstrap = errors.New("no bootstrap info")
	// ErrNoState is the error indicating that there is no state record in the db
	ErrNoState = errors.New("no state record")
)

// db is basically an instance of the core tan storage, it holds required
// resources and manages log and index data.
type db struct {
	name     string
	closed   atomic.Value
	closedCh chan struct{}
	opts     *Options
	dataDir  vfs.File
	dirname  string

	// for asynchronously delete files in the background
	deleteObsoleteCh     chan struct{}
	deleteobsoleteWorker *syncutil.Stopper

	// help to determine what is the current visible view of the data
	readState struct {
		sync.RWMutex
		val *readState
	}

	// where/how log data is maintained
	mu struct {
		sync.Mutex
		offset     int64
		logNum     fileNum
		logFile    vfs.File
		logWriter  *writer
		versions   *versionSet
		nodeStates *nodeStates
	}
}

func stateSyncChange(a, b pb.State) bool {
	return a.Term != b.Term || a.Vote != b.Vote
}

// write writes the update instance to the log and returns a boolean flag
// indicating whether a fsync() operation is required. Each pb.Update instance
// contains any number of raft entries, it is also possible to have raft
// snapshot info and raft state in it. Each of such pb.Update written into the
// db is the unit of
func (d *db) write(u pb.Update, buf []byte) (bool, error) {
	sz := u.SizeUpperLimit()
	if sz > len(buf) {
		buf = make([]byte, sz)
	}
	data := pb.MustMarshalTo(&u, buf)
	if _, ok := isCompactionUpdate(u); ok {
		panic("trying to write a compaction update")
	}
	d.mu.Lock()
	defer d.mu.Unlock()
	st := d.mu.nodeStates.getState(u.ShardID, u.ReplicaID)
	if pb.IsStateEqual(u.State, st) &&
		pb.IsEmptySnapshot(u.Snapshot) && len(u.EntriesToSave) == 0 {
		return false, nil
	}
	sync := !pb.IsEmptySnapshot(u.Snapshot) ||
		len(u.EntriesToSave) > 0 || stateSyncChange(u.State, st)
	return sync, d.doWriteLocked(u, data)
}

func (d *db) doWriteLocked(u pb.Update, data []byte) error {
	if err := d.makeRoomForWrite(); err != nil {
		return err
	}
	offset, err := d.mu.logWriter.writeRecord(data)
	if err != nil {
		return err
	}
	d.updateIndex(u, d.mu.offset, d.mu.logNum)
	d.mu.offset = offset
	d.mu.nodeStates.setState(u.ShardID, u.ReplicaID, u.State)
	return nil
}

// sync issues a fsync() operation on the underlying log file.
func (d *db) sync() error {
	return d.mu.logFile.Sync()
}

// updateIndex records the fileNum and position of the written update into the
// index.
func (d *db) updateIndex(update pb.Update, pos int64, logNum fileNum) {
	index := d.mu.nodeStates.getIndex(update.ShardID, update.ReplicaID)
	compactedTo, compactionUpdate := isCompactionUpdate(update)
	ei := indexEntry{
		pos:     pos,
		fileNum: logNum,
	}
	if compactionUpdate {
		// entry compaction
		index.currEntries.setCompactedTo(compactedTo)
		index.entries.setCompactedTo(compactedTo)
	} else {
		// regular entries
		if len(update.EntriesToSave) > 0 {
			ei.start = update.EntriesToSave[0].Index
			ei.end = update.EntriesToSave[len(update.EntriesToSave)-1].Index
			index.entries.update(ei)
			index.currEntries.update(ei)
		}
		// regular snapshot
		if !pb.IsEmptySnapshot(update.Snapshot) {
			ei.start = update.Snapshot.Index
			ei.end = snapshotFlag
			if index.snapshot.start < ei.start {
				index.snapshot = ei
			}
		}
		// regular state
		if !pb.IsEmptyState(update.State) {
			ei.start = update.Commit
			ei.end = stateFlag
			index.state = ei
		}
	}
}

func (d *db) makeRoomForWrite() error {
	if d.mu.offset < d.opts.MaxLogFileSize {
		return nil
	}
	return d.switchToNewLog()
}

// switchToNewLog flushes the index of the current log file to disk, update the
// readState hold by the db and then switch to a new log file.
func (d *db) switchToNewLog() error {
	if err := d.saveIndex(); err != nil {
		return err
	}
	defer d.updateReadStateLocked(nil)
	return d.createNewLog()
}

// getSnapshot returns the latest snapshot in the db. we record all seen
// snapshots into the db, but will only query for the most recent snapshot
// inserted into the db.
func (d *db) getSnapshot(shardID uint64, replicaID uint64) (pb.Snapshot, error) {
	d.mu.Lock()
	readState := d.loadReadState()
	ies, ok := readState.nodeStates.querySnapshot(shardID, replicaID)
	d.mu.Unlock()
	defer readState.unref()
	if !ok {
		return pb.Snapshot{}, nil
	}
	var snapshot pb.Snapshot
	f := func(u pb.Update, _ int64) bool {
		if pb.IsEmptySnapshot(u.Snapshot) {
			panic("empty snapshot")
		}
		snapshot = u.Snapshot
		return false
	}
	if err := d.readLog(ies, f); err != nil {
		return pb.Snapshot{}, err
	}
	return snapshot, nil
}

// getRaftState returns the raft state. Such a raft state record contains a
// pb.State value and raft entry details expressed as FirstIndex of the entry
// and EntryCount. The pb.State value returned in the State field of
// raftio.RaftState is the latest raft state written into the db.
func (d *db) getRaftState(shardID uint64, replicaID uint64,
	lastIndex uint64) (raftio.RaftState, error) {
	d.mu.Lock()
	readState := d.loadReadState()
	ie, ok := readState.nodeStates.queryState(shardID, replicaID)
	ies, _ := readState.nodeStates.query(shardID, replicaID, lastIndex+1, math.MaxUint64)
	d.mu.Unlock()
	defer readState.unref()
	if !ok {
		return raftio.RaftState{}, raftio.ErrNoSavedLog
	}
	var st raftio.RaftState
	if err := d.readLog(ie, func(u pb.Update, _ int64) bool {
		if pb.IsEmptyState(u.State) {
			panic("empty state")
		}
		st.State = u.State
		return false
	}); err != nil {
		return raftio.RaftState{}, err
	}
	prevIndex := uint64(0)
	for _, e := range ies {
		if prevIndex != 0 && prevIndex+1 != e.start {
			panic("gap in indexes")
		}
		prevIndex = e.end
	}
	if len(ies) > 0 {
		st.FirstIndex = lastIndex + 1
		st.EntryCount = ies[len(ies)-1].end - st.FirstIndex + 1
	}
	return st, nil
}

// getEntries queries the db to return raft entries between [low, high), the
// max size of the returned entries is maxSize bytes. The results will be
// appended into the input entries slice which is already size bytes in size.
func (d *db) getEntries(shardID uint64, replicaID uint64,
	entries []pb.Entry, size uint64, low uint64,
	high uint64, maxSize uint64) ([]pb.Entry, uint64, error) {
	d.mu.Lock()
	readState := d.loadReadState()
	ies, ok := readState.nodeStates.query(shardID, replicaID, low, high)
	compactedTo := readState.nodeStates.compactedTo(shardID, replicaID)
	d.mu.Unlock()
	defer readState.unref()
	if !ok {
		return entries, size, nil
	}
	if low <= compactedTo {
		return entries, size, nil
	}
	if maxSize == 0 {
		maxSize = math.MaxUint64
	}
	expected := low
	done := false
	for _, ie := range ies {
		queryIndex := ie
		f := func(u pb.Update, _ int64) bool {
			// TODO: optimize this, not to append one by one
			for _, e := range u.EntriesToSave {
				nsz := uint64(e.SizeUpperLimit())
				if e.Index < expected {
					continue
				}
				if e.Index == expected && e.Index < high &&
					e.Index >= queryIndex.start && e.Index <= queryIndex.end {
					size += nsz
					expected++
					if len(entries) > 0 && entries[len(entries)-1].Index+1 != e.Index {
						panic("gap in entry index")
					}
					entries = append(entries, e)
					if size > maxSize {
						done = true
						return false
					}
				} else {
					return false
				}
			}
			return true
		}
		if err := d.readLog(ie, f); err != nil {
			return nil, 0, err
		}
		if done {
			return entries, size, nil
		}
	}
	return entries, size, nil
}

// readLog queries the db for the saved pb.Update record identified by the
// specified indexEntry parameter. For each encountered pb.Update record,
// h will be invoked with the encountered pb.Update value passed to it.
func (d *db) readLog(ie indexEntry,
	h func(u pb.Update, offset int64) bool) (err error) {
	fn := makeFilename(d.opts.FS, d.dirname, fileTypeLog, ie.fileNum)
	f, err := d.opts.FS.Open(fn)
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, f.Close())
	}()
	rr := newReader(f, ie.fileNum)
	if ie.pos > 0 {
		if err := rr.seekRecord(ie.pos); err != nil {
			return errors.WithStack(err)
		}
	}
	var buf bytes.Buffer
	var r io.Reader
	for {
		offset := rr.offset()
		r, err = rr.next()
		if err != nil {
			if err == io.EOF {
				return nil
			}
			return errors.WithStack(err)
		}
		if _, err = io.Copy(&buf, r); err != nil {
			if err == io.EOF {
				return nil
			}
			return errors.Wrap(err, "error when reading WAL")
		}
		var update pb.Update
		pb.MustUnmarshal(&update, buf.Bytes())
		if !h(update, offset) {
			break
		}
		buf.Reset()
	}
	return nil
}
````

## File: internal/tan/filename_test.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"testing"

	"github.com/lni/vfs"
	"github.com/stretchr/testify/require"
)

func TestParseFilename(t *testing.T) {
	tests := []struct {
		fileType    fileType
		fileNum     fileNum
		ok          bool
		needFileNum bool
	}{
		{fileTypeLog, fileNum(10), true, true},
		{fileTypeLogTemp, fileNum(10), true, true},
		{fileTypeIndex, fileNum(10), true, true},
		{fileTypeIndexTemp, fileNum(10), true, true},
		{fileTypeLock, fileNum(0), true, false},
		{fileTypeManifest, fileNum(10), true, true},
		{fileTypeCurrent, fileNum(0), true, false},
		{fileTypeTemp, fileNum(10), true, false},
	}

	fs := vfs.NewMem()
	dbdir := "db-dir"
	for idx, tt := range tests {
		fn := makeFilename(fs, dbdir, tt.fileType, tt.fileNum)
		ft, logNum, ok := parseFilename(fs, fn)
		require.Equalf(t, tt.ok, ok, "idx: %d", idx)
		require.Equalf(t, tt.fileType, ft, "idx: %d", idx)
		if tt.needFileNum {
			require.Equalf(t, tt.fileNum, logNum, "idx: %d", idx)
		}
	}
}

func TestMakeBootstrapFilename(t *testing.T) {
	fs := vfs.NewMem()
	dbdir := "db-dir"
	fn := makeBootstrapFilename(fs, dbdir, 2, 3, false)
	require.Equal(t, "db-dir/BOOTSTRAP-2-3", fn)
	fn = makeBootstrapFilename(fs, dbdir, 2, 3, true)
	require.Equal(t, "db-dir/BOOTSTRAP-2-3.tmp", fn)
}

func TestParseBootstrapFilename(t *testing.T) {
	fn := "BOOTSTRAP-2-3"
	shardID, replicaID, ok := parseBootstrapFilename(fn)
	require.Equal(t, true, ok)
	require.Equal(t, uint64(2), shardID)
	require.Equal(t, uint64(3), replicaID)
	fn = "BOOTSTRAP-2-"
	_, _, ok = parseBootstrapFilename(fn)
	require.Equal(t, false, ok)
}
````

## File: internal/tan/filename.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"fmt"
	"regexp"
	"strconv"
	"strings"

	"github.com/lni/vfs"
)

var (
	bsFilenameRe    = regexp.MustCompile(`^BOOTSTRAP-([0-9]+)-([0-9]+)$`)
	tmpBSFilenameRe = regexp.MustCompile(`^BOOTSTRAP-[0-9]+-[0-9]+.tmp$`)
)

type fileNum uint64

// String returns a string representation of the file number.
func (fn fileNum) String() string { return fmt.Sprintf("%06d", fn) }

// fileType enumerates the types of files found in a DB.
type fileType int

// The FileType enumeration.
const (
	fileTypeLog fileType = iota
	fileTypeLogTemp
	fileTypeIndex
	fileTypeIndexTemp
	fileTypeBootstrap
	fileTypeLock
	fileTypeManifest
	fileTypeCurrent
	fileTypeTemp
	fileTypeBootstrapTemp
)

func setCurrentFile(dirname string, fs vfs.FS, fileNum fileNum) (err error) {
	newFilename := makeFilename(fs, dirname, fileTypeCurrent, fileNum)
	oldFilename := makeFilename(fs, dirname, fileTypeTemp, fileNum)
	if err := fs.RemoveAll(oldFilename); err != nil {
		return err
	}
	f, err := fs.Create(oldFilename)
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, f.Sync())
		err = firstError(err, f.Close())
		if err == nil {
			err = fs.Rename(oldFilename, newFilename)
		}
	}()
	v := []byte(fmt.Sprintf("MANIFEST-%s\n", fileNum))
	w := newWriter(f)
	defer func() {
		err = firstError(err, w.close())
	}()
	if _, err = w.writeRecord(v); err != nil {
		return err
	}
	return nil
}

func makeBootstrapFilename(fs vfs.FS,
	dirname string, shardID uint64, replicaID uint64, tmp bool) string {
	pattern := "BOOTSTRAP-%d-%d"
	if tmp {
		pattern = "BOOTSTRAP-%d-%d.tmp"
	}
	return fs.PathJoin(dirname, fmt.Sprintf(pattern, shardID, replicaID))
}

func parseBootstrapFilename(filename string) (uint64, uint64, bool) {
	if parts := bsFilenameRe.FindStringSubmatch(filename); len(parts) == 3 {
		shardID, err := strconv.ParseUint(parts[1], 10, 64)
		if err != nil {
			return 0, 0, false
		}
		replicaID, err := strconv.ParseUint(parts[2], 10, 64)
		if err != nil {
			return 0, 0, false
		}
		return shardID, replicaID, true
	}
	return 0, 0, false
}

// makeFilename builds a filename from components.
func makeFilename(fs vfs.FS, dirname string, fileType fileType, fileNum fileNum) string {
	switch fileType {
	case fileTypeLog:
		return fs.PathJoin(dirname, fmt.Sprintf("%s.log", fileNum))
	case fileTypeLogTemp:
		return fs.PathJoin(dirname, fmt.Sprintf("%s.logtmp", fileNum))
	case fileTypeIndex:
		return fs.PathJoin(dirname, fmt.Sprintf("%s.index", fileNum))
	case fileTypeIndexTemp:
		return fs.PathJoin(dirname, fmt.Sprintf("%s.idxtmp", fileNum))
	case fileTypeBootstrap:
		panic("use makeBootstrapFilename")
	case fileTypeLock:
		return fs.PathJoin(dirname, "LOCK")
	case fileTypeManifest:
		return fs.PathJoin(dirname, fmt.Sprintf("MANIFEST-%s", fileNum))
	case fileTypeCurrent:
		return fs.PathJoin(dirname, "CURRENT")
	case fileTypeTemp:
		return fs.PathJoin(dirname, fmt.Sprintf("CURRENT-%s.dbtmp", fileNum))
	case fileTypeBootstrapTemp:
		panic("use makeBootstrapFilename")
	}
	panic("unreachable")
}

// parseFilename parses the components from a filename.
func parseFilename(fs vfs.FS, filename string) (fileType fileType, fileNum fileNum, ok bool) {
	filename = fs.PathBase(filename)
	switch {
	case filename == "CURRENT":
		return fileTypeCurrent, 0, true
	case filename == "LOCK":
		return fileTypeLock, 0, true
	case filename == "BOOTSTRAP":
		panic("unexpected BOOTSTRAP file")
	case filename == "BOOTSTRAP.tmp":
		panic("unexpected BOOTSTRAP.tmp")
	case strings.HasPrefix(filename, "MANIFEST-"):
		fileNum, ok = parseFileNum(filename[len("MANIFEST-"):])
		if !ok {
			break
		}
		return fileTypeManifest, fileNum, true
	case strings.HasPrefix(filename, "CURRENT-") && strings.HasSuffix(filename, ".dbtmp"):
		s := strings.TrimSuffix(filename[len("CURRENT-"):], ".dbtmp")
		fileNum, ok = parseFileNum(s)
		if !ok {
			break
		}
		return fileTypeTemp, fileNum, ok
	default:
		i := strings.IndexByte(filename, '.')
		if i < 0 {
			break
		}
		fileNum, ok = parseFileNum(filename[:i])
		if !ok {
			break
		}
		switch filename[i+1:] {
		case "log":
			return fileTypeLog, fileNum, true
		case "index":
			return fileTypeIndex, fileNum, true
		case "idxtmp":
			return fileTypeIndexTemp, fileNum, true
		case "logtmp":
			return fileTypeLogTemp, fileNum, true
		}
	}
	return 0, fileNum, false
}

func parseFileNum(s string) (fileNum, bool) {
	u, err := strconv.ParseUint(s, 10, 64)
	if err != nil {
		return 0, false
	}
	return fileNum(u), true
}
````

## File: internal/tan/index_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"bufio"
	"bytes"
	"math"
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/raftio"
	"github.com/lni/vfs"
)

func TestEntryAppend(t *testing.T) {
	tests := []struct {
		existing indexEntry
		newEntry indexEntry
		result1  indexEntry
		result2  indexEntry
		merged   bool
	}{
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{20, 30, 100, 100, 10},
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{20, 30, 100, 100, 10},
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{18, 30, 100, 100, 10},
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{18, 30, 100, 100, 10},
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{10, 30, 100, 100, 10},
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{10, 30, 100, 100, 10},
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{21, 30, 100, 110, 10},
			indexEntry{10, 30, 100, 100, 20},
			indexEntry{},
			true,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{22, 30, 100, 100, 10},
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{22, 30, 100, 100, 10},
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{8, 30, 100, 100, 10},
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{8, 30, 100, 100, 10},
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{21, 30, 101, 100, 10},
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{21, 30, 101, 100, 10},
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{21, 30, 100, 100 + indexBlockSize, 10},
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{21, 30, 100, 100 + indexBlockSize, 10},
			false,
		},
		{
			indexEntry{10, 20, 100, 0, indexBlockSize - 1},
			indexEntry{21, 30, 100, indexBlockSize - 1, 10},
			indexEntry{10, 30, 100, 0, indexBlockSize + 9},
			indexEntry{},
			true,
		},
	}
	for idx, tt := range tests {
		e1, e2, merged := tt.existing.merge(tt.newEntry)
		require.Equalf(t, tt.result1, e1, "idx: %d", idx)
		require.Equalf(t, tt.result2, e2, "idx: %d", idx)
		require.Equalf(t, tt.merged, merged, "idx: %d", idx)
	}
}

func TestEntryAndSingleEntryIndexUpdate(t *testing.T) {
	tests := []struct {
		existing indexEntry
		newEntry indexEntry
		result1  indexEntry
		result2  indexEntry
		merged   bool
		more     bool
	}{
		// #0
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{21, 30, 100, 110, 10},
			indexEntry{10, 30, 100, 100, 20},
			indexEntry{},
			true,
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{21, 30, 101, 105, 10},
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{21, 30, 101, 105, 10},
			false,
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{20, 30, 101, 105, 10},
			indexEntry{10, 19, 100, 100, 10},
			indexEntry{20, 30, 101, 105, 10},
			false,
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{20, 30, 100, 105, 10},
			indexEntry{10, 19, 100, 100, 10},
			indexEntry{20, 30, 100, 105, 10},
			false,
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{15, 30, 100, 105, 10},
			indexEntry{10, 14, 100, 100, 10},
			indexEntry{15, 30, 100, 105, 10},
			false,
			false,
		},
		// #5
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{15, 30, 101, 105, 10},
			indexEntry{10, 14, 100, 100, 10},
			indexEntry{15, 30, 101, 105, 10},
			false,
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{15, 16, 100, 105, 10},
			indexEntry{10, 14, 100, 100, 10},
			indexEntry{15, 16, 100, 105, 10},
			false,
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{5, 30, 100, 105, 10},
			indexEntry{5, 30, 100, 105, 10},
			indexEntry{},
			true,
			true,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{5, 30, 101, 105, 10},
			indexEntry{5, 30, 101, 105, 10},
			indexEntry{},
			true,
			true,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{5, 20, 100, 100, 10},
			indexEntry{5, 20, 100, 100, 10},
			indexEntry{},
			true,
			true,
		},
		// #10
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{5, 20, 101, 105, 10},
			indexEntry{5, 20, 101, 105, 10},
			indexEntry{},
			true,
			true,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{5, 12, 100, 100, 10},
			indexEntry{5, 12, 100, 100, 10},
			indexEntry{},
			true,
			true,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{5, 12, 101, 105, 10},
			indexEntry{5, 12, 101, 105, 10},
			indexEntry{},
			true,
			true,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{5, 9, 100, 105, 10},
			indexEntry{5, 9, 100, 105, 10},
			indexEntry{},
			true,
			true,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{5, 9, 101, 105, 10},
			indexEntry{5, 9, 101, 105, 10},
			indexEntry{},
			true,
			true,
		},
		// #15
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{10, 20, 101, 105, 10},
			indexEntry{10, 20, 101, 105, 10},
			indexEntry{},
			true,
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{5, 5, 101, 105, 10},
			indexEntry{5, 5, 101, 105, 10},
			indexEntry{},
			true,
			true,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{10, 10, 101, 105, 10},
			indexEntry{10, 10, 101, 105, 10},
			indexEntry{},
			true,
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{15, 15, 101, 105, 10},
			indexEntry{10, 14, 100, 100, 10},
			indexEntry{15, 15, 101, 105, 10},
			false,
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{20, 20, 101, 105, 10},
			indexEntry{10, 19, 100, 100, 10},
			indexEntry{20, 20, 101, 105, 10},
			false,
			false,
		},
		// #20
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{21, 21, 101, 105, 10},
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{21, 21, 101, 105, 10},
			false,
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{5, 5, 100, 105, 10},
			indexEntry{5, 5, 100, 105, 10},
			indexEntry{},
			true,
			true,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{10, 10, 100, 105, 10},
			indexEntry{10, 10, 100, 105, 10},
			indexEntry{},
			true,
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{15, 15, 100, 105, 10},
			indexEntry{10, 14, 100, 100, 10},
			indexEntry{15, 15, 100, 105, 10},
			false,
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{20, 20, 100, 105, 10},
			indexEntry{10, 19, 100, 100, 10},
			indexEntry{20, 20, 100, 105, 10},
			false,
			false,
		},
		// #25
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{21, 21, 100, 110, 10},
			indexEntry{10, 21, 100, 100, 20},
			indexEntry{},
			true,
			false,
		},
		{
			indexEntry{10, 10, 100, 100, 10},
			indexEntry{5, 8, 101, 105, 10},
			indexEntry{5, 8, 101, 105, 10},
			indexEntry{},
			true,
			true,
		},
		{
			indexEntry{10, 10, 100, 100, 10},
			indexEntry{5, 10, 101, 105, 10},
			indexEntry{5, 10, 101, 105, 10},
			indexEntry{},
			true,
			true,
		},
		{
			indexEntry{10, 10, 100, 100, 10},
			indexEntry{10, 15, 101, 105, 10},
			indexEntry{10, 15, 101, 105, 10},
			indexEntry{},
			true,
			false,
		},
		{
			indexEntry{10, 10, 100, 100, 10},
			indexEntry{11, 15, 101, 105, 10},
			indexEntry{10, 10, 100, 100, 10},
			indexEntry{11, 15, 101, 105, 10},
			false,
			false,
		},
		// #30
		{
			indexEntry{10, 10, 100, 100, 10},
			indexEntry{5, 10, 101, 105 + indexBlockSize, 10},
			indexEntry{5, 10, 101, 105 + indexBlockSize, 10},
			indexEntry{},
			true,
			true,
		},
		{
			indexEntry{10, 10, 100, 100, 10},
			indexEntry{10, 15, 101, 105 + indexBlockSize, 10},
			indexEntry{10, 15, 101, 105 + indexBlockSize, 10},
			indexEntry{},
			true,
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{20, 20, 100, 100 + indexBlockSize, 10},
			indexEntry{10, 19, 100, 100, 10},
			indexEntry{20, 20, 100, 100 + indexBlockSize, 10},
			false,
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{10, 10, 100, 100 + indexBlockSize, 10},
			indexEntry{10, 10, 100, 100 + indexBlockSize, 10},
			indexEntry{},
			true,
			false,
		},
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{21, 30, 100, 100 + indexBlockSize, 10},
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{21, 30, 100, 100 + indexBlockSize, 10},
			false,
			false,
		},
		// #35
		{
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{30, 35, 100, 100, 10},
			indexEntry{10, 20, 100, 100, 10},
			indexEntry{30, 35, 100, 100, 10},
			false,
			false,
		},
		{
			indexEntry{27, 27, 3, 47, 10},
			indexEntry{27, 40, 5, 23, 10},
			indexEntry{27, 40, 5, 23, 10},
			indexEntry{},
			true,
			false,
		},
	}

	// entry update
	for idx, tt := range tests {
		e1, e2, merged, more := tt.existing.update(tt.newEntry)
		require.Equalf(t, tt.result1, e1, "idx: %d", idx)
		require.Equalf(t, tt.result2, e2, "idx: %d", idx)
		require.Equalf(t, tt.merged, merged, "idx: %d", idx)
		require.Equalf(t, tt.more, more, "idx: %d", idx)
	}
	// single entry index update
	for idx, tt := range tests {
		testIndex := index{entries: []indexEntry{tt.existing}}
		testIndex.update(tt.newEntry)
		if len(testIndex.entries) == 1 {
			require.Equalf(t, tt.result1, testIndex.entries[0], "idx: %d", idx)
			require.Truef(t, tt.merged, "idx: %d", idx)
		} else if len(testIndex.entries) == 2 {
			require.Equalf(t, tt.result1, testIndex.entries[0], "idx: %d", idx)
			require.Equalf(t, tt.result2, testIndex.entries[1], "idx: %d", idx)
			require.Falsef(t, tt.merged, "idx: %d", idx)
		} else {
			t.Errorf("%d unexpected entry count", idx)
		}
	}
}

func TestIndexUpdate(t *testing.T) {
	tests := []struct {
		existing []indexEntry
		newEntry indexEntry
		result   []indexEntry
	}{
		// #0
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{21, 30, 100, 110, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 30, 100, 100, 20}},
		},

		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{21, 30, 101, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}, {21, 30, 101, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{20, 30, 101, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 19, 100, 100, 10}, {20, 30, 101, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{20, 30, 100, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 19, 100, 100, 10}, {20, 30, 100, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{15, 30, 100, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 14, 100, 100, 10}, {15, 30, 100, 105, 10}},
		},
		// #5
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{15, 30, 101, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 14, 100, 100, 10}, {15, 30, 101, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{15, 16, 100, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 14, 100, 100, 10}, {15, 16, 100, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{5, 30, 100, 105, 10},
			[]indexEntry{{1, 4, 99, 50, 10}, {5, 30, 100, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{5, 30, 101, 105, 10},
			[]indexEntry{{1, 4, 99, 50, 10}, {5, 30, 101, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{5, 20, 100, 100, 10},
			[]indexEntry{{1, 4, 99, 50, 10}, {5, 20, 100, 100, 10}},
		},
		// #10
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{5, 20, 101, 105, 10},
			[]indexEntry{{1, 4, 99, 50, 10}, {5, 20, 101, 105, 10}},
		},

		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{5, 12, 100, 100, 10},
			[]indexEntry{{1, 4, 99, 50, 10}, {5, 12, 100, 100, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{5, 12, 101, 105, 10},
			[]indexEntry{{1, 4, 99, 50, 10}, {5, 12, 101, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{5, 9, 100, 105, 10},
			[]indexEntry{{1, 4, 99, 50, 10}, {5, 9, 100, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{5, 9, 101, 105, 10},
			[]indexEntry{{1, 4, 99, 50, 10}, {5, 9, 101, 105, 10}},
		},
		// #15
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{10, 20, 101, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 101, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{5, 5, 101, 105, 10},
			[]indexEntry{{1, 4, 99, 50, 10}, {5, 5, 101, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{10, 10, 101, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 10, 101, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{15, 15, 101, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 14, 100, 100, 10}, {15, 15, 101, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{20, 20, 101, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 19, 100, 100, 10}, {20, 20, 101, 105, 10}},
		},
		// #20
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{21, 21, 101, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}, {21, 21, 101, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{5, 5, 100, 105, 10},
			[]indexEntry{{1, 4, 99, 50, 10}, {5, 5, 100, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{10, 10, 100, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 10, 100, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{15, 15, 100, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 14, 100, 100, 10}, {15, 15, 100, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{20, 20, 100, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 19, 100, 100, 10}, {20, 20, 100, 105, 10}},
		},
		// #25
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 20, 100, 100, 10}},
			indexEntry{21, 21, 100, 110, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 21, 100, 100, 20}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 10, 100, 100, 10}},
			indexEntry{5, 8, 101, 105, 10},
			[]indexEntry{{1, 4, 99, 50, 10}, {5, 8, 101, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 10, 100, 100, 10}},
			indexEntry{5, 10, 101, 105, 10},
			[]indexEntry{{1, 4, 99, 50, 10}, {5, 10, 101, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 10, 100, 100, 10}},
			indexEntry{10, 15, 101, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 15, 101, 105, 10}},
		},
		{
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 10, 100, 100, 10}},
			indexEntry{11, 15, 101, 105, 10},
			[]indexEntry{{1, 9, 99, 50, 10}, {10, 10, 100, 100, 10}, {11, 15, 101, 105, 10}},
		},
		// #30
		{
			[]indexEntry{{1, 4, 98, 40, 10}, {5, 9, 99, 50, 10}, {10, 10, 100, 100, 10}},
			indexEntry{3, 15, 101, 105, 10},
			[]indexEntry{{1, 2, 98, 40, 10}, {3, 15, 101, 105, 10}},
		},
		{
			[]indexEntry{{1, 4, 98, 40, 10}, {5, 9, 99, 50, 10}, {10, 10, 100, 100, 10}},
			indexEntry{3, 4, 101, 105, 10},
			[]indexEntry{{1, 2, 98, 40, 10}, {3, 4, 101, 105, 10}},
		},
		{
			[]indexEntry{{4, 4, 98, 40, 10}, {5, 9, 99, 50, 10}, {10, 10, 100, 100, 10}},
			indexEntry{3, 3, 101, 105, 10},
			[]indexEntry{{3, 3, 101, 105, 10}},
		},
		{
			[]indexEntry{{4, 4, 98, 40, 10}, {5, 9, 99, 50, 10}, {10, 10, 100, 100, 10}},
			indexEntry{4, 4, 101, 105, 10},
			[]indexEntry{{4, 4, 101, 105, 10}},
		},
		{
			[]indexEntry{{2, 4, 98, 40, 10}, {5, 9, 99, 50, 10}, {10, 10, 100, 100, 10}},
			indexEntry{1, 15, 101, 105, 10},
			[]indexEntry{{1, 15, 101, 105, 10}},
		},
	}
	for idx, tt := range tests {
		testIndex := index{tt.existing, 0}
		testIndex.update(tt.newEntry)
		require.Equalf(t, tt.result, testIndex.entries, "idx: %d", idx)
	}
}

func TestIndexQuery(t *testing.T) {
	tests := []struct {
		entries []indexEntry
		start   uint64
		end     uint64
		result  []indexEntry
		ok      bool
	}{
		// #0
		{
			[]indexEntry{{1, 5, 100, 100, 10}},
			2, 3,
			[]indexEntry{{1, 5, 100, 100, 10}},
			true,
		},
		{
			[]indexEntry{{1, 5, 100, 100, 10}},
			3, 5,
			[]indexEntry{{1, 5, 100, 100, 10}},
			true,
		},
		{
			[]indexEntry{{1, 5, 100, 100, 10}},
			1, 4,
			[]indexEntry{{1, 5, 100, 100, 10}},
			true,
		},
		{
			[]indexEntry{{1, 5, 100, 100, 10}},
			2, 6,
			[]indexEntry{{1, 5, 100, 100, 10}},
			true,
		},
		{
			[]indexEntry{{1, 5, 100, 100, 10}},
			6, 7,
			[]indexEntry{},
			false,
		},
		// #5
		{
			[]indexEntry{{5, 10, 100, 100, 10}},
			1, 4,
			[]indexEntry{},
			false,
		},
		{
			[]indexEntry{{1, 5, 100, 100, 10}, {6, 10, 101, 50, 10}, {11, 15, 102, 90, 10}},
			2, 8,
			[]indexEntry{{1, 5, 100, 100, 10}, {6, 10, 101, 50, 10}},
			true,
		},
		{
			[]indexEntry{{1, 5, 100, 100, 10}, {6, 10, 101, 50, 10}, {11, 15, 102, 90, 10}},
			2, 12,
			[]indexEntry{{1, 5, 100, 100, 10}, {6, 10, 101, 50, 10}, {11, 15, 102, 90, 10}},
			true,
		},
		{
			[]indexEntry{{1, 5, 100, 100, 10}, {6, 10, 101, 50, 10}, {11, 15, 102, 90, 10}},
			2, 4,
			[]indexEntry{{1, 5, 100, 100, 10}},
			true,
		},
		{
			[]indexEntry{{1, 5, 100, 100, 10}, {6, 10, 101, 50, 10}, {11, 15, 102, 90, 10}},
			6, 8,
			[]indexEntry{{6, 10, 101, 50, 10}},
			true,
		},
		// #10
		{
			[]indexEntry{{1, 5, 100, 100, 10}, {6, 10, 101, 50, 10}, {11, 15, 102, 90, 10}},
			7, 12,
			[]indexEntry{{6, 10, 101, 50, 10}, {11, 15, 102, 90, 10}},
			true,
		},
		{
			[]indexEntry{{1, 5, 100, 100, 10}, {6, 10, 101, 50, 10}, {11, 15, 102, 90, 10}},
			2, 12,
			[]indexEntry{{1, 5, 100, 100, 10}, {6, 10, 101, 50, 10}, {11, 15, 102, 90, 10}},
			true,
		},
		{
			[]indexEntry{{3, 5, 100, 100, 10}, {6, 10, 101, 50, 10}, {11, 15, 102, 90, 10}},
			2, 6,
			[]indexEntry{},
			false,
		},
		{
			[]indexEntry{{3, 5, 100, 100, 10}, {6, 10, 101, 50, 10}, {11, 15, 102, 90, 10}},
			16, 18,
			[]indexEntry{},
			false,
		},
		{
			[]indexEntry{{3, 5, 100, 100, 10}, {6, 10, 101, 50, 10}, {11, 15, 102, 90, 10}},
			6, 7,
			[]indexEntry{{6, 10, 101, 50, 10}},
			true,
		},
		// #15
		{
			[]indexEntry{},
			6, 7,
			[]indexEntry{},
			false,
		},
		{
			[]indexEntry{{1, 10, 10, 10, 10}, {20, 30, 10, 20, 10}},
			2, 11,
			[]indexEntry{{1, 10, 10, 10, 10}},
			true,
		},
		{
			[]indexEntry{{1, 10, 10, 10, 10}, {20, 30, 10, 20, 10}},
			2, 21,
			[]indexEntry{{1, 10, 10, 10, 10}},
			true,
		},
		{
			[]indexEntry{{1, 10, 10, 10, 10}, {20, 30, 10, 20, 10}},
			11, 15,
			[]indexEntry{},
			false,
		},
		{
			[]indexEntry{{1, 10, 10, 10, 10}, {20, 30, 10, 20, 10}},
			11, 21,
			[]indexEntry{},
			false,
		},
		// #20
		{
			[]indexEntry{{1, 10, 10, 10, 10}, {20, 30, 10, 20, 10}},
			20, 21,
			[]indexEntry{{20, 30, 10, 20, 10}},
			true,
		},
	}
	for idx, tt := range tests {
		testIndex := index{entries: tt.entries}
		result, ok := testIndex.query(tt.start, tt.end)
		require.Equalf(t, tt.ok, ok, "idx: %d", idx)
		require.Equalf(t, tt.result, result, "idx: %d", idx)
	}
}

func TestIndexEncodeDecode(t *testing.T) {
	tests := []struct {
		entries     []indexEntry
		compactedTo uint64
	}{
		{
			nil, 0,
		},
		{
			nil, 10,
		},
		{
			[]indexEntry{}, 0,
		},
		{
			[]indexEntry{{2, 2, 100, 100, 10}}, 0,
		},
		{
			[]indexEntry{{2, 2, 100, 100, 10}, {5, 5, 101, 10, 10}, {10, 10, 102, 10, 10}}, 0,
		},
		{
			[]indexEntry{{2, 2, 100, 100, 10}, {5, 5, 101, 10, 10}, {10, 10, 102, 10, 10}}, 10,
		},
	}
	for idx, tt := range tests {
		input := &index{tt.entries, tt.compactedTo}
		buf := bytes.NewBuffer(nil)
		require.NoErrorf(t, input.encode(buf), "idx: %d", idx)
		decoded := &index{}
		d := &indexDecoder{bufio.NewReader(buf)}
		require.NoErrorf(t, decoded.decode(d), "idx: %d", idx)
		if len(decoded.entries) == 0 {
			require.Truef(t, len(input.entries) == 0, "idx: %d", idx)
		} else {
			require.Equalf(t, input, decoded, "idx: %d", idx)
		}
	}
}

func TestIndexSaveLoad(t *testing.T) {
	i1 := &nodeIndex{
		shardID:   2,
		replicaID: 3,
		currEntries: index{
			entries: []indexEntry{{1, 100, 5, 100, 10}},
		},
		snapshot: indexEntry{30, snapshotFlag, 5, 110, 10},
		state:    indexEntry{6, stateFlag, 5, 20, 10},
	}
	i2 := &nodeIndex{
		shardID:   3,
		replicaID: 4,
		currEntries: index{
			entries: []indexEntry{{1, 100, 5, 100, 10}, {101, 102, 6, 100, 10}},
		},
		snapshot: indexEntry{30, snapshotFlag, 5, 110, 10},
		state:    indexEntry{6, stateFlag, 5, 20, 10},
	}
	ni1 := raftio.NodeInfo{ShardID: 2, ReplicaID: 3}
	ni2 := raftio.NodeInfo{ShardID: 3, ReplicaID: 4}
	nodeStates := newNodeStates()
	nodeStates.indexes[ni1] = i1
	nodeStates.indexes[ni2] = i2
	fs := vfs.NewMem()
	dirname := "db-dir"
	require.NoError(t, fs.MkdirAll(dirname, 0755))
	dir, err := fs.OpenDir(dirname)
	require.NoError(t, err)
	defer func() {
		require.NoError(t, dir.Close())
	}()
	i1entries := i1.currEntries
	i2entries := i2.currEntries
	require.NoError(t, nodeStates.save(dirname, dir, fileNum(1), fs))

	loaded := newNodeStates()
	require.NoError(t, loaded.load(dirname, fileNum(1), fs))
	require.Equal(t, 2, len(loaded.indexes))
	require.Equal(t, i1.shardID, loaded.indexes[ni1].shardID)
	require.Equal(t, i1.replicaID, loaded.indexes[ni1].replicaID)
	require.Equal(t, i1entries, loaded.indexes[ni1].entries)
	require.Equal(t, i2entries, loaded.indexes[ni2].entries)
	require.Equal(t, i1.snapshot, loaded.indexes[ni1].snapshot)
	require.Equal(t, i2.snapshot, loaded.indexes[ni2].snapshot)
	require.Equal(t, i1.state, loaded.indexes[ni1].state)
	require.Equal(t, i2.state, loaded.indexes[ni2].state)
}

func TestIndexLoadIsAppendOnly(t *testing.T) {
	i := &nodeIndex{
		shardID:   2,
		replicaID: 3,
		currEntries: index{
			entries: []indexEntry{{101, 102, 2, 0, 10}, {102, 104, 2, 58, 10}},
		},
	}
	ni1 := raftio.NodeInfo{ShardID: 2, ReplicaID: 3}
	nodeStates := newNodeStates()
	nodeStates.indexes[ni1] = i
	currEntries := i.currEntries
	fs := vfs.NewMem()
	dirname := "db-dir"
	require.NoError(t, fs.MkdirAll(dirname, 0700))
	dir, err := fs.OpenDir(dirname)
	require.NoError(t, err)
	defer func() {
		require.NoError(t, dir.Close())
	}()
	require.NoError(t, nodeStates.save(dirname, dir, fileNum(5), fs))
	loaded := newNodeStates()
	require.NoError(t, loaded.load(dirname, fileNum(5), fs))
	require.Equal(t, currEntries, loaded.indexes[ni1].entries)
}

func TestIndexCompaction(t *testing.T) {
	tests := []struct {
		entries     []indexEntry
		compactedTo uint64
		obsolete    fileNum
	}{
		{
			[]indexEntry{{10, 15, 1, 0, 10}, {16, 17, 2, 0, 10}, {18, 21, 3, 0, 10}},
			21,
			fileNum(2),
		},
		{
			[]indexEntry{{10, 15, 1, 0, 10}, {16, 17, 2, 0, 10}, {18, 21, 3, 0, 10}},
			9,
			fileNum(0),
		},
		{
			[]indexEntry{{10, 15, 1, 0, 10}, {16, 17, 2, 0, 10}, {18, 21, 3, 0, 10}},
			17,
			fileNum(2),
		},
	}
	for idx, tt := range tests {
		entries := index{
			entries:     tt.entries,
			compactedTo: tt.compactedTo,
		}
		fn := entries.compaction()
		require.Equalf(t, tt.obsolete, fn, "idx: %d", idx)
	}
}

func TestIndexRemoveObsolete(t *testing.T) {
	tests := []struct {
		entries            []indexEntry
		maxObsoleteFileNum fileNum
		result             []fileNum
	}{
		{
			[]indexEntry{{10, 15, 1, 0, 10}, {16, 17, 2, 0, 10}, {18, 21, 3, 0, 10}},
			2,
			[]fileNum{1, 2},
		},
		{
			[]indexEntry{{10, 15, 1, 0, 10}, {16, 17, 2, 0, 10}, {18, 21, 3, 0, 10}},
			1,
			[]fileNum{1},
		},
		{
			[]indexEntry{{10, 15, 1, 0, 10}, {16, 17, 2, 0, 10}, {18, 20, 2, 10, 10}, {21, 25, 3, 0, 10}},
			2,
			[]fileNum{1, 2},
		},
		{
			[]indexEntry{{10, 15, 3, 0, 10}, {16, 17, 4, 0, 10}, {18, 21, 5, 0, 10}},
			2,
			nil,
		},
	}
	for idx, tt := range tests {
		entries := index{
			entries: tt.entries,
		}
		result := entries.removeObsolete(tt.maxObsoleteFileNum)
		require.Equalf(t, tt.result, result, "idx: %d", idx)
	}
}

func TestNodeIndexUpdate(t *testing.T) {
	n := nodeIndex{
		entries: index{
			entries: []indexEntry{{1, 5, 5, 10, 10}, {6, 10, 6, 10, 10}},
		},
		snapshot: indexEntry{3, snapshotFlag, 5, 6, 10},
		currEntries: index{
			entries: []indexEntry{{6, 10, 6, 10, 10}},
		},
		state: indexEntry{10, stateFlag, 6, 10, 10},
	}
	exp := nodeIndex{
		entries: index{
			entries: []indexEntry{{1, 5, 5, 10, 10}, {6, 10, 6, 10, 10}, {12, 12, 6, 12, 10}},
		},
		snapshot: indexEntry{11, snapshotFlag, 6, 11, 10},
		currEntries: index{
			entries: []indexEntry{{6, 10, 6, 10, 10}, {12, 12, 6, 12, 10}},
		},
		state: indexEntry{11, stateFlag, 6, 13, 10},
	}
	n.update(indexEntry{12, 12, 6, 12, 10},
		indexEntry{11, snapshotFlag, 6, 11, 10}, indexEntry{11, stateFlag, 6, 13, 10})
	require.Equal(t, exp, n)
}

func TestStateCompaction(t *testing.T) {
	st := nodeIndex{}
	require.Equal(t, fileNum(math.MaxUint64), st.stateCompaction())
	st = nodeIndex{state: indexEntry{10, stateFlag, 20, 10, 10}}
	require.Equal(t, fileNum(19), st.stateCompaction())
}

func TestSnapshotCompaction(t *testing.T) {
	st := nodeIndex{}
	require.Equal(t, fileNum(math.MaxUint64), st.snapshotCompaction())
	st = nodeIndex{snapshot: indexEntry{10, snapshotFlag, 20, 10, 10}}
	require.Equal(t, fileNum(19), st.snapshotCompaction())
}

func TestStateGetObsolete(t *testing.T) {
	nodeStates := newNodeStates()
	ni1 := raftio.NodeInfo{ShardID: 1, ReplicaID: 1}
	ni2 := raftio.NodeInfo{ShardID: 2, ReplicaID: 1}
	index1 := &nodeIndex{}
	index2 := &nodeIndex{}
	index1.entries.append(indexEntry{1, 100, 5, 10, 10})
	index1.entries.append(indexEntry{101, 102, 10, 10, 10})
	index2.entries.append(indexEntry{1, 100, 20, 10, 10})
	nodeStates.indexes[ni1] = index1
	nodeStates.indexes[ni2] = index2
	input := []fileNum{1, 2, 5, 6, 10, 20, 25}
	expected := []fileNum{1, 2, 6, 25}
	result := nodeStates.getObsolete(input)
	require.Equal(t, expected, result)
}
````

## File: internal/tan/index.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"bytes"
	"encoding/binary"
	"io"
	"math"
	"sort"

	"github.com/cockroachdb/errors"
)

var errCorruptIndexFile = errors.New("corrupt index file")

const (
	// each indexEntry covers up to indexBlockSize bytes within the same log file.
	indexBlockSize int64 = 1024 * 128
)

// indexEntry represents an entry in the index. Each indexEntry points to a
// kind of record in the db, it might be a slice of continuous raft entries,
// a raft state record or a raft snapshot record.
type indexEntry struct {
	start   uint64
	end     uint64
	fileNum fileNum
	pos     int64
	length  int64
}

func (e *indexEntry) empty() bool {
	return e.fileNum == 0
}

func (e *indexEntry) indexBlock() int64 {
	return e.pos / indexBlockSize
}

// merge merges two indexEntry records. This is for raft entry indexes as we
// focus on scan performance, we don't need to index individual raft entries in
// the db, they are stored continuously by their raft entry index value anyway.
func (e *indexEntry) merge(n indexEntry) (indexEntry, indexEntry, bool) {
	if e.end+1 == n.start && e.pos+e.length == n.pos &&
		e.fileNum == n.fileNum && e.indexBlock() == n.indexBlock() {
		result := *e
		result.end = n.end
		result.length = e.length + n.length
		return result, indexEntry{}, true
	}
	return *e, n, false
}

// return a tuple of (indexEntry1, indexEntry2, merged, moreMergeRequired)
func (e *indexEntry) update(n indexEntry) (indexEntry, indexEntry, bool, bool) {
	m, _, merged := e.merge(n)
	if merged {
		return m, indexEntry{}, true, false
	}
	// overwrite
	// for raft entries, new writes always overwrite old entries
	if n.start == e.start {
		return n, indexEntry{}, true, false
	}
	// overwrite and more merge required
	if n.start < e.start {
		return n, indexEntry{}, true, true
	}
	keep := *e
	// partial overwrite
	if n.start > e.start && n.start <= e.end {
		keep.end = n.start - 1
		return keep, n, false, false
	}
	return keep, n, false, false
}

// whether the indexEntry points to a state record
func (e *indexEntry) isState() bool {
	return e.end == stateFlag
}

// whether the indexEntry points to a snapshot record
func (e *indexEntry) isSnapshot() bool {
	return e.end == snapshotFlag
}

type indexDecoder struct {
	byteReader
}

func (d indexDecoder) readUvarint() (uint64, error) {
	u, err := binary.ReadUvarint(d)
	if err != nil {
		if err == io.EOF {
			return 0, errCorruptIndexFile
		}
		return 0, err
	}
	return u, nil
}

type indexEncoder struct {
	*bytes.Buffer
}

func (e indexEncoder) writeUvarint(u uint64) {
	var buf [binary.MaxVarintLen64]byte
	n := binary.PutUvarint(buf[:], u)
	e.Write(buf[:n])
}

// index is the index to raft entry records in the db
type index struct {
	entries []indexEntry
	// entries between (0, compactedTo) are ready for compaction
	compactedTo uint64
}

func (i *index) setCompactedTo(index uint64) {
	if index > i.compactedTo {
		i.compactedTo = index
	}
}

func (i *index) size() uint64 {
	return uint64(len(i.entries))
}

func (i *index) last() indexEntry {
	return i.entries[i.size()-1]
}

func (i *index) cut() {
	i.entries = i.entries[:i.size()-1]
}

func (i *index) append(e indexEntry) {
	i.entries = append(i.entries, e)
}

func (i *index) update(e indexEntry) {
	if i.size() == 0 {
		i.append(e)
		return
	}
	last := i.last()
	e1, e2, merged, more := last.update(e)
	if more {
		if i.size() == 1 {
			i.entries = []indexEntry{e1}
			return
		}
		i.cut()
		i.update(e1)
		return
	}
	i.entries[i.size()-1] = e1
	if !merged {
		i.append(e2)
	}
}

func (i *index) query(low uint64, high uint64) ([]indexEntry, bool) {
	if high < low {
		panic("high < low")
	}
	if len(i.entries) == 0 {
		return []indexEntry{}, false
	}

	startIdx := sort.Search(len(i.entries), func(pos int) bool {
		return low <= i.entries[pos].end
	})
	if startIdx >= len(i.entries) {
		return []indexEntry{}, false
	}
	if low < i.entries[startIdx].start {
		return []indexEntry{}, false
	}

	var result []indexEntry
	for idx := startIdx; idx < len(i.entries); idx++ {
		if high <= i.entries[idx].start {
			break
		}
		if len(result) > 0 {
			if result[len(result)-1].end+1 != i.entries[idx].start {
				break
			}
		}
		result = append(result, i.entries[idx])
	}
	return result, true
}

func (i *index) encode(w io.Writer) error {
	e := indexEncoder{new(bytes.Buffer)}
	e.writeUvarint(uint64(len(i.entries)))
	for _, entry := range i.entries {
		e.writeUvarint(entry.start)
		e.writeUvarint(entry.end)
		e.writeUvarint(uint64(entry.fileNum))
		e.writeUvarint(uint64(entry.pos))
		e.writeUvarint(uint64(entry.length))
	}
	e.writeUvarint(i.compactedTo)
	_, err := w.Write(e.Bytes())
	return err
}

func (i *index) decode(d *indexDecoder) error {
	sz, err := d.readUvarint()
	if err != nil {
		return err
	}
	for idx := uint64(0); idx < sz; idx++ {
		start, err := d.readUvarint()
		if err != nil {
			return err
		}
		end, err := d.readUvarint()
		if err != nil {
			return err
		}
		logNum, err := d.readUvarint()
		if err != nil {
			return err
		}
		pos, err := d.readUvarint()
		if err != nil {
			return err
		}
		length, err := d.readUvarint()
		if err != nil {
			return err
		}
		e := indexEntry{
			start:   start,
			end:     end,
			fileNum: fileNum(logNum),
			pos:     int64(pos),
			length:  int64(length),
		}
		i.entries = append(i.entries, e)
	}
	compactedTo, err := d.readUvarint()
	if err != nil {
		return err
	}
	i.compactedTo = compactedTo
	return nil
}

func (i *index) compaction() fileNum {
	return i.entryCompaction()
}

// entryCompaction calculates and returns the max log file fileNum that is
// considered as obsolete and can be safely deleted.
func (i *index) entryCompaction() fileNum {
	maxObsoleteFileNum := fileNum(math.MaxUint64)
	if i.size() == 0 {
		return maxObsoleteFileNum
	}
	fn := i.entries[0].fileNum
	for j := uint64(1); j < i.size(); j++ {
		ie := i.entries[j]
		if ie.fileNum != fn {
			prev := i.entries[j-1]
			if prev.isSnapshot() || prev.isState() {
				panic("not an entry index")
			}
			if prev.end <= i.compactedTo {
				maxObsoleteFileNum = prev.fileNum
			}
			fn = ie.fileNum
		}
	}
	if maxObsoleteFileNum == fileNum(math.MaxUint64) && i.size() > 0 {
		maxObsoleteFileNum = i.entries[0].fileNum - 1
	}
	return maxObsoleteFileNum
}

// removeObsolete removes obsolete indexEntry records from the index
func (i *index) removeObsolete(maxObsoleteFileNum fileNum) []fileNum {
	var obsolete []fileNum
	index := 0
	fn := fileNum(0)
	for j := range i.entries {
		if i.entries[j].fileNum <= maxObsoleteFileNum {
			if fn != i.entries[j].fileNum {
				obsolete = append(obsolete, i.entries[j].fileNum)
				fn = i.entries[j].fileNum
			}
			index = j
		}
	}
	if len(obsolete) > 0 {
		i.entries = append(make([]indexEntry, 0), i.entries[index+1:]...)
		return obsolete
	}
	return nil
}

// nodeIndex is the index for all records that belong to a single raft node
type nodeIndex struct {
	shardID   uint64
	replicaID uint64
	// entries contains all indexEntry records
	entries index
	// currEntries contains only indexEntry records that belong to the current log file
	currEntries index
	snapshot    indexEntry
	state       indexEntry
}

func (n *nodeIndex) removeAll() {
	n.entries = index{}
	n.currEntries = index{}
	n.snapshot = indexEntry{}
	n.state = indexEntry{}
}

func (n *nodeIndex) update(entry indexEntry, ss indexEntry, state indexEntry) {
	n.entries.update(entry)
	n.currEntries.update(entry)
	if ss.start > n.snapshot.start {
		n.snapshot = ss
	}
	if !state.empty() {
		n.state = state
	}
}

func (n *nodeIndex) fileInUse(fn fileNum) bool {
	if n.snapshot.fileNum == fn || n.state.fileNum == fn {
		return true
	}
	for _, ie := range n.entries.entries {
		if ie.fileNum > fn {
			break
		}
		if ie.fileNum == fn {
			return true
		}
	}
	return false
}

func (n *nodeIndex) query(low uint64, high uint64) ([]indexEntry, bool) {
	return n.entries.query(low, high)
}

func (n *nodeIndex) getState() (indexEntry, bool) {
	return n.state, !n.state.empty()
}

func (n *nodeIndex) querySnapshot() (indexEntry, bool) {
	return n.snapshot, !n.snapshot.empty()
}

func (n *nodeIndex) stateCompaction() fileNum {
	if n.state.empty() {
		return fileNum(math.MaxUint64)
	}
	return n.state.fileNum - 1
}

func (n *nodeIndex) snapshotCompaction() fileNum {
	if n.snapshot.empty() {
		return fileNum(math.MaxUint64)
	}
	return n.snapshot.fileNum - 1
}

func (n *nodeIndex) compaction() []fileNum {
	efn := n.entries.compaction()
	sfn := n.snapshotCompaction()
	stateFn := n.stateCompaction()
	maxObsoleteFileNum := fileNum(0)
	if efn < sfn {
		maxObsoleteFileNum = efn
	} else {
		maxObsoleteFileNum = sfn
	}
	if stateFn < maxObsoleteFileNum {
		maxObsoleteFileNum = stateFn
	}
	if maxObsoleteFileNum == fileNum(math.MaxUint64) ||
		maxObsoleteFileNum == fileNum(0) {
		return nil
	}
	obsoleteEntries := n.entries.removeObsolete(maxObsoleteFileNum)
	return obsoleteEntries
}
````

## File: internal/tan/LICENSE.pebble
````
Copyright (c) 2011 The LevelDB-Go Authors. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

   * Redistributions of source code must retain the above copyright
notice, this list of conditions and the following disclaimer.
   * Redistributions in binary form must reproduce the above
copyright notice, this list of conditions and the following disclaimer
in the documentation and/or other materials provided with the
distribution.
   * Neither the name of Google Inc. nor the names of its
contributors may be used to endorse or promote products derived from
this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
````

## File: internal/tan/logdb_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"bytes"
	"flag"
	"math"
	"os"
	"os/exec"
	"testing"

	"github.com/lni/dragonboat/v4/config"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/lni/goutils/leaktest"
	"github.com/lni/vfs"
	"github.com/stretchr/testify/require"
)

var spawnChild = flag.Bool("spawn-child", false, "spawned child")

func spawn(execName string) ([]byte, error) {
	return exec.Command(execName, "-spawn-child",
		"-test.v", "-test.run=TestFileLock$").CombinedOutput()
}

func TestFileLock(t *testing.T) {
	dbdir := "db-dir"
	child := *spawnChild
	msg := "failed to lock tan dir"
	cfg := config.NodeHostConfig{
		Expert: config.ExpertConfig{
			FS: vfs.Default,
		},
	}
	require.NoError(t, cfg.Prepare())
	if !child {
		ldb, err := CreateTan(cfg, nil, []string{dbdir}, nil)
		require.NoError(t, err)
		defer func() {
			require.NoError(t, ldb.Close())
			require.NoError(t, ldb.fs.RemoveAll(dbdir))
		}()
		out, err := spawn(os.Args[0])
		if err == nil {
			t.Fatalf("file lock didn't prevent the second tan to start, %s", out)
		}
		require.True(t, bytes.Contains(out, []byte(msg)))
	} else {
		ldb, err := CreateTan(cfg, nil, []string{dbdir}, nil)
		if err == nil {
			require.NoError(t, ldb.Close())
		} else {
			panic(msg)
		}
	}
}

func TestListNodeInfo(t *testing.T) {
	defer leaktest.AfterTest(t)()
	cfg := config.NodeHostConfig{
		Expert: config.ExpertConfig{
			FS: vfs.Default,
		},
	}
	require.NoError(t, cfg.Prepare())
	ldb, err := CreateTan(cfg, nil, []string{"db-dir"}, nil)
	require.NoError(t, err)
	defer func() {
		require.NoError(t, ldb.Close())
		require.NoError(t, ldb.fs.RemoveAll("db-dir"))
	}()
	rec := pb.Bootstrap{}
	require.NoError(t, ldb.SaveBootstrapInfo(1, 1, rec))
	require.NoError(t, ldb.SaveBootstrapInfo(2, 2, rec))
	require.NoError(t, ldb.SaveBootstrapInfo(3, 3, rec))
	nodes, err := ldb.ListNodeInfo()
	require.NoError(t, err)
	require.Equal(t, 3, len(nodes))
	for _, n := range nodes {
		require.True(t, n.ShardID == 1 && n.ReplicaID == 1 ||
			n.ShardID == 2 && n.ReplicaID == 2 ||
			n.ShardID == 3 && n.ReplicaID == 3)
	}
}

func TestLogDBCanBeCreated(t *testing.T) {
	defer leaktest.AfterTest(t)()
	cfg := config.NodeHostConfig{
		Expert: config.ExpertConfig{FS: vfs.NewMem()},
	}
	require.NoError(t, cfg.Prepare())
	dirs := []string{"db-dir"}
	ldb, err := CreateTan(cfg, nil, dirs, []string{})
	require.Equal(t, tanLogDBName, ldb.Name())
	require.NoError(t, err)
	require.NoError(t, ldb.Close())
}

func TestSaveSnapshots(t *testing.T) {
	defer leaktest.AfterTest(t)()
	cfg := config.NodeHostConfig{
		Expert: config.ExpertConfig{FS: vfs.NewMem()},
	}
	require.NoError(t, cfg.Prepare())
	dirs := []string{"db-dir"}
	ldb, err := CreateTan(cfg, nil, dirs, []string{})
	require.NoError(t, err)
	updates := []pb.Update{
		{
			ShardID:   1,
			ReplicaID: 1,
			Snapshot:  pb.Snapshot{Index: 100, Term: 10},
		},
		{
			ShardID:   2,
			ReplicaID: 1,
			Snapshot:  pb.Snapshot{Index: 200, Term: 10},
		},
	}
	require.NoError(t, ldb.SaveSnapshots(updates))
	ss1, err := ldb.GetSnapshot(1, 1)
	require.NoError(t, err)
	require.Equal(t, uint64(100), ss1.Index)
	ss2, err := ldb.GetSnapshot(2, 1)
	require.NoError(t, err)
	require.Equal(t, uint64(200), ss2.Index)
	require.NoError(t, ldb.Close())
}

func TestSaveRaftState(t *testing.T) {
	defer leaktest.AfterTest(t)()
	cfg := config.NodeHostConfig{
		Expert: config.ExpertConfig{FS: vfs.NewMem()},
	}
	require.NoError(t, cfg.Prepare())
	dirs := []string{"db-dir"}
	ldb, err := CreateTan(cfg, nil, dirs, []string{})
	require.NoError(t, err)
	updates := []pb.Update{
		{
			ShardID:   1,
			ReplicaID: 1,
			Snapshot:  pb.Snapshot{Index: 100, Term: 10},
			State:     pb.State{Commit: 100, Term: 10},
			EntriesToSave: []pb.Entry{
				{Index: 99, Term: 10},
				{Index: 100, Term: 10},
			},
		},
		{
			ShardID:   17,
			ReplicaID: 1,
			Snapshot:  pb.Snapshot{Index: 200, Term: 10},
			State:     pb.State{Commit: 200, Term: 10},
			EntriesToSave: []pb.Entry{
				{Index: 198, Term: 10},
				{Index: 199, Term: 10},
				{Index: 200, Term: 10},
			},
		},
	}
	require.NoError(t, ldb.SaveRaftState(updates, 1))
	ss1, err := ldb.GetSnapshot(1, 1)
	require.NoError(t, err)
	require.Equal(t, updates[0].Snapshot, ss1)

	ss2, err := ldb.GetSnapshot(17, 1)
	require.NoError(t, err)
	require.Equal(t, updates[1].Snapshot, ss2)

	var entries []pb.Entry
	results, _, err := ldb.IterateEntries(entries, 0, 1, 1, 99, 101, math.MaxUint64)
	require.NoError(t, err)
	require.Equal(t, 2, len(results))

	rs, err := ldb.ReadRaftState(1, 1, 98)
	require.NoError(t, err)
	require.Equal(t, updates[0].State, rs.State)
	require.NoError(t, ldb.Close())
}

func TestConcurrentSaveRaftState(t *testing.T) {
	defer leaktest.AfterTest(t)()
	cfg := config.NodeHostConfig{
		Expert: config.ExpertConfig{FS: vfs.NewMem()},
	}
	require.NoError(t, cfg.Prepare())
	dirs := []string{"db-dir"}
	ldb, err := CreateLogMultiplexedTan(cfg, nil, dirs, []string{})
	require.NoError(t, err)
	defer func() {
		require.NoError(t, ldb.Close())
	}()
	for i := uint64(0); i < 16; i++ {
		updates := []pb.Update{
			{
				ShardID:   1,
				ReplicaID: 1,
				Snapshot:  pb.Snapshot{Index: i * uint64(100), Term: 10},
				State:     pb.State{Commit: i * uint64(100), Term: 10},
				EntriesToSave: []pb.Entry{
					{Index: i*2 + 1, Term: 10},
					{Index: i*2 + 2, Term: 10},
				},
			},
			{
				ShardID:   17,
				ReplicaID: 1,
				Snapshot:  pb.Snapshot{Index: i * uint64(200), Term: 20},
				State:     pb.State{Commit: i * uint64(200), Term: 20},
				EntriesToSave: []pb.Entry{
					{Index: i*3 + 1, Term: 20},
					{Index: i*3 + 2, Term: 20},
					{Index: i*3 + 3, Term: 20},
				},
			},
		}
		require.NoError(t, ldb.SaveRaftState(updates, 1))
	}
	for i := uint64(0); i < 16; i++ {
		updates := []pb.Update{
			{
				ShardID:   2,
				ReplicaID: 1,
				Snapshot:  pb.Snapshot{Index: i * uint64(100), Term: 30},
				State:     pb.State{Commit: i * uint64(100), Term: 30},
				EntriesToSave: []pb.Entry{
					{Index: i*2 + 1, Term: 30},
					{Index: i*2 + 2, Term: 30},
				},
			},
			{
				ShardID:   18,
				ReplicaID: 1,
				Snapshot:  pb.Snapshot{Index: i * uint64(100), Term: 40},
				State:     pb.State{Commit: i * uint64(100), Term: 40},
				EntriesToSave: []pb.Entry{
					{Index: i * 3, Term: 40},
					{Index: i*3 + 1, Term: 40},
					{Index: i*3 + 2, Term: 40},
				},
			},
		}
		require.NoError(t, ldb.SaveRaftState(updates, 2))
	}
	// TODO: add checks to see whether there are shard directories named as
	// shard-1 and shard-2
	var entries []pb.Entry
	results, _, err := ldb.IterateEntries(entries, 0, 1, 1, 1, 33, math.MaxUint64)
	require.NoError(t, err)
	require.Equal(t, 32, len(results))
	var entries2 []pb.Entry
	results, _, err = ldb.IterateEntries(entries2, 0, 17, 1, 1, 49, math.MaxUint64)
	require.NoError(t, err)
	require.Equal(t, 48, len(results))
	var entries3 []pb.Entry
	results, _, err = ldb.IterateEntries(entries3, 0, 2, 1, 1, 33, math.MaxUint64)
	require.NoError(t, err)
	require.Equal(t, 32, len(results))

	ss1, err := ldb.GetSnapshot(1, 1)
	require.NoError(t, err)
	require.Equal(t, uint64(1500), ss1.Index)
	ss2, err := ldb.GetSnapshot(17, 1)
	require.NoError(t, err)
	require.Equal(t, uint64(3000), ss2.Index)
	ss3, err := ldb.GetSnapshot(2, 1)
	require.NoError(t, err)
	require.Equal(t, uint64(30), ss3.Term)
	require.Equal(t, uint64(1500), ss3.Index)
}
````

## File: internal/tan/logdb.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

/*
Tan is a log file based LogDB implementation for dragonboat.

Each dragonboat instance owns a tan LogDB instance, which manages all tan
db instances hold by a container instance called collection. Each raft node
is backed one of those tan db instance.

To allow N raft nodes to share M tan db instance, there are two obvious ways
to do it, known as the regular mode, one way is to let each raft node to own
a dedicated tan db. Another way is to share the same tan db among multiple
raft nodes so there won't be an excessive amount of tan db instances, we call
this the multiplexed log mode. Such 1:1 and n:m mapping relationships are
managed by a regularKeeper or a multiplexedKeeper intance both of which are
of the dbKeeper interface as defined in db_keeper.go. This allows the upper
layer to get the relevant tan db by just providing the shardID and replicaID
values of the raft node with the mapping details hidden from the outside.

Each tan db instance owns a log file which will be used for storing all log
data. For data written into the same tan db from different raft nodes, they
will be indexed into different tan nodeIndex instances stored as a part of
db.mu.nodeStates. This means each raft node will have its own nodeIndex.
*/
package tan

import (
	"io"
	"sync"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/lni/vfs"
)

const (
	defaultBufferSize = 1024 * 1024 * 4
	defaultDBName     = "tandb"
	bootstrapDirname  = "bootstrap"
	defaultShards     = 16
	tanLogDBName      = "Tan"
)

var _ raftio.ILogDB = (*LogDB)(nil)

// Factory is the default LogDB factory instance used for creating tan DB
// instances.
var Factory = factory{}

type factory struct{}

// Create creates a new tan LogDB instance in regular mode.
func (factory) Create(cfg config.NodeHostConfig,
	cb config.LogDBCallback, dirs []string, wals []string) (raftio.ILogDB, error) {
	return CreateTan(cfg, cb, dirs, wals)
}

// MultiplexedLogFactory is a LogDB factory instance used for creating an
// tan DB with multiplexed logs.
var MultiplexedLogFactory = multiplexLogFactory{}

type multiplexLogFactory struct{}

// Create creates a tan instance that uses multiplexed log files.
func (multiplexLogFactory) Create(cfg config.NodeHostConfig,
	cb config.LogDBCallback, dirs []string, wals []string) (raftio.ILogDB, error) {
	return CreateLogMultiplexedTan(cfg, cb, dirs, wals)
}

// Name returns the name of the tan instance.
func (factory) Name() string {
	return tanLogDBName
}

// LogDB is the tan ILogDB type used to interface with dragonboat.
type LogDB struct {
	mu         sync.Mutex
	fileLock   io.Closer
	dirname    string
	dir        vfs.File
	bsDirname  string
	bsDir      vfs.File
	fs         vfs.FS
	buffers    [][]byte
	wgs        []*sync.WaitGroup
	collection collection
}

// CreateTan creates and return a regular tan instance. Each raft node will
// be backed by a dedicated log file.
func CreateTan(cfg config.NodeHostConfig, cb config.LogDBCallback,
	dirs []string, wals []string) (*LogDB, error) {
	return createTan(cfg, cb, dirs, wals, true)
}

// CreateLogMultiplexedTan creates and returns a tan instance that uses
// multiplexed log files. A multiplexed log allow multiple raft shards to
// share the same underlying physical log file, this is required when you
// want to run thousands of raft nodes on the same server without having
// thousands action log files.
func CreateLogMultiplexedTan(cfg config.NodeHostConfig, cb config.LogDBCallback,
	dirs []string, wals []string) (*LogDB, error) {
	return createTan(cfg, cb, dirs, wals, false)
}

func createTan(cfg config.NodeHostConfig, cb config.LogDBCallback,
	dirs []string, wals []string, singleNodeLog bool) (*LogDB, error) {
	if cfg.Expert.FS == nil {
		panic("fs not set")
	}
	if cfg.Expert.LogDB.IsEmpty() {
		panic("logdb config is empty")
	}
	dirname := cfg.Expert.FS.PathJoin(dirs[0], defaultDBName)
	ldb := &LogDB{
		dirname:    dirname,
		fs:         cfg.Expert.FS,
		buffers:    make([][]byte, defaultShards),
		wgs:        make([]*sync.WaitGroup, defaultShards),
		collection: newCollection(dirname, cfg.Expert.FS, singleNodeLog),
	}
	for i := 0; i < len(ldb.buffers); i++ {
		ldb.buffers[i] = make([]byte, cfg.Expert.LogDB.KVWriteBufferSize)
	}
	for i := 0; i < len(ldb.wgs); i++ {
		ldb.wgs[i] = new(sync.WaitGroup)
	}
	var err error
	if err := fileutil.MkdirAll(ldb.dirname, ldb.fs); err != nil {
		return nil, err
	}
	bs := ldb.fs.PathJoin(ldb.dirname, bootstrapDirname)
	if err := fileutil.MkdirAll(bs, ldb.fs); err != nil {
		return nil, err
	}
	ldb.bsDirname = bs
	if err := ldb.cleanupBootstrapDir(); err != nil {
		return nil, err
	}
	ldb.dir, err = ldb.fs.Open(ldb.dirname)
	if err != nil {
		return nil, err
	}
	ldb.bsDir, err = ldb.fs.Open(ldb.bsDirname)
	if err != nil {
		return nil, err
	}
	lockFilename := makeFilename(ldb.fs, ldb.dirname, fileTypeLock, 0)
	fileLock, err := ldb.fs.Lock(lockFilename)
	if err != nil {
		return nil, err
	}
	ldb.fileLock = fileLock
	return ldb, nil
}

func (l *LogDB) cleanupBootstrapDir() error {
	ls, err := l.fs.List(l.bsDirname)
	if err != nil {
		return err
	}
	for _, filename := range ls {
		if tmpBSFilenameRe.MatchString(filename) {
			if err := l.fs.Remove(l.fs.PathJoin(l.bsDirname, filename)); err != nil {
				return err
			}
		}
	}
	return nil
}

// TODO: remove the following two methods

// DeleteSnapshot ...
func (l *LogDB) DeleteSnapshot(shardID uint64,
	replicaID uint64, index uint64) error {
	panic("depreciated")
}

// ListSnapshots lists available snapshots associated with the specified
// Raft node for index range (0, index].
func (l *LogDB) ListSnapshots(shardID uint64,
	replicaID uint64, index uint64) ([]pb.Snapshot, error) {
	panic("depreciated")
}

// Name returns the type name of the ILogDB instance.
func (l *LogDB) Name() string {
	return tanLogDBName
}

// Close closes the ILogDB instance.
func (l *LogDB) Close() (err error) {
	func() {
		l.mu.Lock()
		defer l.mu.Unlock()
		err = firstError(err, l.collection.iterate(func(db *db) error {
			return db.close()
		}))
	}()
	err = firstError(err, l.bsDir.Close())
	err = firstError(err, l.dir.Close())
	return firstError(err, l.fileLock.Close())
}

// BinaryFormat returns an constant uint32 value representing the binary
// format version compatible with the ILogDB instance.
func (l *LogDB) BinaryFormat() uint32 {
	return raftio.PlainLogDBBinVersion
}

// ListNodeInfo lists all available NodeInfo found in the log DB.
func (l *LogDB) ListNodeInfo() ([]raftio.NodeInfo, error) {
	files, err := l.fs.List(l.bsDirname)
	if err != nil {
		return nil, err
	}
	result := make([]raftio.NodeInfo, 0)
	for _, file := range files {
		shardID, replicaID, ok := parseBootstrapFilename(file)
		if ok {
			result = append(result, raftio.NodeInfo{ShardID: shardID, ReplicaID: replicaID})
		}
	}
	return result, nil
}

// SaveBootstrapInfo saves the specified bootstrap info to the log DB.
func (l *LogDB) SaveBootstrapInfo(shardID uint64,
	replicaID uint64, rec pb.Bootstrap) error {
	return saveBootstrap(l.fs, l.bsDirname, l.bsDir, shardID, replicaID, rec)
}

// GetBootstrapInfo returns saved bootstrap info from log DB. It returns
// ErrNoBootstrapInfo when there is no previously saved bootstrap info for
// the specified node.
func (l *LogDB) GetBootstrapInfo(shardID uint64,
	replicaID uint64) (pb.Bootstrap, error) {
	return getBootstrap(l.fs, l.bsDirname, shardID, replicaID)
}

// SaveRaftState atomically saves the Raft states, log entries and snapshots
// metadata found in the pb.Update list to the log DB.
func (l *LogDB) SaveRaftState(updates []pb.Update, shardID uint64) error {
	if l.collection.multiplexedLog() {
		return l.concurrentSaveState(updates, shardID)
	}
	return l.sequentialSaveState(updates, shardID)
}

func (l *LogDB) concurrentSaveState(updates []pb.Update, shardID uint64) error {
	var buf []byte
	if shardID-1 < uint64(len(l.buffers)) {
		buf = l.buffers[shardID-1]
	} else {
		buf = make([]byte, defaultBufferSize)
	}
	syncLog := false
	var selected *db
	var usedShardID uint64
	for idx, ud := range updates {
		if idx == 0 {
			usedShardID = l.collection.key(ud.ShardID)
		} else {
			if usedShardID != l.collection.key(ud.ShardID) {
				panic("shard ID changed")
			}
		}
		db, err := l.getDB(ud.ShardID, ud.ReplicaID)
		if err != nil {
			return err
		}
		if selected == nil {
			selected = db
		}
		sync, err := db.write(ud, buf)
		if err != nil {
			return err
		}
		if sync {
			syncLog = true
		}
	}
	if syncLog && selected != nil {
		if err := selected.sync(); err != nil {
			return err
		}
	}
	return nil
}

func (l *LogDB) sequentialSaveState(updates []pb.Update, shardID uint64) error {
	var wg *sync.WaitGroup
	var buf []byte
	if shardID-1 < uint64(len(l.buffers)) {
		buf = l.buffers[shardID-1]
	} else {
		buf = make([]byte, defaultBufferSize)
	}
	if shardID-1 < uint64(len(l.wgs)) {
		wg = l.wgs[shardID-1]
	} else {
		wg = new(sync.WaitGroup)
	}
	for _, ud := range updates {
		db, err := l.getDB(ud.ShardID, ud.ReplicaID)
		if err != nil {
			return err
		}
		sync, err := db.write(ud, buf)
		if err != nil {
			return err
		}
		if sync {
			wg.Add(1)
			go func() {
				if err := db.sync(); err != nil {
					panicNow(err)
				}
				wg.Done()
			}()
		}
	}
	wg.Wait()
	return nil
}

// IterateEntries returns the continuous Raft log entries of the specified
// Raft node between the index value range of [low, high) up to a max size
// limit of maxSize bytes. It returns the located log entries, their total
// size in bytes and the occurred error.
func (l *LogDB) IterateEntries(ents []pb.Entry,
	size uint64, shardID uint64, replicaID uint64, low uint64,
	high uint64, maxSize uint64) ([]pb.Entry, uint64, error) {
	db, err := l.getDB(shardID, replicaID)
	if err != nil {
		return nil, 0, err
	}
	return db.getEntries(shardID, replicaID, ents, size, low, high, maxSize)
}

// ReadRaftState returns the persistented raft state found in Log DB.
func (l *LogDB) ReadRaftState(shardID uint64,
	replicaID uint64, lastIndex uint64) (raftio.RaftState, error) {
	db, err := l.getDB(shardID, replicaID)
	if err != nil {
		return raftio.RaftState{}, err
	}
	return db.getRaftState(shardID, replicaID, lastIndex)
}

// RemoveEntriesTo removes entries between (0, index].
func (l *LogDB) RemoveEntriesTo(shardID uint64,
	replicaID uint64, index uint64) error {
	db, err := l.getDB(shardID, replicaID)
	if err != nil {
		return err
	}
	if err := db.removeEntries(shardID, replicaID, index); err != nil {
		return err
	}
	return db.sync()
}

// CompactEntriesTo reclaims underlying storage space used for storing
// entries up to the specified index.
func (l *LogDB) CompactEntriesTo(shardID uint64,
	replicaID uint64, index uint64) (<-chan struct{}, error) {
	ch := make(chan struct{}, 1)
	ch <- struct{}{}
	return ch, nil
}

// SaveSnapshots saves all snapshot metadata found in the pb.Update list.
func (l *LogDB) SaveSnapshots(updates []pb.Update) error {
	buf := make([]byte, 1024*32)
	for _, ud := range updates {
		if pb.IsEmptySnapshot(ud.Snapshot) {
			continue
		}
		db, err := l.getDB(ud.ShardID, ud.ReplicaID)
		if err != nil {
			return err
		}
		wu := pb.Update{
			ShardID:   ud.ShardID,
			ReplicaID: ud.ReplicaID,
			Snapshot:  ud.Snapshot,
		}
		if _, err := db.write(wu, buf); err != nil {
			return err
		}
		if err := db.sync(); err != nil {
			return err
		}
	}
	return nil
}

// GetSnapshot lists available snapshots associated with the specified
// Raft node for index range (0, index].
func (l *LogDB) GetSnapshot(shardID uint64,
	replicaID uint64) (pb.Snapshot, error) {
	db, err := l.getDB(shardID, replicaID)
	if err != nil {
		return pb.Snapshot{}, err
	}
	return db.getSnapshot(shardID, replicaID)
}

// RemoveNodeData removes all data associated with the specified node.
func (l *LogDB) RemoveNodeData(shardID uint64, replicaID uint64) error {
	db, err := l.getDB(shardID, replicaID)
	if err != nil {
		return err
	}
	if err := db.removeAll(shardID, replicaID); err != nil {
		return err
	}
	if err := removeBootstrap(l.fs,
		l.bsDirname, l.bsDir, shardID, replicaID); err != nil {
		return err
	}
	return db.sync()
}

// ImportSnapshot imports the specified snapshot by creating all required
// metadata in the logdb.
func (l *LogDB) ImportSnapshot(snapshot pb.Snapshot, replicaID uint64) error {
	bs := pb.Bootstrap{
		Join: true,
		Type: snapshot.Type,
	}
	if err := saveBootstrap(l.fs,
		l.bsDirname, l.bsDir, snapshot.ShardID, replicaID, bs); err != nil {
		return err
	}
	db, err := l.getDB(snapshot.ShardID, replicaID)
	if err != nil {
		return err
	}
	if err := db.importSnapshot(snapshot.ShardID, replicaID, snapshot); err != nil {
		return err
	}
	return db.sync()
}

func (l *LogDB) getDB(shardID uint64, replicaID uint64) (*db, error) {
	l.mu.Lock()
	defer l.mu.Unlock()
	return l.collection.getDB(shardID, replicaID)
}
````

## File: internal/tan/node_states.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"bufio"
	"bytes"

	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/lni/vfs"
)

type nodeStates struct {
	indexes map[raftio.NodeInfo]*nodeIndex
	states  map[raftio.NodeInfo]pb.State
}

func newNodeStates() *nodeStates {
	return &nodeStates{
		indexes: make(map[raftio.NodeInfo]*nodeIndex),
		states:  make(map[raftio.NodeInfo]pb.State),
	}
}

func (s *nodeStates) checkNodeInfo(shardID uint64, replicaID uint64) {
	if shardID == 0 && replicaID == 0 {
		panic("shardID/replicaID are both empty")
	}
}

func (s *nodeStates) getState(shardID uint64, replicaID uint64) pb.State {
	s.checkNodeInfo(shardID, replicaID)
	ni := raftio.NodeInfo{ShardID: shardID, ReplicaID: replicaID}
	st, ok := s.states[ni]
	if !ok {
		st = pb.State{}
		s.states[ni] = st
	}
	return st
}

func (s *nodeStates) setState(shardID uint64, replicaID uint64, st pb.State) {
	s.checkNodeInfo(shardID, replicaID)
	ni := raftio.NodeInfo{ShardID: shardID, ReplicaID: replicaID}
	s.states[ni] = st
}

func (s *nodeStates) getIndex(shardID uint64, replicaID uint64) *nodeIndex {
	s.checkNodeInfo(shardID, replicaID)
	ni := raftio.NodeInfo{ShardID: shardID, ReplicaID: replicaID}
	idx, ok := s.indexes[ni]
	if !ok {
		idx = &nodeIndex{shardID: shardID, replicaID: replicaID}
		s.indexes[ni] = idx
	}
	return idx
}

func (s *nodeStates) save(dirname string,
	dir vfs.File, fileNum fileNum, fs vfs.FS) (err error) {
	tmpFn := makeFilename(fs, dirname, fileTypeIndexTemp, fileNum)
	fn := makeFilename(fs, dirname, fileTypeIndex, fileNum)
	file, err := fs.Create(tmpFn)
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, file.Sync())
		err = firstError(err, file.Close())
		if err == nil {
			err = fs.Rename(tmpFn, fn)
		}
		err = firstError(err, dir.Sync())
	}()
	w := newWriter(file)
	defer func() {
		err = firstError(err, w.close())
	}()
	rw, err := w.next()
	if err != nil {
		return err
	}
	e := indexEncoder{new(bytes.Buffer)}
	e.writeUvarint(uint64(len(s.indexes)))
	if _, err := rw.Write(e.Bytes()); err != nil {
		return err
	}
	for ni, n := range s.indexes {
		rw, err = w.next()
		if err != nil {
			return err
		}
		if ni.ShardID != n.shardID || ni.ReplicaID != n.replicaID {
			panic("inconsistent shardID/replicaID")
		}
		e := indexEncoder{new(bytes.Buffer)}
		e.writeUvarint(n.shardID)
		e.writeUvarint(n.replicaID)
		if _, err := rw.Write(e.Bytes()); err != nil {
			return err
		}
		rw, err = w.next()
		if err != nil {
			return err
		}
		if err := n.currEntries.encode(rw); err != nil {
			return err
		}
		n.currEntries = index{}
		rw, err = w.next()
		if err != nil {
			return err
		}
		snapshot := index{[]indexEntry{n.snapshot}, 0}
		if err := snapshot.encode(rw); err != nil {
			return err
		}
		rw, err = w.next()
		if err != nil {
			return err
		}
		state := index{[]indexEntry{n.state}, 0}
		if err := state.encode(rw); err != nil {
			return err
		}
	}
	return nil
}

func (s *nodeStates) load(dirname string, fn fileNum, fs vfs.FS) (err error) {
	file, err := fs.Open(makeFilename(fs, dirname, fileTypeIndex, fn))
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, file.Close())
	}()
	r := newReader(file, fileNum(0))
	rr, err := r.next()
	if err != nil {
		return err
	}
	d := &indexDecoder{bufio.NewReader(rr)}
	sz, err := d.readUvarint()
	if err != nil {
		return err
	}
	for i := uint64(0); i < sz; i++ {
		var entries index
		var snapshots index
		var state index
		rr, err = r.next()
		if err != nil {
			return err
		}
		d = &indexDecoder{bufio.NewReader(rr)}
		shardID, err := d.readUvarint()
		if err != nil {
			return err
		}
		replicaID, err := d.readUvarint()
		if err != nil {
			return err
		}
		n := s.getIndex(shardID, replicaID)
		if n.currEntries.size() > 0 {
			panic("current entries not empty")
		}
		rr, err = r.next()
		if err != nil {
			return err
		}
		d = &indexDecoder{bufio.NewReader(rr)}
		if err := entries.decode(d); err != nil {
			return err
		}
		for idx, e := range entries.entries {
			if e.isSnapshot() || e.isState() {
				plog.Panicf("unexpected type %v", e)
			}
			if idx == 0 {
				// just crossed index file boundary, we must merge here, consider
				// consider existing index entries [{1 26 2 0} {27 27 3 47}], when
				// {27 40 5 23} is loaded as the first index entry from a new index file,
				// it needs to overwrite the {27 27 3 47} entry.
				n.entries.update(e)
			} else {
				// must append only here, we can't merge indexEntries here, consider
				// [{101 101 2 0} {102 104 2 58}] in an index file
				// once merged via update(), the output is {101 104 2 0}, this would be
				// incorrect if there are entries with in index=2 between log file offset
				// 0 and 58
				n.entries.append(e)
			}
		}
		n.entries.setCompactedTo(entries.compactedTo)
		rr, err = r.next()
		if err != nil {
			return err
		}
		d = &indexDecoder{bufio.NewReader(rr)}
		if err := snapshots.decode(d); err != nil {
			return err
		}
		if len(snapshots.entries) > 1 {
			panic("unexpected snapshot entry count")
		}
		if len(snapshots.entries) > 0 {
			n.snapshot = snapshots.entries[0]
		}
		rr, err = r.next()
		if err != nil {
			return err
		}
		d = &indexDecoder{bufio.NewReader(rr)}
		if err := state.decode(d); err != nil {
			return err
		}
		if len(state.entries) > 1 {
			panic("unexpected state entry count")
		}
		if len(state.entries) > 0 && !state.entries[0].empty() {
			n.state = state.entries[0]
		}
	}
	return nil
}

func (s *nodeStates) querySnapshot(shardID uint64, replicaID uint64) (indexEntry, bool) {
	n := s.getIndex(shardID, replicaID)
	return n.querySnapshot()
}

func (s *nodeStates) queryState(shardID uint64, replicaID uint64) (indexEntry, bool) {
	n := s.getIndex(shardID, replicaID)
	return n.getState()
}

func (s *nodeStates) query(shardID uint64, replicaID uint64,
	low uint64, high uint64) ([]indexEntry, bool) {
	n := s.getIndex(shardID, replicaID)
	return n.query(low, high)
}

func (s *nodeStates) compactedTo(shardID uint64, replicaID uint64) uint64 {
	n := s.getIndex(shardID, replicaID)
	return n.entries.compactedTo
}

func (s *nodeStates) getObsolete(fns []fileNum) []fileNum {
	var result []fileNum
	for _, fn := range fns {
		inUse := false
		for _, index := range s.indexes {
			if index.fileInUse(fn) {
				inUse = true
				break
			}
		}
		if !inUse {
			result = append(result, fn)
		}
	}
	return result
}
````

## File: internal/tan/open.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"sort"

	"github.com/cockroachdb/errors"
	"github.com/cockroachdb/errors/oserror"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/lni/goutils/syncutil"
)

// open opens the tan db located in the folder called dirname.
func open(name string, dirname string, opts *Options) (*db, error) {
	opts = opts.EnsureDefaults()
	d := &db{
		name:                 name,
		closedCh:             make(chan struct{}),
		opts:                 opts,
		dirname:              dirname,
		deleteObsoleteCh:     make(chan struct{}, 1),
		deleteobsoleteWorker: syncutil.NewStopper(),
	}
	d.mu.versions = &versionSet{}
	d.mu.nodeStates = newNodeStates()

	d.mu.Lock()
	defer d.mu.Unlock()

	var err error
	d.dataDir, err = opts.FS.OpenDir(dirname)
	if err != nil {
		return nil, err
	}
	currentName := makeFilename(opts.FS, dirname, fileTypeCurrent, 0)
	if _, err := opts.FS.Stat(currentName); oserror.IsNotExist(err) {
		// Create the DB if it did not already exist.
		plog.Infof("%s creating a new tan db", d.id())
		if err := d.mu.versions.create(dirname, opts, d.dataDir, &d.mu.Mutex); err != nil {
			return nil, err
		}
	} else if err != nil {
		return nil, errors.Wrapf(err, "tan: database %q", dirname)
	} else {
		// Load the version set.
		plog.Infof("%s loading an existing tan db", d.id())
		if err := d.mu.versions.load(dirname, opts, &d.mu.Mutex); err != nil {
			return nil, err
		}
		if err := d.mu.versions.currentVersion().checkConsistency(dirname, opts.FS); err != nil {
			return nil, err
		}
	}

	ls, err := opts.FS.List(d.dirname)
	if err != nil {
		return nil, err
	}
	plog.Infof("%s on disk files %v", d.id(), ls)
	type fileNumAndName struct {
		num  fileNum
		name string
	}
	currentVersion := d.mu.versions.currentVersion()
	var indexFiles []fileNumAndName
	logFiles := make(map[fileNum]fileNumAndName)
	lastLogNum := fileNum(0)
	for _, filename := range ls {
		ft, fn, ok := parseFilename(opts.FS, filename)
		if !ok {
			continue
		}
		// Don't reuse any obsolete file numbers
		if d.mu.versions.nextFileNum <= fn {
			d.mu.versions.nextFileNum = fn + 1
		}

		switch ft {
		case fileTypeLog:
			if _, ok := currentVersion.files[fn]; ok {
				if fn > lastLogNum {
					lastLogNum = fn
				}
				logFiles[fn] = fileNumAndName{fn, filename}
			}
		case fileTypeLogTemp:
			fallthrough
		case fileTypeBootstrapTemp:
			fallthrough
		case fileTypeIndexTemp:
			fallthrough
		case fileTypeTemp:
			if err := opts.FS.Remove(opts.FS.PathJoin(dirname, filename)); err != nil {
				return nil, err
			}
		case fileTypeIndex:
			if _, ok := currentVersion.files[fn]; ok {
				indexFiles = append(indexFiles, fileNumAndName{fn, filename})
			}
		}
	}
	sort.Slice(indexFiles, func(i, j int) bool {
		return indexFiles[i].num < indexFiles[j].num
	})
	for _, indexFile := range indexFiles {
		if _, ok := logFiles[indexFile.num]; !ok {
			plog.Panicf("log file %d missing", indexFile.num)
		}
		if err := d.mu.nodeStates.load(dirname, indexFile.num, opts.FS); err != nil {
			// TODO: we can actually regenerate the index when it is corrupted
			return nil, err
		}
		delete(logFiles, indexFile.num)
	}
	plog.Infof("%s logFiles to rebuild: %v", d.id(), logFiles)
	plog.Infof("%s indexFiles: %v", d.id(), indexFiles)
	for _, lf := range logFiles {
		if len(indexFiles) == 0 || lf.num > indexFiles[len(indexFiles)-1].num {
			if lf.num != lastLogNum {
				plog.Panicf("more than one log file have index missing")
			}
			if err := d.rebuildLogAndIndex(lf.num); err != nil {
				return nil, err
			}
		}
	}

	if err := d.createNewLog(); err != nil {
		return nil, err
	}
	d.updateReadStateLocked(nil)

	// indexes are populated when d.mu.state.load() is called above
	for _, index := range d.mu.nodeStates.indexes {
		if index.entries.compactedTo > 0 {
			if err := d.compactionLocked(index); err != nil {
				return nil, err
			}
		}
	}

	d.deleteobsoleteWorker.RunWorker(func() {
		d.deleteObsoleteWorkerMain()
	})
	d.scanObsoleteFiles(ls)
	d.notifyDeleteObsoleteWorker()
	return d, nil
}

func (d *db) id() string {
	return d.name
}

func (d *db) createNewLog() error {
	if d.mu.logFile != nil {
		if err := d.mu.logFile.Close(); err != nil {
			return err
		}
	}
	logNum := d.mu.versions.getNextFileNum()
	logName := makeFilename(d.opts.FS, d.dirname, fileTypeLog, logNum)
	logFile, err := d.opts.FS.Create(logName)
	if err != nil {
		return err
	}
	if err := prealloc(logFile, d.opts.MaxLogFileSize+indexBlockSize); err != nil {
		return err
	}
	if err := d.dataDir.Sync(); err != nil {
		return err
	}
	d.mu.logFile = logFile
	d.mu.logWriter = newWriter(logFile)
	d.mu.logNum = logNum
	d.mu.offset = 0
	ve := versionEdit{
		newFiles: []newFileEntry{{meta: &fileMetadata{fileNum: logNum}}},
	}
	d.mu.versions.logLock()
	return d.mu.versions.logAndApply(&ve, d.dataDir)
}

func (d *db) rebuildLogAndIndex(logNum fileNum) (err error) {
	plog.Infof("%s rebuildLogAndIndex, logNum %d", d.id(), logNum)
	f := func(u pb.Update, offset int64) bool {
		d.updateIndex(u, offset, logNum)
		return true
	}
	if err := d.readLog(indexEntry{fileNum: logNum}, f); err != nil {
		if !IsInvalidRecord(err) {
			return err
		}
		if err := d.rebuildLog(logNum); err != nil {
			return err
		}
	}
	// save to index file
	return d.mu.nodeStates.save(d.dirname, d.dataDir, logNum, d.opts.FS)
}

func (d *db) rebuildLog(logNum fileNum) (err error) {
	// it is possible to have the last log file to contain a corrupted chunk or
	// block on the tail, e.g. power got cut after partially written chunk or
	// block. for those situations, the log file itself is totally fine, we just
	// need to remove the final chunk or block.
	// in theory, we should be able to just truncate the log file to the last
	// reported offset. however, for simplicity, let's just copy the log and skip
	// the last broken chunk or block.
	fn := makeFilename(d.opts.FS, d.dirname, fileTypeLogTemp, logNum)
	ln := makeFilename(d.opts.FS, d.dirname, fileTypeLog, logNum)
	f, err := d.opts.FS.Create(fn)
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, f.Sync())
		err = firstError(err, f.Close())
		err = firstError(err, d.opts.FS.Rename(fn, ln))
		err = firstError(err, d.dataDir.Sync())
	}()
	w := newWriter(f)
	defer func() {
		err = firstError(err, w.close())
	}()
	buf := make([]byte, defaultBufferSize)
	var herr error
	var newOffset int64
	h := func(u pb.Update, offset int64) bool {
		sz := u.SizeUpperLimit()
		if sz > len(buf) {
			buf = make([]byte, sz)
		}
		data := pb.MustMarshalTo(&u, buf)
		updatedOffset, err := w.writeRecord(data)
		if err != nil {
			herr = err
			return false
		}
		if newOffset != offset {
			plog.Panicf("offset changed, %d, %d", offset, newOffset)
		}
		newOffset = updatedOffset
		return true
	}
	if err := d.readLog(indexEntry{fileNum: logNum}, h); err != nil {
		if !IsInvalidRecord(err) {
			return err
		}
	}
	return herr
}

func (d *db) saveIndex() error {
	return d.mu.nodeStates.save(d.dirname, d.dataDir, d.mu.logNum, d.opts.FS)
}

func (d *db) close() error {
	d.deleteobsoleteWorker.Stop()
	d.mu.Lock()
	defer d.mu.Unlock()
	if err := d.closed.Load(); err != nil {
		panic(err)
	}
	d.closed.Store(errors.WithStack(ErrClosed))
	close(d.closedCh)

	var err error
	err = firstError(err, d.mu.logWriter.close())
	err = firstError(err, d.saveIndex())
	// Note that versionSet.close() only closes the MANIFEST. The versions list
	// is still valid for the checks below.
	err = firstError(err, d.mu.versions.close())
	plog.Infof("%s is being closed, logNum %d", d.id(), d.mu.logNum)
	err = firstError(err, d.mu.logFile.Close())
	err = firstError(err, d.dataDir.Close())

	if err == nil {
		d.readState.val.unrefLocked()
	}
	return err
}
````

## File: internal/tan/options.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"github.com/lni/vfs"
)

const (
	// MaxManifestFileSize is the default max manifest file size
	MaxManifestFileSize int64 = 1024 * 1024 * 2
	// MaxLogFileSize is the default max log file size
	MaxLogFileSize int64 = 1024 * 1024 * 64
)

// Options is the option type used by tan
type Options struct {
	MaxLogFileSize      int64
	MaxManifestFileSize int64
	FS                  vfs.FS
}

// EnsureDefaults ensures that the default values for all options are set if a
// valid value was not already specified. Returns the new options.
func (o *Options) EnsureDefaults() *Options {
	if o == nil {
		o = &Options{}
	}
	if o.MaxLogFileSize == 0 {
		o.MaxLogFileSize = MaxLogFileSize
	}
	if o.MaxManifestFileSize == 0 {
		o.MaxManifestFileSize = MaxManifestFileSize
	}
	if o.FS == nil {
		o.FS = vfs.Default
	}
	return o
}
````

## File: internal/tan/prealloc_generic.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//go:build !linux
// +build !linux

package tan

import (
	"github.com/lni/vfs"
)

func prealloc(f vfs.File, size int64) error {
	return nil
}
````

## File: internal/tan/prealloc_linux.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//go:build linux
// +build linux

package tan

import (
	"os"
	"syscall"

	"github.com/lni/vfs"
	"golang.org/x/sys/unix"
)

func prealloc(f vfs.File, size int64) error {
	osf, ok := f.(*os.File)
	if !ok {
		return nil
	}
	return syscall.Fallocate(int(osf.Fd()), unix.FALLOC_FL_KEEP_SIZE, 0, size)
}
````

## File: internal/tan/read_state.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import "sync/atomic"

// readState encapsulates the state needed for reading (the current version and
// list of memtables). Loading the readState is done without grabbing
// DB.mu. Instead, a separate DB.readState.RWMutex is used for
// synchronization. This mutex solely covers the current readState object which
// means it is rarely or ever contended.
//
// Note that various fancy lock-free mechanisms can be imagined for loading the
// readState, but benchmarking showed the ones considered to purely be
// pessimizations. The RWMutex version is a single atomic increment for the
// RLock and an atomic decrement for the RUnlock. It is difficult to do better
// than that without something like thread-local storage which isn't available
// in Go.
type readState struct {
	db         *db
	refcnt     int32
	version    *version
	nodeStates *nodeStates
}

// ref adds a reference to the readState.
func (s *readState) ref() {
	atomic.AddInt32(&s.refcnt, 1)
}

// unref removes a reference to the readState. If this was the last reference,
// the reference the readState holds on the version is released. Requires DB.mu
// is NOT held as version.unref() will acquire it. See unrefLocked() if DB.mu
// is held by the caller.
func (s *readState) unref() {
	if atomic.AddInt32(&s.refcnt, -1) != 0 {
		return
	}
	s.version.unref()

	// TODO:
	// re-enable the obsolete file deletion
	//
	// The last reference to the readState was released. Check to see if there
	// are new obsolete tables to delete.
	// s.db.maybeScheduleObsoleteTableDeletion()
}

// unrefLocked removes a reference to the readState. If this was the last
// reference, the reference the readState holds on the version is
// released. Requires DB.mu is held as version.unrefLocked() requires it. See
// unref() if DB.mu is NOT held by the caller.
func (s *readState) unrefLocked() {
	if atomic.AddInt32(&s.refcnt, -1) != 0 {
		return
	}
	s.version.unrefLocked()

	// NB: Unlike readState.unref(), we don't attempt to cleanup newly obsolete
	// tables as unrefLocked() is only called during DB shutdown to release the
	// current readState.
}

// loadReadState returns the current readState. The returned readState must be
// unreferenced when the caller is finished with it.
func (d *db) loadReadState() *readState {
	d.readState.RLock()
	state := d.readState.val
	state.ref()
	d.readState.RUnlock()
	return state
}

// updateReadStateLocked creates a new readState from the current version and
// list of memtables. Requires DB.mu is held. If checker is not nil, it is
// called after installing the new readState
func (d *db) updateReadStateLocked(checker func(*db) error) {
	s := &readState{
		db:         d,
		refcnt:     1,
		version:    d.mu.versions.currentVersion(),
		nodeStates: d.mu.nodeStates,
	}
	s.version.ref()

	d.readState.Lock()
	old := d.readState.val
	d.readState.val = s
	d.readState.Unlock()
	if checker != nil {
		if err := checker(d); err != nil {
			panic(err)
		}
	}
	if old != nil {
		old.unrefLocked()
	}
}
````

## File: internal/tan/README.md
````markdown
## Tan

Tan is a high performance database for storing Raft log and metadata. 

## Motivation

Early versions of [Dragonboat](https://github.com/lni/dragonboat) employed RocksDB style [LSM](https://en.wikipedia.org/wiki/Log-structured_merge-tree) based Key-Value databases to store Raft log and metadata. Such Key-Value stores are easy to use but such convenience comes at huge costs - 

* redundant MemTables
* redundant keys
* redundant serializations
* storage amplification
* write amplification
* read amplification
* expensive concurrent access control

Tan aims to overcome all these issues by providing a specifically designed database for storing Raft log and metadata.

## License
Tan contains [Pebble](https://github.com/cockroachdb/pebble) code and code derived from Pebble. Pebble itself is built from the [golang version](https://github.com/golang/leveldb) of [Level-DB](https://github.com/google/leveldb). Pebble, Level-DB and the golang version of Level-DB are all BSD licensed.
````

## File: internal/tan/record_test.go
````go
// Copyright 2011 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.

package tan

import (
	"bytes"
	"fmt"
	"io"
	"strings"
	"testing"

	"github.com/cockroachdb/errors"
	"github.com/stretchr/testify/require"
	"golang.org/x/exp/rand"
)

func short(s string) string {
	if len(s) < 64 {
		return s
	}
	return fmt.Sprintf("%s...(skipping %d bytes)...%s", s[:20], len(s)-40, s[len(s)-20:])
}

// big returns a string of length n, composed of repetitions of partial.
func big(partial string, n int) string {
	return strings.Repeat(partial, n/len(partial)+1)[:n]
}

type recordWriter interface {
	writeRecord([]byte) (int64, error)
	close() error
}

func testGeneratorWriter(
	t *testing.T,
	reset func(),
	gen func() (string, bool),
	newWriter func(io.Writer) recordWriter,
) {
	buf := new(bytes.Buffer)

	reset()
	w := newWriter(buf)
	for {
		s, ok := gen()
		if !ok {
			break
		}
		if _, err := w.writeRecord([]byte(s)); err != nil {
			t.Fatalf("Write: %v", err)
		}
	}
	if err := w.close(); err != nil {
		t.Fatalf("Close: %v", err)
	}
	reset()
	r := newReader(buf, 0 /* logNum */)
	for {
		s, ok := gen()
		if !ok {
			break
		}
		rr, err := r.next()
		if err != nil {
			t.Fatalf("reader.Next: %v", err)
		}
		x, err := io.ReadAll(rr)
		if err != nil {
			t.Fatalf("ReadAll: %v", err)
		}
		if string(x) != s {
			t.Fatalf("got %q, want %q", short(string(x)), short(s))
		}
	}
	if _, err := r.next(); err != io.EOF {
		t.Fatalf("got %v, want %v", err, io.EOF)
	}
}

func testGenerator(t *testing.T, reset func(), gen func() (string, bool)) {
	t.Run("Writer", func(t *testing.T) {
		testGeneratorWriter(t, reset, gen, func(w io.Writer) recordWriter {
			return newWriter(w)
		})
	})
}

func testLiterals(t *testing.T, s []string) {
	var i int
	reset := func() {
		i = 0
	}
	gen := func() (string, bool) {
		if i == len(s) {
			return "", false
		}
		i++
		return s[i-1], true
	}
	testGenerator(t, reset, gen)
}

func TestMany(t *testing.T) {
	const n = 1e5
	var i int
	reset := func() {
		i = 0
	}
	gen := func() (string, bool) {
		if i == n {
			return "", false
		}
		i++
		return fmt.Sprintf("%d.", i-1), true
	}
	testGenerator(t, reset, gen)
}

func TestRandom(t *testing.T) {
	const n = 1e2
	var (
		i int
		r *rand.Rand
	)
	reset := func() {
		i, r = 0, rand.New(rand.NewSource(0))
	}
	gen := func() (string, bool) {
		if i == n {
			return "", false
		}
		i++
		return strings.Repeat(string(uint8(i)), r.Intn(2*blockSize+16)), true
	}
	testGenerator(t, reset, gen)
}

func TestBasic(t *testing.T) {
	testLiterals(t, []string{
		strings.Repeat("a", 1000),
		strings.Repeat("b", 97270),
		strings.Repeat("c", 8000),
	})
}

func TestBoundary(t *testing.T) {
	for i := blockSize - 16; i < blockSize+16; i++ {
		s0 := big("abcd", i)
		for j := blockSize - 16; j < blockSize+16; j++ {
			s1 := big("ABCDE", j)
			testLiterals(t, []string{s0, s1})
			testLiterals(t, []string{s0, "", s1})
			testLiterals(t, []string{s0, "x", s1})
		}
	}
}

func TestFlush(t *testing.T) {
	buf := new(bytes.Buffer)
	w := newWriter(buf)
	// Write a couple of records. Everything should still be held
	// in the record.Writer buffer, so that buf.Len should be 0.
	w0, _ := w.next()
	_, err := w0.Write([]byte("0"))
	require.NoError(t, err)
	w1, _ := w.next()
	_, err = w1.Write([]byte("11"))
	require.NoError(t, err)
	if got, want := buf.Len(), 0; got != want {
		t.Fatalf("buffer length #0: got %d want %d", got, want)
	}
	// Flush the record.Writer buffer, which should yield 17 bytes.
	// 17 = 2*7 + 1 + 2, which is two headers and 1 + 2 payload bytes.
	require.NoError(t, w.flush())
	if got, want := buf.Len(), 17; got != want {
		t.Fatalf("buffer length #1: got %d want %d", got, want)
	}
	// Do another write, one that isn't large enough to complete the block.
	// The write should not have flowed through to buf.
	w2, _ := w.next()
	_, err = w2.Write(bytes.Repeat([]byte("2"), 10000))
	require.NoError(t, err)
	if got, want := buf.Len(), 17; got != want {
		t.Fatalf("buffer length #2: got %d want %d", got, want)
	}
	// Flushing should get us up to 10024 bytes written.
	// 10024 = 17 + 7 + 10000.
	require.NoError(t, w.flush())
	if got, want := buf.Len(), 10024; got != want {
		t.Fatalf("buffer length #3: got %d want %d", got, want)
	}
	// Do a bigger write, one that completes the current block.
	// We should now have 32768 bytes (a complete block), without
	// an explicit flush.
	w3, _ := w.next()
	_, err = w3.Write(bytes.Repeat([]byte("3"), 40000))
	require.NoError(t, err)
	if got, want := buf.Len(), 32768; got != want {
		t.Fatalf("buffer length #4: got %d want %d", got, want)
	}
	// Flushing should get us up to 50038 bytes written.
	// 50038 = 10024 + 2*7 + 40000. There are two headers because
	// the one record was split into two chunks.
	require.NoError(t, w.flush())
	if got, want := buf.Len(), 50038; got != want {
		t.Fatalf("buffer length #5: got %d want %d", got, want)
	}
	// Check that reading those records give the right lengths.
	r := newReader(buf, 0 /* logNum */)
	wants := []int64{1, 2, 10000, 40000}
	for i, want := range wants {
		rr, _ := r.next()
		n, err := io.Copy(io.Discard, rr)
		if err != nil {
			t.Fatalf("read #%d: %v", i, err)
		}
		if n != want {
			t.Fatalf("read #%d: got %d bytes want %d", i, n, want)
		}
	}
}

func TestNonExhaustiveRead(t *testing.T) {
	const n = 100
	buf := new(bytes.Buffer)
	p := make([]byte, 10)
	rnd := rand.New(rand.NewSource(1))

	w := newWriter(buf)
	for i := 0; i < n; i++ {
		length := len(p) + rnd.Intn(3*blockSize)
		s := string(uint8(i)) + "123456789abcdefgh"
		_, _ = w.writeRecord([]byte(big(s, length)))
	}
	if err := w.close(); err != nil {
		t.Fatalf("Close: %v", err)
	}

	r := newReader(buf, 0 /* logNum */)
	for i := 0; i < n; i++ {
		rr, _ := r.next()
		_, err := io.ReadFull(rr, p)
		if err != nil {
			t.Fatalf("ReadFull: %v", err)
		}
		want := string(uint8(i)) + "123456789"
		if got := string(p); got != want {
			t.Fatalf("read #%d: got %q want %q", i, got, want)
		}
	}
}

func TestStaleReader(t *testing.T) {
	buf := new(bytes.Buffer)

	w := newWriter(buf)
	_, err := w.writeRecord([]byte("0"))
	require.NoError(t, err)

	_, err = w.writeRecord([]byte("11"))
	require.NoError(t, err)

	require.NoError(t, w.close())

	r := newReader(buf, 0 /* logNum */)
	r0, err := r.next()
	require.NoError(t, err)

	r1, err := r.next()
	require.NoError(t, err)

	p := make([]byte, 1)
	if _, err := r0.Read(p); err == nil || !strings.Contains(err.Error(), "stale") {
		t.Fatalf("stale read #0: unexpected error: %v", err)
	}
	if _, err := r1.Read(p); err != nil {
		t.Fatalf("fresh read #1: got %v want nil error", err)
	}
	if p[0] != '1' {
		t.Fatalf("fresh read #1: byte contents: got '%c' want '1'", p[0])
	}
}

type testRecords struct {
	records [][]byte // The raw value of each record.
	offsets []int64  // The offset of each record within buf, derived from writer.LastRecordOffset.
	buf     []byte   // The serialized records form of all records.
}

// makeTestRecords generates test records of specified lengths.
// The first record will consist of repeating 0x00 bytes, the next record of
// 0x01 bytes, and so forth. The values will loop back to 0x00 after 0xff.
func makeTestRecords(recordLengths ...int) (*testRecords, error) {
	ret := &testRecords{}
	ret.records = make([][]byte, len(recordLengths))
	ret.offsets = make([]int64, len(recordLengths))
	for i, n := range recordLengths {
		ret.records[i] = bytes.Repeat([]byte{byte(i)}, n)
	}

	buf := new(bytes.Buffer)
	w := newWriter(buf)
	for i, rec := range ret.records {
		wRec, err := w.next()
		if err != nil {
			return nil, err
		}

		// Alternate between one big write and many small writes.
		cSize := 8
		if i&1 == 0 {
			cSize = len(rec)
		}
		for ; len(rec) > cSize; rec = rec[cSize:] {
			if _, err := wRec.Write(rec[:cSize]); err != nil {
				return nil, err
			}
		}
		if _, err := wRec.Write(rec); err != nil {
			return nil, err
		}

		ret.offsets[i], err = w.lastRecordOffset()
		if err != nil {
			return nil, err
		}
	}

	if err := w.close(); err != nil {
		return nil, err
	}

	ret.buf = buf.Bytes()
	return ret, nil
}

// corruptBlock corrupts the checksum of the record that starts at the
// specified block offset. The number of the block offset is 0 based.
func corruptBlock(buf []byte, blockNum int) {
	// Ensure we always permute at least 1 byte of the checksum.
	if buf[blockSize*blockNum] == 0x00 {
		buf[blockSize*blockNum] = 0xff
	} else {
		buf[blockSize*blockNum] = 0x00
	}

	buf[blockSize*blockNum+1] = 0x00
	buf[blockSize*blockNum+2] = 0x00
	buf[blockSize*blockNum+3] = 0x00
}

func TestRecoverNoOp(t *testing.T) {
	recs, err := makeTestRecords(
		blockSize-legacyHeaderSize,
		blockSize-legacyHeaderSize,
		blockSize-legacyHeaderSize,
	)
	if err != nil {
		t.Fatalf("makeTestRecords: %v", err)
	}

	r := newReader(bytes.NewReader(recs.buf), 0 /* logNum */)
	_, err = r.next()
	if err != nil || r.err != nil {
		t.Fatalf("reader.Next: %v reader.err: %v", err, r.err)
	}

	seq, begin, end, n := r.seq, r.begin, r.end, r.n

	// Should be a no-op since r.err == nil.
	r.recover()

	// r.err was nil, nothing should have changed.
	if seq != r.seq || begin != r.begin || end != r.end || n != r.n {
		t.Fatal("reader.Recover when no error existed, was not a no-op")
	}
}

func TestBasicRecover(t *testing.T) {
	recs, err := makeTestRecords(
		blockSize-legacyHeaderSize,
		blockSize-legacyHeaderSize,
		blockSize-legacyHeaderSize,
	)
	if err != nil {
		t.Fatalf("makeTestRecords: %v", err)
	}

	// Corrupt the checksum of the second record r1 in our file.
	corruptBlock(recs.buf, 1)

	underlyingReader := bytes.NewReader(recs.buf)
	r := newReader(underlyingReader, 0 /* logNum */)

	// The first record r0 should be read just fine.
	r0, err := r.next()
	if err != nil {
		t.Fatalf("Next: %v", err)
	}
	r0Data, err := io.ReadAll(r0)
	if err != nil {
		t.Fatalf("ReadAll: %v", err)
	}
	if !bytes.Equal(r0Data, recs.records[0]) {
		t.Fatal("Unexpected output in r0's data")
	}

	// The next record should have a checksum mismatch.
	_, err = r.next()
	if err == nil {
		t.Fatal("Expected an error while reading a corrupted record")
	}
	if err != ErrCRCMismatch {
		t.Fatalf("Unexpected error returned: %v", err)
	}

	// Recover from that checksum mismatch.
	r.recover()
	currentOffset, err := underlyingReader.Seek(0, io.SeekCurrent)
	if err != nil {
		t.Fatalf("current offset: %v", err)
	}
	if currentOffset != blockSize*2 {
		t.Fatalf("current offset: got %d, want %d", currentOffset, blockSize*2)
	}

	// The third record r2 should be read just fine.
	r2, err := r.next()
	if err != nil {
		t.Fatalf("Next: %v", err)
	}
	r2Data, err := io.ReadAll(r2)
	if err != nil {
		t.Fatalf("ReadAll: %v", err)
	}
	if !bytes.Equal(r2Data, recs.records[2]) {
		t.Fatal("Unexpected output in r2's data")
	}
}

func TestRecoverSingleBlock(t *testing.T) {
	// The first record will be blockSize * 3 bytes long. Since each block has
	// a 7 byte header, the first record will roll over into 4 blocks.
	recs, err := makeTestRecords(
		blockSize*3,
		blockSize-legacyHeaderSize,
		blockSize/2,
	)
	if err != nil {
		t.Fatalf("makeTestRecords: %v", err)
	}

	// Corrupt the checksum for the portion of the first record that exists in
	// the 4th block.
	corruptBlock(recs.buf, 3)

	// The first record should fail, but only when we read deeper beyond the
	// first block.
	r := newReader(bytes.NewReader(recs.buf), 0 /* logNum */)
	r0, err := r.next()
	if err != nil {
		t.Fatalf("Next: %v", err)
	}

	// Reading deeper should yield a checksum mismatch.
	_, err = io.ReadAll(r0)
	if err == nil {
		t.Fatal("Expected a checksum mismatch error, got nil")
	}
	if err != ErrCRCMismatch {
		t.Fatalf("Unexpected error returned: %v", err)
	}

	// Recover from that checksum mismatch.
	r.recover()

	// All of the data in the second record r1 is lost because the first record
	// r0 shared a partial block with it. The second record also overlapped
	// into the block with the third record r2. Recovery should jump to that
	// block, skipping over the end of the second record and start parsing the
	// third record.
	r2, err := r.next()
	if err != nil {
		t.Fatalf("Next: %v", err)
	}
	r2Data, _ := io.ReadAll(r2)
	if !bytes.Equal(r2Data, recs.records[2]) {
		t.Fatal("Unexpected output in r2's data")
	}
}

func TestRecoverMultipleBlocks(t *testing.T) {
	recs, err := makeTestRecords(
		// The first record will consume 3 entire blocks but a fraction of the 4th.
		blockSize*3,
		// The second record will completely fill the remainder of the 4th block.
		3*(blockSize-legacyHeaderSize)-2*blockSize-2*legacyHeaderSize,
		// Consume the entirety of the 5th block.
		blockSize-legacyHeaderSize,
		// Consume the entirety of the 6th block.
		blockSize-legacyHeaderSize,
		// Consume roughly half of the 7th block.
		blockSize/2,
	)
	if err != nil {
		t.Fatalf("makeTestRecords: %v", err)
	}

	// Corrupt the checksum for the portion of the first record that exists in the 4th block.
	corruptBlock(recs.buf, 3)

	// Now corrupt the two blocks in a row that correspond to recs.records[2:4].
	corruptBlock(recs.buf, 4)
	corruptBlock(recs.buf, 5)

	// The first record should fail, but only when we read deeper beyond the first block.
	r := newReader(bytes.NewReader(recs.buf), 0 /* logNum */)
	r0, err := r.next()
	if err != nil {
		t.Fatalf("Next: %v", err)
	}

	// Reading deeper should yield a checksum mismatch.
	_, err = io.ReadAll(r0)
	if err == nil {
		t.Fatal("Exptected a checksum mismatch error, got nil")
	}
	if err != ErrCRCMismatch {
		t.Fatalf("Unexpected error returned: %v", err)
	}

	// Recover from that checksum mismatch.
	r.recover()

	// All of the data in the second record is lost because the first
	// record shared a partial block with it. The following two records
	// have corrupted checksums as well, so the call above to r.Recover
	// should result in r.next() being a reader to the 5th record.
	r4, err := r.next()
	if err != nil {
		t.Fatalf("Next: %v", err)
	}

	r4Data, _ := io.ReadAll(r4)
	if !bytes.Equal(r4Data, recs.records[4]) {
		t.Fatal("Unexpected output in r4's data")
	}
}

// verifyLastBlockRecover reads each record from recs expecting that the
// last record will be corrupted. It will then try Recover and verify that EOF
// is returned.
func verifyLastBlockRecover(recs *testRecords) error {
	r := newReader(bytes.NewReader(recs.buf), 0 /* logNum */)
	// Loop to one element larger than the number of records to verify EOF.
	for i := 0; i < len(recs.records)+1; i++ {
		_, err := r.next()
		switch i {
		case len(recs.records) - 1:
			if err == nil {
				return errors.New("Expected a checksum mismatch error, got nil")
			}
			r.recover()
		case len(recs.records):
			if err != io.EOF {
				return errors.Errorf("Expected io.EOF, got %v", err)
			}
		default:
			if err != nil {
				return errors.Errorf("Next: %v", err)
			}
		}
	}
	return nil
}

func TestRecoverLastPartialBlock(t *testing.T) {
	recs, err := makeTestRecords(
		// The first record will consume 3 entire blocks but a fraction of the 4th.
		blockSize*3,
		// The second record will completely fill the remainder of the 4th block.
		3*(blockSize-legacyHeaderSize)-2*blockSize-2*legacyHeaderSize,
		// Consume roughly half of the 5th block.
		blockSize/2,
	)
	if err != nil {
		t.Fatalf("makeTestRecords: %v", err)
	}

	// Corrupt the 5th block.
	corruptBlock(recs.buf, 4)

	// Verify Recover works when the last block is corrupted.
	if err := verifyLastBlockRecover(recs); err != nil {
		t.Fatalf("verifyLastBlockRecover: %v", err)
	}
}

func TestRecoverLastCompleteBlock(t *testing.T) {
	recs, err := makeTestRecords(
		// The first record will consume 3 entire blocks but a fraction of the 4th.
		blockSize*3,
		// The second record will completely fill the remainder of the 4th block.
		3*(blockSize-legacyHeaderSize)-2*blockSize-2*legacyHeaderSize,
		// Consume the entire 5th block.
		blockSize-legacyHeaderSize,
	)
	if err != nil {
		t.Fatalf("makeTestRecords: %v", err)
	}

	// Corrupt the 5th block.
	corruptBlock(recs.buf, 4)

	// Verify Recover works when the last block is corrupted.
	if err := verifyLastBlockRecover(recs); err != nil {
		t.Fatalf("verifyLastBlockRecover: %v", err)
	}
}

func TestReaderOffset(t *testing.T) {
	recs, err := makeTestRecords(
		blockSize*2,
		400,
		500,
		600,
		700,
		800,
		9000,
		1000,
	)
	if err != nil {
		t.Fatalf("makeTestRecords: %v", err)
	}

	// The first record should fail, but only when we read deeper beyond the first block.
	r := newReader(bytes.NewReader(recs.buf), 0 /* logNum */)
	for i, offset := range recs.offsets {
		if offset != r.offset() {
			t.Fatalf("%d: expected offset %d, but found %d", i, offset, r.offset())
		}
		rec, err := r.next()
		if err != nil {
			t.Fatalf("Next: %v", err)
		}
		if _, err = io.ReadAll(rec); err != nil {
			t.Fatalf("ReadAll: %v", err)
		}
	}
}

func TestSeekRecord(t *testing.T) {
	recs, err := makeTestRecords(
		// The first record will consume 3 entire blocks but a fraction of the 4th.
		blockSize*3,
		// The second record will completely fill the remainder of the 4th block.
		3*(blockSize-legacyHeaderSize)-2*blockSize-2*legacyHeaderSize,
		// Consume the entirety of the 5th block.
		blockSize-legacyHeaderSize,
		// Consume the entirety of the 6th block.
		blockSize-legacyHeaderSize,
		// Consume roughly half of the 7th block.
		blockSize/2,
	)
	if err != nil {
		t.Fatalf("makeTestRecords: %v", err)
	}

	r := newReader(bytes.NewReader(recs.buf), 0 /* logNum */)
	// Seek to a valid block offset, but within a multiblock record. This should cause the next call to
	// Next after SeekRecord to return the next valid FIRST/FULL chunk of the subsequent record.
	err = r.seekRecord(blockSize)
	if err != nil {
		t.Fatalf("SeekRecord: %v", err)
	}
	rec, err := r.next()
	if err != nil {
		t.Fatalf("Next: %v", err)
	}
	rData, _ := io.ReadAll(rec)
	if !bytes.Equal(rData, recs.records[1]) {
		t.Fatalf("Unexpected output in record 1's data, got %v want %v", rData, recs.records[1])
	}

	// Seek 3 bytes into the second block, which is still in the middle of the first record, but not
	// at a valid chunk boundary. Should result in an error upon calling r.Next.
	err = r.seekRecord(blockSize + 3)
	if err != nil {
		t.Fatalf("SeekRecord: %v", err)
	}
	if _, err = r.next(); err == nil {
		t.Fatalf("Expected an error seeking to an invalid chunk boundary")
	}
	r.recover()

	// Seek to the fifth block and verify all records can be read as appropriate.
	err = r.seekRecord(blockSize * 4)
	if err != nil {
		t.Fatalf("SeekRecord: %v", err)
	}

	check := func(i int) {
		for ; i < len(recs.records); i++ {
			rec, err := r.next()
			if err != nil {
				t.Fatalf("Next: %v", err)
			}

			rData, _ := io.ReadAll(rec)
			if !bytes.Equal(rData, recs.records[i]) {
				t.Fatalf("Unexpected output in record #%d's data, got %v want %v", i, rData, recs.records[i])
			}
		}
	}
	check(2)

	// Seek back to the fourth block, and read all subsequent records and verify them.
	err = r.seekRecord(blockSize * 3)
	if err != nil {
		t.Fatalf("SeekRecord: %v", err)
	}
	check(1)

	// Now seek past the end of the file and verify it causes an error.
	err = r.seekRecord(1 << 20)
	if err == nil {
		t.Fatalf("Seek past the end of a file didn't cause an error")
	}
	if err != io.ErrUnexpectedEOF {
		t.Fatalf("Seeking past EOF raised unexpected error: %v", err)
	}
	r.recover() // Verify recovery works.

	// Validate the current records are returned after seeking to a valid offset.
	err = r.seekRecord(blockSize * 4)
	if err != nil {
		t.Fatalf("SeekRecord: %v", err)
	}
	check(2)
}

func TestLastRecordOffset(t *testing.T) {
	recs, err := makeTestRecords(
		// The first record will consume 3 entire blocks but a fraction of the 4th.
		blockSize*3,
		// The second record will completely fill the remainder of the 4th block.
		3*(blockSize-legacyHeaderSize)-2*blockSize-2*legacyHeaderSize,
		// Consume the entirety of the 5th block.
		blockSize-legacyHeaderSize,
		// Consume the entirety of the 6th block.
		blockSize-legacyHeaderSize,
		// Consume roughly half of the 7th block.
		blockSize/2,
	)
	if err != nil {
		t.Fatalf("makeTestRecords: %v", err)
	}

	wants := []int64{0, 98332, 131072, 163840, 196608}
	for i, got := range recs.offsets {
		if want := wants[i]; got != want {
			t.Errorf("record #%d: got %d, want %d", i, got, want)
		}
	}
}

func TestNoLastRecordOffset(t *testing.T) {
	buf := new(bytes.Buffer)
	w := newWriter(buf)
	defer func() {
		require.NoError(t, w.close())
	}()

	if _, err := w.lastRecordOffset(); err != ErrNoLastRecord {
		t.Fatalf("Expected ErrNoLastRecord, got: %v", err)
	}

	require.NoError(t, w.flush())

	if _, err := w.lastRecordOffset(); err != ErrNoLastRecord {
		t.Fatalf("LastRecordOffset: got: %v, want ErrNoLastRecord", err)
	}

	_, err := w.writeRecord([]byte("testrecord"))
	require.NoError(t, err)

	if off, err := w.lastRecordOffset(); err != nil {
		t.Fatalf("LastRecordOffset: %v", err)
	} else if off != 0 {
		t.Fatalf("LastRecordOffset: got %d, want 0", off)
	}
}

func TestSize(t *testing.T) {
	var buf bytes.Buffer
	zeroes := make([]byte, 8<<10)
	w := newWriter(&buf)
	for i := 0; i < 100; i++ {
		n := rand.Intn(len(zeroes))
		_, err := w.writeRecord(zeroes[:n])
		require.NoError(t, err)
		require.NoError(t, w.flush())
		if buf.Len() != int(w.size()) {
			t.Fatalf("expected %d, but found %d", buf.Len(), w.size())
		}
	}
	require.NoError(t, w.close())
}
````

## File: internal/tan/record.go
````go
// Copyright 2011 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.

// Package record reads and writes sequences of records. Each record is a stream
// of bytes that completes before the next record starts.
//
// When reading, call Next to obtain an io.Reader for the next record. Next will
// return io.EOF when there are no more records. It is valid to call Next
// without reading the current record to exhaustion.
//
// When writing, call Next to obtain an io.Writer for the next record. Calling
// Next finishes the current record. Call Close to finish the final record.
//
// Optionally, call Flush to finish the current record and flush the underlying
// writer without starting a new record. To start a new record after flushing,
// call Next.
//
// Neither Readers or Writers are safe to use concurrently.
//
// Example code:
//	func read(r io.Reader) ([]string, error) {
//		var ss []string
//		records := record.NewReader(r)
//		for {
//			rec, err := records.Next()
//			if err == io.EOF {
//				break
//			}
//			if err != nil {
//				log.Printf("recovering from %v", err)
//				r.Recover()
//				continue
//			}
//			s, err := ioutil.ReadAll(rec)
//			if err != nil {
//				log.Printf("recovering from %v", err)
//				r.Recover()
//				continue
//			}
//			ss = append(ss, string(s))
//		}
//		return ss, nil
//	}
//
//	func write(w io.Writer, ss []string) error {
//		records := record.NewWriter(w)
//		for _, s := range ss {
//			rec, err := records.Next()
//			if err != nil {
//				return err
//			}
//			if _, err := rec.Write([]byte(s)), err != nil {
//				return err
//			}
//		}
//		return records.Close()
//	}
//
// The wire format is that the stream is divided into 32KiB blocks, and each
// block contains a number of tightly packed chunks. Chunks cannot cross block
// boundaries. The last block may be shorter than 32 KiB. Any unused bytes in a
// block must be zero.
//
// A record maps to one or more chunks. There are two chunk formats: legacy and
// recyclable. The legacy chunk format:
//
//   +----------+-----------+-----------+--- ... ---+
//   | CRC (4B) | Size (2B) | Type (1B) | Payload   |
//   +----------+-----------+-----------+--- ... ---+
//
// CRC is computed over the type and payload
// Size is the length of the payload in bytes
// Type is the chunk type
//
// There are four chunk types: whether the chunk is the full record, or the
// first, middle or last chunk of a multi-chunk record. A multi-chunk record
// has one first chunk, zero or more middle chunks, and one last chunk.
//
// The recyclyable chunk format is similar to the legacy format, but extends
// the chunk header with an additional log number field. This allows reuse
// (recycling) of log files which can provide significantly better performance
// when syncing frequently as it avoids needing to update the file
// metadata. Additionally, recycling log files is a prequisite for using direct
// IO with log writing. The recyclyable format is:
//
//   +----------+-----------+-----------+----------------+--- ... ---+
//   | CRC (4B) | Size (2B) | Type (1B) | Log number (4B)| Payload   |
//   +----------+-----------+-----------+----------------+--- ... ---+
//
// Recyclable chunks are distinguished from legacy chunks by the addition of 4
// extra "recyclable" chunk types that map directly to the legacy chunk types
// (i.e. full, first, middle, last). The CRC is computed over the type, log
// number, and payload.
//
// The wire format allows for limited recovery in the face of data corruption:
// on a format error (such as a checksum mismatch), the reader moves to the
// next block and looks for the next full or first chunk.

package tan

// The C++ Level-DB code calls this the log, but it has been renamed to record
// to avoid clashing with the standard log package, and because it is generally
// useful outside of logging. The C++ code also uses the term "physical record"
// instead of "chunk", but "chunk" is shorter and less confusing.

import (
	"encoding/binary"
	"io"

	"github.com/cockroachdb/errors"
)

// These constants are part of the wire format and should not be changed.
const (
	fullChunkType   = 1
	firstChunkType  = 2
	middleChunkType = 3
	lastChunkType   = 4

	recyclableFullChunkType = 5
	// recyclableFirstChunkType = 6
	// recyclableMiddleChunkType = 7
	recyclableLastChunkType = 8
)

const (
	blockSize            = 32 * 1024
	blockSizeMask        = blockSize - 1
	legacyHeaderSize     = 7
	recyclableHeaderSize = legacyHeaderSize + 4
)

var (
	// ErrNotAnIOSeeker is returned if the io.Reader underlying a Reader does not implement io.Seeker.
	ErrNotAnIOSeeker = errors.New("reader does not implement io.Seeker")
	// ErrNoLastRecord is returned if LastRecordOffset is called and there is no previous record.
	ErrNoLastRecord = errors.New("no last record exists")
	// ErrZeroedChunk is returned if a chunk is encountered that is zeroed. This
	// usually occurs due to log file preallocation.
	ErrZeroedChunk = errors.New("zeroed chunk")
	// ErrInvalidChunk is returned if a chunk is encountered with an invalid
	// header, length, or checksum. This usually occurs when a log is recycled,
	// but can also occur due to corruption.
	ErrInvalidChunk = errors.New("invalid chunk")
	// ErrCRCMismatch is returned to indicate that CRC mismatch has been found.
	ErrCRCMismatch = errors.New("tan: crc mismatch")
)

// IsInvalidRecord returns true if the error matches one of the error types
// returned for invalid records. These are treated in a way similar to io.EOF
// in recovery code.
func IsInvalidRecord(err error) bool {
	return errors.Is(err, ErrZeroedChunk) ||
		errors.Is(err, ErrInvalidChunk) ||
		errors.Is(err, io.ErrUnexpectedEOF)
}

type flusher interface {
	Flush() error
}

// reader reads records from an underlying io.Reader.
type reader struct {
	// r is the underlying reader.
	r io.Reader
	// logNum is the low 32-bits of the log's file number. May be zero when used
	// with log files that do not have a file number (e.g. the MANIFEST).
	logNum uint32
	// blockNum is the zero based block number currently held in buf.
	blockNum int64
	// seq is the sequence number of the current record.
	seq int
	// buf[begin:end] is the unread portion of the current chunk's payload. The
	// low bound, begin, excludes the chunk header.
	begin, end int
	// n is the number of bytes of buf that are valid. Once reading has started,
	// only the final block can have n < blockSize.
	n int
	// recovering is true when recovering from corruption.
	recovering bool
	// last is whether the current chunk is the last chunk of the record.
	last bool
	// err is any accumulated error.
	err error
	// buf is the buffer.
	buf [blockSize]byte
}

// newReader returns a new reader. If the file contains records encoded using
// the recyclable record format, then the log number in those records must
// match the specified logNum.
func newReader(r io.Reader, logNum fileNum) *reader {
	return &reader{
		r:        r,
		logNum:   uint32(logNum),
		blockNum: -1,
	}
}

// nextChunk sets r.buf[r.i:r.j] to hold the next chunk's payload, reading the
// next block into the buffer if necessary.
func (r *reader) nextChunk(wantFirst bool) error {
	for {
		if r.end+legacyHeaderSize <= r.n {
			checksum := binary.LittleEndian.Uint32(r.buf[r.end+0 : r.end+4])
			length := binary.LittleEndian.Uint16(r.buf[r.end+4 : r.end+6])
			chunkType := r.buf[r.end+6]

			if checksum == 0 && length == 0 && chunkType == 0 {
				if r.end+recyclableHeaderSize > r.n {
					// Skip the rest of the block if the recyclable header size does not
					// fit within it.
					r.end = r.n
					continue
				}
				if r.recovering {
					// Skip the rest of the block, if it looks like it is all
					// zeroes. This is common with WAL preallocation.
					//
					// Set r.err to be an error so r.recover actually recovers.
					r.err = ErrZeroedChunk
					r.recover()
					continue
				}
				return ErrZeroedChunk
			}

			headerSize := legacyHeaderSize
			if chunkType >= recyclableFullChunkType && chunkType <= recyclableLastChunkType {
				headerSize = recyclableHeaderSize
				if r.end+headerSize > r.n {
					return ErrInvalidChunk
				}

				logNum := binary.LittleEndian.Uint32(r.buf[r.end+7 : r.end+11])
				if logNum != r.logNum {
					if wantFirst {
						// If we're looking for the first chunk of a record, we can treat a
						// previous instance of the log as EOF.
						return io.EOF
					}
					// Otherwise, treat this chunk as invalid in order to prevent reading
					// of a partial record.
					return ErrInvalidChunk
				}

				chunkType -= (recyclableFullChunkType - 1)
			}

			r.begin = r.end + headerSize
			r.end = r.begin + int(length)
			if r.end > r.n {
				if r.recovering {
					r.recover()
					continue
				}
				return ErrInvalidChunk
			}
			if checksum != getCRC(r.buf[r.begin-headerSize+6:r.end]) {
				if r.recovering {
					r.recover()
					continue
				}
				return ErrCRCMismatch
			}
			if wantFirst {
				if chunkType != fullChunkType && chunkType != firstChunkType {
					continue
				}
			}
			r.last = chunkType == fullChunkType || chunkType == lastChunkType
			r.recovering = false
			return nil
		}
		if r.n < blockSize && r.blockNum >= 0 {
			if !wantFirst || r.end != r.n {
				// This can happen if the previous instance of the log ended with a
				// partial block at the same blockNum as the new log but extended
				// beyond the partial block of the new log.
				return ErrInvalidChunk
			}
			return io.EOF
		}
		n, err := io.ReadFull(r.r, r.buf[:])
		if err != nil && err != io.ErrUnexpectedEOF {
			if err == io.EOF && !wantFirst {
				return io.ErrUnexpectedEOF
			}
			return err
		}
		r.begin, r.end, r.n = 0, 0, n
		r.blockNum++
	}
}

// next returns a reader for the next record. It returns io.EOF if there are no
// more records. The reader returned becomes stale after the next Next call,
// and should no longer be used.
func (r *reader) next() (io.Reader, error) {
	r.seq++
	if r.err != nil {
		return nil, r.err
	}
	r.begin = r.end
	r.err = r.nextChunk(true)
	if r.err != nil {
		return nil, r.err
	}
	return singleReader{r, r.seq}, nil
}

// offset returns the current offset within the file. If called immediately
// before a call to Next(), Offset() will return the record offset.
func (r *reader) offset() int64 {
	if r.blockNum < 0 {
		return 0
	}
	return r.blockNum*blockSize + int64(r.end)
}

// recover clears any errors read so far, so that calling Next will start
// reading from the next good 32KiB block. If there are no such blocks, Next
// will return io.EOF. recover also marks the current reader, the one most
// recently returned by Next, as stale. If recover is called without any
// prior error, then recover is a no-op.
func (r *reader) recover() {
	if r.err == nil {
		return
	}
	r.recovering = true
	r.err = nil
	// Discard the rest of the current block.
	r.begin, r.end, r.last = r.n, r.n, false
	// Invalidate any outstanding singleReader.
	r.seq++
}

// seekRecord seeks in the underlying io.Reader such that calling r.Next
// returns the record whose first chunk header starts at the provided offset.
// Its behavior is undefined if the argument given is not such an offset, as
// the bytes at that offset may coincidentally appear to be a valid header.
//
// It returns ErrNotAnIOSeeker if the underlying io.Reader does not implement
// io.Seeker.
//
// seekRecord will fail and return an error if the Reader previously
// encountered an error, including io.EOF. Such errors can be cleared by
// calling Recover. Calling seekRecord after Recover will make calling Next
// return the record at the given offset, instead of the record at the next
// good 32KiB block as Recover normally would. Calling seekRecord before
// Recover has no effect on Recover's semantics other than changing the
// starting point for determining the next good 32KiB block.
//
// The offset is always relative to the start of the underlying io.Reader, so
// negative values will result in an error as per io.Seeker.
func (r *reader) seekRecord(offset int64) error {
	r.seq++
	if r.err != nil {
		return r.err
	}

	s, ok := r.r.(io.Seeker)
	if !ok {
		return ErrNotAnIOSeeker
	}

	// Only seek to an exact block offset.
	c := int(offset & blockSizeMask)
	if _, r.err = s.Seek(offset&^blockSizeMask, io.SeekStart); r.err != nil {
		return r.err
	}

	// Clear the state of the internal reader.
	r.begin, r.end, r.n = 0, 0, 0
	r.blockNum, r.recovering, r.last = -1, false, false
	if r.err = r.nextChunk(false); r.err != nil {
		return r.err
	}

	// Now skip to the offset requested within the block. A subsequent
	// call to Next will return the block at the requested offset.
	r.begin, r.end = c, c

	return nil
}

type singleReader struct {
	r   *reader
	seq int
}

func (x singleReader) Read(p []byte) (int, error) {
	r := x.r
	if r.seq != x.seq {
		return 0, errors.New("pebble/record: stale reader")
	}
	if r.err != nil {
		return 0, r.err
	}
	for r.begin == r.end {
		if r.last {
			return 0, io.EOF
		}
		if r.err = r.nextChunk(false); r.err != nil {
			return 0, r.err
		}
	}
	n := copy(p, r.buf[r.begin:r.end])
	r.begin += n
	return n, nil
}

// writer writes records to an underlying io.Writer.
type writer struct {
	// w is the underlying writer.
	w io.Writer
	// seq is the sequence number of the current record.
	seq int
	// f is w as a flusher.
	f flusher
	// buf[i:j] is the bytes that will become the current chunk.
	// The low bound, i, includes the chunk header.
	i, j int
	// buf[:written] has already been written to w.
	// written is zero unless Flush has been called.
	written int
	// baseOffset is the base offset in w at which writing started. If
	// w implements io.Seeker, it's relative to the start of w, 0 otherwise.
	baseOffset int64
	// blockNumber is the zero based block number currently held in buf.
	blockNumber int64
	// lastOffset is the offset in w where the last record was
	// written (including the chunk header). It is a relative offset to
	// baseOffset, thus the absolute offset of the last record is
	// baseOffset + lastRecordOffset.
	lastOffset int64
	// first is whether the current chunk is the first chunk of the record.
	first bool
	// pending is whether a chunk is buffered but not yet written.
	pending bool
	// err is any accumulated error.
	err error
	// buf is the buffer.
	buf [blockSize]byte
}

// newWriter returns a new Writer.
func newWriter(w io.Writer) *writer {
	f, _ := w.(flusher)

	var o int64
	if s, ok := w.(io.Seeker); ok {
		var err error
		if o, err = s.Seek(0, io.SeekCurrent); err != nil {
			o = 0
		}
	}
	return &writer{
		w:          w,
		f:          f,
		baseOffset: o,
		lastOffset: -1,
	}
}

// fillHeader fills in the header for the pending chunk.
func (w *writer) fillHeader(last bool) {
	if w.i+legacyHeaderSize > w.j || w.j > blockSize {
		panic("pebble/record: bad writer state")
	}
	if last {
		if w.first {
			w.buf[w.i+6] = fullChunkType
		} else {
			w.buf[w.i+6] = lastChunkType
		}
	} else {
		if w.first {
			w.buf[w.i+6] = firstChunkType
		} else {
			w.buf[w.i+6] = middleChunkType
		}
	}
	binary.LittleEndian.PutUint32(w.buf[w.i+0:w.i+4], getCRC(w.buf[w.i+6:w.j]))
	binary.LittleEndian.PutUint16(w.buf[w.i+4:w.i+6], uint16(w.j-w.i-legacyHeaderSize))
}

// writeBlock writes the buffered block to the underlying writer, and reserves
// space for the next chunk's header.
func (w *writer) writeBlock() {
	_, w.err = w.w.Write(w.buf[w.written:])
	w.i = 0
	w.j = legacyHeaderSize
	w.written = 0
	w.blockNumber++
}

// writePending finishes the current record and writes the buffer to the
// underlying writer.
func (w *writer) writePending() {
	if w.err != nil {
		return
	}
	if w.pending {
		w.fillHeader(true)
		w.pending = false
	}
	_, w.err = w.w.Write(w.buf[w.written:w.j])
	w.written = w.j
}

// close finishes the current record and closes the writer.
func (w *writer) close() error {
	w.seq++
	w.writePending()
	if w.err != nil {
		return w.err
	}
	w.err = errors.New("pebble/record: closed Writer")
	return nil
}

// flush finishes the current record, writes to the underlying writer, and
// flushes it if that writer implements interface{ Flush() error }.
func (w *writer) flush() error {
	w.seq++
	w.writePending()
	if w.err != nil {
		return w.err
	}
	if w.f != nil {
		w.err = w.f.Flush()
		return w.err
	}
	return nil
}

// next returns a writer for the next record. The writer returned becomes stale
// after the next Close, Flush or Next call, and should no longer be used.
func (w *writer) next() (io.Writer, error) {
	if err := w.getNext(); err != nil {
		return nil, err
	}
	return singleWriter{w, w.seq}, nil
}

func (w *writer) getNext() error {
	w.seq++
	if w.err != nil {
		return w.err
	}
	if w.pending {
		w.fillHeader(true)
	}
	w.i = w.j
	w.j = w.j + legacyHeaderSize
	// Check if there is room in the block for the header.
	if w.j > blockSize {
		// Fill in the rest of the block with zeroes.
		for k := w.i; k < blockSize; k++ {
			w.buf[k] = 0
		}
		w.writeBlock()
		if w.err != nil {
			return w.err
		}
	}
	w.lastOffset = w.baseOffset + w.blockNumber*blockSize + int64(w.i)
	w.first = true
	w.pending = true
	return nil
}

// writeRecord writes a complete record. Returns the offset just past the end
// of the record.
func (w *writer) writeRecord(p []byte) (int64, error) {
	if w.err != nil {
		return -1, w.err
	}
	if err := w.getNext(); err != nil {
		return -1, err
	}
	t := singleWriter{w, w.seq}
	if _, err := t.Write(p); err != nil {
		return -1, err
	}
	w.writePending()
	offset := w.blockNumber*blockSize + int64(w.j)
	return offset, w.err
}

// size returns the current size of the file.
func (w *writer) size() int64 {
	if w == nil {
		return 0
	}
	return w.blockNumber*blockSize + int64(w.j)
}

// lastOffset returns the offset in the underlying io.Writer of the last
// record so far - the one created by the most recent Next call. It is the
// offset of the first chunk header, suitable to pass to Reader.SeekRecord.
//
// If that io.Writer also implements io.Seeker, the return value is an absolute
// offset, in the sense of io.SeekStart, regardless of whether the io.Writer
// was initially at the zero position when passed to NewWriter. Otherwise, the
// return value is a relative offset, being the number of bytes written between
// the NewWriter call and any records written prior to the last record.
//
// If there is no last record, i.e. nothing was written, LastRecordOffset will
// return ErrNoLastRecord.
func (w *writer) lastRecordOffset() (int64, error) {
	if w.err != nil {
		return 0, w.err
	}
	if w.lastOffset < 0 {
		return 0, ErrNoLastRecord
	}
	return w.lastOffset, nil
}

type singleWriter struct {
	w   *writer
	seq int
}

func (x singleWriter) Write(p []byte) (int, error) {
	w := x.w
	if w.seq != x.seq {
		return 0, errors.New("pebble/record: stale writer")
	}
	if w.err != nil {
		return 0, w.err
	}
	n0 := len(p)
	for len(p) > 0 {
		// Write a block, if it is full.
		if w.j == blockSize {
			w.fillHeader(false)
			w.writeBlock()
			if w.err != nil {
				return 0, w.err
			}
			w.first = false
		}
		// Copy bytes into the buffer.
		n := copy(w.buf[w.j:], p)
		w.j += n
		p = p[n:]
	}
	return n0, nil
}
````

## File: internal/tan/utils.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"fmt"
)

func firstError(err1 error, err2 error) error {
	if err1 != nil {
		return err1
	}
	return err2
}

func panicNow(err error) {
	panic(fmt.Sprintf("%+v", err))
}
````

## File: internal/tan/version_edit_test.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"bytes"
	"strings"
	"testing"

	"github.com/cockroachdb/errors"
	"github.com/kr/pretty"
)

func checkRoundTrip(e0 versionEdit) error {
	var e1 versionEdit
	buf := new(bytes.Buffer)
	if err := e0.encode(buf); err != nil {
		return errors.Wrap(err, "encode")
	}
	if err := e1.decode(buf); err != nil {
		return errors.Wrap(err, "decode")
	}
	if diff := pretty.Diff(e0, e1); diff != nil {
		return errors.Errorf("%s", strings.Join(diff, "\n"))
	}
	return nil
}

func TestVersionEditRoundTrip(t *testing.T) {
	testCases := []versionEdit{
		// An empty version edit.
		{},
		// A complete version edit.
		{
			nextFileNum: 44,
			deletedFiles: map[deletedFileEntry]*fileMetadata{
				{
					fileNum: 703,
				}: nil,
				{
					fileNum: 704,
				}: nil,
			},
			newFiles: []newFileEntry{
				{
					meta: &fileMetadata{
						fileNum: 805,
					},
				},
				{
					meta: &fileMetadata{
						fileNum: 806,
					},
				},
			},
		},
	}
	for _, tc := range testCases {
		if err := checkRoundTrip(tc); err != nil {
			t.Error(err)
		}
	}
}

// TODO:
// crc is now xxhash64, need to update the tset MANIFEST files as well

/*
func TestVersionEditDecode(t *testing.T) {
	testCases := []struct {
		filename     string
		encodedEdits []string
		edits        []VersionEdit
	}{
		// db-stage-1 and db-stage-2 have the same manifest.
		{
			filename: "db-stage-1/MANIFEST-000001",
			encodedEdits: []string{
				"\x02\x00\x03\x02\x04\x00",
			},
			edits: []VersionEdit{
				{
					NextFileNum: 2,
				},
			},
		},
		// db-stage-3 and db-stage-4 have the same manifest.
		{
			filename: "db-stage-3/MANIFEST-000005",
			encodedEdits: []string{
				"\x02\x00",
				"\x02\x04\t\x00\x03\x06\x04\x05d\x00\x04\xda\a\vbar" +
					"\x00\x05\x00\x00\x00\x00\x00\x00\vfoo\x01\x04\x00" +
					"\x00\x00\x00\x00\x00\x03\x05",
			},
			edits: []VersionEdit{
				{},
				{
					NextFileNum: 6,
					NewFiles: []NewFileEntry{
						{
							Meta: &FileMetadata{
								FileNum: 4,
							},
						},
					},
				},
			},
		},
	}

	for _, tc := range testCases {
		t.Run("", func(t *testing.T) {
			f, err := os.Open("./testdata/" + tc.filename)
			if err != nil {
				t.Fatalf("filename=%q: open error: %v", tc.filename, err)
			}
			defer f.Close()
			i, r := 0, NewReader(f, 0)
			for {
				rr, err := r.Next()
				if err == io.EOF {
					break
				}
				if err != nil {
					t.Fatalf("filename=%q i=%d: record reader error: %v", tc.filename, i, err)
				}
				if i >= len(tc.edits) {
					t.Fatalf("filename=%q i=%d: too many version edits", tc.filename, i+1)
				}

				encodedEdit, err := ioutil.ReadAll(rr)
				if err != nil {
					t.Fatalf("filename=%q i=%d: read error: %v", tc.filename, i, err)
				}
				if s := string(encodedEdit); s != tc.encodedEdits[i] {
					t.Fatalf("filename=%q i=%d: got encoded %q, want %q", tc.filename, i, s, tc.encodedEdits[i])
				}

				var edit VersionEdit
				err = edit.Decode(bytes.NewReader(encodedEdit))
				if err != nil {
					t.Fatalf("filename=%q i=%d: decode error: %v", tc.filename, i, err)
				}
				if !reflect.DeepEqual(edit, tc.edits[i]) {
					t.Fatalf("filename=%q i=%d: decode\n\tgot  %#v\n\twant %#v\n%s", tc.filename, i, edit, tc.edits[i],
						strings.Join(pretty.Diff(edit, tc.edits[i]), "\n"))
				}
				if err := checkRoundTrip(edit); err != nil {
					t.Fatalf("filename=%q i=%d: round trip: %v", tc.filename, i, err)
				}

				i++
			}
			if i != len(tc.edits) {
				t.Fatalf("filename=%q: got %d edits, want %d", tc.filename, i, len(tc.edits))
			}
		})
	}
}*/
````

## File: internal/tan/version_edit.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"bufio"
	"bytes"
	"encoding/binary"
	"io"
	"sync/atomic"

	"github.com/cockroachdb/errors"
)

var errCorruptManifest = errors.New("corrupt manifest")

type byteReader interface {
	io.ByteReader
	io.Reader
}

// Tags for the versionEdit disk format.
const (
	tagNextFileNumber = 1
	tagDeletedFile    = 2
	tagNewFile        = 3
)

// deletedFileEntry holds the state for a file deletion. The file itself might
// still be referenced by another level.
type deletedFileEntry struct {
	fileNum fileNum
}

// newFileEntry holds the state for a new file.
type newFileEntry struct {
	meta *fileMetadata
}

// versionEdit holds the state for an edit to a Version along with other
// on-disk state
type versionEdit struct {
	// The next file number. A single counter is used to assign file numbers
	// for the WAL, MANIFEST and OPTIONS files.
	nextFileNum  fileNum
	deletedFiles map[deletedFileEntry]*fileMetadata
	newFiles     []newFileEntry
}

// decode decodes an edit from the specified reader.
func (v *versionEdit) decode(r io.Reader) error {
	br, ok := r.(byteReader)
	if !ok {
		br = bufio.NewReader(r)
	}
	d := versionEditDecoder{br}
	for {
		tag, err := binary.ReadUvarint(br)
		if err == io.EOF {
			break
		}
		if err != nil {
			return err
		}
		switch tag {
		case tagNextFileNumber:
			n, err := d.readfileNum()
			if err != nil {
				return err
			}
			v.nextFileNum = n

		case tagDeletedFile:
			fileNum, err := d.readfileNum()
			if err != nil {
				return err
			}
			if v.deletedFiles == nil {
				v.deletedFiles = make(map[deletedFileEntry]*fileMetadata)
			}
			v.deletedFiles[deletedFileEntry{fileNum}] = nil

		case tagNewFile:
			fileNum, err := d.readfileNum()
			if err != nil {
				return err
			}
			v.newFiles = append(v.newFiles, newFileEntry{
				meta: &fileMetadata{
					fileNum: fileNum,
				},
			})

		default:
			return errCorruptManifest
		}
	}
	return nil
}

// encode encodes an edit to the specified writer.
func (v *versionEdit) encode(w io.Writer) error {
	e := versionEditEncoder{new(bytes.Buffer)}

	if v.nextFileNum != 0 {
		e.writeUvarint(tagNextFileNumber)
		e.writeUvarint(uint64(v.nextFileNum))
	}
	for x := range v.deletedFiles {
		e.writeUvarint(tagDeletedFile)
		e.writeUvarint(uint64(x.fileNum))
	}
	for _, x := range v.newFiles {
		e.writeUvarint(tagNewFile)
		e.writeUvarint(uint64(x.meta.fileNum))
	}
	_, err := w.Write(e.Bytes())
	return err
}

type versionEditDecoder struct {
	byteReader
}

func (d versionEditDecoder) readfileNum() (fileNum, error) {
	u, err := d.readUvarint()
	if err != nil {
		return 0, err
	}
	return fileNum(u), nil
}

func (d versionEditDecoder) readUvarint() (uint64, error) {
	u, err := binary.ReadUvarint(d)
	if err != nil {
		if err == io.EOF {
			return 0, errCorruptManifest
		}
		return 0, err
	}
	return u, nil
}

type versionEditEncoder struct {
	*bytes.Buffer
}

func (e versionEditEncoder) writeUvarint(u uint64) {
	var buf [binary.MaxVarintLen64]byte
	n := binary.PutUvarint(buf[:], u)
	e.Write(buf[:n])
}

// bulkVersionEdit summarizes the files added and deleted from a set of version
// edits.
type bulkVersionEdit struct {
	added   []*fileMetadata
	deleted map[fileNum]*fileMetadata

	// AddedByFileNum maps file number to file metadata for all added files
	// from accumulated version edits. AddedByFileNum is only populated if set
	// to non-nil by a caller. It must be set to non-nil when replaying
	// version edits read from a MANIFEST (as opposed to VersionEdits
	// constructed in-memory).  While replaying a MANIFEST file,
	// VersionEdit.deletedFiles map entries have nil values, because the
	// on-disk deletion record encodes only the file number. Accumulate
	// uses AddedByFileNum to correctly populate the bulkVersionEdit's Deleted
	// field with non-nil *fileMetadata.
	addedByFileNum map[fileNum]*fileMetadata
}

// accumulate adds the file addition and deletions in the specified version
// edit to the bulk edit's internal state.
func (b *bulkVersionEdit) accumulate(ve *versionEdit) error {
	for fn, m := range ve.deletedFiles {
		if b.deleted == nil {
			b.deleted = make(map[fileNum]*fileMetadata)
		}
		if m == nil {
			// m is nil only when replaying a MANIFEST.
			if b.addedByFileNum == nil {
				return errors.Errorf("deleted file %s's metadata is absent and bve.addedByFileNum is nil",
					fn.fileNum)
			}
			m = b.addedByFileNum[fn.fileNum]
			if m == nil {
				return errors.Errorf("file deleted %s before it was inserted", fn.fileNum)
			}
		}
		b.deleted[m.fileNum] = m
	}

	for _, nf := range ve.newFiles {
		if b.deleted != nil {
			// A new file should not have been deleted in this or a preceding
			// VersionEdit at the same level (though files can move across levels).
			if _, ok := b.deleted[nf.meta.fileNum]; ok {
				return errors.Errorf("file deleted %s before it was inserted", nf.meta.fileNum)
			}
		}
		b.added = append(b.added, nf.meta)
		if b.addedByFileNum != nil {
			b.addedByFileNum[nf.meta.fileNum] = nf.meta
		}
	}
	return nil
}

// apply applies the delta b to the current version to produce a new
// version. The new version is consistent with respect to the comparer cmp.
//
// curr may be nil, which is equivalent to a pointer to a zero version.
//
// On success, a map of zombie files containing the file numbers and sizes of
// deleted files is returned. These files are considered zombies because they
// are no longer referenced by the returned Version, but cannot be deleted from
// disk as they are still in use by the incoming Version.
func (b *bulkVersionEdit) apply(curr *version,
) (_ *version, zombies map[fileNum]uint64, _ error) {
	addZombie := func(fn fileNum) {
		if zombies == nil {
			zombies = make(map[fileNum]uint64)
		}
		zombies[fn] = 0
	}
	// The remove zombie function is used to handle tables that are moved from
	// one level to another during a version edit (i.e. a "move" compaction).
	removeZombie := func(fileNum fileNum) {
		if zombies != nil {
			delete(zombies, fileNum)
		}
	}

	v := new(version)
	if curr == nil || curr.files == nil {
		v.files = make(map[fileNum]*fileMetadata)
	} else {
		v.files = curr.clone()
	}

	if len(b.added) == 0 && len(b.deleted) == 0 {
		return v, zombies, nil
	}

	// Some edits on this level.
	addedFiles := b.added
	deletedMap := b.deleted
	if n := len(v.files) + len(addedFiles); n == 0 {
		return nil, nil, errors.New("No current or added files but have deleted files")
	}

	// NB: addedFiles may be empty and it also is not necessarily
	// internally consistent: it does not reflect deletions in deletedMap.

	for _, f := range deletedMap {
		addZombie(f.fileNum)
		f, ok := v.files[f.fileNum]
		if ok {
			// Deleting a file from the B-Tree may decrement its
			// reference count. However, because we cloned the
			// previous level's B-Tree, this should never result in a
			// file's reference count dropping to zero.
			if atomic.AddInt32(&f.refs, -1) == 0 {
				err := errors.Errorf("file %s obsolete during B-Tree removal", f.fileNum)
				return nil, nil, err
			}
			delete(v.files, f.fileNum)
		}
	}

	for _, f := range addedFiles {
		if _, ok := deletedMap[f.fileNum]; ok {
			// Already called addZombie on this file in the preceding
			// loop, so we don't need to do it here.
			continue
		}
		atomic.AddInt32(&f.refs, 1)
		v.files[f.fileNum] = f
		removeZombie(f.fileNum)
	}
	return v, zombies, nil
}
````

## File: internal/tan/version_set_test.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"sync"
	"testing"

	"github.com/lni/vfs"
	"github.com/stretchr/testify/require"
)

func TestVersionSetCanBeCreated(t *testing.T) {
	var vs versionSet
	var mu sync.Mutex
	dirname := "db-dir"
	fs := vfs.NewMem()
	defer vfs.ReportLeakedFD(fs, t)
	opt := &Options{MaxManifestFileSize: MaxManifestFileSize, FS: fs}
	require.NoError(t, fs.MkdirAll(dirname, 0700))
	dir, err := fs.OpenDir(dirname)
	require.NoError(t, err)
	defer func() {
		require.NoError(t, dir.Close())
	}()
	require.NoError(t, vs.create(dirname, opt, dir, &mu))
	require.NoError(t, vs.close())
	// manifest file is accessible from the fs
	manifestFn := makeFilename(fs, dirname, fileTypeManifest, vs.manifestFileNum)
	f, err := fs.Open(manifestFn)
	require.NoError(t, err)
	require.NoError(t, f.Close())
}

func TestVersionSetCanBeApplied(t *testing.T) {
	fs := vfs.NewMem()
	testVersionSetCanBeApplied(t, fs)
}

func testVersionSetCanBeApplied(t *testing.T, fs vfs.FS) {
	var vs versionSet
	defer func() {
		require.NoError(t, vs.close())
	}()
	var mu sync.Mutex
	dirname := "db-dir"
	opt := &Options{MaxManifestFileSize: MaxManifestFileSize, FS: fs}
	require.NoError(t, fs.MkdirAll(dirname, 0700))
	dir, err := fs.OpenDir(dirname)
	require.NoError(t, err)
	defer func() {
		require.NoError(t, dir.Close())
	}()
	require.NoError(t, vs.create(dirname, opt, dir, &mu))

	var ve1 versionEdit
	var ve2 versionEdit
	nfn := vs.getNextFileNum()
	require.Equal(t, fileNum(2), nfn)
	ve1.newFiles = append(ve1.newFiles, newFileEntry{
		meta: &fileMetadata{
			fileNum: 2,
		},
	})

	mu.Lock()
	vs.logLock()
	require.NoError(t, vs.logAndApply(&ve1, dir))
	mu.Unlock()

	v := vs.currentVersion()
	require.Equal(t, 1, len(v.files))
	require.Equal(t, int32(1), v.refcnt)
	f, ok := v.files[2]
	require.True(t, ok)
	require.Equal(t, int32(1), f.refs)
	require.Equal(t, ve1.nextFileNum, vs.nextFileNum)

	nfn = vs.getNextFileNum()
	require.Equal(t, fileNum(3), nfn)
	ve2.newFiles = append(ve2.newFiles, newFileEntry{
		meta: &fileMetadata{
			fileNum: 3,
		},
	})
	ve2.deletedFiles = make(map[deletedFileEntry]*fileMetadata)
	ve2.deletedFiles[deletedFileEntry{fileNum: 2}] = &fileMetadata{fileNum: 2}
	mu.Lock()
	vs.logLock()
	require.NoError(t, vs.logAndApply(&ve2, dir))
	mu.Unlock()

	v = vs.currentVersion()
	require.Equal(t, 1, len(v.files))
	require.Equal(t, int32(1), v.refcnt)
	_, ok2 := v.files[2]
	f3, ok3 := v.files[3]
	require.False(t, ok2)
	require.True(t, ok3)
	require.Equal(t, int32(1), f3.refs)
	require.Equal(t, ve2.nextFileNum, vs.nextFileNum)
	require.Equal(t, 1, len(vs.obsoleteTables))
	require.Equal(t, fileNum(2), vs.obsoleteTables[0].fileNum)
	require.Equal(t, fileNum(1), vs.manifestFileNum)
}

func TestSwitchManifestFile(t *testing.T) {
	var vs versionSet
	var mu sync.Mutex
	dirname := "db-dir"
	fs := vfs.NewMem()
	defer vfs.ReportLeakedFD(fs, t)
	opt := &Options{MaxManifestFileSize: 1, FS: fs}
	require.NoError(t, fs.MkdirAll(dirname, 0700))
	dir, err := fs.OpenDir(dirname)
	require.NoError(t, err)
	defer func() {
		require.NoError(t, dir.Close())
	}()
	require.NoError(t, vs.create(dirname, opt, dir, &mu))
	require.Equal(t, fileNum(1), vs.manifestFileNum)

	var ve1 versionEdit
	fn := vs.getNextFileNum()
	require.Equal(t, fileNum(2), fn)
	ve1.nextFileNum = fn + 1
	ve1.newFiles = append(ve1.newFiles, newFileEntry{
		meta: &fileMetadata{
			fileNum: fn,
		},
	})

	mu.Lock()
	vs.logLock()
	require.NoError(t, vs.logAndApply(&ve1, dir))
	mu.Unlock()

	require.Equal(t, fileNum(3), vs.manifestFileNum)
	require.Equal(t, 1, len(vs.obsoleteManifests))
	require.NoError(t, vs.close())
}

func TestLoadManifest(t *testing.T) {
	fs := vfs.NewMem()
	defer vfs.ReportLeakedFD(fs, t)
	testVersionSetCanBeApplied(t, fs)
	var vs versionSet
	defer func() {
		require.NoError(t, vs.close())
	}()
	var mu sync.Mutex
	dirname := "db-dir"
	opt := &Options{MaxManifestFileSize: MaxManifestFileSize, FS: fs}
	require.NoError(t, vs.load(dirname, opt, &mu))
	v := vs.currentVersion()
	require.Equal(t, 1, len(v.files))
	require.Equal(t, int32(1), v.refcnt)
	_, ok2 := v.files[2]
	f3, ok3 := v.files[3]
	require.False(t, ok2)
	require.True(t, ok3)
	require.Equal(t, int32(1), f3.refs)
	require.Equal(t, fileNum(4), vs.nextFileNum)
	require.Equal(t, 0, len(vs.obsoleteTables))
	require.Equal(t, fileNum(1), vs.manifestFileNum)
	require.Nil(t, vs.manifest)
}
````

## File: internal/tan/version_set.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"bytes"
	"io"
	"sync"

	"github.com/cockroachdb/errors"
	"github.com/lni/vfs"
)

// versionSet manages a collection of immutable versions, and manages the
// creation of a new version from the most recent version. A new version is
// created from an existing version by applying a version edit which is just
// like it sounds: a delta from the previous version. Version edits are logged
// to the MANIFEST file, which is replayed at startup.
type versionSet struct {
	maxManifestFileSize int64
	// Immutable fields.
	dirname string
	// Set to DB.mu.
	mu *sync.Mutex
	fs vfs.FS
	// Mutable fields.
	versions versionList
	// A pointer to versionSet.addObsoleteLocked. Avoids allocating a new closure
	// on the creation of every version.
	obsoleteFn        func(obsolete []*fileMetadata)
	obsoleteTables    []*fileMetadata
	obsoleteManifests []fileNum
	// Zombie tables which have been removed from the current version but are
	// still referenced by an inuse iterator.
	zombieTables map[fileNum]uint64 // filenum -> size
	// The next file number. A single counter is used to assign file numbers
	// for the WAL, MANIFEST, sstable, and OPTIONS files.
	nextFileNum fileNum
	// The current manifest file number.
	manifestFileNum fileNum
	manifestFile    vfs.File
	manifest        *writer
	writing         bool
	writerCond      sync.Cond
}

func (vs *versionSet) init(dirname string, opts *Options, mu *sync.Mutex) {
	vs.maxManifestFileSize = opts.MaxManifestFileSize
	vs.dirname = dirname
	vs.mu = mu
	vs.writerCond.L = mu
	vs.fs = opts.FS
	vs.versions.init(mu)
	vs.obsoleteFn = vs.addObsoleteLocked
	vs.zombieTables = make(map[fileNum]uint64)
	vs.nextFileNum = 1
}

// create creates a version set for a fresh DB.
func (vs *versionSet) create(dirname string, opt *Options, dir vfs.File,
	mu *sync.Mutex) error {
	vs.init(dirname, opt, mu)
	newVersion := &version{}
	vs.append(newVersion)

	// Note that a "snapshot" version edit is written to the manifest when it is
	// created.
	vs.manifestFileNum = vs.getNextFileNum()
	if err := vs.createManifest(vs.dirname, vs.manifestFileNum, vs.nextFileNum); err != nil {
		return err
	}
	if err := vs.manifest.flush(); err != nil {
		return err
	}
	if err := vs.manifestFile.Sync(); err != nil {
		return err
	}
	if err := setCurrentFile(vs.dirname, vs.fs, vs.manifestFileNum); err != nil {
		return err
	}
	return dir.Sync()
}

// load loads the version set from the manifest file.
func (vs *versionSet) load(dirname string,
	opt *Options, mu *sync.Mutex) (err error) {
	vs.init(dirname, opt, mu)
	// Read the CURRENT file to find the current manifest file.
	current, err := vs.fs.Open(makeFilename(vs.fs, dirname, fileTypeCurrent, 0))
	if err != nil {
		return errors.Wrapf(err, "could not open CURRENT file for DB %q", dirname)
	}
	defer func() {
		err = firstError(err, current.Close())
	}()
	stat, err := current.Stat()
	if err != nil {
		return err
	}
	n := stat.Size()
	if n == 0 {
		return errors.Errorf("CURRENT file for DB %q is empty", dirname)
	}
	if n > 4096 {
		return errors.Errorf("CURRENT file for DB %q is too large", dirname)
	}
	r := newReader(current, fileNum(0))
	cr, err := r.next()
	if err != nil {
		return err
	}
	var buf bytes.Buffer
	if _, err = io.Copy(&buf, cr); err != nil {
		return err
	}
	b := buf.Bytes()
	if b[len(b)-1] != '\n' {
		return errors.Errorf("CURRENT file for DB %q is malformed", dirname)
	}
	b = bytes.TrimSpace(b)

	var ok bool
	if _, vs.manifestFileNum, ok = parseFilename(vs.fs, string(b)); !ok {
		return errors.Errorf("MANIFEST name %q is malformed", errors.Safe(b))
	}

	// Read the versionEdits in the manifest file.
	var bve bulkVersionEdit
	bve.addedByFileNum = make(map[fileNum]*fileMetadata)
	manifest, err := vs.fs.Open(vs.fs.PathJoin(dirname, string(b)))
	if err != nil {
		return errors.Wrapf(err, "could not open manifest file %q for DB %q",
			errors.Safe(b), dirname)
	}
	defer func() {
		err = firstError(err, manifest.Close())
	}()
	rr := newReader(manifest, 0 /* logNum */)
	for {
		r, err := rr.next()
		if err == io.EOF || IsInvalidRecord(err) {
			// FIXME:
			// not to tolerate actually corrupted manifest
			break
		}
		if err != nil {
			return errors.Wrapf(err, "error when loading manifest file %q",
				errors.Safe(b))
		}
		var ve versionEdit
		err = ve.decode(r)
		if err != nil {
			// Break instead of returning an error if the record is corrupted
			// or invalid.
			if err == io.EOF || IsInvalidRecord(err) {
				break
			}
			return err
		}
		if err := bve.accumulate(&ve); err != nil {
			return err
		}
		if ve.nextFileNum != 0 {
			vs.nextFileNum = ve.nextFileNum
		}
	}

	newVersion, _, err := bve.apply(nil)
	if err != nil {
		return err
	}
	vs.append(newVersion)
	return nil
}

func (vs *versionSet) close() error {
	if vs.manifestFile != nil {
		if err := vs.manifestFile.Close(); err != nil {
			return err
		}
	}
	return nil
}

// logLock locks the manifest for writing. The lock must be released by either
// a call to logUnlock or logAndApply.
//
// DB.mu must be held when calling this method, but the mutex may be dropped and
// re-acquired during the course of this method.
func (vs *versionSet) logLock() {
	// Wait for any existing writing to the manifest to complete, then mark the
	// manifest as busy.
	for vs.writing {
		vs.writerCond.Wait()
	}
	vs.writing = true
}

// logUnlock releases the lock for manifest writing.
//
// DB.mu must be held when calling this method.
func (vs *versionSet) logUnlock() {
	if !vs.writing {
		panic("MANIFEST not locked for writing")
	}
	vs.writing = false
	vs.writerCond.Signal()
}

// logAndApply logs the version edit to the manifest, applies the version edit
// to the current version, and installs the new version.
//
// DB.mu must be held when calling this method and will be released temporarily
// while performing file I/O. Requires that the manifest is locked for writing
// (see logLock). Will unconditionally release the manifest lock (via
// logUnlock) even if an error occurs.
//
// inProgressCompactions is called while DB.mu is held, to get the list of
// in-progress compactions.
func (vs *versionSet) logAndApply(
	ve *versionEdit,
	dir vfs.File,
) error {
	if !vs.writing {
		panic("MANIFEST not locked for writing")
	}
	defer vs.logUnlock()

	// This is the next manifest filenum, but if the current file is too big we
	// will write this ve to the next file which means what ve encodes is the
	// current filenum and not the next one.
	//
	// TODO(sbhola): figure out why this is correct and update comment.
	ve.nextFileNum = vs.nextFileNum

	currentVersion := vs.currentVersion()
	var newVersion *version

	// Generate a new manifest if we don't currently have one, or the current one
	// is too large.
	var newManifestFileNum fileNum
	if vs.manifest == nil || vs.manifest.size() >= vs.maxManifestFileSize {
		newManifestFileNum = vs.getNextFileNum()
	}

	// Grab certain values before releasing vs.mu, in case createManifest() needs
	// to be called.
	nextFileNum := vs.nextFileNum

	var zombies map[fileNum]uint64
	if err := func() error {
		var bve bulkVersionEdit
		if err := bve.accumulate(ve); err != nil {
			return err
		}

		var err error
		newVersion, zombies, err = bve.apply(currentVersion)
		if err != nil {
			return err
		}

		if newManifestFileNum != 0 {
			if err := vs.createManifest(vs.dirname, newManifestFileNum, nextFileNum); err != nil {
				return err
			}
		}

		w, err := vs.manifest.next()
		if err != nil {
			return err
		}
		// NB: Any error from this point on is considered fatal as we don't now if
		// the MANIFEST write occurred or not. Trying to determine that is
		// fraught. Instead we rely on the standard recovery mechanism run when a
		// database is open. In particular, that mechanism generates a new MANIFEST
		// and ensures it is synced.
		if err := ve.encode(w); err != nil {
			return err
		}
		if err := vs.manifest.flush(); err != nil {
			return err
		}
		if err := vs.manifestFile.Sync(); err != nil {
			return err
		}
		if newManifestFileNum != 0 {
			if err := setCurrentFile(vs.dirname, vs.fs, newManifestFileNum); err != nil {
				return err
			}
			if err := dir.Sync(); err != nil {
				return err
			}
		}
		return nil
	}(); err != nil {
		return err
	}

	// Update the zombie tables set first, as installation of the new version
	// will unref the previous version which could result in addObsoleteLocked
	// being called.
	for fileNum, size := range zombies {
		vs.zombieTables[fileNum] = size
	}
	// Install the new version.
	vs.append(newVersion)
	if newManifestFileNum != 0 {
		if vs.manifestFileNum != 0 {
			vs.obsoleteManifests = append(vs.obsoleteManifests, vs.manifestFileNum)
		}
		vs.manifestFileNum = newManifestFileNum
	}

	return nil
}

// createManifest creates a manifest file that contains a snapshot of vs.
func (vs *versionSet) createManifest(
	dirname string, fileNum, nextFileNum fileNum,
) (err error) {
	var (
		filename     = makeFilename(vs.fs, dirname, fileTypeManifest, fileNum)
		manifestFile vfs.File
		manifest     *writer
	)
	defer func() {
		if manifest != nil {
			err = firstError(err, manifest.close())
		}
		if manifestFile != nil {
			err = firstError(err, manifestFile.Close())
		}
		if err != nil {
			err = firstError(err, vs.fs.Remove(filename))
		}
	}()
	manifestFile, err = vs.fs.Create(filename)
	if err != nil {
		return err
	}
	manifest = newWriter(manifestFile)

	snapshot := versionEdit{}
	cv := vs.currentVersion()
	for _, meta := range cv.files {
		snapshot.newFiles = append(snapshot.newFiles, newFileEntry{
			meta: meta,
		})
	}

	// When creating a version snapshot for an existing DB, this snapshot VersionEdit will be
	// immediately followed by another VersionEdit (being written in logAndApply()). That
	// VersionEdit always contains a LastSeqNum, so we don't need to include that in the snapshot.
	// But it does not necessarily include MinUnflushedLogNum, NextFileNum, so we initialize those
	// using the corresponding fields in the versionSet (which came from the latest preceding
	// VersionEdit that had those fields).
	snapshot.nextFileNum = nextFileNum

	w, err1 := manifest.next()
	if err1 != nil {
		return err1
	}
	if err := snapshot.encode(w); err != nil {
		return err
	}

	if vs.manifest != nil {
		if err := vs.manifest.close(); err != nil {
			return err
		}
		vs.manifest = nil
	}
	if vs.manifestFile != nil {
		if err := vs.manifestFile.Close(); err != nil {
			return err
		}
		vs.manifestFile = nil
	}

	vs.manifest, manifest = manifest, nil
	vs.manifestFile, manifestFile = manifestFile, nil
	return nil
}

func (vs *versionSet) getNextFileNum() fileNum {
	x := vs.nextFileNum
	vs.nextFileNum++
	return x
}

func (vs *versionSet) append(v *version) {
	if v.refs() != 0 {
		panic("version should be unreferenced")
	}
	if !vs.versions.empty() {
		vs.versions.back().unrefLocked()
	}
	v.deleted = vs.obsoleteFn
	v.ref()
	vs.versions.pushBack(v)
}

func (vs *versionSet) currentVersion() *version {
	return vs.versions.back()
}

func (vs *versionSet) addObsoleteLocked(obsolete []*fileMetadata) {
	for _, fileMeta := range obsolete {
		// Note that the obsolete tables are no longer zombie by the definition of
		// zombie, but we leave them in the zombie tables map until they are
		// deleted from disk.
		if _, ok := vs.zombieTables[fileMeta.fileNum]; !ok {
			panic("MANIFEST obsolete table not marked as zombie")
		}
	}
	vs.obsoleteTables = append(vs.obsoleteTables, obsolete...)
}
````

## File: internal/tan/version_test.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"sync"
	"testing"

	"github.com/stretchr/testify/require"
)

func TestVersionUnref(t *testing.T) {
	list := &versionList{}
	list.init(&sync.Mutex{})
	v := &version{deleted: func([]*fileMetadata) {}}
	v.ref()
	list.pushBack(v)
	v.unref()
	require.True(t, list.empty(), "expected version list to be empty")
}
````

## File: internal/tan/version.go
````go
// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use
// of this source code is governed by a BSD-style license that can be found in
// the LICENSE file.
//
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"bytes"
	"sync"
	"sync/atomic"

	"github.com/cockroachdb/errors"
	"github.com/lni/vfs"
)

// fileMetadata holds the metadata for an on-disk table.
type fileMetadata struct {
	// refs is the reference count for the file: incremented when a file is added
	// to a version and decremented when the version is unreferenced. The file is
	// obsolete when the reference count falls to zero.
	refs int32
	// fileNum is the file number.
	fileNum fileNum
}

// version is a collection of file metadata for on-disk tables at various
// levels. In-memory DBs are written to level-0 tables, and compactions
// migrate data from level N to level N+1. The tables map internal keys (which
// are a user key, a delete or set bit, and a sequence number) to user values.
//
// The tables at level 0 are sorted by largest sequence number. Due to file
// ingestion, there may be overlap in the ranges of sequence numbers contain in
// level 0 sstables. In particular, it is valid for one level 0 sstable to have
// the seqnum range [1,100] while an adjacent sstable has the seqnum range
// [50,50]. This occurs when the [50,50] table was ingested and given a global
// seqnum. The ingestion code will have ensured that the [50,50] sstable will
// not have any keys that overlap with the [1,100] in the seqnum range
// [1,49]. The range of internal keys [fileMetadata.smallest,
// fileMetadata.largest] in each level 0 table may overlap.
//
// The tables at any non-0 level are sorted by their internal key range and any
// two tables at the same non-0 level do not overlap.
//
// The internal key ranges of two tables at different levels X and Y may
// overlap, for any X != Y.
//
// Finally, for every internal key in a table at level X, there is no internal
// key in a higher level table that has both the same user key and a higher
// sequence number.
type version struct {
	refcnt int32

	files map[fileNum]*fileMetadata

	// The callback to invoke when the last reference to a version is
	// removed. Will be called with list.mu held.
	deleted func(obsolete []*fileMetadata)

	// The list the version is linked into.
	list *versionList

	// The next/prev link for the versionList doubly-linked list of versions.
	prev, next *version
}

func (v *version) clone() map[fileNum]*fileMetadata {
	fs := make(map[fileNum]*fileMetadata)
	for _, f := range v.files {
		atomic.AddInt32(&f.refs, 1)
		fs[f.fileNum] = f
	}
	return fs
}

// refs returns the number of references to the version.
func (v *version) refs() int32 {
	return atomic.LoadInt32(&v.refcnt)
}

// ref increments the version refcount.
func (v *version) ref() {
	atomic.AddInt32(&v.refcnt, 1)
}

// unref decrements the version refcount. If the last reference to the version
// was removed, the version is removed from the list of versions and the
// Deleted callback is invoked. Requires that the VersionList mutex is NOT
// locked.
func (v *version) unref() {
	if atomic.AddInt32(&v.refcnt, -1) == 0 {
		obsolete := v.unrefFiles()
		l := v.list
		l.mu.Lock()
		l.remove(v)
		v.deleted(obsolete)
		l.mu.Unlock()
	}
}

// unrefLocked decrements the version refcount. If the last reference to the
// version was removed, the version is removed from the list of versions and
// the Deleted callback is invoked. Requires that the VersionList mutex is
// already locked.
func (v *version) unrefLocked() {
	if atomic.AddInt32(&v.refcnt, -1) == 0 {
		v.list.remove(v)
		v.deleted(v.unrefFiles())
	}
}

func (v *version) unrefFiles() []*fileMetadata {
	var obsolete []*fileMetadata
	for _, f := range v.files {
		if atomic.AddInt32(&f.refs, -1) == 0 {
			obsolete = append(obsolete, f)
		}
	}
	return obsolete
}

// checkConsistency checks that all of the files listed in the version exist
func (v *version) checkConsistency(dirname string, fs vfs.FS) error {
	var buf bytes.Buffer
	var args []interface{}
	for fileNum := range v.files {
		path := makeFilename(fs, dirname, fileTypeLog, fileNum)
		if _, err := fs.Stat(path); err != nil {
			buf.WriteString("%s: %v\n")
			args = append(args, errors.Safe(fileNum), err)
			continue
		}
	}
	if buf.Len() == 0 {
		return nil
	}
	return errors.Errorf(buf.String(), args...)
}

// versionList holds a list of versions. The versions are ordered from oldest
// to newest.
type versionList struct {
	mu   *sync.Mutex
	root version
}

// init initializes the version list.
func (l *versionList) init(mu *sync.Mutex) {
	l.mu = mu
	l.root.next = &l.root
	l.root.prev = &l.root
}

// empty returns true if the list is empty, and false otherwise.
func (l *versionList) empty() bool {
	return l.root.next == &l.root
}

// back returns the newest version in the list. Note that this version is only
// valid if Empty() returns true.
func (l *versionList) back() *version {
	return l.root.prev
}

// pushBack adds a new version to the back of the list. This new version
// becomes the "newest" version in the list.
func (l *versionList) pushBack(v *version) {
	if v.list != nil || v.prev != nil || v.next != nil {
		panic("pebble: version list is inconsistent")
	}
	v.prev = l.root.prev
	v.prev.next = v
	v.next = &l.root
	v.next.prev = v
	v.list = l
}

// remove removes the specified version from the list.
func (l *versionList) remove(v *version) {
	if v == &l.root {
		panic("pebble: cannot remove version list root node")
	}
	if v.list != l {
		panic("pebble: version list is inconsistent")
	}
	v.prev.next = v.next
	v.next.prev = v.prev
	v.next = nil // avoid memory leaks
	v.prev = nil // avoid memory leaks
	v.list = nil // avoid memory leaks
}
````

## File: internal/tests/kvpb/kv.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: kv.proto

package kvpb

import (
	"fmt"
	"io"
)

type PBKV struct {
	Key string
	Val string
}

func (m *PBKV) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *PBKV) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	dAtA[i] = 0xa
	i++
	i = encodeVarintKv(dAtA, i, uint64(len(m.Key)))
	i += copy(dAtA[i:], m.Key)
	dAtA[i] = 0x12
	i++
	i = encodeVarintKv(dAtA, i, uint64(len(m.Val)))
	i += copy(dAtA[i:], m.Val)
	return i, nil
}

func encodeVarintKv(dAtA []byte, offset int, v uint64) int {
	for v >= 1<<7 {
		dAtA[offset] = uint8(v&0x7f | 0x80)
		v >>= 7
		offset++
	}
	dAtA[offset] = uint8(v)
	return offset + 1
}

func (m *PBKV) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	l = len(m.Key)
	n += 1 + l + sovKv(uint64(l))
	l = len(m.Val)
	n += 1 + l + sovKv(uint64(l))
	return n
}

func sovKv(x uint64) (n int) {
	for {
		n++
		x >>= 7
		if x == 0 {
			break
		}
	}
	return n
}

func (m *PBKV) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowKv
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: PBKV: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: PBKV: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Key", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowKv
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthKv
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Key = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Val", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowKv
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthKv
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Val = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipKv(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthKv
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}

func skipKv(dAtA []byte) (n int, err error) {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return 0, ErrIntOverflowKv
			}
			if iNdEx >= l {
				return 0, io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		wireType := int(wire & 0x7)
		switch wireType {
		case 0:
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowKv
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				iNdEx++
				if dAtA[iNdEx-1] < 0x80 {
					break
				}
			}
			return iNdEx, nil
		case 1:
			iNdEx += 8
			return iNdEx, nil
		case 2:
			var length int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowKv
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				length |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			iNdEx += length
			if length < 0 {
				return 0, ErrInvalidLengthKv
			}
			return iNdEx, nil
		case 3:
			for {
				var innerWire uint64
				var start int = iNdEx
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return 0, ErrIntOverflowKv
					}
					if iNdEx >= l {
						return 0, io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					innerWire |= (uint64(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				innerWireType := int(innerWire & 0x7)
				if innerWireType == 4 {
					break
				}
				next, err := skipKv(dAtA[start:])
				if err != nil {
					return 0, err
				}
				iNdEx = start + next
			}
			return iNdEx, nil
		case 4:
			return iNdEx, nil
		case 5:
			iNdEx += 4
			return iNdEx, nil
		default:
			return 0, fmt.Errorf("proto: illegal wireType %d", wireType)
		}
	}
	panic("unreachable")
}

func (m *PBKV) GetKey() string {
	if m != nil {
		return m.Key
	}
	return ""
}

func (m *PBKV) GetVal() string {
	if m != nil {
		return m.Val
	}
	return ""
}

var (
	ErrInvalidLengthKv = fmt.Errorf("proto: negative length found during unmarshaling")
	ErrIntOverflowKv   = fmt.Errorf("proto: integer overflow")
)
````

## File: internal/tests/concurrent.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tests

import (
	"encoding/binary"
	"io"
	"math/rand"
	"sync/atomic"
	"time"

	sm "github.com/lni/dragonboat/v4/statemachine"
)

// TestUpdate is a IStateMachine used for testing purposes.
type TestUpdate struct {
	val uint32
}

// Update updates the state machine.
func (c *TestUpdate) Update(e sm.Entry) (sm.Result, error) {
	atomic.StoreUint32(&c.val, 1)
	for i := 0; i < 5; i++ {
		time.Sleep(1 * time.Millisecond)
	}
	atomic.StoreUint32(&c.val, 0)
	return sm.Result{Value: 100}, nil
}

// Lookup queries the state machine.
func (c *TestUpdate) Lookup(query interface{}) (interface{}, error) {
	result := make([]byte, 4)
	v := atomic.LoadUint32(&c.val)
	binary.LittleEndian.PutUint32(result, v)
	return result, nil
}

// SaveSnapshot saves the snapshot.
func (c *TestUpdate) SaveSnapshot(w io.Writer,
	fc sm.ISnapshotFileCollection, stopc <-chan struct{}) error {
	panic("not implemented")
}

// RecoverFromSnapshot recovers the state machine from a snapshot.
func (c *TestUpdate) RecoverFromSnapshot(r io.Reader,
	files []sm.SnapshotFile, stopc <-chan struct{}) error {
	panic("not implemented")
}

// Close closes the state machine.
func (c *TestUpdate) Close() error {
	return nil
}

// GetHash returns the uint64 hash value representing the state of a state
// machine.
func (c *TestUpdate) GetHash() (uint64, error) {
	return 0, nil
}

// ConcurrentUpdate is a IConcurrentStateMachine used for testing purposes.
type ConcurrentUpdate struct {
	UpdateCount int
	val         uint32
}

// Update updates the state machine.
func (c *ConcurrentUpdate) Update(entries []sm.Entry) ([]sm.Entry, error) {
	if c.UpdateCount == 0 {
		c.UpdateCount = len(entries)
	}
	for i := 0; i < 40; i++ {
		atomic.AddUint32(&c.val, 1)
		time.Sleep(1 * time.Millisecond)
	}
	entries[0].Result = sm.Result{Value: 100}
	return entries, nil
}

// Lookup queries the state machine.
func (c *ConcurrentUpdate) Lookup(query interface{}) (interface{}, error) {
	st := time.Duration(rand.Uint64()%10) * time.Millisecond
	time.Sleep(st)
	result := make([]byte, 4)
	v := atomic.LoadUint32(&c.val)
	binary.LittleEndian.PutUint32(result, v)
	return result, nil
}

// PrepareSnapshot makes preparations for taking concurrent snapshot.
func (c *ConcurrentUpdate) PrepareSnapshot() (interface{}, error) {
	panic("not implemented")
}

// SaveSnapshot saves the snapshot.
func (c *ConcurrentUpdate) SaveSnapshot(ctx interface{},
	w io.Writer,
	fc sm.ISnapshotFileCollection, stopc <-chan struct{}) error {
	panic("not implemented")
}

// RecoverFromSnapshot recovers the state machine from a snapshot.
func (c *ConcurrentUpdate) RecoverFromSnapshot(r io.Reader,
	files []sm.SnapshotFile, stopc <-chan struct{}) error {
	panic("not implemented")
}

// Close closes the state machine.
func (c *ConcurrentUpdate) Close() error {
	return nil
}

// GetHash returns the uint64 hash value representing the state of a state
// machine.
func (c *ConcurrentUpdate) GetHash() (uint64, error) {
	return 0, nil
}

// TestSnapshot is a IConcurrentStateMachine used for testing purposes.
type TestSnapshot struct {
	val uint32
}

// Update updates the state machine.
func (c *TestSnapshot) Update(e sm.Entry) (sm.Result, error) {
	return sm.Result{Value: uint64(atomic.LoadUint32(&c.val))}, nil
}

// Lookup queries the state machine.
func (c *TestSnapshot) Lookup(query interface{}) (interface{}, error) {
	return nil, nil
}

// SaveSnapshot saves the snapshot.
func (c *TestSnapshot) SaveSnapshot(w io.Writer,
	fc sm.ISnapshotFileCollection, stopc <-chan struct{}) error {
	atomic.StoreUint32(&c.val, 0)
	for i := 0; i < 100; i++ {
		atomic.StoreUint32(&c.val, 1)
		time.Sleep(time.Millisecond)
	}
	atomic.StoreUint32(&c.val, 0)
	data := make([]byte, 4)
	_, err := w.Write(data)
	if err != nil {
		panic(err)
	}
	return nil
}

// RecoverFromSnapshot recovers the state machine from a snapshot.
func (c *TestSnapshot) RecoverFromSnapshot(r io.Reader,
	files []sm.SnapshotFile, stopc <-chan struct{}) error {
	panic("not implemented")
}

// Close closes the state machine.
func (c *TestSnapshot) Close() error {
	return nil
}

// GetHash returns the uint64 hash value representing the state of a state
// machine.
func (c *TestSnapshot) GetHash() (uint64, error) {
	return 0, nil
}

// ConcurrentSnapshot is a IConcurrentStateMachine used for testing purposes.
type ConcurrentSnapshot struct {
	val uint32
}

// Update updates the state machine.
func (c *ConcurrentSnapshot) Update(entries []sm.Entry) ([]sm.Entry, error) {
	entries[0].Result = sm.Result{Value: uint64(atomic.LoadUint32(&c.val))}
	return entries, nil
}

// Lookup queries the state machine.
func (c *ConcurrentSnapshot) Lookup(query interface{}) (interface{}, error) {
	return nil, sm.ErrSnapshotStopped
}

// PrepareSnapshot makes preparations for taking concurrent snapshot.
func (c *ConcurrentSnapshot) PrepareSnapshot() (interface{}, error) {
	return nil, nil
}

// SaveSnapshot saves the snapshot.
func (c *ConcurrentSnapshot) SaveSnapshot(ctx interface{},
	w io.Writer,
	fc sm.ISnapshotFileCollection, stopc <-chan struct{}) error {
	atomic.StoreUint32(&c.val, 0)
	for i := 0; i < 100; i++ {
		atomic.StoreUint32(&c.val, 1)
		time.Sleep(time.Millisecond)
	}
	atomic.StoreUint32(&c.val, 0)
	data := make([]byte, 4)
	_, err := w.Write(data)
	if err != nil {
		panic(err)
	}
	return nil
}

// RecoverFromSnapshot recovers the state machine from a snapshot.
func (c *ConcurrentSnapshot) RecoverFromSnapshot(r io.Reader,
	files []sm.SnapshotFile, stopc <-chan struct{}) error {
	panic("not implemented")
}

// Close closes the state machine.
func (c *ConcurrentSnapshot) Close() error {
	return nil
}

// GetHash returns the uint64 hash value representing the state of a state
// machine.
func (c *ConcurrentSnapshot) GetHash() (uint64, error) {
	return 0, nil
}
````

## File: internal/tests/concurrentkv.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tests

import (
	"crypto/md5"
	"encoding/binary"
	"encoding/json"
	"fmt"
	"io"
	"os"
	"sync"
	"sync/atomic"
	"time"
	"unsafe"

	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/tests/kvpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
)

type kvdata struct {
	kvs   sync.Map
	count uint64
	junk  []byte
}

// KVJson is an util struct for serializing and deserializing data.
type KVJson struct {
	KVStore map[string]string `json:"KVStore"`
	Count   uint64            `json:"Count"`
	Junk    []byte            `json:"Junk"`
}

// ConcurrentKVTest is a in memory key-value store struct used for testing
// purposes. Note that both key/value are suppose to be valid utf-8 strings.
type ConcurrentKVTest struct {
	ShardID          uint64
	ReplicaID        uint64
	kvdata           unsafe.Pointer
	externalFileTest bool
	closed           uint32
}

// NewConcurrentKVTest creates and return a new KVTest object.
func NewConcurrentKVTest(shardID uint64, replicaID uint64) sm.IConcurrentStateMachine {
	s := &ConcurrentKVTest{
		ShardID:   shardID,
		ReplicaID: replicaID,
	}
	kvdata := &kvdata{junk: make([]byte, 3*1024)}
	// write some junk data consistent across the shard
	for i := 0; i < len(kvdata.junk); i++ {
		kvdata.junk[i] = 2
	}
	s.kvdata = unsafe.Pointer(kvdata)

	v := os.Getenv("EXTERNALFILETEST")
	s.externalFileTest = len(v) > 0
	fmt.Printf("junk data inserted, external file test %t\n", s.externalFileTest)
	return s
}

// Lookup performances local looks up for the sepcified data.
func (s *ConcurrentKVTest) Lookup(key interface{}) (interface{}, error) {
	kvdata := (*kvdata)(atomic.LoadPointer(&(s.kvdata)))
	query := string(key.([]byte))
	v, ok := kvdata.kvs.Load(query)
	if ok {
		return []byte(v.(string)), nil
	}
	return []byte(""), nil
}

// Update updates the object using the specified committed raft entry.
func (s *ConcurrentKVTest) Update(ents []sm.Entry) ([]sm.Entry, error) {
	for i := 0; i < len(ents); i++ {
		dataKv := &kvpb.PBKV{}
		err := dataKv.Unmarshal(ents[i].Cmd)
		if err != nil {
			panic(err)
		}
		key := dataKv.GetKey()
		val := dataKv.GetVal()
		kvdata := (*kvdata)(atomic.LoadPointer(&(s.kvdata)))
		kvdata.kvs.Store(key, val)
		ents[i].Result = sm.Result{Value: uint64(len(ents[i].Cmd))}
	}
	return ents, nil
}

// PrepareSnapshot makes preparations for taking concurrent snapshot.
func (s *ConcurrentKVTest) PrepareSnapshot() (interface{}, error) {
	p := (*kvdata)(atomic.LoadPointer(&(s.kvdata)))
	data := &kvdata{
		count: p.count,
	}
	data.junk = append(data.junk, p.junk...)
	p.kvs.Range(func(k, v interface{}) bool {
		key := k.(string)
		val := v.(string)
		data.kvs.Store(key, val)
		return true
	})
	return data, nil
}

// SaveSnapshot saves the current object state into a snapshot using the
// specified io.Writer object.
func (s *ConcurrentKVTest) SaveSnapshot(ctx interface{},
	w io.Writer,
	fileCollection sm.ISnapshotFileCollection,
	done <-chan struct{}) error {
	if s.isClosed() {
		panic("save snapshot called after Close()")
	}
	delay := getLargeRandomDelay(s.ShardID)
	fmt.Printf("random delay %d ms\n", delay)
	for delay > 0 {
		delay -= 10
		time.Sleep(10 * time.Millisecond)
		select {
		case <-done:
			return sm.ErrSnapshotStopped
		default:
		}
	}
	ctxdata := ctx.(*kvdata)
	jsondata := &KVJson{
		KVStore: make(map[string]string),
		Count:   ctxdata.count,
		Junk:    ctxdata.junk,
	}
	ctxdata.kvs.Range(func(k, v interface{}) bool {
		key := k.(string)
		val := v.(string)
		jsondata.KVStore[key] = val
		return true
	})
	data, err := json.Marshal(jsondata)
	if err != nil {
		panic(err)
	}
	n, err := w.Write(data)
	if err != nil {
		return err
	}
	if n != len(data) {
		panic("didn't write the whole data buf")
	}
	return nil
}

// RecoverFromSnapshot recovers the state using the provided snapshot.
func (s *ConcurrentKVTest) RecoverFromSnapshot(r io.Reader,
	files []sm.SnapshotFile,
	done <-chan struct{}) error {
	if s.isClosed() {
		panic("recover from snapshot called after Close()")
	}
	delay := getLargeRandomDelay(s.ShardID)
	fmt.Printf("random delay %d ms\n", delay)
	for delay > 0 {
		delay -= 10
		time.Sleep(10 * time.Millisecond)
		select {
		case <-done:
			return sm.ErrSnapshotStopped
		default:
		}
	}
	kvdata := &kvdata{}
	jsondata := &KVJson{}
	data, err := fileutil.ReadAll(r)
	if err != nil {
		return err
	}
	if err := json.Unmarshal(data, jsondata); err != nil {
		return err
	}
	for k, v := range jsondata.KVStore {
		kvdata.kvs.Store(k, v)
	}
	kvdata.count = jsondata.Count
	kvdata.junk = jsondata.Junk
	atomic.StorePointer(&(s.kvdata), unsafe.Pointer(kvdata))
	return nil
}

// Close closes the IStateMachine instance
func (s *ConcurrentKVTest) Close() error {
	atomic.StoreUint32(&s.closed, 1)
	return nil
}

// GetHash returns a uint64 representing the current object state.
func (s *ConcurrentKVTest) GetHash() (uint64, error) {
	p := (*kvdata)(atomic.LoadPointer(&(s.kvdata)))
	jsondata := &KVJson{
		KVStore: make(map[string]string),
		Count:   p.count,
	}
	p.kvs.Range(func(k, v interface{}) bool {
		key := k.(string)
		val := v.(string)
		jsondata.KVStore[key] = val
		return true
	})
	data, err := json.Marshal(jsondata)
	if err != nil {
		panic(err)
	}
	hash := md5.New()
	if _, err = hash.Write(data); err != nil {
		panic(err)
	}
	md5sum := hash.Sum(nil)
	return binary.LittleEndian.Uint64(md5sum[:8]), nil
}

func (s *ConcurrentKVTest) isClosed() bool {
	return atomic.LoadUint32(&s.closed) == 1
}
````

## File: internal/tests/error.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tests

import (
	"github.com/cockroachdb/errors/errbase"
)

type stackTracer interface {
	StackTrace() errbase.StackTrace
}

// HasStack returns a boolean value indicating whether the specified error has
// stacktrace attached to it.
func HasStack(err error) bool {
	if err == nil {
		return false
	}
	if _, ok := err.(stackTracer); ok {
		return true
	}
	return HasStack(errbase.UnwrapOnce(err))
}
````

## File: internal/tests/fakedisk_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tests

import (
	"bytes"
	"testing"

	sm "github.com/lni/dragonboat/v4/statemachine"
	"github.com/stretchr/testify/require"
)

func TestSimDiskSMCanBeOpened(t *testing.T) {
	sm := NewSimDiskSM(102)
	v, err := sm.Open(nil)
	require.NoError(t, err, "open failed")
	require.Equal(t, uint64(102), v, "unexpected return value")
}

func TestSimDiskSMCanBeUpdatedAndQueried(t *testing.T) {
	m := NewSimDiskSM(1)
	ents := []sm.Entry{{Index: 2}, {Index: 3}, {Index: 4}}
	_, _ = m.Update(ents)
	require.Equal(t, uint64(4), m.applied, "sm not updated")
	v, err := m.Lookup(nil)
	require.NoError(t, err, "lookup failed")
	require.Equal(t, uint64(4), v.(uint64), "unexpected result")
}

func TestSimDiskSnapshotWorks(t *testing.T) {
	m := NewSimDiskSM(1)
	ents := []sm.Entry{{Index: 2}, {Index: 3}, {Index: 4}}
	_, _ = m.Update(ents)
	require.Equal(t, uint64(4), m.applied, "sm not updated")
	ctx, err := m.PrepareSnapshot()
	require.NoError(t, err, "prepare snapshot failed")
	buf := bytes.NewBuffer(make([]byte, 0, 128))
	err = m.SaveSnapshot(ctx, buf, nil)
	require.NoError(t, err, "save snapshot failed")
	reader := bytes.NewBuffer(buf.Bytes())
	m2 := NewSimDiskSM(100)
	v, err := m2.Lookup(nil)
	require.NoError(t, err, "lookup failed")
	require.Equal(t, uint64(100), v.(uint64), "unexpected result")
	err = m2.RecoverFromSnapshot(reader, nil)
	require.NoError(t, err, "recover from snapshot failed")
	v, err = m2.Lookup(nil)
	require.NoError(t, err, "lookup failed")
	require.Equal(t, uint64(4), v.(uint64), "unexpected result")
}
````

## File: internal/tests/fakedisk.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tests

import (
	"encoding/binary"
	"fmt"
	"io"
	"sync/atomic"
	"time"

	sm "github.com/lni/dragonboat/v4/statemachine"
)

// FakeDiskSM is a test state machine.
type FakeDiskSM struct {
	SlowOpen       uint32
	initialApplied uint64
	count          uint64
	aborted        bool
	recovered      bool
}

// NewFakeDiskSM creates a new fake disk sm for testing purpose.
func NewFakeDiskSM(initialApplied uint64) *FakeDiskSM {
	return &FakeDiskSM{initialApplied: initialApplied}
}

// Open opens the state machine.
func (f *FakeDiskSM) Open(stopc <-chan struct{}) (uint64, error) {
	for atomic.LoadUint32(&f.SlowOpen) > 0 {
		time.Sleep(10 * time.Millisecond)
	}
	return f.initialApplied, nil
}

// SetAborted ...
func (f *FakeDiskSM) SetAborted() {
	f.aborted = true
}

// ClearAborted ...
func (f *FakeDiskSM) ClearAborted() {
	f.aborted = false
}

// Aborted ...
func (f *FakeDiskSM) Aborted() bool {
	return f.aborted
}

// Recovered ...
func (f *FakeDiskSM) Recovered() bool {
	return f.recovered
}

// Update updates the state machine.
func (f *FakeDiskSM) Update(ents []sm.Entry) ([]sm.Entry, error) {
	for _, e := range ents {
		if e.Index <= f.initialApplied {
			panic("already applied index received again")
		} else {
			f.count = f.count + 1
			e.Result = sm.Result{Value: f.count}
		}
	}
	return ents, nil
}

// Lookup queries the state machine.
func (f *FakeDiskSM) Lookup(query interface{}) (interface{}, error) {
	result := make([]byte, 8)
	binary.LittleEndian.PutUint64(result, f.count)
	return result, nil
}

// PrepareSnapshot prepares snapshotting.
func (f *FakeDiskSM) PrepareSnapshot() (interface{}, error) {
	pit := &FakeDiskSM{initialApplied: f.initialApplied, count: f.count}
	return pit, nil
}

// Sync synchronize all in-core state.
func (f *FakeDiskSM) Sync() error {
	return nil
}

// SaveSnapshot saves the state to a snapshot.
func (f *FakeDiskSM) SaveSnapshot(ctx interface{},
	w io.Writer, stopc <-chan struct{}) error {
	if !f.aborted {
		f.aborted = true
		return sm.ErrSnapshotAborted
	}
	pit := ctx.(*FakeDiskSM)
	fmt.Printf("saving initial %d, count %d\n", pit.initialApplied, pit.count)
	v := make([]byte, 8)
	binary.LittleEndian.PutUint64(v, pit.initialApplied)
	if _, err := w.Write(v); err != nil {
		return err
	}
	binary.LittleEndian.PutUint64(v, pit.count)
	if _, err := w.Write(v); err != nil {
		return err
	}
	return nil
}

// RecoverFromSnapshot recovers the state of the state machine from a snapshot.
func (f *FakeDiskSM) RecoverFromSnapshot(r io.Reader,
	stopc <-chan struct{}) error {
	f.recovered = true
	v := make([]byte, 8)
	if _, err := io.ReadFull(r, v); err != nil {
		return err
	}
	f.initialApplied = binary.LittleEndian.Uint64(v)
	if _, err := io.ReadFull(r, v); err != nil {
		return err
	}
	f.count = binary.LittleEndian.Uint64(v)
	fmt.Printf("loading initial %d, count %d\n", f.initialApplied, f.count)
	return nil
}

// Close closes the state machine.
func (f *FakeDiskSM) Close() error {
	return nil
}

// GetHash returns the hash of the state.
func (f *FakeDiskSM) GetHash() (uint64, error) {
	return 0, nil
}

// SimDiskSM is a fake disk based state machine used for testing purposes
type SimDiskSM struct {
	applied   uint64
	recovered uint64
}

// NewSimDiskSM ...
func NewSimDiskSM(applied uint64) *SimDiskSM {
	return &SimDiskSM{applied: applied}
}

// GetApplied ...
func (s *SimDiskSM) GetApplied() uint64 {
	return s.applied
}

// GetRecovered ...
func (s *SimDiskSM) GetRecovered() uint64 {
	return atomic.LoadUint64(&s.recovered)
}

// Open ...
func (s *SimDiskSM) Open(stopc <-chan struct{}) (uint64, error) {
	return s.applied, nil
}

// Update ...
func (s *SimDiskSM) Update(ents []sm.Entry) ([]sm.Entry, error) {
	for _, e := range ents {
		s.applied = e.Index
		e.Result = sm.Result{Value: e.Index}
	}
	return ents, nil
}

// Lookup ...
func (s *SimDiskSM) Lookup(query interface{}) (interface{}, error) {
	result := s.applied
	return result, nil
}

// PrepareSnapshot ...
func (s *SimDiskSM) PrepareSnapshot() (interface{}, error) {
	v := &SimDiskSM{applied: s.applied}
	return v, nil
}

// SaveSnapshot ...
func (s *SimDiskSM) SaveSnapshot(ctx interface{},
	w io.Writer, stopc <-chan struct{}) error {
	pit := ctx.(*SimDiskSM)
	v := make([]byte, 8)
	binary.LittleEndian.PutUint64(v, pit.applied)
	_, err := w.Write(v)
	return err
}

// RecoverFromSnapshot ...
func (s *SimDiskSM) RecoverFromSnapshot(r io.Reader,
	stopc <-chan struct{}) error {
	atomic.AddUint64(&s.recovered, 1)
	v := make([]byte, 8)
	if _, err := io.ReadFull(r, v); err != nil {
		return err
	}
	s.applied = binary.LittleEndian.Uint64(v)
	return nil
}

// Sync ...
func (s *SimDiskSM) Sync() error {
	return nil
}

// Close ...
func (s *SimDiskSM) Close() error {
	return nil
}
````

## File: internal/tests/kvtest.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

/*
Package tests contains various helper functions and modules used in tests.

This package is internally used by Dragonboat, applications are not expected to
import this package.
*/
package tests

import (
	"crypto/md5"
	"encoding/binary"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"math/rand"
	"os"
	"path/filepath"
	"sync"
	"time"

	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/tests/kvpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
	"github.com/lni/goutils/random"
)

// random delays
func generateRandomDelay() {
	v := rand.Uint64()
	if v%10000 == 0 {
		time.Sleep(300 * time.Millisecond)
	} else if v%1000 == 0 {
		time.Sleep(100 * time.Millisecond)
	} else if v%100 == 0 {
		time.Sleep(10 * time.Millisecond)
	} else if v%20 == 0 {
		time.Sleep(2 * time.Millisecond)
	}
}

func getLargeRandomDelay(shardID uint64) uint64 {
	// in IO error injection test, we don't want such delays
	ioei := os.Getenv("IOEI")
	if len(ioei) > 0 {
		return 0
	}
	pcs := fmt.Sprintf("IOEI-%d", shardID)
	ioei = os.Getenv(pcs)
	if len(ioei) > 0 {
		return 0
	}
	v := rand.Uint64() % 100
	if v == 0 {
		return 30 * 1000
	}
	if v < 10 {
		return 1 * 1000
	}
	if v < 30 {
		return 500
	}
	if v < 50 {
		return 100
	}
	return 50
}

// KVTest is a in memory key-value store struct used for testing purposes.
// Note that both key/value are suppose to be valid utf-8 strings.
type KVTest struct {
	ShardID          uint64            `json:"-"`
	ReplicaID        uint64            `json:"-"`
	KVStore          map[string]string `json:"KVStore"`
	Count            uint64            `json:"Count"`
	Junk             []byte            `json:"Junk"`
	closed           bool
	aborted          bool
	externalFileTest bool
	noLargeDelay     bool
	pbkvPool         *sync.Pool
}

// NewKVTest creates and return a new KVTest object.
func NewKVTest(shardID uint64, replicaID uint64) sm.IStateMachine {
	s := &KVTest{
		KVStore:   make(map[string]string),
		ShardID:   shardID,
		ReplicaID: replicaID,
		Junk:      make([]byte, 3*1024),
	}
	v := os.Getenv("EXTERNALFILETEST")
	s.externalFileTest = len(v) > 0
	// write some junk data consistent across the shard
	for i := 0; i < len(s.Junk); i++ {
		s.Junk[i] = 2
	}
	s.pbkvPool = &sync.Pool{
		New: func() interface{} {
			return &kvpb.PBKV{}
		},
	}

	return s
}

// DisableLargeDelay disables random large delays.
func (s *KVTest) DisableLargeDelay() {
	s.noLargeDelay = true
}

// Lookup performances local looks up for the sepcified data.
func (s *KVTest) Lookup(key interface{}) (interface{}, error) {
	if s.closed {
		panic("lookup called after Close()")
	}

	if s.aborted {
		panic("Lookup() called after abort set to true")
	}
	v, ok := s.KVStore[string(key.([]byte))]
	generateRandomDelay()
	if ok {
		return []byte(v), nil
	}

	return []byte(""), nil
}

// Update updates the object using the specified committed raft entry.
func (s *KVTest) Update(e sm.Entry) (sm.Result, error) {
	s.Count++
	if s.aborted {
		panic("update() called after abort set to true")
	}
	if s.closed {
		panic("update called after Close()")
	}
	generateRandomDelay()
	dataKv := s.pbkvPool.Get().(*kvpb.PBKV)
	err := dataKv.Unmarshal(e.Cmd)
	if err != nil {
		panic(err)
	}
	s.updateStore(dataKv.GetKey(), dataKv.GetVal())
	s.pbkvPool.Put(dataKv)
	return sm.Result{Value: uint64(len(e.Cmd))}, nil
}

func (s *KVTest) saveExternalFile(fileCollection sm.ISnapshotFileCollection) {
	dir, err := os.Getwd()
	if err != nil {
		panic(err)
	}
	rn := random.LockGuardedRand.Uint64()
	fn := fmt.Sprintf("external-%d-%d-%d-%d.data",
		s.ShardID, s.ReplicaID, s.Count, rn)
	fp := filepath.Join(dir, fn)
	f, err := os.Create(fp)
	if err != nil {
		panic(err)
	}
	content := fmt.Sprintf("external-test-data-%d", s.Count)
	_, err = f.Write([]byte(content))
	if err != nil {
		panic(err)
	}
	if err = f.Close(); err != nil {
		panic(err)
	}
	fmt.Printf("adding an external file, path %s", fp)
	fileCollection.AddFile(1, fp, []byte(content))
}

func checkExternalFile(files []sm.SnapshotFile, shardID uint64) {
	if len(files) != 1 {
		panic("snapshot external file missing")
	}
	fr := files[0]
	if fr.FileID != 1 {
		panic("FileID value not expected")
	}
	wcontent := string(fr.Metadata)
	content, err := fileutil.ReadFile(fr.Filepath)
	if err != nil {
		panic(err)
	}
	if string(content) != wcontent {
		panic(fmt.Sprintf("unexpected external file content got %s, want %s, fp %s",
			string(content), wcontent, fr.Filepath))
	}
	log.Printf("external file check done")
}

// SaveSnapshot saves the current object state into a snapshot using the
// specified io.Writer object.
func (s *KVTest) SaveSnapshot(w io.Writer,
	fileCollection sm.ISnapshotFileCollection,
	done <-chan struct{}) error {
	if s.closed {
		panic("save snapshot called after Close()")
	}
	if s.externalFileTest {
		s.saveExternalFile(fileCollection)
	}
	delay := getLargeRandomDelay(s.ShardID)
	if s.noLargeDelay {
		delay = 0
	}
	fmt.Printf("random delay %d ms\n", delay)
	for delay > 0 {
		delay -= 10
		time.Sleep(10 * time.Millisecond)
		select {
		case <-done:
			return sm.ErrSnapshotStopped
		default:
		}
	}
	data, err := json.Marshal(s)
	if err != nil {
		panic(err)
	}
	n, err := w.Write(data)
	if err != nil {
		return err
	}
	if n != len(data) {
		panic("didn't write the whole data buf")
	}
	return nil
}

// RecoverFromSnapshot recovers the state using the provided snapshot.
func (s *KVTest) RecoverFromSnapshot(r io.Reader,
	files []sm.SnapshotFile,
	done <-chan struct{}) error {
	if s.closed {
		panic("recover from snapshot called after Close()")
	}
	if s.externalFileTest {
		checkExternalFile(files, s.ShardID)
	}
	delay := getLargeRandomDelay(s.ShardID)
	if s.noLargeDelay {
		delay = 0
	}
	fmt.Printf("random delay %d ms\n", delay)
	for delay > 0 {
		delay -= 10
		time.Sleep(10 * time.Millisecond)
		select {
		case <-done:
			s.aborted = true
			return sm.ErrSnapshotStopped
		default:
		}
	}

	var store KVTest
	data, err := fileutil.ReadAll(r)
	if err != nil {
		return err
	}
	if err := json.Unmarshal(data, &store); err != nil {
		return err
	}
	store.aborted = false
	s.KVStore = store.KVStore
	s.Count = store.Count
	s.Junk = store.Junk
	return nil
}

// Close closes the IStateMachine instance
func (s *KVTest) Close() error {
	s.closed = true
	log.Printf("%d:%dKVStore has been closed", s.ShardID, s.ReplicaID)
	return nil
}

// GetHash returns a uint64 representing the current object state.
func (s *KVTest) GetHash() (uint64, error) {
	data, err := json.Marshal(s)
	if err != nil {
		panic(err)
	}

	hash := md5.New()
	if _, err = hash.Write(data); err != nil {
		panic(err)
	}
	md5sum := hash.Sum(nil)
	return binary.LittleEndian.Uint64(md5sum[:8]), nil
}

func (s *KVTest) updateStore(key string, value string) {
	s.KVStore[key] = value
}

// VerboseSnapshotSM ...
type VerboseSnapshotSM struct {
	sz uint64
}

// Update ...
func (v *VerboseSnapshotSM) Update(e sm.Entry) (sm.Result, error) {
	return sm.Result{}, nil
}

// Lookup ...
func (v *VerboseSnapshotSM) Lookup(q interface{}) (interface{}, error) {
	return v.sz, nil
}

// SaveSnapshot ...
func (v *VerboseSnapshotSM) SaveSnapshot(w io.Writer,
	collection sm.ISnapshotFileCollection, stopc <-chan struct{}) error {
	empty := make([]byte, 1024)
	for i := 0; i < 1024; i++ {
		if _, err := w.Write(empty); err != nil {
			return err
		}
	}
	return nil
}

// RecoverFromSnapshot ...
func (v *VerboseSnapshotSM) RecoverFromSnapshot(r io.Reader,
	collection []sm.SnapshotFile, stopc <-chan struct{}) error {
	data := make([]byte, 1024)
	total := uint64(0)
	defer func() {
		v.sz = total
	}()
	for {
		n, err := r.Read(data)
		if n > 0 {
			total += uint64(n)
		}
		if err != nil {
			if err == io.EOF {
				break
			}
			return err
		}
	}
	return nil
}

// Close ...
func (v *VerboseSnapshotSM) Close() error {
	return nil
}
````

## File: internal/tests/noop.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tests

import (
	"encoding/json"
	"io"
	"sync/atomic"
	"time"

	"github.com/lni/dragonboat/v4/internal/fileutil"
	sm "github.com/lni/dragonboat/v4/statemachine"
)

// NoOP is a IStateMachine struct used for testing purpose.
type NoOP struct {
	MillisecondToSleep uint64
	NoAlloc            bool
}

// SetSleepTime sets the sleep time of the state machine.
func (n *NoOP) SetSleepTime(v uint64) {
	atomic.StoreUint64(&n.MillisecondToSleep, v)
}

// Lookup locally looks up the data.
func (n *NoOP) Lookup(key interface{}) (interface{}, error) {
	return make([]byte, 1), nil
}

// NALookup locally looks up the data.
func (n *NoOP) NALookup(key []byte) ([]byte, error) {
	return key, nil
}

// Update updates the object.
func (n *NoOP) Update(e sm.Entry) (sm.Result, error) {
	sleep := atomic.LoadUint64(&n.MillisecondToSleep)
	if sleep > 0 {
		time.Sleep(time.Duration(sleep) * time.Millisecond)
	}
	if n.NoAlloc {
		return sm.Result{Value: uint64(len(e.Cmd))}, nil
	}
	v := make([]byte, len(e.Cmd))
	copy(v, e.Cmd)
	return sm.Result{Value: uint64(len(e.Cmd)), Data: v}, nil
}

// SaveSnapshot saves the state of the object to the provided io.Writer object.
func (n *NoOP) SaveSnapshot(w io.Writer,
	fileCollection sm.ISnapshotFileCollection,
	done <-chan struct{}) error {
	data, err := json.Marshal(n)
	if err != nil {
		panic(err)
	}
	_, err = w.Write(data)
	if err != nil {
		return err
	}
	return nil
}

// RecoverFromSnapshot recovers the object from the snapshot specified by the
// io.Reader object.
func (n *NoOP) RecoverFromSnapshot(r io.Reader,
	files []sm.SnapshotFile,
	done <-chan struct{}) error {
	var sn NoOP
	data, err := fileutil.ReadAll(r)
	if err != nil {
		return err
	}
	err = json.Unmarshal(data, &sn)
	if err != nil {
		panic("failed to unmarshal snapshot")
	}
	return nil
}

// Close closes the NoOP IStateMachine.
func (n *NoOP) Close() error { return nil }

// GetHash returns a uint64 value representing the current state of the object.
func (n *NoOP) GetHash() (uint64, error) {
	return 0, nil
}
````

## File: internal/transport/tests/localhost.crt
````
-----BEGIN CERTIFICATE-----
MIIDRzCCAi+gAwIBAgIUA4XDtE2nuJePMOG1wtIQ36NpxDkwDQYJKoZIhvcNAQEL
BQAwUjELMAkGA1UEBhMCQ04xCzAJBgNVBAgMAkdEMQswCQYDVQQHDAJTWjESMBAG
A1UECgwJdGVzdCBJbmMuMRUwEwYDVQQDDAx0ZXN0IFJvb3QgQ0EwHhcNMjAwNjE1
MDcyMTQ0WhcNMzAwNjEzMDcyMTQ0WjBPMQswCQYDVQQGEwJDTjELMAkGA1UECAwC
R0QxCzAJBgNVBAcMAlNaMRIwEAYDVQQKDAl0ZXN0IEluYy4xEjAQBgNVBAMMCWxv
Y2FsaG9zdDCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBANGMrIG7kfTr
C2jjJBxq8w3i8nairjHh0EP8ldIQFN9+ue5t8aLpm4Kp2KrkaHyIwQocZDkwLPx4
yEvHiXnUmYxyMDyb0XrZ0fziJE1+BJyWmJ5KI0/3JjbQW1xCye9ERfj4+OKrH/VO
4UNybo5no1SDkieSUoFc4a4eRqjWwC8gfCTfbLMb9uEv+q4ZSgMjDZhWtfSjlQZD
DVnJUJcnFXBrLlaPFxeoji/yePXm0ftiTcAvKZE49ANMG1IcD1ZUSREWqfmncByp
ljLPqKWt7Hhdi4w8jZ5JAf32+aGx6fQj6qTng8K5/qefy3Qjn+uMtK0FHZ04XgjU
ib5qfYX7QSsCAwEAAaMYMBYwFAYDVR0RBA0wC4IJbG9jYWxob3N0MA0GCSqGSIb3
DQEBCwUAA4IBAQBeLle0JFC3kWYflY1Nd0ojzvOYSwEMbumPKEwWgGCTACBTRnui
N/bGijgUl1q5YSnqhUK0XjPsSQwSNk1crOVgP26WcB71UuYgDFJui1fCpJex4sG/
vXwid9vpcLxjbc527vbDckvfPfDEHkrNZ2LT9bnyPIHf6brlzg1y3pcZ2FCSz1Ng
Al8Es2iEH44VSyBnsV+Lp/CpqVHquxXCimZgss6a/sdjweTbzTdFdCvMBP6h/YZP
oAbLXjroG7whwBMB6QSXToMpkEtASkC9vr+Q+//nxDGHcNT4+ymnrSg1d6gluq9c
yUZucsOkQup188ZHvXWouSwGSMA4Y0GsrGiG
-----END CERTIFICATE-----
````

## File: internal/transport/tests/localhost.csr
````
-----BEGIN CERTIFICATE REQUEST-----
MIIClDCCAXwCAQAwTzELMAkGA1UEBhMCQ04xCzAJBgNVBAgMAkdEMQswCQYDVQQH
DAJTWjESMBAGA1UECgwJdGVzdCBJbmMuMRIwEAYDVQQDDAlsb2NhbGhvc3QwggEi
MA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDRjKyBu5H06wto4yQcavMN4vJ2
oq4x4dBD/JXSEBTffrnubfGi6ZuCqdiq5Gh8iMEKHGQ5MCz8eMhLx4l51JmMcjA8
m9F62dH84iRNfgSclpieSiNP9yY20FtcQsnvREX4+Pjiqx/1TuFDcm6OZ6NUg5In
klKBXOGuHkao1sAvIHwk32yzG/bhL/quGUoDIw2YVrX0o5UGQw1ZyVCXJxVway5W
jxcXqI4v8nj15tH7Yk3ALymROPQDTBtSHA9WVEkRFqn5p3AcqZYyz6ilrex4XYuM
PI2eSQH99vmhsen0I+qk54PCuf6nn8t0I5/rjLStBR2dOF4I1Im+an2F+0ErAgMB
AAGgADANBgkqhkiG9w0BAQsFAAOCAQEAnGA+u8XJ4s+tVlYSbE6w8eb/X+qllX6Y
N5OOzvIzBK1aIeRLXzqOgsgWYut+AtaR/WoQ3iO+nOz2gvElNFVp8fSrTo0TsOzD
ht7VZKOmyzgUY5SQpYfTRmoXbY5xNMcNUPQ5g/f8LQnsifnjJsA2R5ZuGoXLWAcf
HSXKa9cZq3e5bnFMuZM3VfJH1LV/Ma+xcXky4A3h86TwsTwBnqMDhbJYkYBxn3m5
qXRE7N6L01SGFOwLgbJVuEaj/4/ZyOZnRF2O/4jnqegXoEVETPNe3Ovytt/t3GeN
Q7DAgpxnpkOMNRkf+A5T/rIUghWdy//RQzxQhRfgQDrGN0Y0CyQDpw==
-----END CERTIFICATE REQUEST-----
````

## File: internal/transport/tests/localhost.key
````
-----BEGIN PRIVATE KEY-----
MIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQDRjKyBu5H06wto
4yQcavMN4vJ2oq4x4dBD/JXSEBTffrnubfGi6ZuCqdiq5Gh8iMEKHGQ5MCz8eMhL
x4l51JmMcjA8m9F62dH84iRNfgSclpieSiNP9yY20FtcQsnvREX4+Pjiqx/1TuFD
cm6OZ6NUg5InklKBXOGuHkao1sAvIHwk32yzG/bhL/quGUoDIw2YVrX0o5UGQw1Z
yVCXJxVway5WjxcXqI4v8nj15tH7Yk3ALymROPQDTBtSHA9WVEkRFqn5p3AcqZYy
z6ilrex4XYuMPI2eSQH99vmhsen0I+qk54PCuf6nn8t0I5/rjLStBR2dOF4I1Im+
an2F+0ErAgMBAAECggEBAJbC/g3slGtAfBn/aaikbbAs+sLN8kPjMkLnxCa4+Tt+
4mifYz0Rk5fjeQYihOYsQa5zlMvsVEFk6L5Ulh04Dy0Tro68kG8xrbmiPfoe01SR
qMYhktxidXT0XPa4+q4RUcJ55h62fayxDrLG7rG38LPKYWnlBUUbPSdXkwDAWM7N
44AzhkPmafY3KqoHEWRiff9UFHB3N3VcCNI114IWe6hyz1LK4xyDxNJCurebR2aa
e4LCRw8ftYY+wVx5AyYW0WGjhgYewtWe00ROjWUjlSwgC0VvftbqS+wCfwb+h2vq
9Ed1Hd9igOJ9KwmeYiPXZiY0ozhXenfw+QKKA3+dPyECgYEA9hK4xKinecOJKkgr
IwyYMmrQq8DxFamOH5VQ9XJ9lbDT0g2y/b4gB4qC0Xbo9v4U1sPwfH6WGXqkKEv4
ftoIvCVcSDkwFko7TJYktx7+lObXvTi9ZSvOzoJ+rkupKggSQdeZrNim7HUdHg9h
GcPHlSU1FDLxFSWO9aPbu9yUpzECgYEA2gDCty5YShONY5TvClN9A788idJW+aDZ
GSWSG+nH2yFjS0r4e/jTe2FQ++IDLRkwPkpkDYSSmDhEcSnGdrTTavPE+1bymmb4
H5qxQ83sev8j8IMkAiFCPHBoc4FaINFOO9Z6P/sPmapaQLnbgYBMXmNv4sZDr6Lg
izJhroOtzxsCgYBmL0oZKGw7rU51h3iaWm2NajvzRvEWbM2/IdN3ya6LDd44eUXe
Byjn2MYQU8/UWZivwrhICo0sUXh/32R20PYVo3IZ792fM/5CBIuEm9NeZCoswrYr
oDCGIzyWBodeE/H48542X9WZldyckghhmaJ0tfm/xJPGk001hnEGJ0h30QKBgQDW
k6cU3PYkO6iQmxjKZjEIDiTNV48qeEsYOGXfObXxF6T/iFSa2el3aOtocidELaXR
ujFPKO2ODNw5MrBZSVVL0ZNS/Vsf5ThXC1Ft9vPMgPPkGYDI4+mpSc9V2b1Xzg0e
UtbTwQ+y5f2d9hJ8HkeNJH0Njq8y4rP37ug8+PmMfwKBgQD10tXkATxSSB5eZHiC
haH9SvtnhMOk/Q8ZebPjsUXlAsAPjwuZ46F+yG5SS2Q7AYZ8iUmpj1+2jq/8R6Yd
7hOMTOeo8p70nPj4EGjlRtHkOwu21w27r+tdtYvUocl2LLJgkwqEuIMklxuru3dk
FICuuPMb2nbZM/sI5GYnwEz/KA==
-----END PRIVATE KEY-----
````

## File: internal/transport/tests/README.md
````markdown
## Keys and certificates in this folder ##
They are used by unit tests. You should not use any of them for any production purposes.
````

## File: internal/transport/tests/test-root-ca.crt
````
-----BEGIN CERTIFICATE-----
MIIDhTCCAm2gAwIBAgIUdxLGwtJhO+xoOhqxVkPiUNisrzgwDQYJKoZIhvcNAQEL
BQAwUjELMAkGA1UEBhMCQ04xCzAJBgNVBAgMAkdEMQswCQYDVQQHDAJTWjESMBAG
A1UECgwJdGVzdCBJbmMuMRUwEwYDVQQDDAx0ZXN0IFJvb3QgQ0EwHhcNMjAwNjE1
MDcxOTE3WhcNMzAwNjEzMDcxOTE3WjBSMQswCQYDVQQGEwJDTjELMAkGA1UECAwC
R0QxCzAJBgNVBAcMAlNaMRIwEAYDVQQKDAl0ZXN0IEluYy4xFTATBgNVBAMMDHRl
c3QgUm9vdCBDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKwsGf3j
HPFlx+me6fRHQVj8Lct6Ju1JnUhQ81ertACArdhBZ5AzsAwgzGZP+8HDIMui83GJ
+qiiFSTsMuQ/RS6zW2APtKWquaU6FanudqzpT2JXGTqaKEa4V5NP3NEYARWCtQ5A
xNi0p2xBL187+1/cOrgQXY9BZeuObG9KJ7jUgURJ+FroF7L4psRRJYW5pu76aSqp
0uqGQa3jTuU2bCG/rvZ0fV7RIS5sZldSXmy3M9mHwBZRoGj0/Tt+kaN77bA9QbmM
FyHAwZtGLuxgr9F6JTzwlrL/e2u2WUqRHRHc72bCdZlFWjz6H95XcYVywKshNFGU
aasW/esVpxc2XOUCAwEAAaNTMFEwHQYDVR0OBBYEFPq5PU0Bao0qGlzTPG7XXDqB
/uHOMB8GA1UdIwQYMBaAFPq5PU0Bao0qGlzTPG7XXDqB/uHOMA8GA1UdEwEB/wQF
MAMBAf8wDQYJKoZIhvcNAQELBQADggEBAHTVjU2U3BMm/2P/OqJ9ZDy0Z548uu9S
d9g0yToBCLfYTfWJgeyh8DVBSnyUervtmsmE6SJO/cLfYDiOxOtDSarp2iEIcr69
dOzu+4YvF4MkgBDgS4C4jFEhZD6iwEj3ocMV6JpQEuZKAHuOU4PfoMxAFNDFVGLc
4y3H2RXqH8QzEebbR6dWBxt/XofskOgp0R6dcwqMX83cg1cVwU1dyOIV04aUdL8a
rmOA217e2qwW0KE8soVokoDJzV+35v5HYtBtJ3A5EsQSppnP6uVn/5bjjMeQh/w/
b98YXpOiCbFLW7mxtKUaYKpeAwwj/dX1rgDwlpw0Ddh4PYewaY+yYKg=
-----END CERTIFICATE-----
````

## File: internal/transport/chunk_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package transport

import (
	"crypto/rand"
	"testing"

	"github.com/lni/goutils/leaktest"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/rsm"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

func getTestChunk() []pb.Chunk {
	result := make([]pb.Chunk, 0)
	for chunkID := uint64(0); chunkID < 10; chunkID++ {
		c := pb.Chunk{
			DeploymentId:   settings.UnmanagedDeploymentID,
			BinVer:         raftio.TransportBinVersion,
			ShardID:        100,
			ReplicaID:      2,
			From:           12,
			FileChunkId:    chunkID,
			FileChunkCount: 10,
			ChunkId:        chunkID,
			ChunkSize:      100,
			ChunkCount:     10,
			Index:          1,
			Term:           1,
			Filepath:       "snapshot-0000000000000001.gbsnap",
			FileSize:       10 * rsm.HeaderSize,
		}
		data := make([]byte, rsm.HeaderSize)
		if _, err := rand.Read(data); err != nil {
			panic(err)
		}
		c.Data = data
		result = append(result, c)
	}
	return result
}

func hasSnapshotTempFile(cs *Chunk, c pb.Chunk) bool {
	env := cs.getEnv(c)
	fp := env.GetTempFilepath()
	if _, err := cs.fs.Stat(fp); vfs.IsNotExist(err) {
		return false
	}
	return true
}

func hasExternalFile(cs *Chunk,
	c pb.Chunk, fn string, sz uint64, ifs vfs.IFS) bool {
	env := cs.getEnv(c)
	efp := ifs.PathJoin(env.GetFinalDir(), fn)
	fs, err := cs.fs.Stat(efp)
	if vfs.IsNotExist(err) {
		return false
	}
	return uint64(fs.Size()) == sz
}

func runChunkTest(t *testing.T,
	fn func(*testing.T, *Chunk, *testMessageHandler), fs vfs.IFS) {
	defer func() {
		if err := fs.RemoveAll(snapshotDir); err != nil {
			t.Fatalf("%v", err)
		}
	}()
	defer leaktest.AfterTest(t)()
	handler := newTestMessageHandler()
	trans, _, stopper, tt := newTestTransport(handler, false, fs)
	defer func() {
		if err := trans.env.Close(); err != nil {
			t.Fatalf("failed to stop the env %v", err)
		}
	}()
	defer func() {
		if err := trans.Close(); err != nil {
			t.Fatalf("failed to close the transport module %v", err)
		}
	}()
	defer stopper.Stop()
	defer tt.cleanup()
	chunks := NewChunk(trans.handleRequest,
		trans.snapshotReceived, trans.dir, trans.nhConfig.GetDeploymentID(), fs)
	ts := getTestChunk()
	snapDir := chunks.dir(ts[0].ShardID, ts[0].ReplicaID)
	if err := fs.MkdirAll(snapDir, 0755); err != nil {
		t.Fatalf("%v", err)
	}
	fn(t, chunks, handler)
}

func TestMaxSlotIsEnforced(t *testing.T) {
	fn := func(t *testing.T, chunks *Chunk, handler *testMessageHandler) {
		defer func() {
			if err := chunks.fs.RemoveAll(snapshotDir); err != nil {
				t.Fatalf("%v", err)
			}
		}()
		inputs := getTestChunk()
		chunks.validate = false
		v := uint64(1)
		c := inputs[0]
		for i := uint64(0); i < maxConcurrentSlot; i++ {
			v++
			c.ShardID = v
			snapDir := chunks.dir(v, c.ReplicaID)
			if err := chunks.fs.MkdirAll(snapDir, 0755); err != nil {
				t.Fatalf("%v", err)
			}
			if !chunks.addLocked(c) {
				t.Errorf("failed to add chunk")
			}
		}
		count := len(chunks.tracked)
		for i := uint64(0); i < maxConcurrentSlot; i++ {
			v++
			c.ShardID = v
			if chunks.addLocked(c) {
				t.Errorf("not rejected")
			}
		}
		assert.Equal(t, count, len(chunks.tracked))
	}
	fs := vfs.GetTestFS()
	runChunkTest(t, fn, fs)
}

func TestOutOfOrderChunkWillBeIgnored(t *testing.T) {
	fn := func(t *testing.T, chunks *Chunk, handler *testMessageHandler) {
		inputs := getTestChunk()
		chunks.validate = false
		chunks.addLocked(inputs[0])
		key := chunkKey(inputs[0])
		td := chunks.tracked[key]
		next := td.next
		td.next = next + 10
		assert.Nil(t, chunks.record(inputs[1]))
		td = chunks.tracked[key]
		assert.Equal(t, next+10, td.next)
	}
	fs := vfs.GetTestFS()
	runChunkTest(t, fn, fs)
}

func TestChunkFromANewLeaderIsIgnored(t *testing.T) {
	fn := func(t *testing.T, chunks *Chunk, handler *testMessageHandler) {
		inputs := getTestChunk()
		chunks.validate = false
		chunks.addLocked(inputs[0])
		key := chunkKey(inputs[0])
		td := chunks.tracked[key]
		next := td.next
		td.first.From = td.first.From + 1
		assert.Nil(t, chunks.record(inputs[1]))
		td = chunks.tracked[key]
		assert.Equal(t, next, td.next)
	}
	fs := vfs.GetTestFS()
	runChunkTest(t, fn, fs)
}

func TestNotTrackedChunkWillBeIgnored(t *testing.T) {
	fn := func(t *testing.T, chunks *Chunk, handler *testMessageHandler) {
		inputs := getTestChunk()
		assert.Nil(t, chunks.record(inputs[1]))
	}
	fs := vfs.GetTestFS()
	runChunkTest(t, fn, fs)
}

func TestGetOrCreateSnapshotLock(t *testing.T) {
	fn := func(t *testing.T, chunks *Chunk, handler *testMessageHandler) {
		l := chunks.getSnapshotLock("k1")
		l1, ok := chunks.locks["k1"]
		assert.True(t, ok)
		assert.Equal(t, l, l1)
		l2 := chunks.getSnapshotLock("k2")
		assert.NotNil(t, l2)
		l3 := chunks.getSnapshotLock("k3")
		assert.NotNil(t, l3)
		ll := chunks.getSnapshotLock("k1")
		assert.Equal(t, l1, ll)
		assert.Equal(t, 3, len(chunks.locks))
	}
	fs := vfs.GetTestFS()
	runChunkTest(t, fn, fs)
}

func TestShouldUpdateValidator(t *testing.T) {
	tests := []struct {
		validate    bool
		hasFileInfo bool
		chunkID     uint64
		result      bool
	}{
		{true, true, 0, false},
		{true, false, 0, false},
		{false, true, 0, false},
		{false, false, 0, false},
		{true, true, 1, false},
		{true, false, 1, true},
		{false, true, 1, false},
		{false, false, 1, false},
	}
	for _, tt := range tests {
		c := &Chunk{validate: tt.validate}
		input := pb.Chunk{ChunkId: tt.chunkID, HasFileInfo: tt.hasFileInfo}
		assert.Equal(t, tt.result, c.shouldValidate(input))
	}
}

func TestAddFirstChunkRecordsTheSnapshotAndCreatesTheTempFile(t *testing.T) {
	fn := func(t *testing.T, chunks *Chunk, handler *testMessageHandler) {
		inputs := getTestChunk()
		chunks.validate = false
		chunks.addLocked(inputs[0])
		td, ok := chunks.tracked[chunkKey(inputs[0])]
		assert.True(t, ok)
		assert.Equal(t, chunks.getTick(), td.tick)
		recordedChunk, ok := chunks.tracked[chunkKey(inputs[0])]
		assert.True(t, ok)
		assert.Equal(t, inputs[0], recordedChunk.first)
		assert.True(t, hasSnapshotTempFile(chunks, inputs[0]))
	}
	fs := vfs.GetTestFS()
	runChunkTest(t, fn, fs)
}

func TestGcRemovesRecordAndTempFile(t *testing.T) {
	fn := func(t *testing.T, chunks *Chunk, handler *testMessageHandler) {
		inputs := getTestChunk()
		chunks.validate = false
		chunks.addLocked(inputs[0])
		assert.True(t, chunks.addLocked(inputs[0]))
		count := chunks.timeout + chunks.gcTick
		for i := uint64(0); i < count; i++ {
			chunks.Tick()
		}
		_, ok := chunks.tracked[chunkKey(inputs[0])]
		assert.False(t, ok)
		assert.False(t, hasSnapshotTempFile(chunks, inputs[0]))
		assert.Equal(t, uint64(0), handler.getSnapshotCount(100, 2))
	}
	fs := vfs.GetTestFS()
	runChunkTest(t, fn, fs)
}

func TestReceivedCompleteChunkWillBeMergedIntoSnapshotFile(t *testing.T) {
	fn := func(t *testing.T, chunks *Chunk, handler *testMessageHandler) {
		inputs := getTestChunk()
		chunks.validate = false
		for _, c := range inputs {
			assert.True(t, chunks.addLocked(c))
		}
		_, ok := chunks.tracked[chunkKey(inputs[0])]
		assert.False(t, ok)
		assert.False(t, hasSnapshotTempFile(chunks, inputs[0]))
		assert.Equal(t, uint64(1), handler.getSnapshotCount(100, 2))
	}
	fs := vfs.GetTestFS()
	runChunkTest(t, fn, fs)
}

func TestChunkAreIgnoredWhenNodeIsRemoved(t *testing.T) {
	fn := func(t *testing.T, chunks *Chunk, handler *testMessageHandler) {
		inputs := getTestChunk()
		env := chunks.getEnv(inputs[0])
		chunks.validate = false
		assert.True(t, chunks.addLocked(inputs[0]))
		assert.True(t, chunks.addLocked(inputs[1]))
		snapshotDir := env.GetRootDir()
		assert.Nil(t, fileutil.MarkDirAsDeleted(snapshotDir, &pb.Message{}, chunks.fs))
		for idx, c := range inputs {
			if idx <= 1 {
				continue
			}
			assert.False(t, chunks.addLocked(c))
		}
		tmpSnapDir := env.GetTempDir()
		_, err := chunks.fs.Stat(tmpSnapDir)
		assert.True(t, vfs.IsNotExist(err))
	}
	fs := vfs.GetTestFS()
	runChunkTest(t, fn, fs)
}

// when there is no flag file
func TestOutOfDateChunkCanBeHandled(t *testing.T) {
	fn := func(t *testing.T, chunks *Chunk, handler *testMessageHandler) {
		inputs := getTestChunk()
		env := chunks.getEnv(inputs[0])
		snapDir := env.GetFinalDir()
		assert.Nil(t, chunks.fs.MkdirAll(snapDir, 0755))
		chunks.validate = false
		for _, c := range inputs {
			chunks.addLocked(c)
		}
		_, ok := chunks.tracked[chunkKey(inputs[0])]
		assert.False(t, ok)
		assert.Equal(t, uint64(0), handler.getSnapshotCount(100, 2))
		tmpSnapDir := env.GetTempDir()
		_, err := chunks.fs.Stat(tmpSnapDir)
		assert.True(t, vfs.IsNotExist(err))
	}
	fs := vfs.GetTestFS()
	runChunkTest(t, fn, fs)
}

func TestSignificantlyDelayedNonFirstChunkAreIgnored(t *testing.T) {
	fn := func(t *testing.T, chunks *Chunk, handler *testMessageHandler) {
		inputs := getTestChunk()
		chunks.validate = false
		chunks.addLocked(inputs[0])
		count := chunks.timeout + chunks.gcTick
		for i := uint64(0); i < count; i++ {
			chunks.Tick()
		}
		_, ok := chunks.tracked[chunkKey(inputs[0])]
		assert.False(t, ok)
		assert.False(t, hasSnapshotTempFile(chunks, inputs[0]))
		assert.Equal(t, uint64(0), handler.getSnapshotCount(100, 2))
		// now we have the remaining chunks
		for _, c := range inputs[1:] {
			assert.False(t, chunks.addLocked(c))
		}
		_, ok = chunks.tracked[chunkKey(inputs[0])]
		assert.False(t, ok)
		assert.False(t, hasSnapshotTempFile(chunks, inputs[0]))
		assert.Equal(t, uint64(0), handler.getSnapshotCount(100, 2))
	}
	fs := vfs.GetTestFS()
	runChunkTest(t, fn, fs)
}

func checkTestSnapshotFile(chunks *Chunk,
	chunk pb.Chunk, size uint64) bool {
	env := chunks.getEnv(chunk)
	finalFp := env.GetFilepath()
	f, err := chunks.fs.Open(finalFp)
	if err != nil {
		plog.Errorf("no final fp file %s", finalFp)
		return false
	}
	defer func() {
		if err := f.Close(); err != nil {
			panic(err)
		}
	}()
	fi, _ := f.Stat()
	if uint64(fi.Size()) != size {
		plog.Errorf("size doesn't match %d, %d, %s", fi.Size(), size, finalFp)
		return false
	}
	return true
}

func TestAddingFirstChunkAgainResetsTempFile(t *testing.T) {
	fn := func(t *testing.T, chunks *Chunk, handler *testMessageHandler) {
		inputs := getTestChunk()
		chunks.validate = false
		chunks.addLocked(inputs[0])
		chunks.addLocked(inputs[1])
		chunks.addLocked(inputs[2])
		inputs = getTestChunk()
		// now add everything
		for _, c := range inputs {
			assert.True(t, chunks.addLocked(c))
		}
		_, ok := chunks.tracked[chunkKey(inputs[0])]
		assert.False(t, ok)
		assert.False(t, hasSnapshotTempFile(chunks, inputs[0]))
		assert.Equal(t, uint64(1), handler.getSnapshotCount(100, 2))
		assert.True(t, checkTestSnapshotFile(chunks, inputs[0], settings.SnapshotHeaderSize*10))
	}
	fs := vfs.GetTestFS()
	runChunkTest(t, fn, fs)
}

func testSnapshotWithExternalFilesAreHandledByChunk(t *testing.T,
	validate bool, snapshotCount uint64, fs vfs.IFS) {
	fn := func(t *testing.T, chunks *Chunk, handler *testMessageHandler) {
		chunks.validate = validate
		sf1 := &pb.SnapshotFile{
			Filepath: "/data/external1.data",
			FileSize: 100,
			FileId:   1,
			Metadata: make([]byte, 16),
		}
		sf2 := &pb.SnapshotFile{
			Filepath: "/data/external2.data",
			FileSize: snapshotChunkSize + 100,
			FileId:   2,
			Metadata: make([]byte, 32),
		}
		ss := pb.Snapshot{
			Filepath: "filepath.data",
			FileSize: snapshotChunkSize*3 + 100,
			Index:    100,
			Term:     200,
			Files:    []*pb.SnapshotFile{sf1, sf2},
		}
		msg := pb.Message{
			Type:     pb.InstallSnapshot,
			To:       2,
			From:     1,
			ShardID:  100,
			Snapshot: ss,
		}
		inputs, err := splitSnapshotMessage(msg, chunks.fs)
		require.Nil(t, err)
		for _, c := range inputs {
			c.DeploymentId = settings.UnmanagedDeploymentID
			c.Data = make([]byte, c.ChunkSize)
			added := chunks.addLocked(c)
			if snapshotCount == 0 {
				assert.False(t, added)
			} else {
				assert.True(t, added)
			}
		}
		if snapshotCount > 0 {
			assert.Equal(t, uint64(1), handler.getSnapshotCount(100, 2))
			assert.True(t, hasExternalFile(chunks, inputs[0], "external1.data", 100, fs) &&
				hasExternalFile(chunks, inputs[0], "external2.data", snapshotChunkSize+100, fs))
		} else {
			assert.Equal(t, uint64(0), handler.getSnapshotCount(100, 2))
		}
	}
	runChunkTest(t, fn, fs)
}

func TestSnapshotWithExternalFilesAreHandledByChunk(t *testing.T) {
	fs := vfs.GetTestFS()
	testSnapshotWithExternalFilesAreHandledByChunk(t, true, 0, fs)
	testSnapshotWithExternalFilesAreHandledByChunk(t, false, 1, fs)
}

func TestWitnessSnapshotCanBeHandled(t *testing.T) {
	fn := func(t *testing.T, chunks *Chunk, handler *testMessageHandler) {
		ss := pb.Snapshot{
			Filepath: "",
			FileSize: 0,
			Index:    100,
			Term:     200,
			Files:    nil,
			Dummy:    false,
			Witness:  true,
		}
		msg := pb.Message{
			Type:     pb.InstallSnapshot,
			To:       2,
			From:     1,
			ShardID:  100,
			Snapshot: ss,
		}
		inputs, err := splitSnapshotMessage(msg, chunks.fs)
		require.Nil(t, err)
		assert.Equal(t, 1, len(inputs))
		chunk := inputs[0]
		assert.Equal(t, raftio.TransportBinVersion, chunk.BinVer)
		assert.True(t, chunk.Witness)
		assert.Equal(t, uint64(100), chunk.ShardID)
		assert.Equal(t, uint64(1), chunk.From)
		assert.Equal(t, uint64(2), chunk.ReplicaID)
		for _, c := range inputs {
			assert.NotEmpty(t, c.Data)
			c.DeploymentId = settings.UnmanagedDeploymentID
			assert.True(t, chunks.addLocked(c))
		}
		assert.Equal(t, uint64(1), handler.getSnapshotCount(100, 2))
	}
	fs := vfs.GetTestFS()
	runChunkTest(t, fn, fs)
}

func TestSnapshotRecordWithoutExternalFilesCanBeSplitIntoChunk(t *testing.T) {
	fs := vfs.GetTestFS()
	ss := pb.Snapshot{
		Filepath: "filepath.data",
		FileSize: snapshotChunkSize*3 + 100,
		Index:    100,
		Term:     200,
	}
	msg := pb.Message{
		Type:     pb.InstallSnapshot,
		To:       2,
		From:     1,
		ShardID:  100,
		Snapshot: ss,
	}
	chunks, err := splitSnapshotMessage(msg, fs)
	require.Nil(t, err)
	assert.Equal(t, 4, len(chunks))
	for _, c := range chunks {
		assert.Equal(t, raftio.TransportBinVersion, c.BinVer)
		assert.Equal(t, uint64(100), c.ShardID)
		assert.Equal(t, uint64(2), c.ReplicaID)
		assert.Equal(t, uint64(1), c.From)
		assert.Equal(t, uint64(100), c.Index)
		assert.Equal(t, uint64(200), c.Term)
	}
	assert.False(t, chunks[0].HasFileInfo)
	assert.False(t, chunks[1].HasFileInfo)
	assert.False(t, chunks[2].HasFileInfo)
	assert.False(t, chunks[3].HasFileInfo)
	assert.Equal(t, uint64(0), chunks[0].FileChunkId)
	assert.Equal(t, uint64(1), chunks[1].FileChunkId)
	assert.Equal(t, uint64(2), chunks[2].FileChunkId)
	assert.Equal(t, uint64(3), chunks[3].FileChunkId)
	assert.Equal(t, uint64(4), chunks[0].FileChunkCount)
	assert.Equal(t, uint64(4), chunks[1].FileChunkCount)
	assert.Equal(t, uint64(4), chunks[2].FileChunkCount)
	assert.Equal(t, uint64(4), chunks[3].FileChunkCount)
	assert.Equal(t, uint64(0), chunks[0].ChunkId)
	assert.Equal(t, uint64(1), chunks[1].ChunkId)
	assert.Equal(t, uint64(2), chunks[2].ChunkId)
	assert.Equal(t, uint64(3), chunks[3].ChunkId)
	assert.Equal(t, ss.FileSize, chunks[0].ChunkSize+chunks[1].ChunkSize+
		chunks[2].ChunkSize+chunks[3].ChunkSize)
}

func TestSnapshotRecordWithTwoExternalFilesCanBeSplitIntoChunk(t *testing.T) {
	fs := vfs.GetTestFS()
	sf1 := &pb.SnapshotFile{
		Filepath: "/data/external1.data",
		FileSize: 100,
		FileId:   1,
		Metadata: make([]byte, 16),
	}
	sf2 := &pb.SnapshotFile{
		Filepath: "/data/external2.data",
		FileSize: snapshotChunkSize + 100,
		FileId:   2,
		Metadata: make([]byte, 32),
	}
	ss := pb.Snapshot{
		Filepath: "filepath.data",
		FileSize: snapshotChunkSize*3 + 100,
		Index:    100,
		Term:     200,
		Files:    []*pb.SnapshotFile{sf1, sf2},
	}
	msg := pb.Message{
		Type:     pb.InstallSnapshot,
		To:       2,
		From:     1,
		ShardID:  100,
		Snapshot: ss,
	}
	chunks, err := splitSnapshotMessage(msg, fs)
	require.Nil(t, err)
	assert.Equal(t, 7, len(chunks))
	total := uint64(0)
	for idx, c := range chunks {
		assert.Equal(t, uint64(idx), c.ChunkId)
		total += c.ChunkSize
	}
	assert.Equal(t, sf1.FileSize+sf2.FileSize+ss.FileSize, total)
	assert.Equal(t, uint64(0), chunks[0].FileChunkId)
	assert.Equal(t, uint64(0), chunks[4].FileChunkId)
	assert.Equal(t, uint64(0), chunks[5].FileChunkId)
	assert.Equal(t, uint64(1), chunks[4].FileChunkCount)
	assert.Equal(t, uint64(2), chunks[5].FileChunkCount)
	assert.Equal(t, uint64(100), chunks[4].FileSize)
	assert.Equal(t, snapshotChunkSize+100, chunks[5].FileSize)
	for idx := range chunks {
		if idx >= 0 && idx < 4 {
			assert.False(t, chunks[idx].HasFileInfo)
		} else {
			assert.True(t, chunks[idx].HasFileInfo)
		}
	}
	assert.Equal(t, sf1.FileId, chunks[4].FileInfo.FileId)
	assert.Equal(t, sf2.FileId, chunks[5].FileInfo.FileId)
	assert.Equal(t, 16, len(chunks[4].FileInfo.Metadata))
	assert.Equal(t, 32, len(chunks[5].FileInfo.Metadata))
}

func TestGetMessageFromChunk(t *testing.T) {
	fn := func(t *testing.T, chunks *Chunk, handler *testMessageHandler) {
		sf1 := &pb.SnapshotFile{
			Filepath: "/data/external1.data",
			FileSize: 100,
			FileId:   1,
			Metadata: make([]byte, 16),
		}
		sf2 := &pb.SnapshotFile{
			Filepath: "/data/external2.data",
			FileSize: snapshotChunkSize + 100,
			FileId:   2,
			Metadata: make([]byte, 32),
		}
		files := []*pb.SnapshotFile{sf1, sf2}
		chunk := pb.Chunk{
			ShardID:      123,
			ReplicaID:    3,
			From:         2,
			ChunkId:      0,
			Index:        200,
			Term:         300,
			FileSize:     350,
			DeploymentId: 2345,
			Filepath:     "test.data",
		}
		msg := chunks.toMessage(chunk, files)
		assert.Equal(t, 1, len(msg.Requests))
		assert.Equal(t, chunk.BinVer, msg.BinVer)
		req := msg.Requests[0]
		assert.Equal(t, chunk.DeploymentId, msg.DeploymentId)
		assert.Equal(t, pb.InstallSnapshot, req.Type)
		assert.Equal(t, chunk.From, req.From)
		assert.Equal(t, chunk.ReplicaID, req.To)
		assert.Equal(t, chunk.ShardID, req.ShardID)
		ss := req.Snapshot
		assert.Equal(t, len(files), len(ss.Files))
		assert.Equal(t, chunk.FileSize, ss.FileSize)
		assert.Equal(t, chunks.fs.PathJoin("gtransport_test_data_safe_to_delete",
			"snapshot-123-3", "snapshot-00000000000000C8", "test.data"), ss.Filepath)
		assert.Equal(t, len(sf1.Metadata), len(ss.Files[0].Metadata))
		assert.Equal(t, len(sf2.Metadata), len(ss.Files[1].Metadata))
		assert.Equal(t, chunks.fs.PathJoin("gtransport_test_data_safe_to_delete",
			"snapshot-123-3", "snapshot-00000000000000C8", "external-file-1"), ss.Files[0].Filepath)
		assert.Equal(t, chunks.fs.PathJoin("gtransport_test_data_safe_to_delete",
			"snapshot-123-3", "snapshot-00000000000000C8", "external-file-2"), ss.Files[1].Filepath)
	}
	fs := vfs.GetTestFS()
	runChunkTest(t, fn, fs)
}
````

## File: internal/transport/chunk.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package transport

import (
	"fmt"
	"io"
	"sync"
	"sync/atomic"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/logutil"

	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/rsm"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/utils"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

var (
	// ErrSnapshotOutOfDate is returned when the snapshot being received is
	// considered as out of date.
	ErrSnapshotOutOfDate     = errors.New("snapshot is out of date")
	gcIntervalTick           = settings.Soft.SnapshotGCTick
	snapshotChunkTimeoutTick = settings.Soft.SnapshotChunkTimeoutTick
	maxConcurrentSlot        = settings.Soft.MaxConcurrentStreamingSnapshot
)

var firstError = utils.FirstError

func chunkKey(c pb.Chunk) string {
	return fmt.Sprintf("%d:%d:%d", c.ShardID, c.ReplicaID, c.Index)
}

type tracked struct {
	validator *rsm.SnapshotValidator
	files     []*pb.SnapshotFile
	first     pb.Chunk
	tick      uint64
	next      uint64
}

type ssLock struct {
	mu sync.Mutex
}

func (l *ssLock) lock() {
	l.mu.Lock()
}

func (l *ssLock) unlock() {
	l.mu.Unlock()
}

// Chunk managed on the receiving side
type Chunk struct {
	fs        vfs.IFS
	tracked   map[string]*tracked
	locks     map[string]*ssLock
	dir       server.SnapshotDirFunc
	confirm   func(uint64, uint64, uint64)
	onReceive func(pb.MessageBatch)
	timeout   uint64
	did       uint64
	tick      uint64
	gcTick    uint64
	mu        sync.Mutex
	validate  bool
}

// NewChunk creates and returns a new snapshot chunks instance.
func NewChunk(onReceive func(pb.MessageBatch),
	confirm func(uint64, uint64, uint64), dir server.SnapshotDirFunc,
	did uint64, fs vfs.IFS) *Chunk {
	return &Chunk{
		did:       did,
		validate:  true,
		onReceive: onReceive,
		confirm:   confirm,
		tracked:   make(map[string]*tracked),
		locks:     make(map[string]*ssLock),
		timeout:   snapshotChunkTimeoutTick,
		gcTick:    gcIntervalTick,
		dir:       dir,
		fs:        fs,
	}
}

// Add adds a received trunk to chunks.
func (c *Chunk) Add(chunk pb.Chunk) bool {
	if chunk.DeploymentId != c.did ||
		chunk.BinVer != raftio.TransportBinVersion {
		plog.Errorf("invalid did or binver, %d, %d, %d, %d",
			chunk.DeploymentId, c.did, chunk.BinVer, raftio.TransportBinVersion)
		return false
	}
	key := chunkKey(chunk)
	lock := c.getSnapshotLock(key)
	lock.lock()
	defer lock.unlock()
	return c.addLocked(chunk)
}

// Tick moves the internal logical clock forward.
func (c *Chunk) Tick() {
	ct := atomic.AddUint64(&c.tick, 1)
	if ct%c.gcTick == 0 {
		c.gc()
	}
}

// Close closes the chunks instance.
func (c *Chunk) Close() {
	tracked := c.getTracked()
	for key, td := range tracked {
		func() {
			l := c.getSnapshotLock(key)
			l.lock()
			defer l.unlock()
			c.removeTempDir(td.first)
			c.reset(key)
		}()
	}
}

func (c *Chunk) gc() {
	tracked := c.getTracked()
	tick := c.getTick()
	for key, td := range tracked {
		func() {
			l := c.getSnapshotLock(key)
			l.lock()
			defer l.unlock()
			if tick-td.tick >= c.timeout {
				c.removeTempDir(td.first)
				c.reset(key)
			}
		}()
	}
}

func (c *Chunk) getTick() uint64 {
	return atomic.LoadUint64(&c.tick)
}

func (c *Chunk) reset(key string) {
	c.mu.Lock()
	defer c.mu.Unlock()
	c.resetLocked(key)
}

func (c *Chunk) getTracked() map[string]*tracked {
	m := make(map[string]*tracked)
	c.mu.Lock()
	defer c.mu.Unlock()
	for k, v := range c.tracked {
		m[k] = v
	}
	return m
}

func (c *Chunk) resetLocked(key string) {
	delete(c.tracked, key)
}

func (c *Chunk) getSnapshotLock(key string) *ssLock {
	c.mu.Lock()
	defer c.mu.Unlock()
	l, ok := c.locks[key]
	if !ok {
		l = &ssLock{}
		c.locks[key] = l
	}
	return l
}

func (c *Chunk) full() bool {
	return uint64(len(c.tracked)) >= maxConcurrentSlot
}

func (c *Chunk) record(chunk pb.Chunk) *tracked {
	c.mu.Lock()
	defer c.mu.Unlock()
	key := chunkKey(chunk)
	td := c.tracked[key]
	if chunk.ChunkId == 0 {
		plog.Debugf("first chunk of %s received", c.ssid(chunk))
		if td != nil {
			plog.Warningf("removing unclaimed chunks %s", key)
			c.removeTempDir(td.first)
		} else {
			if c.full() {
				plog.Errorf("max slot count reached, dropped a chunk %s", key)
				return nil
			}
		}
		validator := rsm.NewSnapshotValidator()
		if c.validate && !chunk.HasFileInfo {
			if !validator.AddChunk(chunk.Data, chunk.ChunkId) {
				return nil
			}
		}
		td = &tracked{
			next:      1,
			first:     chunk,
			validator: validator,
			files:     make([]*pb.SnapshotFile, 0),
		}
		c.tracked[key] = td
	} else {
		if td == nil {
			plog.Errorf("not tracked chunk %s ignored, id %d", key, chunk.ChunkId)
			return nil
		}
		if td.next != chunk.ChunkId {
			plog.Errorf("out of order, %s, want %d, got %d",
				key, td.next, chunk.ChunkId)
			return nil
		}
		from := chunk.From
		want := td.first.From
		if want != from {
			from := chunk.From
			want := td.first.From
			plog.Errorf("ignored %s, from %d, want %d", key, from, want)
			return nil
		}
		td.next = chunk.ChunkId + 1
	}
	if chunk.FileChunkId == 0 && chunk.HasFileInfo {
		td.files = append(td.files, &chunk.FileInfo)
	}
	td.tick = c.getTick()
	return td
}

func (c *Chunk) shouldValidate(chunk pb.Chunk) bool {
	return c.validate && !chunk.HasFileInfo && chunk.ChunkId != 0
}

func (c *Chunk) addLocked(chunk pb.Chunk) bool {
	key := chunkKey(chunk)
	td := c.record(chunk)
	if td == nil {
		plog.Warningf("ignored a chunk belongs to %s", key)
		return false
	}
	removed, err := c.nodeRemoved(chunk)
	if err != nil {
		panicNow(err)
	}
	if removed {
		c.removeTempDir(chunk)
		plog.Warningf("node removed, ignored chunk %s", key)
		return false
	}
	if c.shouldValidate(chunk) {
		if !td.validator.AddChunk(chunk.Data, chunk.ChunkId) {
			plog.Warningf("ignored a invalid chunk %s", key)
			return false
		}
	}
	if err := c.save(chunk); err != nil {
		err = errors.Wrapf(err, "failed to save chunk %s", key)
		c.removeTempDir(chunk)
		panicNow(err)
	}
	if chunk.IsLastChunk() {
		plog.Debugf("last chunk %s received", key)
		defer c.reset(key)
		if c.validate {
			if !td.validator.Validate() {
				plog.Warningf("dropped an invalid snapshot %s", key)
				c.removeTempDir(chunk)
				return false
			}
		}
		if err := c.finalize(chunk, td); err != nil {
			c.removeTempDir(chunk)
			if !errors.Is(err, ErrSnapshotOutOfDate) {
				plog.Panicf("%s failed when finalizing, %v", key, err)
			}
			return false
		}
		snapshotMessage := c.toMessage(td.first, td.files)
		plog.Debugf("%s received from %d, term %d",
			c.ssid(chunk), chunk.From, chunk.Term)
		c.onReceive(snapshotMessage)
		c.confirm(chunk.ShardID, chunk.ReplicaID, chunk.From)
	}
	return true
}

func (c *Chunk) nodeRemoved(chunk pb.Chunk) (bool, error) {
	env := c.getEnv(chunk)
	dir := env.GetRootDir()
	return fileutil.IsDirMarkedAsDeleted(dir, c.fs)
}

func (c *Chunk) save(chunk pb.Chunk) (err error) {
	env := c.getEnv(chunk)
	if chunk.ChunkId == 0 {
		if err := env.CreateTempDir(); err != nil {
			return err
		}
	}
	fn := c.fs.PathBase(chunk.Filepath)
	fp := c.fs.PathJoin(env.GetTempDir(), fn)
	var f *chunkFile
	if chunk.FileChunkId == 0 {
		f, err = createChunkFile(fp, c.fs)
	} else {
		f, err = openChunkFileForAppend(fp, c.fs)
	}
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, f.close())
	}()
	n, err := f.write(chunk.Data)
	if err != nil {
		return err
	}
	if len(chunk.Data) != n {
		return io.ErrShortWrite
	}
	if chunk.IsLastChunk() || chunk.IsLastFileChunk() {
		if err := f.sync(); err != nil {
			return err
		}
	}
	return nil
}

func (c *Chunk) getEnv(chunk pb.Chunk) server.SSEnv {
	return server.NewSSEnv(c.dir, chunk.ShardID, chunk.ReplicaID,
		chunk.Index, chunk.From, server.ReceivingMode, c.fs)
}

func (c *Chunk) finalize(chunk pb.Chunk, td *tracked) error {
	env := c.getEnv(chunk)
	msg := c.toMessage(td.first, td.files)
	if len(msg.Requests) != 1 || msg.Requests[0].Type != pb.InstallSnapshot {
		panic("invalid message")
	}
	ss := &msg.Requests[0].Snapshot
	err := env.FinalizeSnapshot(ss)
	if err == server.ErrSnapshotOutOfDate {
		return ErrSnapshotOutOfDate
	}
	return err
}

func (c *Chunk) removeTempDir(chunk pb.Chunk) {
	env := c.getEnv(chunk)
	env.MustRemoveTempDir()
}

func (c *Chunk) toMessage(chunk pb.Chunk,
	files []*pb.SnapshotFile) pb.MessageBatch {
	if chunk.ChunkId != 0 {
		panic("not first chunk")
	}
	env := c.getEnv(chunk)
	snapDir := env.GetFinalDir()
	m := pb.Message{}
	m.Type = pb.InstallSnapshot
	m.From = chunk.From
	m.To = chunk.ReplicaID
	m.ShardID = chunk.ShardID
	s := pb.Snapshot{}
	s.Index = chunk.Index
	s.Term = chunk.Term
	s.OnDiskIndex = chunk.OnDiskIndex
	s.Membership = chunk.Membership
	fn := c.fs.PathBase(chunk.Filepath)
	s.Filepath = c.fs.PathJoin(snapDir, fn)
	s.FileSize = chunk.FileSize
	s.Witness = chunk.Witness
	m.Snapshot = s
	m.Snapshot.Files = files
	for idx := range m.Snapshot.Files {
		fp := c.fs.PathJoin(snapDir, m.Snapshot.Files[idx].Filename())
		m.Snapshot.Files[idx].Filepath = fp
	}
	return pb.MessageBatch{
		BinVer:       chunk.BinVer,
		DeploymentId: chunk.DeploymentId,
		Requests:     []pb.Message{m},
	}
}

func (c *Chunk) ssid(chunk pb.Chunk) string {
	return logutil.DescribeSS(chunk.ShardID, chunk.ReplicaID, chunk.Index)
}

func panicNow(err error) {
	plog.Panicf("%+v", err)
	panic(err)
}
````

## File: internal/transport/chunkfile.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package transport

import (
	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/vfs"
)

// chunkFile is the snapshot chunk file being transferred.
type chunkFile struct {
	file    vfs.File
	fs      vfs.IFS
	dir     string
	syncDir bool
}

// openChunkFileForAppend opens the chunk file at fp for appending.
func openChunkFileForAppend(fp string, fs vfs.IFS) (*chunkFile, error) {
	f, err := fs.OpenForAppend(fp)
	if err != nil {
		return nil, err
	}
	return &chunkFile{file: f, fs: fs}, nil
}

// openChunkFileForRead opens for the chunk file for read-only operation.
func openChunkFileForRead(fp string, fs vfs.IFS) (*chunkFile, error) {
	f, err := fs.Open(fp)
	if err != nil {
		return nil, err
	}
	return &chunkFile{file: f, fs: fs}, nil
}

// createChunkFile creates a new chunk file.
func createChunkFile(fp string, fs vfs.IFS) (*chunkFile, error) {
	f, err := fs.Create(fp)
	if err != nil {
		return nil, err
	}
	return &chunkFile{file: f, syncDir: true, dir: fs.PathDir(fp), fs: fs}, nil
}

// readAt reads from the file.
func (cf *chunkFile) readAt(data []byte, offset int64) (int, error) {
	return cf.file.ReadAt(data, offset)
}

// write writes the specified data to the chunk file.
func (cf *chunkFile) write(data []byte) (int, error) {
	return cf.file.Write(data)
}

// close closes the chunk file.
func (cf *chunkFile) close() error {
	if err := cf.file.Close(); err != nil {
		return err
	}
	if cf.syncDir {
		return fileutil.SyncDir(cf.dir, cf.fs)
	}
	return nil
}

// sync syncs the chunk file.
func (cf *chunkFile) sync() error {
	return cf.file.Sync()
}
````

## File: internal/transport/fuzz.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +build gofuzz

package transport

import (
	"bytes"
	"net"
	"time"
)

type fuzzROConn struct {
	buf *bytes.Buffer
}

func newFuzzROConn(data []byte) *fuzzROConn {
	conn := &fuzzROConn{
		buf: bytes.NewBuffer(data),
	}
	return conn
}

func (f *fuzzROConn) Read(b []byte) (n int, err error) {
	n, err = f.buf.Read(b)
	return n, err
}

func (f *fuzzROConn) Write(b []byte) (n int, err error) {
	panic("not suppose to be called")
}

func (f *fuzzROConn) Close() error {
	panic("not suppose to be called")
}

func (f *fuzzROConn) LocalAddr() net.Addr {
	panic("not suppose to be called")
}

func (f *fuzzROConn) RemoteAddr() net.Addr {
	panic("not suppose to be called")
}

func (f *fuzzROConn) SetDeadline(t time.Time) error {
	panic("not suppose to be called")
}

func (f *fuzzROConn) SetReadDeadline(t time.Time) error {
	return nil
}

func (f *fuzzROConn) SetWriteDeadline(t time.Time) error {
	panic("not suppose to be called")
}

func Fuzz(data []byte) int {
	roconn := newFuzzROConn(data)
	header := make([]byte, requestHeaderSize)
	tbuf := make([]byte, payloadBufferSize)
	if _, _, err := readMessage(roconn, header, tbuf); err != nil {
		return 0
	}
	return 1
}
````

## File: internal/transport/job_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package transport

import (
	"context"
	"testing"

	"github.com/lni/goutils/syncutil"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/vfs"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

func TestSnapshotJobCanBeCreatedInSavedMode(t *testing.T) {
	fs := vfs.GetTestFS()
	cfg := config.NodeHostConfig{}
	transport := NewNOOPTransport(cfg, nil, nil)
	c := newJob(context.Background(), 1, 1, 1, false, 201, transport, nil, fs)
	require.Equal(t, 201, cap(c.ch), "unexpected chan length")
}

func TestSnapshotJobCanBeCreatedInStreamingMode(t *testing.T) {
	fs := vfs.GetTestFS()
	cfg := config.NodeHostConfig{}
	transport := NewNOOPTransport(cfg, nil, nil)
	c := newJob(context.Background(), 1, 1, 1, true, 201, transport, nil, fs)
	require.Equal(t, streamingChanLength, cap(c.ch), "unexpected chan length")
}

func TestSendSavedSnapshotPutsAllChunksInCh(t *testing.T) {
	fs := vfs.GetTestFS()
	m := pb.Message{
		Type: pb.InstallSnapshot,
		Snapshot: pb.Snapshot{
			FileSize: 1024 * 1024 * 512,
		},
	}
	chunks, err := splitSnapshotMessage(m, fs)
	require.NoError(t, err, "failed to get chunks")
	transport := NewNOOPTransport(config.NodeHostConfig{}, nil, nil)
	c := newJob(context.Background(), 1, 1, 1, false, len(chunks),
		transport, nil, fs)
	require.Equal(t, len(chunks), cap(c.ch), "unexpected chan length")
	c.addSnapshot(chunks)
	require.Equal(t, len(chunks), len(c.ch), "not all chunks pushed to ch")
}

func TestKeepSendingChunksUsingFailedJobWillNotBlock(t *testing.T) {
	fs := vfs.GetTestFS()
	cfg := config.NodeHostConfig{}
	transport := NewNOOPTransport(cfg, nil, nil)
	c := newJob(context.Background(), 1, 1, 1, true, 0, transport, nil, fs)
	require.Equal(t, streamingChanLength, cap(c.ch), "unexpected chan length")
	err := c.connect("a1")
	require.NoError(t, err, "connect failed")
	stopper := syncutil.NewStopper()
	var perr error
	stopper.RunWorker(func() {
		perr = c.process()
	})
	noopConn, ok := c.conn.(*NOOPSnapshotConnection)
	require.True(t, ok, "failed to get noopConn")
	noopConn.req.SetToFail(true)
	sent, stopped := c.AddChunk(pb.Chunk{})
	require.True(t, sent, "failed to send")
	require.False(t, stopped, "unexpectedly stopped")
	stopper.Stop()
	require.NotNil(t, perr, "error didn't return from process()")
	for i := 0; i < streamingChanLength*10; i++ {
		c.AddChunk(pb.Chunk{})
	}
	select {
	case <-c.failed:
	default:
		require.Fail(t, "failed chan not closed")
	}
	c.close()
}

func testSpecialChunkCanStopTheProcessLoop(t *testing.T,
	tt uint64, experr error, fs vfs.IFS) {
	cfg := config.NodeHostConfig{}
	transport := NewNOOPTransport(cfg, nil, nil)
	c := newJob(context.Background(), 1, 1, 1, true, 0, transport, nil, fs)
	err := c.connect("a1")
	require.NoError(t, err, "connect failed")
	stopper := syncutil.NewStopper()
	var perr error
	stopper.RunWorker(func() {
		perr = c.process()
	})
	poison := pb.Chunk{
		ChunkCount: tt,
	}
	sent, stopped := c.AddChunk(poison)
	require.True(t, sent, "failed to send")
	require.False(t, stopped, "unexpectedly stopped")
	stopper.Stop()
	require.Equal(t, experr, perr, "unexpected error val")
}

func TestPoisonChunkCanStopTheProcessLoop(t *testing.T) {
	fs := vfs.GetTestFS()
	testSpecialChunkCanStopTheProcessLoop(t,
		pb.PoisonChunkCount, ErrStreamSnapshot, fs)
}

func TestLastChunkCanStopTheProcessLoop(t *testing.T) {
	fs := vfs.GetTestFS()
	testSpecialChunkCanStopTheProcessLoop(t, pb.LastChunkCount, nil, fs)
}
````

## File: internal/transport/job.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmaij.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package transport

import (
	"context"
	"sync/atomic"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/logutil"

	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

const (
	streamingChanLength = 4
)

var (
	// ErrStopped is the error returned to indicate that the connection has
	// already been stopped.
	ErrStopped = errors.New("connection stopped")
	// ErrStreamSnapshot is the error returned to indicate that snapshot
	// streaming failed.
	ErrStreamSnapshot = errors.New("stream snapshot failed")
)

// Sink is the chunk sink for receiving generated snapshot chunk.
type Sink struct {
	j *job
}

// Receive receives a snapshot chunk.
func (s *Sink) Receive(chunk pb.Chunk) (bool, bool) {
	return s.j.AddChunk(chunk)
}

// Close closes the sink processing.
func (s *Sink) Close() error {
	s.Receive(pb.Chunk{ChunkCount: pb.PoisonChunkCount})
	return nil
}

// ShardID returns the shard ID of the source node.
func (s *Sink) ShardID() uint64 {
	return s.j.shardID
}

// ToReplicaID returns the node ID of the node intended to get and handle the
// received snapshot chunk.
func (s *Sink) ToReplicaID() uint64 {
	return s.j.replicaID
}

type job struct {
	conn         raftio.ISnapshotConnection
	preSend      atomic.Value
	postSend     atomic.Value
	fs           vfs.IFS
	ctx          context.Context
	transport    raftio.ITransport
	ch           chan pb.Chunk
	completed    chan struct{}
	stopc        chan struct{}
	failed       chan struct{}
	deploymentID uint64
	replicaID    uint64
	shardID      uint64
	streaming    bool
}

func newJob(ctx context.Context,
	shardID uint64, replicaID uint64,
	did uint64, streaming bool, sz int, transport raftio.ITransport,
	stopc chan struct{}, fs vfs.IFS) *job {
	j := &job{
		shardID:      shardID,
		replicaID:    replicaID,
		deploymentID: did,
		streaming:    streaming,
		ctx:          ctx,
		transport:    transport,
		stopc:        stopc,
		failed:       make(chan struct{}),
		completed:    make(chan struct{}),
		fs:           fs,
	}
	var chsz int
	if streaming {
		chsz = streamingChanLength
	} else {
		chsz = sz
	}
	j.ch = make(chan pb.Chunk, chsz)
	return j
}

func (j *job) close() {
	if j.conn != nil {
		j.conn.Close()
	}
}

func (j *job) connect(addr string) error {
	conn, err := j.transport.GetSnapshotConnection(j.ctx, addr)
	if err != nil {
		plog.Errorf("failed to get a job to %s, %v", addr, err)
		return err
	}
	j.conn = conn
	return nil
}

func (j *job) addSnapshot(chunks []pb.Chunk) {
	if len(chunks) != cap(j.ch) {
		plog.Panicf("cap of ch is %d, want %d", cap(j.ch), len(chunks))
	}
	for _, chunk := range chunks {
		j.ch <- chunk
	}
}

func (j *job) AddChunk(chunk pb.Chunk) (bool, bool) {
	if !chunk.IsPoisonChunk() {
		plog.Debugf("%s is sending chunk %d to %s",
			logutil.ReplicaID(chunk.From), chunk.ChunkId,
			dn(chunk.ShardID, chunk.ReplicaID))
	} else {
		plog.Debugf("sending a poison chunk to %s", dn(j.shardID, j.replicaID))
	}

	select {
	case j.ch <- chunk:
		return true, false
	case <-j.completed:
		if !chunk.IsPoisonChunk() {
			plog.Panicf("more chunk received for completed job")
		}
		return true, false
	case <-j.failed:
		plog.Warningf("stream snapshot to %s failed", dn(j.shardID, j.replicaID))
		return false, false
	case <-j.stopc:
		return false, true
	}
}

func (j *job) process() error {
	if j.conn == nil {
		panic("nil connection")
	}
	if j.streaming {
		err := j.streamSnapshot()
		if err != nil {
			close(j.failed)
		}
		return err
	}
	return j.sendSnapshot()
}

func (j *job) streamSnapshot() error {
	for {
		select {
		case <-j.stopc:
			plog.Warningf("stream snapshot to %s stopped", dn(j.shardID, j.replicaID))
			return ErrStopped
		case chunk := <-j.ch:
			chunk.DeploymentId = j.deploymentID
			if chunk.IsPoisonChunk() {
				return ErrStreamSnapshot
			}
			if err := j.sendChunk(chunk, j.conn); err != nil {
				plog.Errorf("streaming snapshot chunk to %s failed, %v",
					dn(chunk.ShardID, chunk.ReplicaID), err)
				return err
			}
			if chunk.ChunkCount == pb.LastChunkCount {
				plog.Debugf("node %d just sent all chunks to %s",
					chunk.From, dn(chunk.ShardID, chunk.ReplicaID))
				close(j.completed)
				return nil
			}
		}
	}
}

func (j *job) sendSnapshot() error {
	chunks := make([]pb.Chunk, 0)
	for {
		select {
		case <-j.stopc:
			return ErrStopped
		case chunk := <-j.ch:
			if len(chunks) == 0 && chunk.ChunkId != 0 {
				panic("chunk alignment error")
			}
			chunks = append(chunks, chunk)
			if chunk.ChunkId+1 == chunk.ChunkCount {
				return j.sendChunks(chunks)
			}
		}
	}
}

func (j *job) sendChunks(chunks []pb.Chunk) error {
	chunkData := make([]byte, snapshotChunkSize)
	for _, chunk := range chunks {
		select {
		case <-j.stopc:
			return ErrStopped
		default:
		}
		chunk.DeploymentId = j.deploymentID
		if !chunk.Witness {
			// TODO: add a test for such error
			// TODO: add a test to show that failed sendChunks for other reasons will
			// 			 be reported
			data, err := loadChunkData(chunk, chunkData, j.fs)
			if err != nil {
				panicNow(err)
			}
			chunk.Data = data
		}
		if err := j.sendChunk(chunk, j.conn); err != nil {
			return err
		}
		if f := j.postSend.Load(); f != nil {
			f.(func(pb.Chunk))(chunk)
		}
	}
	return nil
}

func (j *job) sendChunk(c pb.Chunk,
	conn raftio.ISnapshotConnection) error {
	if f := j.preSend.Load(); f != nil {
		updated, shouldSend := f.(StreamChunkSendFunc)(c)
		if !shouldSend {
			plog.Debugf("chunk to %s skipped", dn(c.ShardID, c.ReplicaID))
			return errChunkSendSkipped
		}
		return conn.SendChunk(updated)
	}
	return conn.SendChunk(c)
}
````

## File: internal/transport/metrics.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package transport

import (
	"github.com/VictoriaMetrics/metrics"
)

type transportMetrics struct {
	snapshotSent       *metrics.Counter
	messageConns       *metrics.Gauge
	snapshotConns      *metrics.Gauge
	messageDropped     *metrics.Counter
	messageSent        *metrics.Counter
	snapshotDropped    *metrics.Counter
	messageConnFailed  *metrics.Counter
	snapshotConnFailed *metrics.Counter
	messageReceived    *metrics.Counter
	messageRecvDropped *metrics.Counter
	snapshotReceived   *metrics.Counter
	useMetrics         bool
}

func newTransportMetrics(useMetrics bool,
	msgCount func() float64, ssCount func() float64) *transportMetrics {
	tm := &transportMetrics{useMetrics: useMetrics}
	if useMetrics {
		name := "dragonboat_transport_message_connections"
		tm.messageConns = metrics.GetOrCreateGauge(name, msgCount)
		name = "dragonboat_transport_snapshot_connections"
		tm.snapshotConns = metrics.GetOrCreateGauge(name, ssCount)
		name = "dragonboat_transport_message_send_failure_total"
		tm.messageDropped = metrics.GetOrCreateCounter(name)
		name = "dragonboat_transport_message_send_success_total"
		tm.messageSent = metrics.GetOrCreateCounter(name)
		name = "dragonboat_transport_snapshot_send_failure_total"
		tm.snapshotDropped = metrics.GetOrCreateCounter(name)
		name = "dragonboat_transport_snapshot_send_success_total"
		tm.snapshotSent = metrics.GetOrCreateCounter(name)
		name = "dragonboat_transport_received_message_total"
		tm.messageReceived = metrics.GetOrCreateCounter(name)
		name = "dragonboat_transport_received_message_dropped_total"
		tm.messageRecvDropped = metrics.GetOrCreateCounter(name)
		name = "dragonboat_transport_received_snapshot_total"
		tm.snapshotReceived = metrics.GetOrCreateCounter(name)
		name = "dragonboat_transport_failed_message_connection_attempt_total"
		tm.messageConnFailed = metrics.GetOrCreateCounter(name)
		name = "dragonboat_transport_failed_snapshot_connection_attempt_total"
		tm.snapshotConnFailed = metrics.GetOrCreateCounter(name)
	}
	return tm
}

func (tm *transportMetrics) messageConnectionFailure() {
	if tm.useMetrics {
		tm.messageConnFailed.Add(1)
	}
}

func (tm *transportMetrics) snapshotCnnectionFailure() {
	if tm.useMetrics {
		tm.snapshotConnFailed.Add(1)
	}
}

func (tm *transportMetrics) receivedMessages(ss uint64,
	msg uint64, dropped uint64) {
	if tm.useMetrics {
		tm.messageReceived.Add(int(msg))
		tm.messageRecvDropped.Add(int(dropped))
		tm.snapshotReceived.Add(int(ss))
	}
}

func (tm *transportMetrics) messageSendSuccess(count uint64) {
	if tm.useMetrics {
		tm.messageSent.Add(int(count))
	}
}

func (tm *transportMetrics) messageSendFailure(count uint64) {
	if tm.useMetrics {
		tm.messageDropped.Add(int(count))
	}
}

func (tm *transportMetrics) snapshotSendSuccess() {
	if tm.useMetrics {
		tm.snapshotSent.Add(1)
	}
}

func (tm *transportMetrics) snapshotSendFailure() {
	if tm.useMetrics {
		tm.snapshotDropped.Add(1)
	}
}
````

## File: internal/transport/monkey.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +build dragonboat_monkeytest

package transport

// ApplyMonkeySettings applies monkey settings
func ApplyMonkeySettings() {
	perConnBufSize = 1024 * 64
	sendQueueLen = 64
	snapshotChunkSize = 1024
	payloadBufferSize = 1024 * 8
}
````

## File: internal/transport/noop.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package transport

import (
	"context"
	"sync"
	"sync/atomic"
	"time"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/stringutil"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/raftio"
	"github.com/lni/dragonboat/v4/raftpb"
)

var (
	// NOOPRaftName is the module name for the NOOP transport module.
	NOOPRaftName = "noop-test-transport"
	// ErrRequestedToFail is the error used to indicate that the error is
	// requested.
	ErrRequestedToFail = errors.New("requested to returned error")
)

type noopRequest struct {
	mu      sync.Mutex
	fail    bool
	blocked bool
}

func (r *noopRequest) SetToFail(v bool) {
	r.mu.Lock()
	defer r.mu.Unlock()
	r.fail = v
}

func (r *noopRequest) Fail() bool {
	r.mu.Lock()
	defer r.mu.Unlock()
	return r.fail
}

func (r *noopRequest) SetBlocked(v bool) {
	r.mu.Lock()
	defer r.mu.Unlock()
	r.blocked = v
}

func (r *noopRequest) Blocked() bool {
	r.mu.Lock()
	defer r.mu.Unlock()
	return r.blocked
}

type noopConnectRequest struct {
	mu   sync.Mutex
	fail bool
}

func (r *noopConnectRequest) SetToFail(v bool) {
	r.mu.Lock()
	defer r.mu.Unlock()
	r.fail = v
}

func (r *noopConnectRequest) Fail() bool {
	r.mu.Lock()
	defer r.mu.Unlock()
	return r.fail
}

// NOOPConnection is the connection used to exchange messages between node hosts.
type NOOPConnection struct {
	req *noopRequest
}

// Close closes the NOOPConnection instance.
func (c *NOOPConnection) Close() {
}

// SendMessageBatch return ErrRequestedToFail when requested.
func (c *NOOPConnection) SendMessageBatch(batch raftpb.MessageBatch) error {
	if c.req.Fail() {
		return ErrRequestedToFail
	}
	for c.req.Blocked() {
		time.Sleep(50 * time.Millisecond)
	}
	return nil
}

// NOOPSnapshotConnection is the connection used to send snapshots.
type NOOPSnapshotConnection struct {
	req             *noopRequest
	sendChunksCount uint64
}

// Close closes the NOOPSnapshotConnection.
func (c *NOOPSnapshotConnection) Close() {
}

// SendChunk returns ErrRequestedToFail when requested.
func (c *NOOPSnapshotConnection) SendChunk(chunk raftpb.Chunk) error {
	if c.req.Fail() {
		return ErrRequestedToFail
	}
	c.sendChunksCount++
	return nil
}

// NOOPTransportFactory is a NOOP transport module used in testing
type NOOPTransportFactory struct{}

// Create creates a noop transport instance.
func (n *NOOPTransportFactory) Create(nhConfig config.NodeHostConfig,
	handler raftio.MessageHandler,
	chunkHandler raftio.ChunkHandler) raftio.ITransport {
	return NewNOOPTransport(nhConfig, handler, chunkHandler)
}

// Validate returns a boolean value indicating whether the input address is
// valid.
func (n *NOOPTransportFactory) Validate(addr string) bool {
	return stringutil.IsValidAddress(addr)
}

// NOOPTransport is a transport module for testing purposes. It does not
// actually has the ability to exchange messages or snapshots between
// nodehosts.
type NOOPTransport struct {
	req        *noopRequest
	connReq    *noopConnectRequest
	connected  uint64
	tryConnect uint64
}

// NewNOOPTransport creates a new NOOPTransport instance.
func NewNOOPTransport(nhConfig config.NodeHostConfig,
	requestHandler raftio.MessageHandler,
	chunkHandler raftio.ChunkHandler) raftio.ITransport {
	return &NOOPTransport{
		req:     &noopRequest{},
		connReq: &noopConnectRequest{},
	}
}

// Start starts the NOOPTransport instance.
func (g *NOOPTransport) Start() error {
	return nil
}

// Close closes the NOOPTransport instance.
func (g *NOOPTransport) Close() error {
	return nil
}

// GetConnection returns a connection.
func (g *NOOPTransport) GetConnection(ctx context.Context,
	target string) (raftio.IConnection, error) {
	atomic.AddUint64(&g.tryConnect, 1)
	if g.connReq.Fail() {
		return nil, ErrRequestedToFail
	}
	atomic.AddUint64(&g.connected, 1)
	return &NOOPConnection{req: g.req}, nil
}

// GetSnapshotConnection returns a snapshot connection.
func (g *NOOPTransport) GetSnapshotConnection(ctx context.Context,
	target string) (raftio.ISnapshotConnection, error) {
	atomic.AddUint64(&g.tryConnect, 1)
	if g.connReq.Fail() {
		return nil, ErrRequestedToFail
	}
	atomic.AddUint64(&g.connected, 1)
	return &NOOPSnapshotConnection{req: g.req}, nil
}

// Name returns the module name.
func (g *NOOPTransport) Name() string {
	return NOOPRaftName
}
````

## File: internal/transport/snapshot.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
//
// This file contains code derived from CockroachDB. The asyncSendSnapshot
// method, connectAndProcessSnapshot method and the processSnapshotQueue
// method is similar to the one used in CockroachDB.
//
// Copyright 2014 The Cockroach Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
// implied. See the License for the specific language governing
// permissions and limitations under the License.

package transport

import (
	"sync/atomic"

	"github.com/cockroachdb/errors"

	"github.com/lni/dragonboat/v4/internal/rsm"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

var (
	snapshotChunkSize  = settings.SnapshotChunkSize
	maxConnectionCount = settings.Soft.MaxSnapshotConnections
)

// SendSnapshot asynchronously sends raft snapshot message to its target.
func (t *Transport) SendSnapshot(m pb.Message) bool {
	if !t.sendSnapshot(m) {
		plog.Errorf("failed to send snapshot to %s", dn(m.ShardID, m.To))
		t.sendSnapshotNotification(m.ShardID, m.To, true)
		return false
	}
	return true
}

// GetStreamSink returns a connection used for streaming snapshot.
func (t *Transport) GetStreamSink(shardID uint64, replicaID uint64) *Sink {
	s := t.getStreamSink(shardID, replicaID)
	if s == nil {
		plog.Errorf("failed to connect to %s", dn(shardID, replicaID))
		t.sendSnapshotNotification(shardID, replicaID, true)
	}
	return s
}

func (t *Transport) getStreamSink(shardID uint64, replicaID uint64) *Sink {
	addr, _, err := t.resolver.Resolve(shardID, replicaID)
	if err != nil {
		return nil
	}
	if !t.GetCircuitBreaker(addr).Ready() {
		plog.Warningf("circuit breaker for %s is not ready", addr)
		return nil
	}
	key := raftio.GetNodeInfo(shardID, replicaID)
	if job := t.createJob(key, addr, true, 0); job != nil {
		shutdown := func() {
			atomic.AddUint64(&t.jobs, ^uint64(0))
		}
		t.stopper.RunWorker(func() {
			t.processSnapshot(job, addr)
			shutdown()
		})
		return &Sink{j: job}
	}
	return nil
}

func (t *Transport) sendSnapshot(m pb.Message) bool {
	if !t.doSendSnapshot(m) {
		if err := m.Snapshot.Unref(); err != nil {
			panic(err)
		}
		return false
	}
	return true
}

func (t *Transport) doSendSnapshot(m pb.Message) bool {
	toReplicaID := m.To
	shardID := m.ShardID
	if m.Type != pb.InstallSnapshot {
		panic("not a snapshot message")
	}
	chunks, err := splitSnapshotMessage(m, t.fs)
	if err != nil {
		plog.Errorf("failed to get snapshot chunks %+v", err)
		return false
	}
	addr, _, err := t.resolver.Resolve(shardID, toReplicaID)
	if err != nil {
		return false
	}
	if !t.GetCircuitBreaker(addr).Ready() {
		t.metrics.snapshotCnnectionFailure()
		return false
	}
	key := raftio.GetNodeInfo(shardID, toReplicaID)
	job := t.createJob(key, addr, false, len(chunks))
	if job == nil {
		return false
	}
	shutdown := func() {
		atomic.AddUint64(&t.jobs, ^uint64(0))
		if err := m.Snapshot.Unref(); err != nil {
			panic(err)
		}
	}
	t.stopper.RunWorker(func() {
		t.processSnapshot(job, addr)
		shutdown()
	})
	job.addSnapshot(chunks)
	return true
}

func (t *Transport) createJob(key raftio.NodeInfo,
	addr string, streaming bool, sz int) *job {
	if v := atomic.AddUint64(&t.jobs, 1); v > maxConnectionCount {
		r := atomic.AddUint64(&t.jobs, ^uint64(0))
		plog.Warningf("job count is rate limited %d", r)
		return nil
	}
	job := newJob(t.ctx, key.ShardID, key.ReplicaID, t.nhConfig.GetDeploymentID(),
		streaming, sz, t.trans, t.stopper.ShouldStop(), t.fs)
	job.postSend = t.postSend
	job.preSend = t.preSend
	return job
}

func (t *Transport) processSnapshot(c *job, addr string) {
	breaker := t.GetCircuitBreaker(addr)
	successes := breaker.Successes()
	consecFailures := breaker.ConsecFailures()
	shardID := c.shardID
	replicaID := c.replicaID
	if err := func() error {
		if err := c.connect(addr); err != nil {
			plog.Warningf("failed to get snapshot conn to %s", dn(shardID, replicaID))
			t.sendSnapshotNotification(shardID, replicaID, true)
			close(c.failed)
			t.metrics.snapshotCnnectionFailure()
			return err
		}
		defer c.close()
		breaker.Success()
		if successes == 0 || consecFailures > 0 {
			plog.Debugf("snapshot stream to %s (%s) established",
				dn(shardID, replicaID), addr)
			t.sysEvents.ConnectionEstablished(addr, true)
		}
		err := c.process()
		if err != nil {
			plog.Errorf("snapshot chunk processing failed: %v", err)
		}
		t.sendSnapshotNotification(shardID, replicaID, err != nil)
		return err
	}(); err != nil {
		plog.Warningf("processSnapshot failed: %v", err)
		breaker.Fail()
		t.sysEvents.ConnectionFailed(addr, true)
	}
}

func (t *Transport) sendSnapshotNotification(shardID uint64,
	replicaID uint64, rejected bool) {
	if rejected {
		t.metrics.snapshotSendFailure()
	} else {
		t.metrics.snapshotSendSuccess()
	}
	t.msgHandler.HandleSnapshotStatus(shardID, replicaID, rejected)
	plog.Debugf("snapshot notification to %s added, reject %t",
		dn(shardID, replicaID), rejected)
}

func splitBySnapshotFile(msg pb.Message,
	filepath string, filesize uint64, startChunkID uint64,
	sf *pb.SnapshotFile) []pb.Chunk {
	if filesize == 0 {
		panic("empty file")
	}
	results := make([]pb.Chunk, 0)
	chunkCount := (filesize-1)/snapshotChunkSize + 1
	for i := uint64(0); i < chunkCount; i++ {
		var csz uint64
		if i == chunkCount-1 {
			csz = filesize - (chunkCount-1)*snapshotChunkSize
		} else {
			csz = snapshotChunkSize
		}
		c := pb.Chunk{
			BinVer:         raftio.TransportBinVersion,
			ShardID:        msg.ShardID,
			ReplicaID:      msg.To,
			From:           msg.From,
			FileChunkId:    i,
			FileChunkCount: chunkCount,
			ChunkId:        startChunkID + i,
			ChunkSize:      csz,
			Index:          msg.Snapshot.Index,
			Term:           msg.Snapshot.Term,
			OnDiskIndex:    msg.Snapshot.OnDiskIndex,
			Membership:     msg.Snapshot.Membership,
			Filepath:       filepath,
			FileSize:       filesize,
			Witness:        msg.Snapshot.Witness,
		}
		if sf != nil {
			c.HasFileInfo = true
			c.FileInfo = *sf
		}
		results = append(results, c)
	}
	return results
}

func getChunks(m pb.Message) []pb.Chunk {
	startChunkID := uint64(0)
	results := splitBySnapshotFile(m,
		m.Snapshot.Filepath, m.Snapshot.FileSize, startChunkID, nil)
	startChunkID += uint64(len(results))
	for _, snapshotFile := range m.Snapshot.Files {
		chunks := splitBySnapshotFile(m,
			snapshotFile.Filepath, snapshotFile.FileSize, startChunkID, snapshotFile)
		results = append(results, chunks...)
		startChunkID += uint64(len(chunks))
	}
	for idx := range results {
		results[idx].ChunkCount = uint64(len(results))
	}
	return results
}

func getWitnessChunk(m pb.Message, fs vfs.IFS) ([]pb.Chunk, error) {
	ss, err := rsm.GetWitnessSnapshot(fs)
	if err != nil {
		return nil, err
	}
	results := make([]pb.Chunk, 0)
	results = append(results, pb.Chunk{
		BinVer:         raftio.TransportBinVersion,
		ShardID:        m.ShardID,
		ReplicaID:      m.To,
		From:           m.From,
		FileChunkId:    0,
		FileChunkCount: 1,
		ChunkId:        0,
		ChunkCount:     1,
		ChunkSize:      uint64(len(ss)),
		Index:          m.Snapshot.Index,
		Term:           m.Snapshot.Term,
		OnDiskIndex:    0,
		Membership:     m.Snapshot.Membership,
		Filepath:       "witness.snapshot",
		FileSize:       uint64(len(ss)),
		Witness:        true,
		Data:           ss,
	})
	return results, nil
}

func splitSnapshotMessage(m pb.Message, fs vfs.IFS) ([]pb.Chunk, error) {
	if m.Type != pb.InstallSnapshot {
		panic("not a snapshot message")
	}
	if m.Snapshot.Witness {
		return getWitnessChunk(m, fs)
	}
	return getChunks(m), nil
}

func loadChunkData(chunk pb.Chunk,
	data []byte, fs vfs.IFS) (result []byte, err error) {
	f, err := openChunkFileForRead(chunk.Filepath, fs)
	if err != nil {
		return nil, err
	}
	defer func() {
		err = firstError(err, f.close())
	}()
	offset := chunk.FileChunkId * snapshotChunkSize
	if chunk.ChunkSize != uint64(len(data)) {
		data = make([]byte, chunk.ChunkSize)
	}
	n, err := f.readAt(data, int64(offset))
	if err != nil {
		return nil, err
	}
	if uint64(n) != chunk.ChunkSize {
		return nil, errors.New("failed to read the snapshot chunk")
	}
	return data, nil
}
````

## File: internal/transport/tcp_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package transport

import (
	"encoding/binary"
	"reflect"
	"testing"

	"github.com/stretchr/testify/require"
)

func TestRequstHeaderCanBeEncodedAndDecoded(t *testing.T) {
	r := requestHeader{
		method: raftType,
		size:   1024,
		crc:    1000,
	}
	buf := make([]byte, requestHeaderSize)
	result := r.encode(buf)
	require.Equal(t, requestHeaderSize, len(result), "unexpected size")
	rr := requestHeader{}
	require.True(t, rr.decode(result), "decode failed")
	require.True(t, reflect.DeepEqual(&r, &rr), "request header changed")
}

func TestRequestHeaderCRCIsChecked(t *testing.T) {
	r := requestHeader{
		method: raftType,
		size:   1024,
		crc:    1000,
	}
	buf := make([]byte, requestHeaderSize)
	result := r.encode(buf)
	require.Equal(t, requestHeaderSize, len(result), "unexpected size")
	rr := requestHeader{}
	require.True(t, rr.decode(result), "decode failed")
	crc := binary.BigEndian.Uint32(result[10:])
	binary.BigEndian.PutUint32(result[10:], crc+1)
	require.False(t, rr.decode(result), "crc error not reported")
	binary.BigEndian.PutUint32(result[10:], crc)
	require.True(t, rr.decode(result), "decode failed")
	binary.BigEndian.PutUint64(result[2:], 0)
	require.False(t, rr.decode(result), "crc error not reported")
}

func TestInvalidMethodNameIsReported(t *testing.T) {
	r := requestHeader{
		method: 1024,
		size:   1024,
		crc:    1000,
	}
	buf := make([]byte, requestHeaderSize)
	result := r.encode(buf)
	require.Equal(t, requestHeaderSize, len(result), "unexpected size")
	rr := requestHeader{}
	require.False(t, rr.decode(result), "decode did not report invalid method name")
}
````

## File: internal/transport/tcp.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package transport

import (
	"bytes"
	"context"
	"crypto/tls"
	"encoding/binary"
	"hash/crc32"
	"io"
	"net"
	"sync"
	"time"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/netutil"
	"github.com/lni/goutils/syncutil"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

var (
	// ErrBadMessage is the error returned to indicate the incoming message is
	// corrupted.
	ErrBadMessage       = errors.New("invalid message")
	errPoisonReceived   = errors.New("poison received")
	magicNumber         = [2]byte{0xAE, 0x7D}
	poisonNumber        = [2]byte{0x0, 0x0}
	payloadBufferSize   = settings.SnapshotChunkSize + 1024*128
	tlsHandshackTimeout = 10 * time.Second
	magicNumberDuration = 1 * time.Second
	headerDuration      = 2 * time.Second
	readDuration        = 5 * time.Second
	writeDuration       = 5 * time.Second
	keepAlivePeriod     = 10 * time.Second
	perConnBufSize      = settings.Soft.PerConnectionSendBufSize
	recvBufSize         = settings.Soft.PerConnectionRecvBufSize
)

const (
	// TCPTransportName is the name of the tcp transport module.
	TCPTransportName         = "go-tcp-transport"
	requestHeaderSize        = 18
	raftType          uint16 = 100
	snapshotType      uint16 = 200
)

type requestHeader struct {
	size   uint64
	crc    uint32
	method uint16
}

// TODO:
// TCP is never reliable [1]. dragonboat uses application layer crc32 checksum
// to help protecting raft state and log from some faulty network switches or
// buggy kernels. However, this is not necessary when TLS encryption is used.
// Update tcp.go to stop crc32 checking messages when TLS is used.
//
// [1] twitter's 2015 data corruption accident -
// https://www.evanjones.ca/checksum-failure-is-a-kernel-bug.html
// https://www.evanjones.ca/tcp-and-ethernet-checksums-fail.html
func (h *requestHeader) encode(buf []byte) []byte {
	if len(buf) < requestHeaderSize {
		panic("input buf too small")
	}
	binary.BigEndian.PutUint16(buf, h.method)
	binary.BigEndian.PutUint64(buf[2:], h.size)
	binary.BigEndian.PutUint32(buf[10:], 0)
	binary.BigEndian.PutUint32(buf[14:], h.crc)
	v := crc32.ChecksumIEEE(buf[:requestHeaderSize])
	binary.BigEndian.PutUint32(buf[10:], v)
	return buf[:requestHeaderSize]
}

func (h *requestHeader) decode(buf []byte) bool {
	if len(buf) < requestHeaderSize {
		return false
	}
	incoming := binary.BigEndian.Uint32(buf[10:])
	binary.BigEndian.PutUint32(buf[10:], 0)
	expected := crc32.ChecksumIEEE(buf[:requestHeaderSize])
	if incoming != expected {
		plog.Errorf("header crc check failed")
		return false
	}
	binary.BigEndian.PutUint32(buf[10:], incoming)
	method := binary.BigEndian.Uint16(buf)
	if method != raftType && method != snapshotType {
		plog.Errorf("invalid method type")
		return false
	}
	h.method = method
	h.size = binary.BigEndian.Uint64(buf[2:])
	h.crc = binary.BigEndian.Uint32(buf[14:])
	return true
}

func sendPoison(conn net.Conn, poison []byte) error {
	tt := time.Now().Add(magicNumberDuration).Add(magicNumberDuration)
	if err := conn.SetWriteDeadline(tt); err != nil {
		return err
	}
	if _, err := conn.Write(poison); err != nil {
		return err
	}
	return nil
}

func sendPoisonAck(conn net.Conn, poisonAck []byte) error {
	return sendPoison(conn, poisonAck)
}

func waitPoisonAck(conn net.Conn) {
	ack := make([]byte, len(poisonNumber))
	tt := time.Now().Add(keepAlivePeriod)
	if err := conn.SetReadDeadline(tt); err != nil {
		return
	}
	if _, err := io.ReadFull(conn, ack); err != nil {
		plog.Errorf("failed to get poison ack %v", err)
		return
	}
}

func writeMessage(conn net.Conn,
	header requestHeader, buf []byte, headerBuf []byte, encrypted bool) error {
	header.size = uint64(len(buf))
	if !encrypted {
		header.crc = crc32.ChecksumIEEE(buf)
	}
	headerBuf = header.encode(headerBuf)
	tt := time.Now().Add(magicNumberDuration).Add(headerDuration)
	if err := conn.SetWriteDeadline(tt); err != nil {
		return err
	}
	if _, err := conn.Write(magicNumber[:]); err != nil {
		return err
	}
	if _, err := conn.Write(headerBuf); err != nil {
		return err
	}
	sent := 0
	bufSize := int(recvBufSize)
	for sent < len(buf) {
		if sent+bufSize > len(buf) {
			bufSize = len(buf) - sent
		}
		tt = time.Now().Add(writeDuration)
		if err := conn.SetWriteDeadline(tt); err != nil {
			return err
		}
		if _, err := conn.Write(buf[sent : sent+bufSize]); err != nil {
			return err
		}
		sent += bufSize
	}
	if sent != len(buf) {
		plog.Panicf("sent %d, buf len %d", sent, len(buf))
	}
	return nil
}

func readMessage(conn net.Conn,
	header []byte, rbuf []byte, encrypted bool) (requestHeader, []byte, error) {
	tt := time.Now().Add(headerDuration)
	if err := conn.SetReadDeadline(tt); err != nil {
		return requestHeader{}, nil, err
	}
	if _, err := io.ReadFull(conn, header); err != nil {
		plog.Errorf("failed to get the header")
		return requestHeader{}, nil, err
	}
	rheader := requestHeader{}
	if !rheader.decode(header) {
		plog.Errorf("invalid header")
		return requestHeader{}, nil, ErrBadMessage
	}
	if rheader.size == 0 {
		plog.Errorf("invalid payload length")
		return requestHeader{}, nil, ErrBadMessage
	}
	var buf []byte
	if rheader.size > uint64(len(rbuf)) {
		buf = make([]byte, rheader.size)
	} else {
		buf = rbuf[:rheader.size]
	}
	received := uint64(0)
	var recvBuf []byte
	if rheader.size < recvBufSize {
		recvBuf = buf[:rheader.size]
	} else {
		recvBuf = buf[:recvBufSize]
	}
	toRead := rheader.size
	for toRead > 0 {
		tt = time.Now().Add(readDuration)
		if err := conn.SetReadDeadline(tt); err != nil {
			return requestHeader{}, nil, err
		}
		if _, err := io.ReadFull(conn, recvBuf); err != nil {
			return requestHeader{}, nil, err
		}
		toRead -= uint64(len(recvBuf))
		received += uint64(len(recvBuf))
		if toRead < recvBufSize {
			recvBuf = buf[received : received+toRead]
		} else {
			recvBuf = buf[received : received+recvBufSize]
		}
	}
	if received != rheader.size {
		panic("unexpected size")
	}
	if !encrypted && crc32.ChecksumIEEE(buf) != rheader.crc {
		plog.Errorf("invalid payload checksum")
		return requestHeader{}, nil, ErrBadMessage
	}
	return rheader, buf, nil
}

func readMagicNumber(conn net.Conn, magicNum []byte) error {
	tt := time.Now().Add(magicNumberDuration)
	if err := conn.SetReadDeadline(tt); err != nil {
		return err
	}
	if _, err := io.ReadFull(conn, magicNum); err != nil {
		return err
	}
	if bytes.Equal(magicNum, poisonNumber[:]) {
		return errPoisonReceived
	}
	if !bytes.Equal(magicNum, magicNumber[:]) {
		return ErrBadMessage
	}
	return nil
}

type connection struct {
	conn net.Conn
}

func newConnection(conn net.Conn) net.Conn {
	return &connection{conn: conn}
}

func (c *connection) Close() error {
	return c.conn.Close()
}

func (c *connection) Read(b []byte) (int, error) {
	return c.conn.Read(b)
}

func (c *connection) Write(b []byte) (int, error) {
	return c.conn.Write(b)
}

func (c *connection) LocalAddr() net.Addr {
	panic("not implemented")
}

func (c *connection) RemoteAddr() net.Addr {
	panic("not implemented")
}

func (c *connection) SetDeadline(t time.Time) error {
	return c.conn.SetDeadline(t)
}

func (c *connection) SetReadDeadline(t time.Time) error {
	return c.conn.SetReadDeadline(t)
}

func (c *connection) SetWriteDeadline(t time.Time) error {
	return c.conn.SetWriteDeadline(t)
}

// TCPConnection is the connection used for sending raft messages to remote
// nodes.
type TCPConnection struct {
	conn      net.Conn
	header    []byte
	payload   []byte
	encrypted bool
}

var _ raftio.IConnection = (*TCPConnection)(nil)

// NewTCPConnection creates and returns a new TCPConnection instance.
func NewTCPConnection(conn net.Conn, encrypted bool) *TCPConnection {
	return &TCPConnection{
		conn:      newConnection(conn),
		header:    make([]byte, requestHeaderSize),
		payload:   make([]byte, perConnBufSize),
		encrypted: encrypted,
	}
}

// Close closes the TCPConnection instance.
func (c *TCPConnection) Close() {
	if err := c.conn.Close(); err != nil {
		plog.Errorf("failed to close the connection %v", err)
	}
}

// SendMessageBatch sends a raft message batch to remote node.
func (c *TCPConnection) SendMessageBatch(batch pb.MessageBatch) error {
	header := requestHeader{method: raftType}
	sz := batch.SizeUpperLimit()
	var buf []byte
	if len(c.payload) < sz {
		buf = make([]byte, sz)
	} else {
		buf = c.payload
	}
	buf = pb.MustMarshalTo(&batch, buf)
	return writeMessage(c.conn, header, buf, c.header, c.encrypted)
}

// TCPSnapshotConnection is the connection for sending raft snapshot chunks to
// remote nodes.
type TCPSnapshotConnection struct {
	conn      net.Conn
	header    []byte
	encrypted bool
}

var _ raftio.ISnapshotConnection = (*TCPSnapshotConnection)(nil)

// NewTCPSnapshotConnection creates and returns a new snapshot connection.
func NewTCPSnapshotConnection(conn net.Conn,
	encrypted bool) *TCPSnapshotConnection {
	return &TCPSnapshotConnection{
		conn:      newConnection(conn),
		header:    make([]byte, requestHeaderSize),
		encrypted: encrypted,
	}
}

// Close closes the snapshot connection.
func (c *TCPSnapshotConnection) Close() {
	defer func() {
		if err := c.conn.Close(); err != nil {
			plog.Debugf("failed to close the connection %v", err)
		}
	}()
	if err := sendPoison(c.conn, poisonNumber[:]); err != nil {
		return
	}
	waitPoisonAck(c.conn)
}

// SendChunk sends the specified snapshot chunk to remote node.
func (c *TCPSnapshotConnection) SendChunk(chunk pb.Chunk) error {
	header := requestHeader{method: snapshotType}
	sz := chunk.Size()
	buf := make([]byte, sz)
	buf = pb.MustMarshalTo(&chunk, buf)
	return writeMessage(c.conn, header, buf, c.header, c.encrypted)
}

// TCP is a TCP based transport module for exchanging raft messages and
// snapshots between NodeHost instances.
type TCP struct {
	stopper        *syncutil.Stopper
	connStopper    *syncutil.Stopper
	requestHandler raftio.MessageHandler
	chunkHandler   raftio.ChunkHandler
	nhConfig       config.NodeHostConfig
	encrypted      bool
}

var _ raftio.ITransport = (*TCP)(nil)

// NewTCPTransport creates and returns a new TCP transport module.
func NewTCPTransport(nhConfig config.NodeHostConfig,
	requestHandler raftio.MessageHandler,
	chunkHandler raftio.ChunkHandler) raftio.ITransport {
	return &TCP{
		nhConfig:       nhConfig,
		stopper:        syncutil.NewStopper(),
		connStopper:    syncutil.NewStopper(),
		requestHandler: requestHandler,
		chunkHandler:   chunkHandler,
		encrypted:      nhConfig.MutualTLS,
	}
}

// Start starts the TCP transport module.
func (t *TCP) Start() error {
	address := t.nhConfig.GetListenAddress()
	tlsConfig, err := t.nhConfig.GetServerTLSConfig()
	if err != nil {
		return err
	}
	listener, err := netutil.NewStoppableListener(address,
		tlsConfig, t.stopper.ShouldStop())
	if err != nil {
		return err
	}
	t.connStopper.RunWorker(func() {
		// sync.WaitGroup's doc mentions that
		// "Note that calls with a positive delta that occur when the counter is
		//  zero must happen before a Wait."
		// It is unclear that whether the stdlib is going complain in future
		// releases when Wait() is called when the counter is zero and Add() with
		// positive delta has never been called.
		<-t.connStopper.ShouldStop()
	})
	t.stopper.RunWorker(func() {
		for {
			conn, err := listener.Accept()
			if err != nil {
				if err == netutil.ErrListenerStopped {
					return
				}
				panic(err)
			}
			var once sync.Once
			connCloseCh := make(chan struct{})
			closeFn := func() {
				once.Do(func() {
					select {
						case connCloseCh <- struct{}{}:
						default:
					}
					if err := conn.Close(); err != nil {
						plog.Errorf("failed to close the connection %v", err)
					}
				})
			}
			t.connStopper.RunWorker(func() {
				select {
				case <-t.stopper.ShouldStop():
				case <-connCloseCh:
				}
				closeFn()
			})
			t.connStopper.RunWorker(func() {
				t.serveConn(conn)
				closeFn()
			})
		}
	})
	return nil
}

// Close closes the TCP transport module.
func (t *TCP) Close() error {
	t.stopper.Stop()
	t.connStopper.Stop()
	return nil
}

// GetConnection returns a new raftio.IConnection for sending raft messages.
func (t *TCP) GetConnection(ctx context.Context,
	target string) (raftio.IConnection, error) {
	conn, err := t.getConnection(ctx, target)
	if err != nil {
		return nil, err
	}
	return NewTCPConnection(conn, t.encrypted), nil
}

// GetSnapshotConnection returns a new raftio.IConnection for sending raft
// snapshots.
func (t *TCP) GetSnapshotConnection(ctx context.Context,
	target string) (raftio.ISnapshotConnection, error) {
	conn, err := t.getConnection(ctx, target)
	if err != nil {
		return nil, err
	}
	return NewTCPSnapshotConnection(conn, t.encrypted), nil
}

// Name returns a human readable name of the TCP transport module.
func (t *TCP) Name() string {
	return TCPTransportName
}

func (t *TCP) serveConn(conn net.Conn) {
	magicNum := make([]byte, len(magicNumber))
	header := make([]byte, requestHeaderSize)
	tbuf := make([]byte, payloadBufferSize)
	for {
		err := readMagicNumber(conn, magicNum)
		if err != nil {
			if errors.Is(err, errPoisonReceived) {
				if err := sendPoisonAck(conn, poisonNumber[:]); err != nil {
					plog.Debugf("failed to send poison ack %v", err)
				}
				return
			}
			if errors.Is(err, ErrBadMessage) {
				return
			}
			operr, ok := err.(net.Error)
			if ok && operr.Timeout() {
				continue
			} else {
				return
			}
		}
		rheader, buf, err := readMessage(conn, header, tbuf, t.encrypted)
		if err != nil {
			return
		}
		if rheader.method == raftType {
			batch := pb.MessageBatch{}
			if err := batch.Unmarshal(buf); err != nil {
				return
			}
			t.requestHandler(batch)
		} else {
			chunk := pb.Chunk{}
			if err := chunk.Unmarshal(buf); err != nil {
				return
			}
			if !t.chunkHandler(chunk) {
				plog.Errorf("chunk rejected %s", chunkKey(chunk))
				return
			}
		}
	}
}

func setTCPConn(conn *net.TCPConn) error {
	if err := conn.SetLinger(0); err != nil {
		return err
	}
	if err := conn.SetKeepAlive(true); err != nil {
		return err
	}
	return conn.SetKeepAlivePeriod(keepAlivePeriod)
}

// FIXME:
// context.Context is ignored
func (t *TCP) getConnection(ctx context.Context,
	target string) (net.Conn, error) {
	timeout := time.Duration(dialTimeoutSecond) * time.Second
	conn, err := net.DialTimeout("tcp", target, timeout)
	if err != nil {
		return nil, err
	}
	tcpconn, ok := conn.(*net.TCPConn)
	if ok {
		if err := setTCPConn(tcpconn); err != nil {
			return nil, err
		}
	}
	tlsConfig, err := t.nhConfig.GetClientTLSConfig(target)
	if err != nil {
		return nil, err
	}
	if tlsConfig != nil {
		conn = tls.Client(conn, tlsConfig)
		tt := time.Now().Add(tlsHandshackTimeout)
		if err := conn.SetDeadline(tt); err != nil {
			return nil, err
		}
		if err := conn.(*tls.Conn).Handshake(); err != nil {
			return nil, err
		}
	}
	return conn, nil
}
````

## File: internal/transport/transport_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package transport

import (
	"bytes"
	"crypto/md5"
	"crypto/rand"
	"fmt"
	"io"
	"os"
	"strconv"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/lni/goutils/leaktest"
	"github.com/lni/goutils/netutil"
	"github.com/lni/goutils/syncutil"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/registry"
	"github.com/lni/dragonboat/v4/internal/rsm"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/raftio"
	"github.com/lni/dragonboat/v4/raftpb"
)

var serverAddress = fmt.Sprintf("localhost:%d", getTestPort())

const (
	snapshotDir       = "gtransport_test_data_safe_to_delete"
	caFile            = "tests/test-root-ca.crt"
	certFile          = "tests/localhost.crt"
	keyFile           = "tests/localhost.key"
	testSnapshotIndex = uint64(12345)
)

const defaultTestPort = 26001

func getTestPort() int {
	pv := os.Getenv("DRAGONBOAT_TEST_PORT")
	if len(pv) > 0 {
		port, err := strconv.Atoi(pv)
		if err != nil {
			panic(err)
		}
		return port
	}
	return defaultTestPort
}

type dummyTransportEvent struct{}

func (d *dummyTransportEvent) ConnectionEstablished(addr string, snapshot bool) {}
func (d *dummyTransportEvent) ConnectionFailed(addr string, snapshot bool)      {}

type testSnapshotDir struct {
	fs vfs.IFS
}

func newTestSnapshotDir(fs vfs.IFS) *testSnapshotDir {
	return &testSnapshotDir{fs: fs}
}

func (g *testSnapshotDir) GetSnapshotRootDir(shardID uint64,
	replicaID uint64) string {
	snapNodeDir := fmt.Sprintf("snapshot-%d-%d", shardID, replicaID)
	return g.fs.PathJoin(snapshotDir, snapNodeDir)
}

func (g *testSnapshotDir) GetSnapshotDir(shardID uint64,
	replicaID uint64, lastApplied uint64) string {
	snapNodeDir := fmt.Sprintf("snapshot-%d-%d", shardID, replicaID)
	snapDir := fmt.Sprintf("snapshot-%016X", lastApplied)
	d := g.fs.PathJoin(snapshotDir, snapNodeDir, snapDir)
	return d
}

func (g *testSnapshotDir) getSnapshotFileMD5(shardID uint64,
	replicaID uint64, index uint64, filename string) ([]byte, error) {
	snapDir := g.GetSnapshotDir(shardID, replicaID, index)
	fp := g.fs.PathJoin(snapDir, filename)
	f, err := g.fs.Open(fp)
	if err != nil {
		return nil, err
	}
	defer func() {
		if err := f.Close(); err != nil {
			panic(err)
		}
	}()
	h := md5.New()
	if _, err := io.Copy(h, f); err != nil {
		return nil, err
	}
	return h.Sum(nil), nil
}

func (g *testSnapshotDir) generateSnapshotExternalFile(shardID uint64,
	replicaID uint64, index uint64, filename string, sz uint64) {
	snapDir := g.GetSnapshotDir(shardID, replicaID, index)
	if err := g.fs.MkdirAll(snapDir, 0755); err != nil {
		panic(err)
	}
	fp := g.fs.PathJoin(snapDir, filename)
	data := make([]byte, sz)
	if _, err := rand.Read(data); err != nil {
		panic(err)
	}
	f, err := g.fs.Create(fp)
	if err != nil {
		panic(err)
	}
	n, err := f.Write(data)
	if n != len(data) {
		panic("failed to write all files")
	}
	if err != nil {
		panic(err)
	}
	if err := f.Close(); err != nil {
		panic(err)
	}
}

func (g *testSnapshotDir) generateSnapshotFile(shardID uint64,
	replicaID uint64, index uint64, filename string, sz uint64, fs vfs.IFS) {
	snapDir := g.GetSnapshotDir(shardID, replicaID, index)
	if err := g.fs.MkdirAll(snapDir, 0755); err != nil {
		panic(err)
	}
	fp := g.fs.PathJoin(snapDir, filename)
	data := make([]byte, sz)
	if _, err := rand.Read(data); err != nil {
		panic(err)
	}
	writer, err := rsm.NewSnapshotWriter(fp, raftpb.NoCompression, fs)
	if err != nil {
		panic(err)
	}
	n, err := writer.Write(data)
	if n != len(data) {
		panic("short write")
	}
	if err != nil {
		panic(err)
	}
	/*n, err = writer.Write(data)
	if n != len(data) {
		panic("short write")
	}
	if err != nil {
		panic(err)
	}*/
	if err := writer.Close(); err != nil {
		panic(err)
	}
}

func (g *testSnapshotDir) cleanup() {
	if err := g.fs.RemoveAll(snapshotDir); err != nil {
		panic(err)
	}
}

type testMessageHandler struct {
	mu                        sync.Mutex
	requestCount              map[raftio.NodeInfo]uint64
	unreachableCount          map[raftio.NodeInfo]uint64
	snapshotCount             map[raftio.NodeInfo]uint64
	snapshotFailedCount       map[raftio.NodeInfo]uint64
	snapshotSuccessCount      map[raftio.NodeInfo]uint64
	receivedSnapshotCount     map[raftio.NodeInfo]uint64
	receivedSnapshotFromCount map[raftio.NodeInfo]uint64
}

func newTestMessageHandler() *testMessageHandler {
	return &testMessageHandler{
		requestCount:              make(map[raftio.NodeInfo]uint64),
		unreachableCount:          make(map[raftio.NodeInfo]uint64),
		snapshotCount:             make(map[raftio.NodeInfo]uint64),
		snapshotFailedCount:       make(map[raftio.NodeInfo]uint64),
		snapshotSuccessCount:      make(map[raftio.NodeInfo]uint64),
		receivedSnapshotCount:     make(map[raftio.NodeInfo]uint64),
		receivedSnapshotFromCount: make(map[raftio.NodeInfo]uint64),
	}
}

func (h *testMessageHandler) HandleMessageBatch(reqs raftpb.MessageBatch) (uint64, uint64) {
	h.mu.Lock()
	defer h.mu.Unlock()
	ss := uint64(0)
	msg := uint64(0)
	for _, req := range reqs.Requests {
		epk := raftio.GetNodeInfo(req.ShardID, req.To)
		v, ok := h.requestCount[epk]
		if ok {
			h.requestCount[epk] = v + 1
		} else {
			h.requestCount[epk] = 1
		}
		if req.Type == raftpb.InstallSnapshot {
			ss++
			v, ok = h.snapshotCount[epk]
			if ok {
				h.snapshotCount[epk] = v + 1
			} else {
				h.snapshotCount[epk] = 1
			}
		} else {
			msg++
		}
	}
	return ss, msg
}

func (h *testMessageHandler) HandleSnapshotStatus(shardID uint64,
	replicaID uint64, failed bool) {
	h.mu.Lock()
	defer h.mu.Unlock()
	epk := raftio.GetNodeInfo(shardID, replicaID)
	var p *map[raftio.NodeInfo]uint64
	if failed {
		p = &h.snapshotFailedCount
	} else {
		p = &h.snapshotSuccessCount
	}
	v, ok := (*p)[epk]
	if ok {
		(*p)[epk] = v + 1
	} else {
		(*p)[epk] = 1
	}
}

func (h *testMessageHandler) HandleUnreachable(shardID uint64,
	replicaID uint64) {
	h.mu.Lock()
	defer h.mu.Unlock()
	epk := raftio.GetNodeInfo(shardID, replicaID)
	v, ok := h.unreachableCount[epk]
	if ok {
		h.unreachableCount[epk] = v + 1
	} else {
		h.unreachableCount[epk] = 1
	}
}

func (h *testMessageHandler) HandleSnapshot(shardID uint64,
	replicaID uint64, from uint64) {
	h.mu.Lock()
	defer h.mu.Unlock()
	epk := raftio.GetNodeInfo(shardID, replicaID)
	v, ok := h.receivedSnapshotCount[epk]
	if ok {
		h.receivedSnapshotCount[epk] = v + 1
	} else {
		h.receivedSnapshotCount[epk] = 1
	}
	epk.ReplicaID = from
	v, ok = h.receivedSnapshotFromCount[epk]
	if ok {
		h.receivedSnapshotFromCount[epk] = v + 1
	} else {
		h.receivedSnapshotFromCount[epk] = 1
	}
}

func (h *testMessageHandler) getReceivedSnapshotCount(shardID uint64,
	replicaID uint64) uint64 {
	return h.getMessageCount(h.receivedSnapshotCount, shardID, replicaID)
}

func (h *testMessageHandler) getReceivedSnapshotFromCount(shardID uint64,
	replicaID uint64) uint64 {
	return h.getMessageCount(h.receivedSnapshotFromCount, shardID, replicaID)
}

func (h *testMessageHandler) getRequestCount(shardID uint64,
	replicaID uint64) uint64 {
	return h.getMessageCount(h.requestCount, shardID, replicaID)
}

func (h *testMessageHandler) getFailedSnapshotCount(shardID uint64,
	replicaID uint64) uint64 {
	return h.getMessageCount(h.snapshotFailedCount, shardID, replicaID)
}

func (h *testMessageHandler) getSnapshotSuccessCount(shardID uint64,
	replicaID uint64) uint64 {
	return h.getMessageCount(h.snapshotSuccessCount, shardID, replicaID)
}

func (h *testMessageHandler) getSnapshotCount(shardID uint64,
	replicaID uint64) uint64 {
	return h.getMessageCount(h.snapshotCount, shardID, replicaID)
}

func (h *testMessageHandler) getMessageCount(m map[raftio.NodeInfo]uint64,
	shardID uint64, replicaID uint64) uint64 {
	h.mu.Lock()
	defer h.mu.Unlock()
	epk := raftio.GetNodeInfo(shardID, replicaID)
	v, ok := m[epk]
	if ok {
		return v
	}
	return 0
}

func newNOOPTestTransport(handler IMessageHandler, fs vfs.IFS) (*Transport,
	*registry.Registry, *NOOPTransport, *noopRequest, *noopConnectRequest) {
	t := newTestSnapshotDir(fs)
	nodes := registry.NewNodeRegistry(settings.Soft.StreamConnections, nil)
	c := config.NodeHostConfig{
		MaxSendQueueSize: 256 * 1024 * 1024,
		RaftAddress:      "localhost:9876",
		Expert: config.ExpertConfig{
			TransportFactory: &NOOPTransportFactory{},
		},
	}
	env, err := server.NewEnv(c, fs)
	if err != nil {
		panic(err)
	}
	transport, err := NewTransport(c,
		handler, env, nodes, t.GetSnapshotRootDir, &dummyTransportEvent{}, fs)
	if err != nil {
		panic(err)
	}
	trans, ok := transport.trans.(*NOOPTransport)
	if !ok {
		panic("not a noop transport")
	}
	return transport, nodes, trans, trans.req, trans.connReq
}

func newTestTransport(handler IMessageHandler,
	mutualTLS bool, fs vfs.IFS) (*Transport, *registry.Registry,
	*syncutil.Stopper, *testSnapshotDir) {
	stopper := syncutil.NewStopper()
	nodes := registry.NewNodeRegistry(settings.Soft.StreamConnections, nil)
	t := newTestSnapshotDir(fs)
	c := config.NodeHostConfig{
		RaftAddress: serverAddress,
	}
	if mutualTLS {
		c.MutualTLS = true
		c.CAFile = caFile
		c.CertFile = certFile
		c.KeyFile = keyFile
	}
	env, err := server.NewEnv(c, fs)
	if err != nil {
		panic(err)
	}
	transport, err := NewTransport(c,
		handler, env, nodes, t.GetSnapshotRootDir, &dummyTransportEvent{}, fs)
	if err != nil {
		panic(err)
	}
	return transport, nodes, stopper, t
}

func testMessageCanBeSent(t *testing.T, mutualTLS bool, sz uint64, fs vfs.IFS) {
	handler := newTestMessageHandler()
	trans, nodes, stopper, _ := newTestTransport(handler, mutualTLS, fs)
	defer func() {
		err := trans.env.Close()
		require.NoError(t, err, "failed to stop the env")
	}()
	defer func() {
		err := trans.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	defer stopper.Stop()
	nodes.Add(100, 2, serverAddress)
	for i := 0; i < 20; i++ {
		msg := raftpb.Message{
			Type:    raftpb.Heartbeat,
			To:      2,
			ShardID: 100,
		}
		done := trans.Send(msg)
		assert.True(t, done, "failed to send message")
	}
	done := false
	for i := 0; i < 200; i++ {
		time.Sleep(100 * time.Millisecond)
		count := handler.getRequestCount(100, 2)
		plog.Infof("%d test messages received", count)
		if count == 20 {
			done = true
			break
		} else {
			plog.Infof("count: %d, want 20, will wait for 100ms", count)
		}
	}
	assert.True(t, done, "failed to get all 20 sent messages")
	// test to ensure a single big message can be sent/received.
	plog.Infof("sending a test msg with payload sz %d", sz)
	payload := make([]byte, sz)
	m := raftpb.Message{
		Type:    raftpb.Replicate,
		To:      2,
		ShardID: 100,
		Entries: []raftpb.Entry{
			{
				Cmd: payload,
			},
		},
	}
	plog.Infof("msg ready to be sent")
	ok := trans.Send(m)
	assert.True(t, ok, "failed to send the large msg")
	received := false
	for i := 0; i < 400; i++ {
		time.Sleep(100 * time.Millisecond)
		if handler.getRequestCount(100, 2) == 21 {
			received = true
			break
		}
	}
	assert.True(t, received, "got %d, want %d",
		handler.getRequestCount(100, 2), 21)
}

func TestMessageCanBeSent(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	testMessageCanBeSent(t, false, settings.LargeEntitySize+1, fs)
	testMessageCanBeSent(t, false, recvBufSize/2, fs)
	testMessageCanBeSent(t, false, recvBufSize+1, fs)
	testMessageCanBeSent(t, false, perConnBufSize+1, fs)
	testMessageCanBeSent(t, false, perConnBufSize/2, fs)
	testMessageCanBeSent(t, false, 1, fs)
	testMessageCanBeSent(t, true, settings.LargeEntitySize+1, fs)
	testMessageCanBeSent(t, true, recvBufSize/2, fs)
	testMessageCanBeSent(t, true, recvBufSize+1, fs)
	testMessageCanBeSent(t, true, perConnBufSize+1, fs)
	testMessageCanBeSent(t, true, perConnBufSize/2, fs)
	testMessageCanBeSent(t, true, 1, fs)
}

// add some latency to localhost
// sudo tc qdisc add dev lo root handle 1:0 netem delay 100msec
// remove latency
// sudo tc qdisc del dev lo root
// don't forget to change your TCP window size if necessary
// e.g. in our dev environment, we have -
// net.core.wmem_max = 25165824
// net.core.rmem_max = 25165824
// net.ipv4.tcp_rmem = 4096 87380 25165824
// net.ipv4.tcp_wmem = 4096 87380 25165824
func testMessageCanBeSentWithLargeLatency(t *testing.T, mutualTLS bool, fs vfs.IFS) {
	handler := newTestMessageHandler()
	trans, nodes, stopper, _ := newTestTransport(handler, mutualTLS, fs)
	defer func() {
		err := trans.env.Close()
		require.NoError(t, err, "failed to stop the env")
	}()
	defer func() {
		err := trans.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	defer stopper.Stop()
	nodes.Add(100, 2, serverAddress)
	for i := 0; i < 128; i++ {
		msg := raftpb.Message{
			Type:    raftpb.Replicate,
			To:      2,
			ShardID: 100,
			Entries: []raftpb.Entry{{Cmd: make([]byte, 1024)}},
		}
		done := trans.Send(msg)
		assert.True(t, done, "failed to send message")
	}
	done := false
	for i := 0; i < 400; i++ {
		time.Sleep(100 * time.Millisecond)
		if handler.getRequestCount(100, 2) == 128 {
			done = true
			break
		}
	}
	assert.True(t, done, "failed to send/receive all messages")
}

// latency need to be simulated by configuring your environment
func TestMessageCanBeSentWithLargeLatency(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	testMessageCanBeSentWithLargeLatency(t, true, fs)
	testMessageCanBeSentWithLargeLatency(t, false, fs)
}

func testMessageBatchWithNotMatchedDBVAreDropped(t *testing.T,
	f SendMessageBatchFunc, mutualTLS bool, fs vfs.IFS) {
	handler := newTestMessageHandler()
	trans, nodes, stopper, _ := newTestTransport(handler, mutualTLS, fs)
	defer func() {
		err := trans.env.Close()
		require.NoError(t, err, "failed to stop the env")
	}()
	defer func() {
		err := trans.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	defer stopper.Stop()
	nodes.Add(100, 2, serverAddress)
	trans.SetPreSendBatchHook(f)
	for i := 0; i < 100; i++ {
		msg := raftpb.Message{
			Type:    raftpb.Heartbeat,
			To:      2,
			ShardID: 100,
		}
		// silently drop
		done := trans.Send(msg)
		assert.True(t, done, "failed to send message")
	}
	time.Sleep(100 * time.Millisecond)
	assert.Equal(t, uint64(0), handler.getRequestCount(100, 2),
		"got %d, want %d", handler.getRequestCount(100, 2), 0)
}

func TestMessageBatchWithNotMatchedDeploymentIDAreDropped(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	f := func(b raftpb.MessageBatch) (raftpb.MessageBatch, bool) {
		b.DeploymentId = 2
		return b, true
	}
	testMessageBatchWithNotMatchedDBVAreDropped(t, f, true, fs)
	testMessageBatchWithNotMatchedDBVAreDropped(t, f, false, fs)
}

func TestMessageBatchWithNotMatchedBinVerAreDropped(t *testing.T) {
	defer leaktest.AfterTest(t)()
	f := func(b raftpb.MessageBatch) (raftpb.MessageBatch, bool) {
		b.BinVer = raftio.TransportBinVersion + 1
		return b, true
	}
	fs := vfs.GetTestFS()
	testMessageBatchWithNotMatchedDBVAreDropped(t, f, true, fs)
	testMessageBatchWithNotMatchedDBVAreDropped(t, f, false, fs)
}

func TestCircuitBreaker(t *testing.T) {
	defer leaktest.AfterTest(t)()
	breaker := netutil.NewBreaker()
	breaker.Fail()
	assert.False(t, breaker.Ready(), "breaker is still ready?")
}

func (t *Transport) queueSize() int {
	t.mu.Lock()
	defer t.mu.Unlock()
	return len(t.mu.queues)
}

func TestCircuitBreakerKicksInOnConnectivityIssue(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	handler := newTestMessageHandler()
	trans, nodes, stopper, _ := newTestTransport(handler, false, fs)
	defer func() {
		err := trans.env.Close()
		require.NoError(t, err, "failed to stop the env")
	}()
	defer func() {
		err := trans.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	defer stopper.Stop()
	nodes.Add(100, 2, "nosuchhost:39001")
	msg := raftpb.Message{
		Type:    raftpb.Heartbeat,
		To:      2,
		From:    1,
		ShardID: 100,
	}
	done := trans.Send(msg)
	assert.True(t, done, "not suppose to fail")
	for trans.queueSize() != 0 {
		time.Sleep(100 * time.Millisecond)
	}
	time.Sleep(20 * time.Millisecond)
	breaker := trans.GetCircuitBreaker("nosuchhost:39001")
	assert.False(t, breaker.Ready(), "breaker is still ready?")
	time.Sleep(time.Second)
	assert.True(t, breaker.Ready(), "breaker is not ready after wait")
}

func getTestSnapshotMessage(to uint64) raftpb.Message {
	m := raftpb.Message{
		Type:    raftpb.InstallSnapshot,
		From:    12,
		To:      to,
		ShardID: 100,
		Snapshot: raftpb.Snapshot{
			Membership: raftpb.Membership{
				ConfigChangeId: 178,
			},
			Index: testSnapshotIndex,
			Term:  19,
		},
	}
	m.Snapshot.Load(&noopCompactor{})
	return m
}

func TestSnapshotCanBeSent(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	mutualTLSValues := []bool{true, false}
	for _, v := range mutualTLSValues {
		testSnapshotCanBeSent(t, snapshotChunkSize-1, 10000, v, fs)
		testSnapshotCanBeSent(t, snapshotChunkSize/2, 10000, v, fs)
		testSnapshotCanBeSent(t, snapshotChunkSize+1, 10000, v, fs)
		testSnapshotCanBeSent(t, snapshotChunkSize*3, 10000, v, fs)
		testSnapshotCanBeSent(t, snapshotChunkSize*3+1, 10000, v, fs)
		testSnapshotCanBeSent(t, snapshotChunkSize*3-1, 10000, v, fs)
	}
}

// FIXME: re-enable this test
/*
func testSourceAddressWillBeAddedToNodeRegistry(t *testing.T, mutualTLS bool, fs vfs.IFS) {
	handler := newTestMessageHandler()
	trans, nodes, stopper, _ := newTestTransport(handler, mutualTLS, fs)
	defer func() {
		if err := trans.env.Close(); err != nil {
			t.Fatalf("failed to stop the env %v", err)
		}
	}()
	defer func() {
		if err := trans.Close(); err != nil {
			t.Fatalf("failed to close the transport module %v", err)
		}
	}()
	defer stopper.Stop()
	nodes.Add(100, 2, serverAddress)
	msg := raftpb.Message{
		Type:      raftpb.Heartbeat,
		To:        2,
		From:      200,
		ShardID:   100,
	}
	done := trans.Send(msg)
	if !done {
		t.Errorf("not suppose to fail")
	}
	count := 0
	for count < 200 && handler.getRequestCount(100, 2) == 0 {
		count++
		time.Sleep(5 * time.Millisecond)
	}
	if count == 200 {
		t.Errorf("failed to send the message")
	}
	vc := 0
	nodes.addr.Range(func(k, v interface{}) bool {
		vc++
		return true
	})
	if vc != 2 {
		t.Errorf("remote address not updated")
	}
	key := raftio.GetNodeInfo(100, 200)
	v, ok := nodes.addr.Load(key)
	if !ok {
		t.Errorf("did not record source address")
	}
	if v.(string) != serverAddress {
		t.Errorf("v %s, want %s", v, serverAddress)
	}
}

func TestSourceAddressWillBeAddedToNodeRegistry(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	testSourceAddressWillBeAddedToNodeRegistry(t, true, fs)
	testSourceAddressWillBeAddedToNodeRegistry(t, false, fs)
}*/

func waitForTotalSnapshotStatusUpdateCount(handler *testMessageHandler,
	maxWait uint64, count uint64) {
	total := uint64(0)

	for total < maxWait {
		time.Sleep(10 * time.Millisecond)
		total += 10
		handler.mu.Lock()
		c := uint64(0)
		for _, v := range handler.snapshotFailedCount {
			c += v
		}
		for _, v := range handler.snapshotSuccessCount {
			c += v
		}
		handler.mu.Unlock()

		if c >= count {
			return
		}
	}

}

func waitForFirstSnapshotStatusUpdate(handler *testMessageHandler,
	maxWait uint64) {
	total := uint64(0)
	for total < maxWait {
		time.Sleep(10 * time.Millisecond)
		total += 10
		if handler.getFailedSnapshotCount(100, 2) > 0 ||
			handler.getSnapshotSuccessCount(100, 2) > 0 {
			return
		}
	}
}

func waitForSnapshotCountUpdate(handler *testMessageHandler, maxWait uint64) {
	total := uint64(0)
	for total < maxWait {
		time.Sleep(10 * time.Millisecond)
		total += 10
		count := handler.getReceivedSnapshotCount(100, 2)
		if count > 0 {
			return
		}
	}
}

func getTestSnapshotFileSize(sz uint64) uint64 {
	switch rsm.DefaultVersion {
	case rsm.V1:
		return sz + rsm.HeaderSize
	case rsm.V2:
		return rsm.GetV2PayloadSize(sz) + rsm.HeaderSize
	default:
		panic("unknown snapshot version")
	}
}

type noopCompactor struct{}

func (noopCompactor) Compact(uint64) error { return nil }

func testSnapshotCanBeSent(t *testing.T,
	sz uint64, maxWait uint64, mutualTLS bool, fs vfs.IFS) {
	handler := newTestMessageHandler()
	trans, nodes, stopper, tt := newTestTransport(handler, mutualTLS, fs)
	defer func() {
		err := fs.RemoveAll(snapshotDir)
		require.NoError(t, err)
	}()
	defer func() {
		err := trans.env.Close()
		require.NoError(t, err, "failed to stop the env")
	}()
	defer tt.cleanup()
	defer func() {
		err := trans.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	defer stopper.Stop()
	nodes.Add(100, 2, serverAddress)
	plog.Infof("going to generate snapshot file")
	tt.generateSnapshotFile(100, 12, testSnapshotIndex, "testsnapshot.gbsnap", sz, fs)
	plog.Infof("snapshot file created")
	m := getTestSnapshotMessage(2)
	m.Snapshot.FileSize = getTestSnapshotFileSize(sz)
	dir := tt.GetSnapshotDir(100, 12, testSnapshotIndex)
	chunks := NewChunk(trans.handleRequest,
		trans.snapshotReceived, trans.dir, trans.nhConfig.GetDeploymentID(), fs)
	snapDir := chunks.dir(100, 2)
	err := fs.MkdirAll(snapDir, 0755)
	require.NoError(t, err)
	m.Snapshot.Filepath = fs.PathJoin(dir, "testsnapshot.gbsnap")
	// send the snapshot file
	plog.Infof("send snapshot will be called")
	done := trans.SendSnapshot(m)
	plog.Infof("send snapshot returned")
	assert.True(t, done, "failed to send the snapshot")
	plog.Infof("waiting for snapshot status update")
	waitForFirstSnapshotStatusUpdate(handler, maxWait)
	plog.Infof("waiting for snapshot count update")
	waitForSnapshotCountUpdate(handler, maxWait)
	plog.Infof("snapshot count updated")
	assert.Equal(t, uint64(1), handler.getSnapshotCount(100, 2),
		"got %d, want %d", handler.getSnapshotCount(100, 2), 1)
	assert.Equal(t, uint64(0), handler.getFailedSnapshotCount(100, 2),
		"got %d, want 0", handler.getFailedSnapshotCount(100, 2))
	assert.Equal(t, uint64(1), handler.getSnapshotSuccessCount(100, 2),
		"got %d, want 1", handler.getSnapshotSuccessCount(100, 2))
	assert.Equal(t, uint64(1), handler.getReceivedSnapshotFromCount(100, 12),
		"got %d, want 1", handler.getReceivedSnapshotFromCount(100, 12))
	assert.Equal(t, uint64(1), handler.getReceivedSnapshotCount(100, 2),
		"got %d, want 1", handler.getReceivedSnapshotFromCount(100, 12))
	md5Original, err := tt.getSnapshotFileMD5(100,
		2, testSnapshotIndex, "testsnapshot.gbsnap")
	assert.NoError(t, err, "err %v, want nil", err)
	md5Received, err := tt.getSnapshotFileMD5(100,
		12, testSnapshotIndex, "testsnapshot.gbsnap")
	assert.NoError(t, err, "err %v, want nil", err)
	assert.True(t, bytes.Equal(md5Original, md5Received),
		"snapshot content changed during transmission")
}

func testSnapshotWithNotMatchedDBVWillBeDropped(t *testing.T,
	f StreamChunkSendFunc, mutualTLS bool, fs vfs.IFS) {
	handler := newTestMessageHandler()
	trans, nodes, stopper, tt := newTestTransport(handler, mutualTLS, fs)
	defer func() {
		err := trans.env.Close()
		require.NoError(t, err, "failed to stop the env")
	}()
	defer tt.cleanup()
	defer func() {
		err := trans.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	defer stopper.Stop()
	nodes.Add(100, 2, serverAddress)
	tt.generateSnapshotFile(100, 12, testSnapshotIndex, "testsnapshot.gbsnap", 1024, fs)
	m := getTestSnapshotMessage(2)
	m.Snapshot.FileSize = getTestSnapshotFileSize(1024)
	dir := tt.GetSnapshotDir(100, 12, testSnapshotIndex)
	m.Snapshot.Filepath = fs.PathJoin(dir, "testsnapshot.gbsnap")
	// send the snapshot file
	trans.SetPreStreamChunkSendHook(f)
	done := trans.SendSnapshot(m)
	assert.True(t, done, "failed to send the snapshot")
	waitForFirstSnapshotStatusUpdate(handler, 1000)
	waitForSnapshotCountUpdate(handler, 1000)
	assert.Equal(t, uint64(0), handler.getSnapshotCount(100, 2),
		"got %d, want %d", handler.getSnapshotCount(100, 2), 0)
	// such snapshot dropped on the sending side should be reported.
	assert.Equal(t, uint64(0), handler.getFailedSnapshotCount(100, 2),
		"got %d, want 0", handler.getFailedSnapshotCount(100, 2))
	assert.Equal(t, uint64(1), handler.getSnapshotSuccessCount(100, 2),
		"got %d, want 1", handler.getSnapshotSuccessCount(100, 2))
}

func TestSnapshotWithNotMatchedDeploymentIDWillBeDropped(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	f := func(c raftpb.Chunk) (raftpb.Chunk, bool) {
		c.DeploymentId = 2
		return c, true
	}
	testSnapshotWithNotMatchedDBVWillBeDropped(t, f, true, fs)
	testSnapshotWithNotMatchedDBVWillBeDropped(t, f, false, fs)
}

func TestSnapshotWithNotMatchedBinVerWillBeDropped(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	f := func(c raftpb.Chunk) (raftpb.Chunk, bool) {
		c.BinVer = raftio.TransportBinVersion + 1
		return c, true
	}
	testSnapshotWithNotMatchedDBVWillBeDropped(t, f, true, fs)
	testSnapshotWithNotMatchedDBVWillBeDropped(t, f, false, fs)
}

func TestMaxSnapshotConnectionIsLimited(t *testing.T) {
	fs := vfs.GetTestFS()
	handler := newTestMessageHandler()
	trans, nodes, stopper, tt := newTestTransport(handler, false, fs)
	defer func() {
		err := trans.env.Close()
		require.NoError(t, err, "failed to stop the env")
	}()
	defer tt.cleanup()
	defer func() {
		err := trans.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	defer stopper.Stop()
	nodes.Add(100, 2, serverAddress)
	conns := make([]*Sink, 0)
	for i := uint64(0); i < maxConnectionCount; i++ {
		sink := trans.GetStreamSink(100, 2)
		assert.NotNil(t, sink, "failed to get sink")
		conns = append(conns, sink)
	}
	for i := uint64(0); i < maxConnectionCount; i++ {
		sink := trans.GetStreamSink(100, 2)
		assert.Nil(t, sink, "connection is not limited")
	}
	for _, v := range conns {
		close(v.j.ch)
	}
	for {
		if atomic.LoadUint64(&trans.jobs) != 0 {
			time.Sleep(time.Millisecond)
		} else {
			break
		}
	}
	breaker := trans.GetCircuitBreaker(serverAddress)
	breaker.Reset()
	plog.Infof("circuit breaker for %s is now ready", serverAddress)
	for i := uint64(0); i < maxConnectionCount; i++ {
		sink := trans.GetStreamSink(100, 2)
		assert.NotNil(t, sink, "failed to get sink again %d", i)
	}
}

func testFailedConnectionReportsSnapshotFailure(t *testing.T,
	mutualTLS bool, fs vfs.IFS) {
	snapshotSize := snapshotChunkSize * 10
	handler := newTestMessageHandler()
	trans, nodes, stopper, tt := newTestTransport(handler, mutualTLS, fs)
	defer func() {
		err := trans.env.Close()
		require.NoError(t, err, "failed to stop the env")
	}()
	defer tt.cleanup()
	defer func() {
		err := trans.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	defer stopper.Stop()
	// invalid address
	nodes.Add(100, 2, "localhost:12345")
	tt.generateSnapshotFile(100, 12, testSnapshotIndex, "testsnapshot.gbsnap", snapshotSize, fs)
	m := getTestSnapshotMessage(2)
	m.Snapshot.FileSize = getTestSnapshotFileSize(snapshotSize)
	dir := tt.GetSnapshotDir(100, 12, testSnapshotIndex)
	m.Snapshot.Filepath = fs.PathJoin(dir, "testsnapshot.gbsnap")
	// send the snapshot file
	done := trans.SendSnapshot(m)
	assert.True(t, done, "failed to send the snapshot")
	waitForTotalSnapshotStatusUpdateCount(handler, 6000, 1)
	assert.Equal(t, uint64(0), handler.getSnapshotCount(100, 2),
		"got %d, want %d", handler.getSnapshotCount(100, 2), 0)
	assert.Greater(t, handler.getFailedSnapshotCount(100, 2), uint64(0),
		"got %d, want > 0", handler.getFailedSnapshotCount(100, 2))
}

func TestFailedConnectionReportsSnapshotFailure(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	testFailedConnectionReportsSnapshotFailure(t, true, fs)
	testFailedConnectionReportsSnapshotFailure(t, false, fs)
}

func testSnapshotWithExternalFilesCanBeSend(t *testing.T,
	sz uint64, maxWait uint64, mutualTLS bool, fs vfs.IFS) {
	handler := newTestMessageHandler()
	trans, nodes, stopper, tt := newTestTransport(handler, mutualTLS, fs)
	defer func() {
		err := fs.RemoveAll(snapshotDir)
		require.NoError(t, err)
	}()
	defer func() {
		err := trans.env.Close()
		require.NoError(t, err, "failed to stop the env")
	}()
	defer tt.cleanup()
	defer func() {
		err := trans.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	defer stopper.Stop()
	chunks := NewChunk(trans.handleRequest,
		trans.snapshotReceived, trans.dir, trans.nhConfig.GetDeploymentID(), fs)
	ts := getTestChunk()
	snapDir := chunks.dir(ts[0].ShardID, ts[0].ReplicaID)
	err := fs.MkdirAll(snapDir, 0755)
	require.NoError(t, err)
	nodes.Add(100, 2, serverAddress)
	tt.generateSnapshotFile(100, 12, testSnapshotIndex, "testsnapshot.gbsnap", sz, fs)
	tt.generateSnapshotExternalFile(100, 12, testSnapshotIndex, "external1.data", sz)
	tt.generateSnapshotExternalFile(100, 12, testSnapshotIndex, "external2.data", sz)
	m := getTestSnapshotMessage(2)
	dir := tt.GetSnapshotDir(100, 12, testSnapshotIndex)
	m.Snapshot.FileSize = getTestSnapshotFileSize(sz)
	m.Snapshot.Filepath = fs.PathJoin(dir, "testsnapshot.gbsnap")
	f1 := &raftpb.SnapshotFile{
		Filepath: fs.PathJoin(dir, "external1.data"),
		FileSize: sz,
		FileId:   1,
	}
	f2 := &raftpb.SnapshotFile{
		Filepath: fs.PathJoin(dir, "external2.data"),
		FileSize: sz,
		FileId:   2,
	}
	m.Snapshot.Files = []*raftpb.SnapshotFile{f1, f2}
	// send the snapshot file
	done := trans.SendSnapshot(m)
	assert.True(t, done, "failed to send the snapshot")
	waitForFirstSnapshotStatusUpdate(handler, maxWait)
	waitForSnapshotCountUpdate(handler, maxWait)
	assert.Equal(t, uint64(1), handler.getSnapshotCount(100, 2),
		"got %d, want %d", handler.getSnapshotCount(100, 2), 1)
	assert.Equal(t, uint64(0), handler.getFailedSnapshotCount(100, 2),
		"got %d, want 0", handler.getFailedSnapshotCount(100, 2))
	assert.Equal(t, uint64(1), handler.getSnapshotSuccessCount(100, 2),
		"got %d, want 1", handler.getSnapshotSuccessCount(100, 2))
	assert.Equal(t, uint64(1), handler.getReceivedSnapshotFromCount(100, 12),
		"got %d, want 1", handler.getReceivedSnapshotFromCount(100, 12))
	assert.Equal(t, uint64(1), handler.getReceivedSnapshotCount(100, 2),
		"got %d, want 1", handler.getReceivedSnapshotFromCount(100, 12))
	filenames := []string{"testsnapshot.gbsnap", "external1.data", "external2.data"}
	for _, fn := range filenames {
		md5Original, err := tt.getSnapshotFileMD5(100, 2, testSnapshotIndex, fn)
		assert.NoError(t, err, "err %v, want nil", err)
		md5Received, err := tt.getSnapshotFileMD5(100, 12, testSnapshotIndex, fn)
		assert.NoError(t, err, "err %v, want nil", err)
		assert.True(t, bytes.Equal(md5Original, md5Received),
			"snapshot content changed during transmission")
	}
}

func TestSnapshotWithExternalFilesCanBeSend(t *testing.T) {
	fs := vfs.GetTestFS()
	testSnapshotWithExternalFilesCanBeSend(t, snapshotChunkSize/2, 3000, false, fs)
	testSnapshotWithExternalFilesCanBeSend(t, snapshotChunkSize*3+100, 3000, false, fs)
	testSnapshotWithExternalFilesCanBeSend(t, snapshotChunkSize/2, 3000, true, fs)
	testSnapshotWithExternalFilesCanBeSend(t, snapshotChunkSize*3+100, 3000, true, fs)
}

func TestNoOPTransportCanBeCreated(t *testing.T) {
	fs := vfs.GetTestFS()
	handler := newTestMessageHandler()
	tt, _, _, _, _ := newNOOPTestTransport(handler, fs)
	defer func() {
		err := tt.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
}

func TestInitialMessageCanBeSent(t *testing.T) {
	fs := vfs.GetTestFS()
	handler := newTestMessageHandler()
	tt, nodes, noopTransport, req, connReq := newNOOPTestTransport(handler, fs)
	defer func() {
		err := tt.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	nodes.Add(100, 2, serverAddress)
	msg := raftpb.Message{
		Type:    raftpb.Heartbeat,
		To:      2,
		ShardID: 100,
	}
	connReq.SetToFail(false)
	req.SetToFail(false)
	ok := tt.Send(msg)
	assert.True(t, ok, "send failed")
	for i := 0; i < 1000; i++ {
		if atomic.LoadUint64(&noopTransport.connected) != 0 {
			break
		}
		time.Sleep(time.Millisecond)
	}
	assert.Equal(t, 1, tt.queueSize(), "queue len %d, want 1", tt.queueSize())
	assert.Equal(t, 1, len(tt.mu.breakers), "breakers len %d, want 1",
		len(tt.mu.breakers))
	assert.Equal(t, uint64(1), noopTransport.connected, "connected %d, want 1",
		noopTransport.connected)
}

func TestFailedConnectionIsRemovedFromTransport(t *testing.T) {
	fs := vfs.GetTestFS()
	handler := newTestMessageHandler()
	tt, nodes, _, req, connReq := newNOOPTestTransport(handler, fs)
	defer func() {
		err := tt.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	nodes.Add(100, 2, serverAddress)
	msg := raftpb.Message{
		Type:    raftpb.Heartbeat,
		To:      2,
		ShardID: 100,
	}
	connReq.SetToFail(false)
	req.SetToFail(false)
	ok := tt.Send(msg)
	assert.True(t, ok, "send failed")
	req.SetToFail(true)
	// requested the noop trans to fail the send batch operation, given the first
	// batch is in the chan, we don't know whether the first batch is going to
	// fail or the one below is going to fail. after the send below, failure will
	// eventually be triggered and we just need to check & wait.
	tt.Send(msg)
	for i := 0; i < 5000; i++ {
		if tt.queueSize() != 0 {
			time.Sleep(time.Millisecond)
		} else {
			break
		}
	}
	assert.Equal(t, 0, tt.queueSize(), "queue len %d, want 0", tt.queueSize())
}

func TestCircuitBreakerCauseFailFast(t *testing.T) {
	fs := vfs.GetTestFS()
	handler := newTestMessageHandler()
	tt, nodes, noopTransport, req, connReq := newNOOPTestTransport(handler, fs)
	defer func() {
		if err := tt.Close(); err != nil {
			t.Fatalf("failed to close the transport module %v", err)
		}
	}()
	nodes.Add(100, 2, serverAddress)
	msg := raftpb.Message{
		Type:    raftpb.Heartbeat,
		To:      2,
		ShardID: 100,
	}
	connReq.SetToFail(false)
	req.SetToFail(false)
	ok := tt.Send(msg)
	assert.True(t, ok, "send failed")
	req.SetToFail(true)
	// see comments in TestFailedConnectionIsRemovedFromTransport for why
	// the returned value of the below Send() is not checked
	tt.Send(msg)
	for i := 0; i < 1000; i++ {
		if tt.queueSize() != 0 {
			time.Sleep(time.Millisecond)
		} else {
			break
		}
	}
	req.SetToFail(false)
	for i := 0; i < 20; i++ {
		ok = tt.Send(msg)
		assert.False(t, ok, "send unexpectedly returned ok")
		time.Sleep(time.Millisecond)
	}
	assert.Equal(t, 0, tt.queueSize(), "queue len %d, want 0", tt.queueSize())
	assert.Equal(t, uint64(1), atomic.LoadUint64(&noopTransport.connected),
		"connected %d, want 1", noopTransport.connected)
	assert.Equal(t, uint64(1), atomic.LoadUint64(&noopTransport.tryConnect),
		"connected %d, want 1", noopTransport.tryConnect)
}

func TestCircuitBreakerForResolveNotShared(t *testing.T) {
	fs := vfs.GetTestFS()
	handler := newTestMessageHandler()
	tt, nodes, noopTransport, req, connReq := newNOOPTestTransport(handler, fs)
	defer func() {
		err := tt.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	nodes.Add(100, 2, serverAddress)
	msg := raftpb.Message{
		Type:    raftpb.Heartbeat,
		To:      2,
		ShardID: 100,
	}
	msgUnknownNode := raftpb.Message{
		Type:    raftpb.Heartbeat,
		To:      3,
		ShardID: 100,
	}
	connReq.SetToFail(false)
	req.SetToFail(false)
	ok := tt.Send(msg)
	assert.True(t, ok, "send failed")
	for i := 0; i < 20; i++ {
		ok := tt.Send(msgUnknownNode)
		assert.False(t, ok, "send unexpectedly returned ok")
		time.Sleep(time.Millisecond)
	}
	for i := 0; i < 20; i++ {
		ok := tt.Send(msg)
		assert.True(t, ok, "send failed for known host")
		time.Sleep(time.Millisecond)
	}
	assert.Equal(t, uint64(1), atomic.LoadUint64(&noopTransport.connected),
		"connected %d, want 1", noopTransport.connected)
	assert.Equal(t, uint64(1), atomic.LoadUint64(&noopTransport.tryConnect),
		"connected %d, want 1", noopTransport.tryConnect)
}

// unknown target
func TestStreamToUnknownTargetWillHaveSnapshotStatusUpdated(t *testing.T) {
	fs := vfs.GetTestFS()
	handler := newTestMessageHandler()
	tt, nodes, _, _, _ := newNOOPTestTransport(handler, fs)
	defer func() {
		err := tt.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	nodes.Add(100, 2, serverAddress)
	sink := tt.GetStreamSink(100, 3)
	assert.Nil(t, sink, "unexpectedly returned a sink")
	assert.Equal(t, uint64(1), handler.getFailedSnapshotCount(100, 3),
		"snapshot failed count %d", handler.snapshotFailedCount)
	assert.Equal(t, uint64(0), handler.getSnapshotSuccessCount(100, 3),
		"snapshot succeed count %d", handler.snapshotSuccessCount)
}

// failed to connect
func TestFailedStreamConnectionWillHaveSnapshotStatusUpdated(t *testing.T) {
	fs := vfs.GetTestFS()
	handler := newTestMessageHandler()
	tt, nodes, _, req, connReq := newNOOPTestTransport(handler, fs)
	defer func() {
		err := tt.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	nodes.Add(100, 2, serverAddress)
	connReq.SetToFail(true)
	req.SetToFail(true)
	tt.GetStreamSink(100, 2)
	failedSnapshotReported := false
	for i := 0; i < 10000; i++ {
		if handler.getFailedSnapshotCount(100, 2) != 1 {
			time.Sleep(time.Millisecond)
			continue
		}
		failedSnapshotReported = true
		break
	}
	assert.True(t, failedSnapshotReported, "failed snapshot not reported")
}

// failed to connect due to too many connections
func TestFailedStreamingDueToTooManyConnectionsHaveStatusUpdated(t *testing.T) {
	fs := vfs.GetTestFS()
	handler := newTestMessageHandler()
	tt, nodes, _, _, _ := newNOOPTestTransport(handler, fs)
	defer func() {
		err := tt.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	nodes.Add(100, 2, serverAddress)
	for i := uint64(0); i < maxConnectionCount; i++ {
		sink := tt.GetStreamSink(100, 2)
		assert.NotNil(t, sink, "failed to connect")
	}
	for i := uint64(0); i < 2*maxConnectionCount; i++ {
		sink := tt.GetStreamSink(100, 2)
		assert.Nil(t, sink, "stream connection not limited")
	}
	failedSnapshotReported := false
	for i := 0; i < 10000; i++ {
		count := handler.getFailedSnapshotCount(100, 2)
		if count != 2*maxConnectionCount {
			time.Sleep(time.Millisecond)
			continue
		}
		failedSnapshotReported = true
		break
	}
	assert.True(t, failedSnapshotReported, "failed snapshot not reported")
}

func TestInMemoryEntrySizeCanBeLimitedWhenSendingMessages(t *testing.T) {
	fs := vfs.GetTestFS()
	handler := newTestMessageHandler()
	tt, nodes, _, req, _ := newNOOPTestTransport(handler, fs)
	defer func() {
		req.SetBlocked(false)
		defer func() {
			err := tt.Close()
			require.NoError(t, err, "failed to close the transport module")
		}()
	}()
	nodes.Add(100, 2, serverAddress)
	e := raftpb.Entry{Cmd: make([]byte, 1024*1024*10)}
	msg := raftpb.Message{
		ShardID: 100,
		To:      2,
		Type:    raftpb.Replicate,
		Entries: []raftpb.Entry{e},
	}
	req.SetBlocked(true)
	for i := 0; i < 1000; i++ {
		sent, reason := tt.send(msg)
		if !sent {
			assert.Equal(t, rateLimited, reason, "not due to rate limit")
			break
		}
		if i == 999 {
			t.Errorf("no message rejected")
		}
	}
}

func TestInMemoryEntrySizeCanDropToZero(t *testing.T) {
	fs := vfs.GetTestFS()
	handler := newTestMessageHandler()
	tt, nodes, _, _, _ := newNOOPTestTransport(handler, fs)
	defer func() {
		err := tt.Close()
		require.NoError(t, err, "failed to close the transport module")
	}()
	nodes.Add(100, 2, serverAddress)
	e := raftpb.Entry{Cmd: make([]byte, 1024*1024*10)}
	msg := raftpb.Message{
		ShardID: 100,
		To:      2,
		Type:    raftpb.Replicate,
		Entries: []raftpb.Entry{e},
	}
	_, key, err := nodes.Resolve(100, 2)
	require.NoError(t, err, "failed to resolve the addr")
	sent := tt.Send(msg)
	assert.True(t, sent, "first send failed")
	sq, ok := tt.mu.queues[key]
	assert.True(t, ok, "failed to get sq")
	for len(sq.ch) != 0 {
		time.Sleep(time.Millisecond)
	}
	for i := 0; i < 20; i++ {
		sent := tt.Send(msg)
		assert.True(t, sent, "failed to send2")
	}
	for len(sq.ch) != 0 {
		time.Sleep(time.Millisecond)
	}
	for i := 0; i < 1000; i++ {
		time.Sleep(10 * time.Millisecond)
		if sq.rl.Get() == 0 {
			return
		}
		if i == 999 {
			t.Errorf("rate limiter failed to report correct size")
		}
	}
}
````

## File: internal/transport/transport.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
//
// This file contains code derived from CockroachDB. The async send message
// pattern used in ASyncSend/connectAndProcess/connectAndProcess is similar
// to the one used in CockroachDB.
//
// Copyright 2014 The Cockroach Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
// implied. See the License for the specific language governing
// permissions and limitations under the License.

/*
Package transport implements the transport component used for exchanging
Raft messages between NodeHosts.

This package is internally used by Dragonboat, applications are not expected
to import this package.
*/
package transport

import (
	"context"
	"sync"
	"sync/atomic"
	"time"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/logutil"
	"github.com/lni/goutils/netutil"
	circuit "github.com/lni/goutils/netutil/rubyist/circuitbreaker"
	"github.com/lni/goutils/syncutil"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/invariants"
	"github.com/lni/dragonboat/v4/internal/registry"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/logger"
	ct "github.com/lni/dragonboat/v4/plugin/chan"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

const (
	maxMsgBatchSize = settings.MaxMessageBatchSize
)

var (
	lazyFreeCycle = settings.Soft.LazyFreeCycle
)

var (
	plog                = logger.GetLogger("transport")
	sendQueueLen        = settings.Soft.SendQueueLength
	dialTimeoutSecond   = settings.Soft.GetConnectedTimeoutSecond
	idleTimeout         = time.Minute
	errChunkSendSkipped = errors.New("chunk skipped")
	errBatchSendSkipped = errors.New("batch skipped")
	dn                  = logutil.DescribeNode
)

// IMessageHandler is the interface required to handle incoming raft requests.
type IMessageHandler interface {
	HandleMessageBatch(batch pb.MessageBatch) (uint64, uint64)
	HandleUnreachable(shardID uint64, replicaID uint64)
	HandleSnapshotStatus(shardID uint64, replicaID uint64, rejected bool)
	HandleSnapshot(shardID uint64, replicaID uint64, from uint64)
}

// ITransport is the interface of the transport layer used for exchanging
// Raft messages.
type ITransport interface {
	Name() string
	Send(pb.Message) bool
	SendSnapshot(pb.Message) bool
	GetStreamSink(shardID uint64, replicaID uint64) *Sink
	Close() error
}

//
// funcs used mainly in testing
//

// StreamChunkSendFunc is a func type that is used to determine whether a
// snapshot chunk should indeed be sent. This func is used in test only.
type StreamChunkSendFunc func(pb.Chunk) (pb.Chunk, bool)

// SendMessageBatchFunc is a func type that is used to determine whether the
// specified message batch should be sent. This func is used in test only.
type SendMessageBatchFunc func(pb.MessageBatch) (pb.MessageBatch, bool)

type sendQueue struct {
	ch chan pb.Message
	rl *server.RateLimiter
}

func (sq *sendQueue) rateLimited() bool {
	return sq.rl.RateLimited()
}

func (sq *sendQueue) increase(msg pb.Message) {
	if msg.Type != pb.Replicate {
		return
	}
	sq.rl.Increase(pb.GetEntrySliceInMemSize(msg.Entries))
}

func (sq *sendQueue) decrease(msg pb.Message) {
	if msg.Type != pb.Replicate {
		return
	}
	sq.rl.Decrease(pb.GetEntrySliceInMemSize(msg.Entries))
}

// ITransportEvent is the interface for notifying connection status changes.
type ITransportEvent interface {
	ConnectionEstablished(string, bool)
	ConnectionFailed(string, bool)
}

type failedSend uint64

type nodeMap map[raftio.NodeInfo]struct{}

const (
	success failedSend = iota
	circuitBreakerNotReady
	unknownTarget
	rateLimited
	chanIsFull
)

// DefaultTransportFactory is the default transport module used.
type DefaultTransportFactory struct{}

// Create creates a default transport instance.
func (dtm *DefaultTransportFactory) Create(nhConfig config.NodeHostConfig,
	handler raftio.MessageHandler,
	chunkHandler raftio.ChunkHandler) raftio.ITransport {
	return NewTCPTransport(nhConfig, handler, chunkHandler)
}

// Validate returns a boolean value indicating whether the specified address is
// valid.
func (dtm *DefaultTransportFactory) Validate(addr string) bool {
	panic("not suppose to be called")
}

// Transport is the transport layer for delivering raft messages and snapshots.
type Transport struct {
	mu struct {
		sync.Mutex
		queues   map[string]sendQueue
		breakers map[string]*circuit.Breaker
	}
	sysEvents    ITransportEvent
	ctx          context.Context
	preSendBatch atomic.Value
	preSend      atomic.Value
	postSend     atomic.Value
	msgHandler   IMessageHandler
	resolver     registry.IResolver
	trans        raftio.ITransport
	fs           vfs.IFS
	stopper      *syncutil.Stopper
	dir          server.SnapshotDirFunc
	env          *server.Env
	metrics      *transportMetrics
	chunks       *Chunk
	cancel       context.CancelFunc
	sourceID     string
	nhConfig     config.NodeHostConfig
	jobs         uint64
}

var _ ITransport = (*Transport)(nil)

// NewTransport creates a new Transport object.
func NewTransport(nhConfig config.NodeHostConfig,
	handler IMessageHandler, env *server.Env, resolver registry.IResolver,
	dir server.SnapshotDirFunc, sysEvents ITransportEvent,
	fs vfs.IFS) (*Transport, error) {
	sourceID := nhConfig.RaftAddress
	if nhConfig.NodeRegistryEnabled() {
		sourceID = env.NodeHostID()
	}
	t := &Transport{
		nhConfig:   nhConfig,
		env:        env,
		sourceID:   sourceID,
		resolver:   resolver,
		stopper:    syncutil.NewStopper(),
		dir:        dir,
		sysEvents:  sysEvents,
		fs:         fs,
		msgHandler: handler,
	}
	chunks := NewChunk(t.handleRequest,
		t.snapshotReceived, t.dir, t.nhConfig.GetDeploymentID(), fs)
	t.trans = create(nhConfig, t.handleRequest, chunks.Add)
	t.chunks = chunks
	t.ctx, t.cancel = context.WithCancel(context.Background())
	t.mu.queues = make(map[string]sendQueue)
	t.mu.breakers = make(map[string]*circuit.Breaker)
	msgConn := func() float64 {
		t.mu.Lock()
		defer t.mu.Unlock()
		return float64(len(t.mu.queues))
	}
	ssCount := func() float64 {
		return float64(atomic.LoadUint64(&t.jobs))
	}
	t.metrics = newTransportMetrics(true, msgConn, ssCount)

	plog.Infof("transport type: %s", t.trans.Name())
	if err := t.trans.Start(); err != nil {
		plog.Errorf("transport failed to start %v", err)
		if cerr := t.trans.Close(); cerr != nil {
			plog.Errorf("failed to close the transport module %v", cerr)
		}
		return nil, err
	}
	t.stopper.RunWorker(func() {
		ticker := time.NewTicker(time.Second)
		defer ticker.Stop()
		for {
			select {
			case <-ticker.C:
				chunks.Tick()
			case <-t.stopper.ShouldStop():
				return
			}
		}
	})
	return t, nil
}

// Name returns the type name of the transport module
func (t *Transport) Name() string {
	return t.trans.Name()
}

// GetTrans returns the transport instance.
func (t *Transport) GetTrans() raftio.ITransport {
	return t.trans
}

// SetPreSendBatchHook set the SendMessageBatch hook.
// This function is only expected to be used in monkey testing.
func (t *Transport) SetPreSendBatchHook(h SendMessageBatchFunc) {
	t.preSendBatch.Store(h)
}

// SetPreStreamChunkSendHook sets the StreamChunkSend hook function that will
// be called before each snapshot chunk is sent.
func (t *Transport) SetPreStreamChunkSendHook(h StreamChunkSendFunc) {
	t.preSend.Store(h)
}

// Close closes the Transport object.
func (t *Transport) Close() error {
	t.cancel()
	t.stopper.Stop()
	t.chunks.Close()
	return t.trans.Close()
}

// GetCircuitBreaker returns the circuit breaker used for the specified
// target node.
func (t *Transport) GetCircuitBreaker(key string) *circuit.Breaker {
	t.mu.Lock()
	breaker, ok := t.mu.breakers[key]
	if !ok {
		breaker = netutil.NewBreaker()
		t.mu.breakers[key] = breaker
	}
	t.mu.Unlock()

	return breaker
}

func (t *Transport) handleRequest(req pb.MessageBatch) {
	did := t.nhConfig.GetDeploymentID()
	if req.DeploymentId != did {
		plog.Warningf("deployment id does not match %d vs %d, message dropped",
			req.DeploymentId, did)
		return
	}
	if req.BinVer != raftio.TransportBinVersion {
		plog.Warningf("binary compatibility version not match %d vs %d",
			req.BinVer, raftio.TransportBinVersion)
		return
	}
	addr := req.SourceAddress
	if len(addr) > 0 {
		for _, r := range req.Requests {
			if r.From != 0 {
				t.resolver.Add(r.ShardID, r.From, addr)
			}
		}
	}
	ssCount, msgCount := t.msgHandler.HandleMessageBatch(req)
	dropedMsgCount := uint64(len(req.Requests)) - ssCount - msgCount
	t.metrics.receivedMessages(ssCount, msgCount, dropedMsgCount)
}

func (t *Transport) snapshotReceived(shardID uint64,
	replicaID uint64, from uint64) {
	t.msgHandler.HandleSnapshot(shardID, replicaID, from)
}

func (t *Transport) notifyUnreachable(addr string, affected nodeMap) {
	plog.Warningf("%s became unreachable, affected %d nodes", addr, len(affected))
	for n := range affected {
		t.msgHandler.HandleUnreachable(n.ShardID, n.ReplicaID)
	}
}

// Send asynchronously sends raft messages to their target nodes.
//
// The generic async send Go pattern used in Send() is found in CockroachDB's
// codebase.
func (t *Transport) Send(req pb.Message) bool {
	v, _ := t.send(req)
	if !v {
		t.metrics.messageSendFailure(1)
	}
	return v
}

func (t *Transport) send(req pb.Message) (bool, failedSend) {
	if req.Type == pb.InstallSnapshot {
		panic("snapshot message must be sent via its own channel.")
	}
	toReplicaID := req.To
	shardID := req.ShardID
	from := req.From
	addr, key, err := t.resolver.Resolve(shardID, toReplicaID)
	if err != nil {
		return false, unknownTarget
	}
	// fail fast
	if !t.GetCircuitBreaker(addr).Ready() {
		t.metrics.messageConnectionFailure()
		return false, circuitBreakerNotReady
	}
	// get the channel, create it in case it is not in the queue map
	t.mu.Lock()
	sq, ok := t.mu.queues[key]
	if !ok {
		sq = sendQueue{
			ch: make(chan pb.Message, sendQueueLen),
			rl: server.NewRateLimiter(t.nhConfig.MaxSendQueueSize),
		}
		t.mu.queues[key] = sq
	}
	t.mu.Unlock()
	if !ok {
		shutdownQueue := func() {
			t.mu.Lock()
			delete(t.mu.queues, key)
			t.mu.Unlock()
		}
		t.stopper.RunWorker(func() {
			affected := make(nodeMap)
			if !t.connectAndProcess(addr, sq, from, affected) {
				t.notifyUnreachable(addr, affected)
			}
			shutdownQueue()
		})
	}
	if sq.rateLimited() {
		return false, rateLimited
	}

	sq.increase(req)

	select {
	case sq.ch <- req:
		return true, success
	default:
		sq.decrease(req)
		return false, chanIsFull
	}
}

// connectAndProcess returns a boolean value indicating whether it is stopped
// gracefully when the system is being shutdown
func (t *Transport) connectAndProcess(remoteHost string,
	sq sendQueue, from uint64, affected nodeMap) bool {
	breaker := t.GetCircuitBreaker(remoteHost)
	successes := breaker.Successes()
	consecFailures := breaker.ConsecFailures()
	if err := func() error {
		plog.Debugf("%s is trying to connect to %s", t.sourceID, remoteHost)
		conn, err := t.trans.GetConnection(t.ctx, remoteHost)
		if err != nil {
			plog.Errorf("Nodehost %s failed to get a connection to %s, %v",
				t.sourceID, remoteHost, err)
			return err
		}
		defer conn.Close()
		breaker.Success()
		if successes == 0 || consecFailures > 0 {
			plog.Debugf("message streaming to %s established", remoteHost)
			t.sysEvents.ConnectionEstablished(remoteHost, false)
		}
		return t.processMessages(remoteHost, sq, conn, affected)
	}(); err != nil {
		plog.Warningf("breaker %s to %s failed, connect and process failed: %s",
			t.sourceID, remoteHost, err.Error())
		breaker.Fail()
		t.metrics.messageConnectionFailure()
		t.sysEvents.ConnectionFailed(remoteHost, false)
		return false
	}
	return true
}

func (t *Transport) processMessages(remoteHost string,
	sq sendQueue, conn raftio.IConnection, affected nodeMap) error {
	idleTimer := time.NewTimer(idleTimeout)
	defer idleTimer.Stop()
	sz := uint64(0)
	batch := pb.MessageBatch{
		SourceAddress: t.sourceID,
		BinVer:        raftio.TransportBinVersion,
	}
	did := t.nhConfig.GetDeploymentID()
	requests := make([]pb.Message, 0)
	for {
		idleTimer.Reset(idleTimeout)
		select {
		case <-t.stopper.ShouldStop():
			return nil
		case <-idleTimer.C:
			return nil
		case req := <-sq.ch:
			n := raftio.NodeInfo{
				ShardID:   req.ShardID,
				ReplicaID: req.From,
			}
			affected[n] = struct{}{}
			sq.decrease(req)
			sz += uint64(req.SizeUpperLimit())
			requests = append(requests, req)
			for done := false; !done && sz < maxMsgBatchSize; {
				select {
				case req = <-sq.ch:
					sq.decrease(req)
					sz += uint64(req.SizeUpperLimit())
					requests = append(requests, req)
				case <-t.stopper.ShouldStop():
					return nil
				default:
					done = true
				}
			}
			batch.DeploymentId = did
			twoBatch := false
			if sz < maxMsgBatchSize || len(requests) == 1 {
				batch.Requests = requests
			} else {
				twoBatch = true
				batch.Requests = requests[:len(requests)-1]
			}
			if err := t.sendMessageBatch(conn, batch); err != nil {
				plog.Errorf("send batch failed, target %s (%v), %d",
					remoteHost, err, len(batch.Requests))
				return err
			}
			if twoBatch {
				batch.Requests = []pb.Message{requests[len(requests)-1]}
				if err := t.sendMessageBatch(conn, batch); err != nil {
					plog.Errorf("send batch failed, taret node %s (%v), %d",
						remoteHost, err, len(batch.Requests))
					return err
				}
			}
			sz = 0
			requests, batch = lazyFree(requests, batch)
			requests = requests[:0]
		}
	}
}

func lazyFree(reqs []pb.Message,
	mb pb.MessageBatch) ([]pb.Message, pb.MessageBatch) {
	if lazyFreeCycle > 0 {
		for i := 0; i < len(reqs); i++ {
			reqs[i].Entries = nil
		}
		mb.Requests = []pb.Message{}
	}
	return reqs, mb
}

func (t *Transport) sendMessageBatch(conn raftio.IConnection,
	batch pb.MessageBatch) error {
	if f := t.preSendBatch.Load(); f != nil {
		updated, shouldSend := f.(SendMessageBatchFunc)(batch)
		if !shouldSend {
			return errBatchSendSkipped
		}
		return conn.SendMessageBatch(updated)
	}
	if err := conn.SendMessageBatch(batch); err != nil {
		t.metrics.messageSendFailure(uint64(len(batch.Requests)))
		return err
	}
	t.metrics.messageSendSuccess(uint64(len(batch.Requests)))
	return nil
}

func create(nhConfig config.NodeHostConfig,
	requestHandler raftio.MessageHandler,
	chunkHandler raftio.ChunkHandler) raftio.ITransport {
	var tm config.TransportFactory
	if nhConfig.Expert.TransportFactory != nil {
		tm = nhConfig.Expert.TransportFactory
	} else if invariants.MemfsTest {
		tm = &ct.ChanTransportFactory{}
	} else {
		tm = &DefaultTransportFactory{}
	}
	return tm.Create(nhConfig, requestHandler, chunkHandler)
}
````

## File: internal/utils/dio/io_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dio

import (
	"bytes"
	"crypto/rand"
	"testing"

	"github.com/stretchr/testify/require"
)

type wc struct{}

func (wc) Write(data []byte) (int, error) {
	return len(data), nil
}

func (wc) Close() error { return nil }

func TestCountedWriterCountsWrittenBytes(t *testing.T) {
	w := NewCountedWriter(&wc{})
	n, err := w.Write(make([]byte, 128))
	require.Equal(t, 128, n)
	require.NoError(t, err)
	n, err = w.Write(make([]byte, 1))
	require.Equal(t, 1, n)
	require.NoError(t, err)
	require.NoError(t, w.Close())
	require.Equal(t, uint64(129), w.BytesWritten())
}

func TestCountedWriteMustBeClosedFirst(t *testing.T) {
	w := NewCountedWriter(&wc{})
	n, err := w.Write(make([]byte, 128))
	require.Equal(t, 128, n)
	require.NoError(t, err)
	n, err = w.Write(make([]byte, 1))
	require.Equal(t, 1, n)
	require.NoError(t, err)
	require.Panics(t, func() {
		_ = w.BytesWritten()
	})
}

func TestSnappyBlockCompression(t *testing.T) {
	for i := 1; i <= 512; i++ {
		src := make([]byte, i*1024)
		sz, ok := MaxEncodedLen(Snappy, uint64(i*1024))
		require.True(t, ok, "failed to get encoded len")
		dst := make([]byte, sz)
		_, err := rand.Read(src)
		require.NoError(t, err)
		n := CompressSnappyBlock(src, dst)
		decompressed := make([]byte, i*1024)
		err = DecompressSnappyBlock(dst[:n], decompressed)
		require.NoError(t, err, "snappy compression failed")
		require.Equal(t, src, decompressed, "content changed")
	}
}

type tb struct {
	buf *bytes.Buffer
}

func (tb) Close() error { return nil }

func (t *tb) Write(data []byte) (int, error) {
	return t.buf.Write(data)
}

func (t *tb) Read(data []byte) (int, error) {
	return t.buf.Read(data)
}

func TestCompressorDecompressor(t *testing.T) {
	for i := 1; i <= 128; i++ {
		src := make([]byte, 10*i*1024)
		dst := make([]byte, 0)
		data := make([]byte, 0)
		_, err := rand.Read(src)
		require.NoError(t, err)
		buf := &tb{buf: bytes.NewBuffer(data)}
		c := NewCompressor(Snappy, buf)
		n, err := c.Write(src)
		require.Equal(t, len(src), n, "failed to write all data")
		require.NoError(t, err)
		require.NoError(t, c.Close())
		d := NewDecompressor(Snappy, buf)
		for {
			r := make([]byte, 1024)
			_, err = d.Read(r)
			require.NoError(t, err, "failed to read")
			dst = append(dst, r...)
			if len(dst) == len(src) {
				break
			}
		}
		require.Equal(t, src, dst, "content changed")
	}
}
````

## File: internal/utils/dio/io.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dio

import (
	"io"
	"math"
	"sync/atomic"

	"github.com/golang/snappy"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

// CompressionType is the type of the compression.
type CompressionType = pb.CompressionType

const (
	// NoCompression is the CompressionType value used to indicate not to use
	// any compression.
	NoCompression CompressionType = pb.NoCompression
	// Snappy is the CompressionType value used to indicate that google snappy
	// is used for data compression.
	Snappy CompressionType = pb.Snappy
)

// CountedWriter is a io.WriteCloser wrapper that keeps the total number of bytes
// written to the underlying writer.
type CountedWriter struct {
	closed uint32
	total  uint64
	w      io.WriteCloser
}

// NewCountedWriter creates a new CountedWriter.
func NewCountedWriter(w io.WriteCloser) *CountedWriter {
	return &CountedWriter{w: w}
}

// Write writes the specified content to the underlying writer.
func (cw *CountedWriter) Write(data []byte) (int, error) {
	cw.total += uint64(len(data))
	return cw.w.Write(data)
}

// Close closes the underlying writer.
func (cw *CountedWriter) Close() error {
	defer func() {
		atomic.StoreUint32(&cw.closed, 1)
	}()
	return cw.w.Close()
}

// BytesWritten returns the total number of bytes written.
func (cw *CountedWriter) BytesWritten() uint64 {
	if atomic.LoadUint32(&cw.closed) == 0 {
		panic("calling BytesWritten before close is called")
	}
	return cw.total
}

// Compressor is a io.WriteCloser that compresses its input data to its
// underlying io.Writer.
type Compressor struct {
	uw io.WriteCloser
	wc io.WriteCloser
	ct CompressionType
}

// NewCompressor returns a Compressor instance.
func NewCompressor(ct CompressionType, wc io.WriteCloser) io.WriteCloser {
	switch ct {
	case NoCompression:
		return wc
	case Snappy:
		c := &Compressor{
			uw: wc,
			wc: snappy.NewBufferedWriter(wc),
			ct: ct,
		}
		return c
	default:
		panic("unknown compression type")
	}
}

// Write compresses the input data and writes to the underlying writer.
func (c *Compressor) Write(data []byte) (int, error) {
	return c.wc.Write(data)
}

// Close closes the compressor.
func (c *Compressor) Close() error {
	if c.ct == NoCompression {
		panic("not suppose to reach here")
	}
	if err := c.wc.Close(); err != nil {
		return err
	}
	if c.uw != nil {
		return c.uw.Close()
	}
	return nil
}

// Decompressor is a io.WriteCloser that decompresses data read from its
// underlying reader.
type Decompressor struct {
	ur io.ReadCloser
	rc io.Reader
	ct CompressionType
}

// NewDecompressor return a decompressor instance.
func NewDecompressor(ct CompressionType, r io.ReadCloser) io.ReadCloser {
	switch ct {
	case NoCompression:
		return r
	case Snappy:
		d := &Decompressor{
			ur: r,
			rc: snappy.NewReader(r),
			ct: ct,
		}
		return d
	default:
		panic("unknown compression type")
	}
}

// Read reads from the underlying reader.
func (dc *Decompressor) Read(data []byte) (int, error) {
	return dc.rc.Read(data)
}

// Close closes the decompressor.
func (dc *Decompressor) Close() error {
	switch dc.ct {
	case NoCompression:
		panic("not suppose to reach here")
	case Snappy:
		return dc.ur.Close()
	default:
		panic("unknown compression type")
	}
}

// MaxEncodedLen returns the maximum length of the encoded block given the
// specified compression type and src length.
func MaxEncodedLen(ct CompressionType, srcLen uint64) (uint64, bool) {
	if ct == Snappy {
		if srcLen > MaxBlockLen(ct) {
			return 0, false
		}
		sz := snappy.MaxEncodedLen(int(srcLen))
		if sz == -1 {
			return 0, false
		}
		return uint64(sz), true
	}
	panic("not supported compression type")
}

// MaxBlockLen returns the maximum length allowed for specified compression
// type.
func MaxBlockLen(ct CompressionType) uint64 {
	if ct == Snappy {
		// https://github.com/golang/snappy/blob/2a8bb927dd31d8daada140a5d09578521ce5c36a/encode.go#L76
		return 6 * (0xffffffff - 32) / 7
	}
	return math.MaxUint64
}

// CompressSnappyBlock compresses the src block using snappy and store the
// compressed block into dst. The length of the compressed block is returned.
func CompressSnappyBlock(src []byte, dst []byte) int {
	dstLen := len(dst)
	result := snappy.Encode(dst, src)
	if len(result) > dstLen {
		panic("dst length is too small")
	}
	return len(result)
}

// DecompressSnappyBlock decompresses the snappy compressed data in src to the
// dst slice. The dst slice must be of the exact length of the uncompressed
// data.
func DecompressSnappyBlock(src []byte, dst []byte) error {
	dstLen := len(dst)
	result, err := snappy.Decode(dst, src)
	if len(result) != dstLen {
		panic("corrupted decodedLen in header")
	}
	if err != nil {
		return err
	}
	return nil
}
````

## File: internal/utils/error.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package utils

// FirstError returns the first error.
func FirstError(err1 error, err2 error) error {
	if err1 != nil {
		return err1
	}
	return err2
}
````

## File: internal/vfs/defaultfs.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//go:build !dragonboat_memfs_test
// +build !dragonboat_memfs_test

package vfs

// GetFS returns the vfs instance used in normal run.
func GetFS() IFS {
	return DefaultFS
}

// GetTestFS returns the vfs instance used in tests.
func GetTestFS() IFS {
	return DefaultFS
}
````

## File: internal/vfs/error.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package vfs

import (
	gvfs "github.com/lni/vfs"
)

// ErrInjected is an error injected for testing purposes.
var ErrInjected = gvfs.ErrInjected

// Injector injects errors into FS.
type Injector = gvfs.Injector

// ErrorFS is a gvfs.FS implementation.
type ErrorFS = gvfs.ErrorFS

// InjectIndex implements Injector
type InjectIndex = gvfs.InjectIndex

// Op is an enum describing the type of FS operations.
type Op = gvfs.Op

// OpRead describes read operations
var OpRead = gvfs.OpRead

// OpWrite describes write operations
var OpWrite = gvfs.OpWrite

// OpSync describes the fsync operation
var OpSync = gvfs.OpSync

// OnIndex creates and returns an injector instance that returns an ErrInjected
// on the (n+1)-th invocation of its MaybeError function.
func OnIndex(index int32, op Op) *InjectIndex {
	return gvfs.OnIndex(index, op)
}

// Wrap wraps an existing IFS implementation with the specified injector.
func Wrap(fs IFS, inj Injector) *ErrorFS {
	return gvfs.Wrap(fs, inj)
}
````

## File: internal/vfs/memfs.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +build dragonboat_memfs_test

package vfs

import (
	gvfs "github.com/lni/vfs"
)

func GetFS() IFS {
	return MemStrictFS
}

func GetTestFS() IFS {
	return gvfs.NewStrictMem()
}
````

## File: internal/vfs/vfs.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package vfs

import (
	"io"
	"os"
	"path/filepath"
	"testing"

	"github.com/cockroachdb/errors/oserror"
	pvfs "github.com/cockroachdb/pebble/vfs"

	gvfs "github.com/lni/vfs"
)

// IFS is the vfs interface used by dragonboat.
type IFS = gvfs.FS

// MemFS is a memory backed file system for testing purposes.
type MemFS = gvfs.MemFS

// DefaultFS is a vfs instance using underlying OS fs.
var DefaultFS IFS = gvfs.Default

// MemStrictFS is a vfs instance using memfs.
var MemStrictFS IFS = gvfs.NewStrictMem()

// File is the file interface returned by IFS.
type File = gvfs.File

// NewMemFS creates a in-memory fs.
func NewMemFS() IFS {
	return gvfs.NewStrictMem()
}

// PebbleFS is a wrapper struct that implements the pebble/vfs.FS interface.
type PebbleFS struct {
	fs IFS
}

var _ pvfs.FS = (*PebbleFS)(nil)

// NewPebbleFS creates a new pebble/vfs.FS instance.
func NewPebbleFS(fs IFS) pvfs.FS {
	return &PebbleFS{fs}
}

// GetDiskUsage ...
func (p *PebbleFS) GetDiskUsage(path string) (pvfs.DiskUsage, error) {
	du, err := p.fs.GetDiskUsage(path)
	return pvfs.DiskUsage{
		AvailBytes: du.AvailBytes,
		TotalBytes: du.TotalBytes,
		UsedBytes:  du.UsedBytes,
	}, err
}

// Create ...
func (p *PebbleFS) Create(name string) (pvfs.File, error) {
	return p.fs.Create(name)
}

// Link ...
func (p *PebbleFS) Link(oldname, newname string) error {
	return p.fs.Link(oldname, newname)
}

// Open ...
func (p *PebbleFS) Open(name string, opts ...pvfs.OpenOption) (pvfs.File, error) {
	f, err := p.fs.Open(name)
	if err != nil {
		return nil, err
	}
	for _, opt := range opts {
		opt.Apply(f)
	}
	return f, nil
}

// OpenDir ...
func (p *PebbleFS) OpenDir(name string) (pvfs.File, error) {
	return p.fs.OpenDir(name)
}

// Remove ...
func (p *PebbleFS) Remove(name string) error {
	return p.fs.Remove(name)
}

// RemoveAll ...
func (p *PebbleFS) RemoveAll(name string) error {
	return p.fs.RemoveAll(name)
}

// Rename ...
func (p *PebbleFS) Rename(oldname, newname string) error {
	return p.fs.Rename(oldname, newname)
}

// ReuseForWrite ...
func (p *PebbleFS) ReuseForWrite(oldname, newname string) (pvfs.File, error) {
	return p.fs.ReuseForWrite(oldname, newname)
}

// MkdirAll ...
func (p *PebbleFS) MkdirAll(dir string, perm os.FileMode) error {
	return p.fs.MkdirAll(dir, perm)
}

// Lock ...
func (p *PebbleFS) Lock(name string) (io.Closer, error) {
	return p.fs.Lock(name)
}

// List ...
func (p *PebbleFS) List(dir string) ([]string, error) {
	return p.fs.List(dir)
}

// Stat ...
func (p *PebbleFS) Stat(name string) (os.FileInfo, error) {
	return p.fs.Stat(name)
}

// PathBase ...
func (p *PebbleFS) PathBase(path string) string {
	return p.fs.PathBase(path)
}

// PathJoin ...
func (p *PebbleFS) PathJoin(elem ...string) string {
	return p.fs.PathJoin(elem...)
}

// PathDir ...
func (p *PebbleFS) PathDir(path string) string {
	return p.fs.PathDir(path)
}

// IsNotExist returns a boolean value indicating whether the specified error is
// to indicate that a file or directory does not exist.
func IsNotExist(err error) bool {
	return oserror.IsNotExist(err)
}

// IsExist returns a boolean value indicating whether the specified error is to
// indicate that a file or directory already exists.
func IsExist(err error) bool {
	return oserror.IsExist(err)
}

// TempDir returns the directory use for storing temporary files.
func TempDir() string {
	return os.TempDir()
}

// Clean is a wrapper for filepath.Clean.
func Clean(dir string) string {
	return filepath.Clean(dir)
}

// ReportLeakedFD reports leaked file fds.
func ReportLeakedFD(fs IFS, t *testing.T) {
	gvfs.ReportLeakedFD(fs, t)
}
````

## File: logger/capnslogger.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package logger

import (
	"github.com/lni/goutils/logutil/capnslog"
)

const (
	// RepoName is the repo name used in capnslog.
	RepoName = "github.com/lni/dragonboat/v4"
)

// CreateCapnsLog creates an ILogger instance based on capnslog.
func CreateCapnsLog(pkgName string) ILogger {
	return &capnsLog{
		logger: capnslog.NewPackageLogger(RepoName, pkgName),
	}
}

type capnsLog struct {
	logger *capnslog.PackageLogger
}

var _ ILogger = (*capnsLog)(nil)

func (c *capnsLog) SetLevel(level LogLevel) {
	var cl capnslog.LogLevel
	switch level {
	case CRITICAL:
		cl = capnslog.CRITICAL
	case ERROR:
		cl = capnslog.ERROR
	case WARNING:
		cl = capnslog.WARNING
	case INFO:
		cl = capnslog.INFO
	case DEBUG:
		cl = capnslog.DEBUG
	default:
		panic("unexpected level")
	}
	c.logger.SetLevel(cl)
}

func (c *capnsLog) Debugf(format string, args ...interface{}) {
	c.logger.Debugf(format, args...)
}

func (c *capnsLog) Infof(format string, args ...interface{}) {
	c.logger.Infof(format, args...)
}

func (c *capnsLog) Warningf(format string, args ...interface{}) {
	c.logger.Warningf(format, args...)
}

func (c *capnsLog) Errorf(format string, args ...interface{}) {
	c.logger.Errorf(format, args...)
}

func (c *capnsLog) Panicf(format string, args ...interface{}) {
	c.logger.Panicf(format, args...)
}
````

## File: logger/logger.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

/*
Package logger manages loggers used in dragonboat.
*/
package logger

import (
	"sync"

	"github.com/lni/dragonboat/v4/internal/invariants"
)

// LogLevel is the log level defined in dragonboat.
type LogLevel int

const (
	// CRITICAL is the CRITICAL log level
	CRITICAL LogLevel = iota - 1
	// ERROR is the ERROR log level
	ERROR
	// WARNING is the WARNING log level
	WARNING
	// INFO is the INFO log level
	INFO
	// DEBUG is the DEBUG log level
	DEBUG
)

// Factory is the factory method for creating logger used for the
// specified package.
type Factory func(pkgName string) ILogger

// ILogger is the interface implemented by loggers that can be used by
// dragonboat. You can implement your own ILogger implementation by building
// wrapper struct on top of your favourite logging library.
type ILogger interface {
	SetLevel(LogLevel)
	Debugf(format string, args ...interface{})
	Infof(format string, args ...interface{})
	Warningf(format string, args ...interface{})
	Errorf(format string, args ...interface{})
	Panicf(format string, args ...interface{})
}

// SetLoggerFactory sets the factory function used to create ILogger instances.
// This method will panic if called multiple times to prevent nil dereferences,
// multiple logging formats, and other potential issues. Ensure this is called
// only once on startup.
func SetLoggerFactory(f Factory) {
	_loggers.mu.Lock()
	defer _loggers.mu.Unlock()
	if _loggers.loggerFactory != nil {
		panic("setting the logger factory again")
	}
	_loggers.loggerFactory = f
}

// GetLogger returns the logger for the specified package name. The most common
// use case for the returned logger is to set its log verbosity level.
func GetLogger(pkgName string) ILogger {
	return getILogger(pkgName, false)
}

// GetMonkeyLogger returns a logger that only works in monkey test mode.
func GetMonkeyLogger(pkgName string) ILogger {
	return getILogger(pkgName, true)
}

func getILogger(pkgName string, monkey bool) ILogger {
	_loggers.mu.Lock()
	defer _loggers.mu.Unlock()
	l, ok := _loggers.loggers[pkgName]
	if !ok {
		l = &dragonboatLogger{pkgName: pkgName, monkeyLogger: monkey}
		_loggers.loggers[pkgName] = l
	}
	return l
}

type dragonboatLogger struct {
	logger       ILogger
	pkgName      string
	mu           sync.Mutex
	monkeyLogger bool
}

var _ ILogger = (*dragonboatLogger)(nil)

func (d *dragonboatLogger) get() ILogger {
	if d.monkeyLogger && !invariants.MonkeyTest {
		return _nullLogger
	}
	d.mu.Lock()
	defer d.mu.Unlock()
	if d.logger == nil {
		d.logger = _loggers.createILogger(d.pkgName)
	}
	return d.logger
}

func (d *dragonboatLogger) SetLevel(l LogLevel) {
	d.get().SetLevel(l)
}

func (d *dragonboatLogger) Debugf(format string, args ...interface{}) {
	d.get().Debugf(format, args...)
}

func (d *dragonboatLogger) Infof(format string, args ...interface{}) {
	d.get().Infof(format, args...)
}

func (d *dragonboatLogger) Warningf(format string, args ...interface{}) {
	d.get().Warningf(format, args...)
}

func (d *dragonboatLogger) Errorf(format string, args ...interface{}) {
	d.get().Errorf(format, args...)
}

func (d *dragonboatLogger) Panicf(format string, args ...interface{}) {
	d.get().Panicf(format, args...)
}

type sysLoggers struct {
	loggers       map[string]*dragonboatLogger
	loggerFactory Factory
	mu            sync.Mutex
}

func (l *sysLoggers) createILogger(pkgName string) ILogger {
	l.mu.Lock()
	defer l.mu.Unlock()
	if l.loggerFactory == nil {
		return createDefaultILogger(pkgName)
	}
	return l.loggerFactory(pkgName)
}

var _loggers = createSysLoggers()

func createSysLoggers() *sysLoggers {
	s := &sysLoggers{
		loggers: make(map[string]*dragonboatLogger),
	}
	return s
}

func createDefaultILogger(pkgName string) ILogger {
	return CreateCapnsLog(pkgName)
}

type nullLogger struct{}

var _ ILogger = (*nullLogger)(nil)
var _nullLogger = nullLogger{}

func (nullLogger) SetLevel(LogLevel)                           {}
func (nullLogger) Debugf(format string, args ...interface{})   {}
func (nullLogger) Infof(format string, args ...interface{})    {}
func (nullLogger) Warningf(format string, args ...interface{}) {}
func (nullLogger) Errorf(format string, args ...interface{})   {}
func (nullLogger) Panicf(format string, args ...interface{})   {}
````

## File: plugin/chan/chan.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package transport

import (
	"context"
	"sync"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/syncutil"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

var (
	// ErrClosed indicates that the connection has been closed.
	ErrClosed = errors.New("connection closed")
	// ErrFailedToConnect indicates that connecting to the remote failed.
	ErrFailedToConnect = errors.New("failed to connect")
)

type acceptChanConn struct {
	ac  chan chanConn
	acc chan struct{}
}

// the terms send/receive are all from user's pow in this file
type chanConn struct {
	snapshot     bool
	senderClosed chan struct{}
	recverClosed chan struct{}
	dataChan     chan []byte
}

var listening = make(map[string]acceptChanConn)
var listeningMu sync.Mutex

// ChanTransportFactory is a channel based module used for testing purposes.
type ChanTransportFactory struct{}

// Create creates a channel based transport instance.
func (ctm *ChanTransportFactory) Create(nhConfig config.NodeHostConfig,
	handler raftio.MessageHandler,
	chunkHandler raftio.ChunkHandler) raftio.ITransport {
	return NewChanTransport(nhConfig, handler, chunkHandler)
}

// Validate returns a boolean value indicating whether the specified address
// is valid.
func (ctm *ChanTransportFactory) Validate(addr string) bool {
	panic("not suppose to be called")
}

// ChanConnection is a channel based connection.
type ChanConnection struct {
	cc chanConn
}

// Close ...
func (cc *ChanConnection) Close() {
	close(cc.cc.senderClosed)
}

// SendMessageBatch ...
func (cc *ChanConnection) SendMessageBatch(batch pb.MessageBatch) error {
	if cc.cc.snapshot {
		panic("sending message on snapshot cc")
	}
	data := pb.MustMarshal(&batch)
	select {
	case <-cc.cc.recverClosed:
		return ErrClosed
	case cc.cc.dataChan <- data:
	}
	return nil
}

// ChanSSConnection is a channel based snapshot connection.
type ChanSSConnection struct {
	cc chanConn
}

// Close ...
func (csc *ChanSSConnection) Close() {
	close(csc.cc.senderClosed)
}

// SendChunk ...
func (csc *ChanSSConnection) SendChunk(chunk pb.Chunk) error {
	if !csc.cc.snapshot {
		panic("sending snapshot data on regular cc")
	}
	data := pb.MustMarshal(&chunk)
	select {
	case <-csc.cc.recverClosed:
		return ErrClosed
	case csc.cc.dataChan <- data:
	}
	return nil
}

// ChanTransport is a channel based transport module used for testing purposes.
type ChanTransport struct {
	nhConfig       config.NodeHostConfig
	requestHandler raftio.MessageHandler
	chunkHandler   raftio.ChunkHandler
	stopper        *syncutil.Stopper
	connStopper    *syncutil.Stopper
}

// NewChanTransport creates a new channel based test transport module.
func NewChanTransport(nhConfig config.NodeHostConfig,
	requestHandler raftio.MessageHandler,
	chunkHandler raftio.ChunkHandler) raftio.ITransport {
	return &ChanTransport{
		nhConfig:       nhConfig,
		requestHandler: requestHandler,
		chunkHandler:   chunkHandler,
		stopper:        syncutil.NewStopper(),
		connStopper:    syncutil.NewStopper(),
	}
}

// Start ...
func (ct *ChanTransport) Start() error {
	acc := acceptChanConn{
		ac:  make(chan chanConn, 1),
		acc: make(chan struct{}),
	}
	func() {
		listeningMu.Lock()
		defer listeningMu.Unlock()
		listening[ct.nhConfig.RaftAddress] = acc
	}()
	ct.stopper.RunWorker(func() {
		for {
			select {
			case <-ct.stopper.ShouldStop():
				func() {
					listeningMu.Lock()
					defer listeningMu.Unlock()
					close(acc.acc)
					delete(listening, ct.nhConfig.RaftAddress)
				}()
				return
			case cc := <-acc.ac:
				ct.connStopper.RunWorker(func() {
					ct.serveConn(cc)
				})
			}
		}
	})
	return nil
}

// Close ...
func (ct *ChanTransport) Close() error {
	ct.stopper.Stop()
	ct.connStopper.Stop()
	return nil
}

// Name ...
func (ct *ChanTransport) Name() string {
	return "ChanTransport"
}

func (ct *ChanTransport) getConnection(target string,
	snapshot bool) (chanConn, error) {
	listeningMu.Lock()
	defer listeningMu.Unlock()
	acc, ok := listening[target]
	if !ok {
		return chanConn{}, ErrFailedToConnect
	}
	cc := createChanConn(snapshot)
	select {
	case <-acc.acc:
		return chanConn{}, ErrFailedToConnect
	case acc.ac <- cc:
	}
	return cc, nil
}

// GetConnection ...
func (ct *ChanTransport) GetConnection(ctx context.Context,
	target string) (raftio.IConnection, error) {
	cc, err := ct.getConnection(target, false)
	if err != nil {
		return nil, err
	}
	return &ChanConnection{cc: cc}, nil
}

// GetSnapshotConnection ...
func (ct *ChanTransport) GetSnapshotConnection(ctx context.Context,
	target string) (raftio.ISnapshotConnection, error) {
	cc, err := ct.getConnection(target, true)
	if err != nil {
		return nil, err
	}
	return &ChanSSConnection{cc: cc}, nil
}

func (ct *ChanTransport) process(data []byte, cc chanConn) bool {
	if cc.snapshot {
		chunk := pb.Chunk{}
		if err := chunk.Unmarshal(data); err != nil {
			return false
		}
		if !ct.chunkHandler(chunk) {
			return false
		}
	} else {
		batch := pb.MessageBatch{}
		if err := batch.Unmarshal(data); err != nil {
			return false
		}
		ct.requestHandler(batch)
	}
	return true
}

func (ct *ChanTransport) serveConn(cc chanConn) {
	defer close(cc.recverClosed)
	done := false
	for !done {
		select {
		case data := <-cc.dataChan:
			if !ct.process(data, cc) {
				return
			}
		case <-ct.stopper.ShouldStop():
			return
		case <-cc.senderClosed:
			done = true
		}
	}
	for {
		select {
		case data := <-cc.dataChan:
			if !ct.process(data, cc) {
				return
			}
		default:
			return
		}
	}
}

func createChanConn(snapshot bool) chanConn {
	return chanConn{
		snapshot:     snapshot,
		senderClosed: make(chan struct{}),
		recverClosed: make(chan struct{}),
		dataChan:     make(chan []byte, 8),
	}
}
````

## File: plugin/tan/tan.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tan

import (
	"github.com/lni/dragonboat/v4/internal/tan"
)

// Factory is the factory variable used to create tan instances.
var Factory = tan.Factory
````

## File: plugin/tee/tee.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tee

import (
	"github.com/lni/dragonboat/v4/config"
	tl "github.com/lni/dragonboat/v4/internal/logdb/tee"
	"github.com/lni/dragonboat/v4/internal/tan"
	"github.com/lni/dragonboat/v4/raftio"
)

// CreateTanPebbleLogDB creates a Tee LogDB backed by Tan and Pebble.
func CreateTanPebbleLogDB(cfg config.NodeHostConfig, cb config.LogDBCallback,
	dirs []string, wals []string) (raftio.ILogDB, error) {
	ndirs := make([]string, 0)
	nwals := make([]string, 0)
	fs := cfg.Expert.FS
	for _, v := range dirs {
		ndirs = append(ndirs, fs.PathJoin(v, "tee-tan"))
	}
	for _, v := range wals {
		nwals = append(nwals, fs.PathJoin(v, "tee-tan"))
	}
	tdb, err := tan.CreateTan(cfg, cb, ndirs, nwals)
	if err != nil {
		return nil, err
	}
	pdb, err := tl.NewPebbleLogDB(cfg, cb, dirs, wals)
	if err != nil {
		return nil, err
	}
	return tl.MakeTeeLogDB(tdb, pdb), nil
}

// TanPebbleLogDBFactory is the factory for creating a tan and pebble backed
// tee LogDB instance.
var TanPebbleLogDBFactory = tanPebbleLogDBFactory{}

type tanPebbleLogDBFactory struct{}

func (tanPebbleLogDBFactory) Create(cfg config.NodeHostConfig,
	cb config.LogDBCallback, dirs []string, wals []string) (raftio.ILogDB, error) {
	return CreateTanPebbleLogDB(cfg, cb, dirs, wals)
}

func (tanPebbleLogDBFactory) Name() string {
	return "tan-pebble-tee"
}
````

## File: raftio/binversion.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raftio

const (
	// LogDBBinVersion is the current logdb binary compatibility version
	// implemented in Dragonboat.
	// For v1.4  BinVersion = 100
	//     v2.0  BinVersion = 210
	LogDBBinVersion uint32 = 210
	// PlainLogDBBinVersion is the logdb binary compatibility version value when
	// plain entries are used in ILogDB.
	PlainLogDBBinVersion uint32 = 100
	// TransportBinVersion is the transport binary compatibility version implemented in
	// Dragonboat.
	// For v1.4  TransportBinLog = 100
	//     v2.0  TransportBinLog = 210
	TransportBinVersion uint32 = 210
)
````

## File: raftio/listener.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raftio

const (
	// NoLeader is a special leader ID value to indicate that there is currently
	// no leader or leader ID is unknown.
	NoLeader uint64 = 0
)

// LeaderInfo contains info on Raft leader.
type LeaderInfo struct {
	ShardID   uint64
	ReplicaID uint64
	Term      uint64
	LeaderID  uint64
}

// IRaftEventListener is the interface to allow users to get notified for
// certain Raft events.
type IRaftEventListener interface {
	LeaderUpdated(info LeaderInfo)
}

// EntryInfo contains info on log entries.
type EntryInfo struct {
	ShardID   uint64
	ReplicaID uint64
	Index     uint64
}

// SnapshotInfo contains info of the snapshot.
type SnapshotInfo struct {
	ShardID   uint64
	ReplicaID uint64
	From      uint64
	Index     uint64
}

// ConnectionInfo contains info of the connection.
type ConnectionInfo struct {
	Address            string
	SnapshotConnection bool
}

// ISystemEventListener is the system event listener used by the NodeHost.
type ISystemEventListener interface {
	NodeHostShuttingDown()
	NodeUnloaded(info NodeInfo)
	NodeDeleted(info NodeInfo)
	NodeReady(info NodeInfo)
	MembershipChanged(info NodeInfo)
	ConnectionEstablished(info ConnectionInfo)
	ConnectionFailed(info ConnectionInfo)
	SendSnapshotStarted(info SnapshotInfo)
	SendSnapshotCompleted(info SnapshotInfo)
	SendSnapshotAborted(info SnapshotInfo)
	SnapshotReceived(info SnapshotInfo)
	SnapshotRecovered(info SnapshotInfo)
	SnapshotCreated(info SnapshotInfo)
	SnapshotCompacted(info SnapshotInfo)
	LogCompacted(info EntryInfo)
	LogDBCompacted(info EntryInfo)
}
````

## File: raftio/logdb.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raftio

import (
	"github.com/cockroachdb/errors"

	pb "github.com/lni/dragonboat/v4/raftpb"
)

var (
	// ErrNoSavedLog indicates no saved log.
	ErrNoSavedLog = errors.New("no saved log")
	// ErrNoBootstrapInfo indicates that there is no saved bootstrap info.
	ErrNoBootstrapInfo = errors.New("no bootstrap info")
)

// Metrics is the metrics of the LogDB.
type Metrics struct {
	// Busy indicates whether the LogDB is busy and not suitable for saving new
	// data into the store.
	Busy bool
}

// NodeInfo is used to identify a Raft node.
type NodeInfo struct {
	ShardID   uint64
	ReplicaID uint64
}

// RaftState is the persistent Raft state found in the Log DB.
type RaftState struct {
	// State is the Raft state persistent to the disk
	State pb.State
	// FirstIndex is the index of the first entry to iterate
	FirstIndex uint64
	// EntryCount is the number of entries to iterate
	EntryCount uint64
}

// GetNodeInfo returns a NodeInfo instance with the specified shard ID
// and replica ID.
func GetNodeInfo(shardID uint64, replicaID uint64) NodeInfo {
	return NodeInfo{ShardID: shardID, ReplicaID: replicaID}
}

// ILogDB is the interface implemented by the log DB for persistently store
// Raft states, log entries and other Raft metadata.
type ILogDB interface {
	// Name returns the type name of the ILogDB instance.
	Name() string
	// Close closes the ILogDB instance.
	Close() error
	// BinaryFormat returns an constant uint32 value representing the binary
	// format version compatible with the ILogDB instance.
	BinaryFormat() uint32
	// ListNodeInfo lists all available NodeInfo found in the log DB.
	ListNodeInfo() ([]NodeInfo, error)
	// SaveBootstrapInfo saves the specified bootstrap info to the log DB.
	SaveBootstrapInfo(shardID uint64,
		replicaID uint64, bootstrap pb.Bootstrap) error
	// GetBootstrapInfo returns saved bootstrap info from log DB. It returns
	// ErrNoBootstrapInfo when there is no previously saved bootstrap info for
	// the specified node.
	GetBootstrapInfo(shardID uint64, replicaID uint64) (pb.Bootstrap, error)
	// SaveRaftState atomically saves the Raft states, log entries and snapshots
	// metadata found in the pb.Update list to the log DB. shardID is a 1-based
	// ID of the worker invoking the SaveRaftState method, as each worker
	// accesses the log DB from its own thread, SaveRaftState will never be
	// concurrently called with the same shardID.
	SaveRaftState(updates []pb.Update, shardID uint64) error
	// IterateEntries returns the continuous Raft log entries of the specified
	// Raft node between the index value range of [low, high) up to a max size
	// limit of maxSize bytes. It returns the located log entries, their total
	// size in bytes and the occurred error.
	IterateEntries(ents []pb.Entry,
		size uint64, shardID uint64, replicaID uint64, low uint64,
		high uint64, maxSize uint64) ([]pb.Entry, uint64, error)
	// ReadRaftState returns the persistented raft state found in Log DB.
	ReadRaftState(shardID uint64,
		replicaID uint64, lastIndex uint64) (RaftState, error)
	// RemoveEntriesTo removes entries with indexes between (0, index].
	RemoveEntriesTo(shardID uint64, replicaID uint64, index uint64) error
	// CompactEntriesTo reclaims underlying storage space used for storing
	// entries up to the specified index.
	CompactEntriesTo(shardID uint64,
		replicaID uint64, index uint64) (<-chan struct{}, error)
	// SaveSnapshots saves all snapshot metadata found in the pb.Update list.
	SaveSnapshots([]pb.Update) error
	// GetSnapshot returns the most recent snapshot associated with the specified
	// shard.
	GetSnapshot(shardID uint64, replicaID uint64) (pb.Snapshot, error)
	// RemoveNodeData removes all data associated with the specified node.
	RemoveNodeData(shardID uint64, replicaID uint64) error
	// ImportSnapshot imports the specified snapshot by creating all required
	// metadata in the logdb.
	ImportSnapshot(snapshot pb.Snapshot, replicaID uint64) error
}
````

## File: raftio/registry.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

/*
Package raftio contains structs, interfaces and function definitions required
to build custom persistent Raft log storage and transport modules.

Structs, interfaces and functions defined in the raftio package are only
required when building your custom persistent Raft log storage or transport
modules. Skip this package if you plan to use the default built-in LogDB and
transport modules provided by Dragonboat.

Structs, interfaces and functions defined in the raftio package are not
considered as a part of Dragonboat's public APIs. Breaking changes might
happen in the coming minor releases.
*/
package raftio

// INodeRegistry is the registry interface used to resolve all known
// NodeHosts and their shards and replicas in the system.
type INodeRegistry interface {
	Close() error
	Add(shardID uint64, replicaID uint64, url string)
	Remove(shardID uint64, replicaID uint64)
	RemoveShard(shardID uint64)
	Resolve(shardID uint64, replicaID uint64) (string, string, error)
}
````

## File: raftio/transport.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

/*
Package raftio contains structs, interfaces and function definitions required
to build custom persistent Raft log storage and transport modules.

Structs, interfaces and functions defined in the raftio package are only
required when building your custom persistent Raft log storage or transport
modules. Skip this package if you plan to use the default built-in LogDB and
transport modules provided by Dragonboat.

Structs, interfaces and functions defined in the raftio package are not
considered as a part of Dragonboat's public APIs. Breaking changes might
happen in the coming minor releases.
*/
package raftio

import (
	"context"

	pb "github.com/lni/dragonboat/v4/raftpb"
)

// MessageHandler is the handler function type for handling received message
// batches. Received message batches should be passed to the message handler to
// be processed.
type MessageHandler func(pb.MessageBatch)

// ChunkHandler is the handler function type for handling received snapshot
// chunks. It adds the new snapshot chunk to the snapshot chunk sink. Chunks
// from the same snapshot are combined into the snapshot image and then
// be passed to dragonboat.
//
// ChunkHandler returns a boolean value indicating whether the snapshot
// connection is still valid for accepting future snapshot chunks.
type ChunkHandler func(pb.Chunk) bool

// IConnection is the interface used by the transport module for sending Raft
// messages. Each IConnection works for a specified target NodeHost instance,
// it is possible for a target to have multiple concurrent IConnection
// instances in use.
type IConnection interface {
	// Close closes the IConnection instance.
	Close()
	// SendMessageBatch sends the specified message batch to the target. It is
	// recommended to deliver the message batch to the target in order to enjoy
	// the best possible performance, but out of order delivery is allowed at the
	// cost of reduced performance.
	SendMessageBatch(batch pb.MessageBatch) error
}

// ISnapshotConnection is the interface used by the transport module for sending
// snapshot chunks. Each ISnapshotConnection works for a specified target
// NodeHost instance.
type ISnapshotConnection interface {
	// Close closes the ISnapshotConnection instance.
	Close()
	// SendChunk sends the snapshot chunk to the target. It is
	// recommended to have the snapshot chunk delivered in order for the best
	// performance, but out of order delivery is allowed at the cost of reduced
	// performance.
	SendChunk(chunk pb.Chunk) error
}

// ITransport is the interface to be implemented by a customized transport
// module. A transport module is responsible for exchanging Raft messages,
// snapshots and other metadata between NodeHost instances.
type ITransport interface {
	// Name returns the type name of the ITransport instance.
	Name() string
	// Start launches the transport module and make it ready to start sending and
	// receiving Raft messages. If necessary, ITransport may take this opportunity
	// to start listening for incoming data.
	Start() error
	// Close closes the transport module.
	Close() error
	// GetConnection returns an IConnection instance used for sending messages
	// to the specified target NodeHost instance.
	GetConnection(ctx context.Context, target string) (IConnection, error)
	// GetSnapshotConnection returns an ISnapshotConnection instance used for
	// sending snapshot chunks to the specified target NodeHost instance.
	GetSnapshotConnection(ctx context.Context,
		target string) (ISnapshotConnection, error)
}
````

## File: raftpb/bootstrap.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: raft.proto

package raftpb

import (
	"fmt"
	"io"
)

type Bootstrap struct {
	Addresses map[uint64]string
	Join      bool
	Type      StateMachineType
}

func (m *Bootstrap) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *Bootstrap) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if len(m.Addresses) > 0 {
		for k, _ := range m.Addresses {
			dAtA[i] = 0xa
			i++
			v := m.Addresses[k]
			mapSize := 1 + sovRaft(uint64(k)) + 1 + len(v) + sovRaft(uint64(len(v)))
			i = encodeVarintRaft(dAtA, i, uint64(mapSize))
			dAtA[i] = 0x8
			i++
			i = encodeVarintRaft(dAtA, i, uint64(k))
			dAtA[i] = 0x12
			i++
			i = encodeVarintRaft(dAtA, i, uint64(len(v)))
			i += copy(dAtA[i:], v)
		}
	}
	dAtA[i] = 0x10
	i++
	if m.Join {
		dAtA[i] = 1
	} else {
		dAtA[i] = 0
	}
	i++
	dAtA[i] = 0x18
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Type))
	return i, nil
}

func (m *Bootstrap) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	if len(m.Addresses) > 0 {
		for k, v := range m.Addresses {
			_ = k
			_ = v
			mapEntrySize := 1 + sovRaft(uint64(k)) + 1 + len(v) + sovRaft(uint64(len(v)))
			n += mapEntrySize + 1 + sovRaft(uint64(mapEntrySize))
		}
	}
	n += 2
	n += 1 + sovRaft(uint64(m.Type))
	return n
}

func (m *Bootstrap) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRaft
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: Bootstrap: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: Bootstrap: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Addresses", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.Addresses == nil {
				m.Addresses = make(map[uint64]string)
			}
			var mapkey uint64
			var mapvalue string
			for iNdEx < postIndex {
				entryPreIndex := iNdEx
				var wire uint64
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowRaft
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					wire |= uint64(b&0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				fieldNum := int32(wire >> 3)
				if fieldNum == 1 {
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowRaft
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						mapkey |= uint64(b&0x7F) << shift
						if b < 0x80 {
							break
						}
					}
				} else if fieldNum == 2 {
					var stringLenmapvalue uint64
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowRaft
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						stringLenmapvalue |= uint64(b&0x7F) << shift
						if b < 0x80 {
							break
						}
					}
					intStringLenmapvalue := int(stringLenmapvalue)
					if intStringLenmapvalue < 0 {
						return ErrInvalidLengthRaft
					}
					postStringIndexmapvalue := iNdEx + intStringLenmapvalue
					if postStringIndexmapvalue < 0 {
						return ErrInvalidLengthRaft
					}
					if postStringIndexmapvalue > l {
						return io.ErrUnexpectedEOF
					}
					mapvalue = string(dAtA[iNdEx:postStringIndexmapvalue])
					iNdEx = postStringIndexmapvalue
				} else {
					iNdEx = entryPreIndex
					skippy, err := skipRaft(dAtA[iNdEx:])
					if err != nil {
						return err
					}
					if skippy < 0 {
						return ErrInvalidLengthRaft
					}
					if (iNdEx + skippy) > postIndex {
						return io.ErrUnexpectedEOF
					}
					iNdEx += skippy
				}
			}
			m.Addresses[mapkey] = mapvalue
			iNdEx = postIndex
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Join", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.Join = bool(v != 0)
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Type", wireType)
			}
			m.Type = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Type |= StateMachineType(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		default:
			iNdEx = preIndex
			skippy, err := skipRaft(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
````

## File: raftpb/chunk.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: raft.proto

package raftpb

import (
	"fmt"
	"io"
)

type Chunk struct {
	ShardID        uint64
	ReplicaID      uint64
	From           uint64
	ChunkId        uint64
	ChunkSize      uint64
	ChunkCount     uint64
	Data           []byte
	Index          uint64
	Term           uint64
	Membership     Membership
	Filepath       string
	FileSize       uint64
	DeploymentId   uint64
	FileChunkId    uint64
	FileChunkCount uint64
	HasFileInfo    bool
	FileInfo       SnapshotFile
	BinVer         uint32
	OnDiskIndex    uint64
	Witness        bool
}

func (m *Chunk) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *Chunk) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	dAtA[i] = 0x8
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.ShardID))
	dAtA[i] = 0x10
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.ReplicaID))
	dAtA[i] = 0x18
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.From))
	dAtA[i] = 0x20
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.ChunkId))
	dAtA[i] = 0x28
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.ChunkSize))
	dAtA[i] = 0x30
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.ChunkCount))
	if m.Data != nil {
		dAtA[i] = 0x3a
		i++
		i = encodeVarintRaft(dAtA, i, uint64(len(m.Data)))
		i += copy(dAtA[i:], m.Data)
	}
	dAtA[i] = 0x40
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Index))
	dAtA[i] = 0x48
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Term))
	dAtA[i] = 0x52
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Membership.Size()))
	n3, err := m.Membership.MarshalTo(dAtA[i:])
	if err != nil {
		return 0, err
	}
	i += n3
	dAtA[i] = 0x62
	i++
	i = encodeVarintRaft(dAtA, i, uint64(len(m.Filepath)))
	i += copy(dAtA[i:], m.Filepath)
	dAtA[i] = 0x68
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.FileSize))
	dAtA[i] = 0x70
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.DeploymentId))
	dAtA[i] = 0x78
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.FileChunkId))
	dAtA[i] = 0x80
	i++
	dAtA[i] = 0x1
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.FileChunkCount))
	dAtA[i] = 0x88
	i++
	dAtA[i] = 0x1
	i++
	if m.HasFileInfo {
		dAtA[i] = 1
	} else {
		dAtA[i] = 0
	}
	i++
	dAtA[i] = 0x92
	i++
	dAtA[i] = 0x1
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.FileInfo.Size()))
	n4, err := m.FileInfo.MarshalTo(dAtA[i:])
	if err != nil {
		return 0, err
	}
	i += n4
	dAtA[i] = 0x98
	i++
	dAtA[i] = 0x1
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.BinVer))
	dAtA[i] = 0xa0
	i++
	dAtA[i] = 0x1
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.OnDiskIndex))
	dAtA[i] = 0xa8
	i++
	dAtA[i] = 0x1
	i++
	if m.Witness {
		dAtA[i] = 1
	} else {
		dAtA[i] = 0
	}
	i++
	return i, nil
}

func (m *Chunk) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	n += 1 + sovRaft(uint64(m.ShardID))
	n += 1 + sovRaft(uint64(m.ReplicaID))
	n += 1 + sovRaft(uint64(m.From))
	n += 1 + sovRaft(uint64(m.ChunkId))
	n += 1 + sovRaft(uint64(m.ChunkSize))
	n += 1 + sovRaft(uint64(m.ChunkCount))
	if m.Data != nil {
		l = len(m.Data)
		n += 1 + l + sovRaft(uint64(l))
	}
	n += 1 + sovRaft(uint64(m.Index))
	n += 1 + sovRaft(uint64(m.Term))
	l = m.Membership.Size()
	n += 1 + l + sovRaft(uint64(l))
	l = len(m.Filepath)
	n += 1 + l + sovRaft(uint64(l))
	n += 1 + sovRaft(uint64(m.FileSize))
	n += 1 + sovRaft(uint64(m.DeploymentId))
	n += 1 + sovRaft(uint64(m.FileChunkId))
	n += 2 + sovRaft(uint64(m.FileChunkCount))
	n += 3
	l = m.FileInfo.Size()
	n += 2 + l + sovRaft(uint64(l))
	n += 2 + sovRaft(uint64(m.BinVer))
	n += 2 + sovRaft(uint64(m.OnDiskIndex))
	n += 3
	return n
}

func (m *Chunk) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRaft
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: Chunk: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: Chunk: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ShardId", wireType)
			}
			m.ShardID = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ShardID |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field NodeId", wireType)
			}
			m.ReplicaID = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ReplicaID |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field From", wireType)
			}
			m.From = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.From |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ChunkId", wireType)
			}
			m.ChunkId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ChunkId |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 5:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ChunkSize", wireType)
			}
			m.ChunkSize = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ChunkSize |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 6:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ChunkCount", wireType)
			}
			m.ChunkCount = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ChunkCount |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 7:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Data", wireType)
			}
			var byteLen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				byteLen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if byteLen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + byteLen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Data = append(m.Data[:0], dAtA[iNdEx:postIndex]...)
			if m.Data == nil {
				m.Data = []byte{}
			}
			iNdEx = postIndex
		case 8:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Index", wireType)
			}
			m.Index = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Index |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 9:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Term", wireType)
			}
			m.Term = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Term |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 10:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Membership", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if err := m.Membership.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex

		case 12:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Filepath", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Filepath = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 13:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field FileSize", wireType)
			}
			m.FileSize = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.FileSize |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 14:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field DeploymentId", wireType)
			}
			m.DeploymentId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.DeploymentId |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 15:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field FileChunkId", wireType)
			}
			m.FileChunkId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.FileChunkId |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 16:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field FileChunkCount", wireType)
			}
			m.FileChunkCount = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.FileChunkCount |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}

		case 17:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field HasFileInfo", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.HasFileInfo = bool(v != 0)
		case 18:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field FileInfo", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if err := m.FileInfo.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 19:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field BinVer", wireType)
			}
			m.BinVer = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.BinVer |= uint32(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 20:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field OnDiskIndex", wireType)
			}
			m.OnDiskIndex = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.OnDiskIndex |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 21:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Witness", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.Witness = bool(v != 0)
		default:
			iNdEx = preIndex
			skippy, err := skipRaft(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}
	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
````

## File: raftpb/common.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: raft.proto

package raftpb

import (
	"fmt"
	"io"
)

func encodeVarintRaft(dAtA []byte, offset int, v uint64) int {
	for v >= 1<<7 {
		dAtA[offset] = uint8(v&0x7f | 0x80)
		v >>= 7
		offset++
	}
	dAtA[offset] = uint8(v)
	return offset + 1
}

func sovRaft(x uint64) (n int) {
	for {
		n++
		x >>= 7
		if x == 0 {
			break
		}
	}
	return n
}

func sozRaft(x uint64) (n int) {
	return sovRaft(uint64((x << 1) ^ uint64((int64(x) >> 63))))
}

func skipRaft(dAtA []byte) (n int, err error) {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return 0, ErrIntOverflowRaft
			}
			if iNdEx >= l {
				return 0, io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		wireType := int(wire & 0x7)
		switch wireType {
		case 0:
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				iNdEx++
				if dAtA[iNdEx-1] < 0x80 {
					break
				}
			}
			return iNdEx, nil
		case 1:
			iNdEx += 8
			return iNdEx, nil
		case 2:
			var length int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				length |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if length < 0 {
				return 0, ErrInvalidLengthRaft
			}
			iNdEx += length
			if iNdEx < 0 {
				return 0, ErrInvalidLengthRaft
			}
			return iNdEx, nil
		case 3:
			for {
				var innerWire uint64
				var start int = iNdEx
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return 0, ErrIntOverflowRaft
					}
					if iNdEx >= l {
						return 0, io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					innerWire |= (uint64(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				innerWireType := int(innerWire & 0x7)
				if innerWireType == 4 {
					break
				}
				next, err := skipRaft(dAtA[start:])
				if err != nil {
					return 0, err
				}
				iNdEx = start + next
				if iNdEx < 0 {
					return 0, ErrInvalidLengthRaft
				}
			}
			return iNdEx, nil
		case 4:
			return iNdEx, nil
		case 5:
			iNdEx += 4
			return iNdEx, nil
		default:
			return 0, fmt.Errorf("proto: illegal wireType %d", wireType)
		}
	}
	panic("unreachable")
}

var (
	ErrInvalidLengthRaft = fmt.Errorf("proto: negative length found during unmarshaling")
	ErrIntOverflowRaft   = fmt.Errorf("proto: integer overflow")
)
````

## File: raftpb/configchange.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: raft.proto

package raftpb

import (
	"fmt"
	"io"
)

type ConfigChange struct {
	ConfigChangeId uint64
	Type           ConfigChangeType
	ReplicaID      uint64
	Address        string
	Initialize     bool
}

func (m *ConfigChange) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *ConfigChange) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	dAtA[i] = 0x8
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.ConfigChangeId))
	dAtA[i] = 0x10
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Type))
	dAtA[i] = 0x18
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.ReplicaID))
	dAtA[i] = 0x22
	i++
	i = encodeVarintRaft(dAtA, i, uint64(len(m.Address)))
	i += copy(dAtA[i:], m.Address)
	dAtA[i] = 0x28
	i++
	if m.Initialize {
		dAtA[i] = 1
	} else {
		dAtA[i] = 0
	}
	i++
	return i, nil
}

func (m *ConfigChange) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	n += 1 + sovRaft(uint64(m.ConfigChangeId))
	n += 1 + sovRaft(uint64(m.Type))
	n += 1 + sovRaft(uint64(m.ReplicaID))
	l = len(m.Address)
	n += 1 + l + sovRaft(uint64(l))
	n += 2
	return n
}

func (m *ConfigChange) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRaft
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: ConfigChange: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: ConfigChange: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ConfigChangeId", wireType)
			}
			m.ConfigChangeId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ConfigChangeId |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Type", wireType)
			}
			m.Type = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Type |= ConfigChangeType(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ReplicaID", wireType)
			}
			m.ReplicaID = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ReplicaID |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Address", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Address = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 5:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Initialize", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.Initialize = bool(v != 0)
		default:
			iNdEx = preIndex
			skippy, err := skipRaft(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
````

## File: raftpb/entry.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: raft.proto

package raftpb

type Entry struct {
	Term        uint64
	Index       uint64
	Type        EntryType
	Key         uint64
	ClientID    uint64
	SeriesID    uint64
	RespondedTo uint64
	Cmd         []byte
}

func (m *Entry) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func EntriesToApply(entries []Entry, applied uint64, strict bool) []Entry {
	if len(entries) == 0 {
		return entries
	}
	lastIndex := entries[len(entries)-1].Index
	firstIndex := entries[0].Index
	if lastIndex <= applied {
		if strict {
			plog.Panicf("got entries [%d-%d] older than current state %d",
				firstIndex, lastIndex, applied)
		}
		return []Entry{}
	}
	if firstIndex > applied+1 {
		plog.Panicf("entry hole found: %d, want: %d", firstIndex, applied+1)
	}
	if applied-firstIndex+1 < uint64(len(entries)) {
		return entries[applied-firstIndex+1:]
	}
	return []Entry{}
}
````

## File: raftpb/entrybatch.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: raft.proto

package raftpb

import (
	"fmt"
	"io"
)

type EntryBatch struct {
	Entries []Entry
}

func (m *EntryBatch) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *EntryBatch) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if len(m.Entries) > 0 {
		for _, msg := range m.Entries {
			dAtA[i] = 0xa
			i++
			i = encodeVarintRaft(dAtA, i, uint64(msg.Size()))
			n, err := msg.MarshalTo(dAtA[i:])
			if err != nil {
				return 0, err
			}
			i += n
		}
	}
	return i, nil
}

func (m *EntryBatch) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	if len(m.Entries) > 0 {
		for _, e := range m.Entries {
			l = e.Size()
			n += 1 + l + sovRaft(uint64(l))
		}
	}
	return n
}

func (m *EntryBatch) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRaft
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: EntryBatch: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: EntryBatch: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Entries", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Entries = append(m.Entries, Entry{})
			if err := m.Entries[len(m.Entries)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipRaft(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
````

## File: raftpb/fuzz.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +build gofuzz

package raftpb

import (
	"fmt"
	"reflect"
)

func Fuzz(data []byte) int {
	e := Entry{}
	if err := e.Unmarshal(data); err != nil {
		return 0
	}
	m, err := e.Marshal()
	if err != nil {
		panic(err)
	}
	e2 := Entry{}
	if err := e2.Unmarshal(m); err != nil {
		panic(err)
	}
	if len(e.Data) == 0 {
		e.Data = nil
	}
	if len(e2.Data) == 0 {
		e.Data = nil
	}
	if !reflect.DeepEqual(&e2, &e) {
		msg := fmt.Sprintf("\ne1: %v\ne2: %v\n", e, e2)
		panic(msg)
	}

	return 1
}
````

## File: raftpb/membership.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: raft.proto

package raftpb

import (
	"fmt"
	"io"
)

type Membership struct {
	ConfigChangeId uint64
	Addresses      map[uint64]string
	Removed        map[uint64]bool
	NonVotings     map[uint64]string
	Witnesses      map[uint64]string
}

func (m *Membership) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *Membership) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	dAtA[i] = 0x8
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.ConfigChangeId))
	if len(m.Addresses) > 0 {
		for k, _ := range m.Addresses {
			dAtA[i] = 0x12
			i++
			v := m.Addresses[k]
			mapSize := 1 + sovRaft(uint64(k)) + 1 + len(v) + sovRaft(uint64(len(v)))
			i = encodeVarintRaft(dAtA, i, uint64(mapSize))
			dAtA[i] = 0x8
			i++
			i = encodeVarintRaft(dAtA, i, uint64(k))
			dAtA[i] = 0x12
			i++
			i = encodeVarintRaft(dAtA, i, uint64(len(v)))
			i += copy(dAtA[i:], v)
		}
	}
	if len(m.Removed) > 0 {
		for k, _ := range m.Removed {
			dAtA[i] = 0x1a
			i++
			v := m.Removed[k]
			mapSize := 1 + sovRaft(uint64(k)) + 1 + 1
			i = encodeVarintRaft(dAtA, i, uint64(mapSize))
			dAtA[i] = 0x8
			i++
			i = encodeVarintRaft(dAtA, i, uint64(k))
			dAtA[i] = 0x10
			i++
			if v {
				dAtA[i] = 1
			} else {
				dAtA[i] = 0
			}
			i++
		}
	}
	if len(m.NonVotings) > 0 {
		for k, _ := range m.NonVotings {
			dAtA[i] = 0x22
			i++
			v := m.NonVotings[k]
			mapSize := 1 + sovRaft(uint64(k)) + 1 + len(v) + sovRaft(uint64(len(v)))
			i = encodeVarintRaft(dAtA, i, uint64(mapSize))
			dAtA[i] = 0x8
			i++
			i = encodeVarintRaft(dAtA, i, uint64(k))
			dAtA[i] = 0x12
			i++
			i = encodeVarintRaft(dAtA, i, uint64(len(v)))
			i += copy(dAtA[i:], v)
		}
	}
	if len(m.Witnesses) > 0 {
		for k, _ := range m.Witnesses {
			dAtA[i] = 0x2a
			i++
			v := m.Witnesses[k]
			mapSize := 1 + sovRaft(uint64(k)) + 1 + len(v) + sovRaft(uint64(len(v)))
			i = encodeVarintRaft(dAtA, i, uint64(mapSize))
			dAtA[i] = 0x8
			i++
			i = encodeVarintRaft(dAtA, i, uint64(k))
			dAtA[i] = 0x12
			i++
			i = encodeVarintRaft(dAtA, i, uint64(len(v)))
			i += copy(dAtA[i:], v)
		}
	}
	return i, nil
}

func (m *Membership) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	n += 1 + sovRaft(uint64(m.ConfigChangeId))
	if len(m.Addresses) > 0 {
		for k, v := range m.Addresses {
			_ = k
			_ = v
			mapEntrySize := 1 + sovRaft(uint64(k)) + 1 + len(v) + sovRaft(uint64(len(v)))
			n += mapEntrySize + 1 + sovRaft(uint64(mapEntrySize))
		}
	}
	if len(m.Removed) > 0 {
		for k, v := range m.Removed {
			_ = k
			_ = v
			mapEntrySize := 1 + sovRaft(uint64(k)) + 1 + 1
			n += mapEntrySize + 1 + sovRaft(uint64(mapEntrySize))
		}
	}
	if len(m.NonVotings) > 0 {
		for k, v := range m.NonVotings {
			_ = k
			_ = v
			mapEntrySize := 1 + sovRaft(uint64(k)) + 1 + len(v) + sovRaft(uint64(len(v)))
			n += mapEntrySize + 1 + sovRaft(uint64(mapEntrySize))
		}
	}
	if len(m.Witnesses) > 0 {
		for k, v := range m.Witnesses {
			_ = k
			_ = v
			mapEntrySize := 1 + sovRaft(uint64(k)) + 1 + len(v) + sovRaft(uint64(len(v)))
			n += mapEntrySize + 1 + sovRaft(uint64(mapEntrySize))
		}
	}
	return n
}

func (m *Membership) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRaft
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: Membership: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: Membership: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ConfigChangeId", wireType)
			}
			m.ConfigChangeId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ConfigChangeId |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Addresses", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.Addresses == nil {
				m.Addresses = make(map[uint64]string)
			}
			var mapkey uint64
			var mapvalue string
			for iNdEx < postIndex {
				entryPreIndex := iNdEx
				var wire uint64
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowRaft
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					wire |= uint64(b&0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				fieldNum := int32(wire >> 3)
				if fieldNum == 1 {
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowRaft
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						mapkey |= uint64(b&0x7F) << shift
						if b < 0x80 {
							break
						}
					}
				} else if fieldNum == 2 {
					var stringLenmapvalue uint64
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowRaft
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						stringLenmapvalue |= uint64(b&0x7F) << shift
						if b < 0x80 {
							break
						}
					}
					intStringLenmapvalue := int(stringLenmapvalue)
					if intStringLenmapvalue < 0 {
						return ErrInvalidLengthRaft
					}
					postStringIndexmapvalue := iNdEx + intStringLenmapvalue
					if postStringIndexmapvalue < 0 {
						return ErrInvalidLengthRaft
					}
					if postStringIndexmapvalue > l {
						return io.ErrUnexpectedEOF
					}
					mapvalue = string(dAtA[iNdEx:postStringIndexmapvalue])
					iNdEx = postStringIndexmapvalue
				} else {
					iNdEx = entryPreIndex
					skippy, err := skipRaft(dAtA[iNdEx:])
					if err != nil {
						return err
					}
					if skippy < 0 {
						return ErrInvalidLengthRaft
					}
					if (iNdEx + skippy) > postIndex {
						return io.ErrUnexpectedEOF
					}
					iNdEx += skippy
				}
			}
			m.Addresses[mapkey] = mapvalue
			iNdEx = postIndex
		case 3:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Removed", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.Removed == nil {
				m.Removed = make(map[uint64]bool)
			}
			var mapkey uint64
			var mapvalue bool
			for iNdEx < postIndex {
				entryPreIndex := iNdEx
				var wire uint64
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowRaft
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					wire |= uint64(b&0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				fieldNum := int32(wire >> 3)
				if fieldNum == 1 {
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowRaft
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						mapkey |= uint64(b&0x7F) << shift
						if b < 0x80 {
							break
						}
					}
				} else if fieldNum == 2 {
					var mapvaluetemp int
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowRaft
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						mapvaluetemp |= int(b&0x7F) << shift
						if b < 0x80 {
							break
						}
					}
					mapvalue = bool(mapvaluetemp != 0)
				} else {
					iNdEx = entryPreIndex
					skippy, err := skipRaft(dAtA[iNdEx:])
					if err != nil {
						return err
					}
					if skippy < 0 {
						return ErrInvalidLengthRaft
					}
					if (iNdEx + skippy) > postIndex {
						return io.ErrUnexpectedEOF
					}
					iNdEx += skippy
				}
			}
			m.Removed[mapkey] = mapvalue
			iNdEx = postIndex

		case 4:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field NonVotings", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.NonVotings == nil {
				m.NonVotings = make(map[uint64]string)
			}
			var mapkey uint64
			var mapvalue string
			for iNdEx < postIndex {
				entryPreIndex := iNdEx
				var wire uint64
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowRaft
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					wire |= uint64(b&0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				fieldNum := int32(wire >> 3)
				if fieldNum == 1 {
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowRaft
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						mapkey |= uint64(b&0x7F) << shift
						if b < 0x80 {
							break
						}
					}
				} else if fieldNum == 2 {
					var stringLenmapvalue uint64
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowRaft
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						stringLenmapvalue |= uint64(b&0x7F) << shift
						if b < 0x80 {
							break
						}
					}
					intStringLenmapvalue := int(stringLenmapvalue)
					if intStringLenmapvalue < 0 {
						return ErrInvalidLengthRaft
					}
					postStringIndexmapvalue := iNdEx + intStringLenmapvalue
					if postStringIndexmapvalue < 0 {
						return ErrInvalidLengthRaft
					}
					if postStringIndexmapvalue > l {
						return io.ErrUnexpectedEOF
					}
					mapvalue = string(dAtA[iNdEx:postStringIndexmapvalue])
					iNdEx = postStringIndexmapvalue
				} else {
					iNdEx = entryPreIndex
					skippy, err := skipRaft(dAtA[iNdEx:])
					if err != nil {
						return err
					}
					if skippy < 0 {
						return ErrInvalidLengthRaft
					}
					if (iNdEx + skippy) > postIndex {
						return io.ErrUnexpectedEOF
					}
					iNdEx += skippy
				}
			}
			m.NonVotings[mapkey] = mapvalue
			iNdEx = postIndex

		case 5:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Witnesses", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.Witnesses == nil {
				m.Witnesses = make(map[uint64]string)
			}
			var mapkey uint64
			var mapvalue string
			for iNdEx < postIndex {
				entryPreIndex := iNdEx
				var wire uint64
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowRaft
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					wire |= uint64(b&0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				fieldNum := int32(wire >> 3)
				if fieldNum == 1 {
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowRaft
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						mapkey |= uint64(b&0x7F) << shift
						if b < 0x80 {
							break
						}
					}
				} else if fieldNum == 2 {
					var stringLenmapvalue uint64
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowRaft
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						stringLenmapvalue |= uint64(b&0x7F) << shift
						if b < 0x80 {
							break
						}
					}
					intStringLenmapvalue := int(stringLenmapvalue)
					if intStringLenmapvalue < 0 {
						return ErrInvalidLengthRaft
					}
					postStringIndexmapvalue := iNdEx + intStringLenmapvalue
					if postStringIndexmapvalue < 0 {
						return ErrInvalidLengthRaft
					}
					if postStringIndexmapvalue > l {
						return io.ErrUnexpectedEOF
					}
					mapvalue = string(dAtA[iNdEx:postStringIndexmapvalue])
					iNdEx = postStringIndexmapvalue
				} else {
					iNdEx = entryPreIndex
					skippy, err := skipRaft(dAtA[iNdEx:])
					if err != nil {
						return err
					}
					if skippy < 0 {
						return ErrInvalidLengthRaft
					}
					if (iNdEx + skippy) > postIndex {
						return io.ErrUnexpectedEOF
					}
					iNdEx += skippy
				}
			}
			m.Witnesses[mapkey] = mapvalue
			iNdEx = postIndex

		default:
			iNdEx = preIndex
			skippy, err := skipRaft(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
````

## File: raftpb/message.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: raft.proto

package raftpb

type Message struct {
	Type     MessageType
	To       uint64
	From     uint64
	ShardID  uint64
	Term     uint64
	LogTerm  uint64
	LogIndex uint64
	Commit   uint64
	Reject   bool
	Hint     uint64
	Entries  []Entry
	Snapshot Snapshot
	HintHigh uint64
}

func (m *Message) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *Message) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	dAtA[i] = 0x8
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Type))
	dAtA[i] = 0x10
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.To))
	dAtA[i] = 0x18
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.From))
	dAtA[i] = 0x20
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.ShardID))
	dAtA[i] = 0x28
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Term))
	dAtA[i] = 0x30
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.LogTerm))
	dAtA[i] = 0x38
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.LogIndex))
	dAtA[i] = 0x40
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Commit))
	dAtA[i] = 0x48
	i++
	if m.Reject {
		dAtA[i] = 1
	} else {
		dAtA[i] = 0
	}
	i++
	dAtA[i] = 0x50
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Hint))
	if len(m.Entries) > 0 {
		for _, msg := range m.Entries {
			dAtA[i] = 0x5a
			i++
			i = encodeVarintRaft(dAtA, i, uint64(msg.Size()))
			n, err := msg.MarshalTo(dAtA[i:])
			if err != nil {
				return 0, err
			}
			i += n
		}
	}
	dAtA[i] = 0x62
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Snapshot.Size()))
	n2, err := m.Snapshot.MarshalTo(dAtA[i:])
	if err != nil {
		return 0, err
	}
	i += n2
	dAtA[i] = 0x68
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.HintHigh))
	return i, nil
}

func (m *Message) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	n += 1 + sovRaft(uint64(m.Type))
	n += 1 + sovRaft(uint64(m.To))
	n += 1 + sovRaft(uint64(m.From))
	n += 1 + sovRaft(uint64(m.ShardID))
	n += 1 + sovRaft(uint64(m.Term))
	n += 1 + sovRaft(uint64(m.LogTerm))
	n += 1 + sovRaft(uint64(m.LogIndex))
	n += 1 + sovRaft(uint64(m.Commit))
	n += 2
	n += 1 + sovRaft(uint64(m.Hint))
	if len(m.Entries) > 0 {
		for _, e := range m.Entries {
			l = e.Size()
			n += 1 + l + sovRaft(uint64(l))
		}
	}
	l = m.Snapshot.Size()
	n += 1 + l + sovRaft(uint64(l))
	n += 1 + sovRaft(uint64(m.HintHigh))
	return n
}
````

## File: raftpb/messagebatch.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: raft.proto

package raftpb

type MessageBatch struct {
	Requests      []Message
	DeploymentId  uint64
	SourceAddress string
	BinVer        uint32
}

func (m *MessageBatch) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *MessageBatch) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if len(m.Requests) > 0 {
		for _, msg := range m.Requests {
			dAtA[i] = 0xa
			i++
			i = encodeVarintRaft(dAtA, i, uint64(msg.Size()))
			n, err := msg.MarshalTo(dAtA[i:])
			if err != nil {
				return 0, err
			}
			i += n
		}
	}
	dAtA[i] = 0x10
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.DeploymentId))
	dAtA[i] = 0x1a
	i++
	i = encodeVarintRaft(dAtA, i, uint64(len(m.SourceAddress)))
	i += copy(dAtA[i:], m.SourceAddress)
	dAtA[i] = 0x20
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.BinVer))
	return i, nil
}

func (m *MessageBatch) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	if len(m.Requests) > 0 {
		for _, e := range m.Requests {
			l = e.Size()
			n += 1 + l + sovRaft(uint64(l))
		}
	}
	n += 1 + sovRaft(uint64(m.DeploymentId))
	l = len(m.SourceAddress)
	n += 1 + l + sovRaft(uint64(l))
	n += 1 + sovRaft(uint64(m.BinVer))
	return n
}
````

## File: raftpb/raft_optimized.go
````go
//
// Code in this file is auto generated by colfer.
//

package raftpb

import (
	"encoding/binary"
	"fmt"
	"io"
	"time"
	"unsafe"

	"github.com/lni/dragonboat/v4/internal/settings"
)

const (
	// RaftEntryEncodingScheme is the scheme name of the codec.
	RaftEntryEncodingScheme = "colfer"
)

var (
	_ = unsafe.Sizeof(0)
	_ = io.ReadFull
	_ = time.Now()
)

var intconv = binary.BigEndian

// Colfer configuration attributes
var (
	// ColferSizeMax is the upper limit for serial byte sizes
	ColferSizeMax uint64 = 8 * 1024 * 1024 * 1024 * 1024
)

// ColferMax signals an upper limit breach.
type ColferMax string

// Error honors the error interface.
func (m ColferMax) Error() string { return string(m) }

// ColferError signals a data mismatch as as a byte index.
type ColferError int

// Error honors the error interface.
func (i ColferError) Error() string {
	return fmt.Sprintf("colfer: unknown header at byte %d", i)
}

// ColferTail signals data continuation as a byte index.
type ColferTail int

// Error honors the error interface.
func (i ColferTail) Error() string {
	return fmt.Sprintf("colfer: data continuation at byte %d", i)
}

// SizeUpperLimit returns the upper limit size of the state instance.
func (m *State) SizeUpperLimit() int {
	return 8 + 16*3
}

// SizeUpperLimit returns the upper limit size of the entry batch.
func (m *EntryBatch) SizeUpperLimit() (n int) {
	var l int
	n += 16
	if len(m.Entries) > 0 {
		for _, e := range m.Entries {
			l = e.SizeUpperLimit()
			n += (l + 16)
		}
	}
	return n
}

// SizeUpperLimit returns the upper limit size of an entry.
func (m *Entry) SizeUpperLimit() int {
	l := settings.EntryNonCmdFieldsSize
	l += len(m.Cmd)
	return l
}

// Size returns the actual size of an entry
func (m *Entry) Size() int {
	l := 1

	if x := m.Term; x >= 1<<49 {
		l += 9
	} else if x != 0 {
		for l += 2; x >= 0x80; l++ {
			x >>= 7
		}
	}

	if x := m.Index; x >= 1<<49 {
		l += 9
	} else if x != 0 {
		for l += 2; x >= 0x80; l++ {
			x >>= 7
		}
	}

	if v := m.Type; v != 0 {
		x := uint32(v)
		if v < 0 {
			x = ^x + 1
		}
		for l += 2; x >= 0x80; l++ {
			x >>= 7
		}
	}

	if x := m.Key; x >= 1<<49 {
		l += 9
	} else if x != 0 {
		for l += 2; x >= 0x80; l++ {
			x >>= 7
		}
	}

	if x := m.ClientID; x >= 1<<49 {
		l += 9
	} else if x != 0 {
		for l += 2; x >= 0x80; l++ {
			x >>= 7
		}
	}

	if x := m.SeriesID; x >= 1<<49 {
		l += 9
	} else if x != 0 {
		for l += 2; x >= 0x80; l++ {
			x >>= 7
		}
	}

	if x := m.RespondedTo; x >= 1<<49 {
		l += 9
	} else if x != 0 {
		for l += 2; x >= 0x80; l++ {
			x >>= 7
		}
	}

	if x := len(m.Cmd); x != 0 {
		if uint64(x) > ColferSizeMax {
			panic("max size reached")
		}
		for l += x + 2; x >= 0x80; l++ {
			x >>= 7
		}
	}

	if uint64(l) > ColferSizeMax {
		panic(fmt.Sprintf("max size reached %d", l))
	}
	return l
}

// MarshalTo marshals the entry to the specified byte slice.
func (m *Entry) MarshalTo(buf []byte) (int, error) {
	v := m.marshalTo(buf)
	return v, nil
}

func (m *Entry) marshalTo(buf []byte) int {
	var i int

	if x := m.Term; x >= 1<<49 {
		buf[i] = 0 | 0x80
		intconv.PutUint64(buf[i+1:], x)
		i += 9
	} else if x != 0 {
		buf[i] = 0
		i++
		for x >= 0x80 {
			buf[i] = byte(x | 0x80)
			x >>= 7
			i++
		}
		buf[i] = byte(x)
		i++
	}

	if x := m.Index; x >= 1<<49 {
		buf[i] = 1 | 0x80
		intconv.PutUint64(buf[i+1:], x)
		i += 9
	} else if x != 0 {
		buf[i] = 1
		i++
		for x >= 0x80 {
			buf[i] = byte(x | 0x80)
			x >>= 7
			i++
		}
		buf[i] = byte(x)
		i++
	}

	if v := m.Type; v != 0 {
		x := uint32(v)
		if v >= 0 {
			buf[i] = 2
		} else {
			x = ^x + 1
			buf[i] = 2 | 0x80
		}
		i++
		for x >= 0x80 {
			buf[i] = byte(x | 0x80)
			x >>= 7
			i++
		}
		buf[i] = byte(x)
		i++
	}

	if x := m.Key; x >= 1<<49 {
		buf[i] = 3 | 0x80
		intconv.PutUint64(buf[i+1:], x)
		i += 9
	} else if x != 0 {
		buf[i] = 3
		i++
		for x >= 0x80 {
			buf[i] = byte(x | 0x80)
			x >>= 7
			i++
		}
		buf[i] = byte(x)
		i++
	}

	if x := m.ClientID; x >= 1<<49 {
		buf[i] = 4 | 0x80
		intconv.PutUint64(buf[i+1:], x)
		i += 9
	} else if x != 0 {
		buf[i] = 4
		i++
		for x >= 0x80 {
			buf[i] = byte(x | 0x80)
			x >>= 7
			i++
		}
		buf[i] = byte(x)
		i++
	}

	if x := m.SeriesID; x >= 1<<49 {
		buf[i] = 5 | 0x80
		intconv.PutUint64(buf[i+1:], x)
		i += 9
	} else if x != 0 {
		buf[i] = 5
		i++
		for x >= 0x80 {
			buf[i] = byte(x | 0x80)
			x >>= 7
			i++
		}
		buf[i] = byte(x)
		i++
	}

	if x := m.RespondedTo; x >= 1<<49 {
		buf[i] = 6 | 0x80
		intconv.PutUint64(buf[i+1:], x)
		i += 9
	} else if x != 0 {
		buf[i] = 6
		i++
		for x >= 0x80 {
			buf[i] = byte(x | 0x80)
			x >>= 7
			i++
		}
		buf[i] = byte(x)
		i++
	}

	if l := len(m.Cmd); l != 0 {
		buf[i] = 7
		i++
		x := uint(l)
		for x >= 0x80 {
			buf[i] = byte(x | 0x80)
			x >>= 7
			i++
		}
		buf[i] = byte(x)
		i++
		i += copy(buf[i:], m.Cmd)
	}

	buf[i] = 0x7f
	i++
	return i
}

// Unmarshal unmarshals the input to the current entry instance.
func (m *Entry) Unmarshal(data []byte) error {
	_, err := m.unmarshal(data)
	return err
}

func (m *Entry) unmarshal(data []byte) (int, error) {
	if len(data) == 0 {
		return 0, io.EOF
	}
	header := data[0]
	i := 1

	if header == 0 {
		start := i
		i++
		if i >= len(data) {
			goto eof
		}
		x := uint64(data[start])

		if x >= 0x80 {
			x &= 0x7f
			for shift := uint(7); ; shift += 7 {
				b := uint64(data[i])
				i++
				if i >= len(data) {
					goto eof
				}

				if b < 0x80 || shift == 56 {
					x |= b << shift
					break
				}
				x |= (b & 0x7f) << shift
			}
		}
		m.Term = x

		header = data[i]
		i++
	} else if header == 0|0x80 {
		start := i
		i += 8
		if i >= len(data) {
			goto eof
		}
		m.Term = intconv.Uint64(data[start:])
		header = data[i]
		i++
	}

	if header == 1 {
		start := i
		i++
		if i >= len(data) {
			goto eof
		}
		x := uint64(data[start])

		if x >= 0x80 {
			x &= 0x7f
			for shift := uint(7); ; shift += 7 {
				b := uint64(data[i])
				i++
				if i >= len(data) {
					goto eof
				}

				if b < 0x80 || shift == 56 {
					x |= b << shift
					break
				}
				x |= (b & 0x7f) << shift
			}
		}
		m.Index = x

		header = data[i]
		i++
	} else if header == 1|0x80 {
		start := i
		i += 8
		if i >= len(data) {
			goto eof
		}
		m.Index = intconv.Uint64(data[start:])
		header = data[i]
		i++
	}
	if header == 2 {
		if i+1 >= len(data) {
			i++
			goto eof
		}
		x := uint32(data[i])
		i++

		if x >= 0x80 {
			x &= 0x7f
			for shift := uint(7); ; shift += 7 {
				b := uint32(data[i])
				i++
				if i >= len(data) {
					goto eof
				}

				if b < 0x80 {
					x |= b << shift
					break
				}
				x |= (b & 0x7f) << shift
			}
		}
		m.Type = EntryType(x)

		header = data[i]
		i++
	} else if header == 2|0x80 {
		if i+1 >= len(data) {
			i++
			goto eof
		}
		x := uint32(data[i])
		i++

		if x >= 0x80 {
			x &= 0x7f
			for shift := uint(7); ; shift += 7 {
				b := uint32(data[i])
				i++
				if i >= len(data) {
					goto eof
				}

				if b < 0x80 {
					x |= b << shift
					break
				}
				x |= (b & 0x7f) << shift
			}
		}
		m.Type = EntryType(^x + 1)

		header = data[i]
		i++
	}

	if header == 3 {
		start := i
		i++
		if i >= len(data) {
			goto eof
		}
		x := uint64(data[start])

		if x >= 0x80 {
			x &= 0x7f
			for shift := uint(7); ; shift += 7 {
				b := uint64(data[i])
				i++
				if i >= len(data) {
					goto eof
				}

				if b < 0x80 || shift == 56 {
					x |= b << shift
					break
				}
				x |= (b & 0x7f) << shift
			}
		}
		m.Key = x

		header = data[i]
		i++
	} else if header == 3|0x80 {
		start := i
		i += 8
		if i >= len(data) {
			goto eof
		}
		m.Key = intconv.Uint64(data[start:])
		header = data[i]
		i++
	}
	if header == 4 {
		start := i
		i++
		if i >= len(data) {
			goto eof
		}
		x := uint64(data[start])

		if x >= 0x80 {
			x &= 0x7f
			for shift := uint(7); ; shift += 7 {
				b := uint64(data[i])
				i++
				if i >= len(data) {
					goto eof
				}

				if b < 0x80 || shift == 56 {
					x |= b << shift
					break
				}
				x |= (b & 0x7f) << shift
			}
		}
		m.ClientID = x

		header = data[i]
		i++
	} else if header == 4|0x80 {
		start := i
		i += 8
		if i >= len(data) {
			goto eof
		}
		m.ClientID = intconv.Uint64(data[start:])
		header = data[i]
		i++
	}

	if header == 5 {
		start := i
		i++
		if i >= len(data) {
			goto eof
		}
		x := uint64(data[start])

		if x >= 0x80 {
			x &= 0x7f
			for shift := uint(7); ; shift += 7 {
				b := uint64(data[i])
				i++
				if i >= len(data) {
					goto eof
				}

				if b < 0x80 || shift == 56 {
					x |= b << shift
					break
				}
				x |= (b & 0x7f) << shift
			}
		}
		m.SeriesID = x

		header = data[i]
		i++
	} else if header == 5|0x80 {
		start := i
		i += 8
		if i >= len(data) {
			goto eof
		}
		m.SeriesID = intconv.Uint64(data[start:])
		header = data[i]
		i++
	}
	if header == 6 {
		start := i
		i++
		if i >= len(data) {
			goto eof
		}
		x := uint64(data[start])

		if x >= 0x80 {
			x &= 0x7f
			for shift := uint(7); ; shift += 7 {
				b := uint64(data[i])
				i++
				if i >= len(data) {
					goto eof
				}

				if b < 0x80 || shift == 56 {
					x |= b << shift
					break
				}
				x |= (b & 0x7f) << shift
			}
		}
		m.RespondedTo = x

		header = data[i]
		i++
	} else if header == 6|0x80 {
		start := i
		i += 8
		if i >= len(data) {
			goto eof
		}
		m.RespondedTo = intconv.Uint64(data[start:])
		header = data[i]
		i++
	}

	if header == 7 {
		if i >= len(data) {
			goto eof
		}
		x := uint(data[i])
		i++

		if x >= 0x80 {
			x &= 0x7f
			for shift := uint(7); ; shift += 7 {
				if i >= len(data) {
					goto eof
				}
				b := uint(data[i])
				i++

				if b < 0x80 {
					x |= b << shift
					break
				}
				x |= (b & 0x7f) << shift
			}
		}

		if x > uint(ColferSizeMax) {
			return 0, ColferMax(fmt.Sprintf("colfer: raftpb.Entry.Cmd size %d exceeds %d bytes", x, ColferSizeMax))
		}

		start := i
		i += int(x)
		if i >= len(data) {
			goto eof
		}
		// https://github.com/golang/go/wiki/SliceTricks
		ic := data[start:i]
		m.Cmd = append(ic[:0:0], ic...)

		header = data[i]
		i++
	}

	if header != 0x7f {
		return 0, ColferError(i - 1)
	}
	if uint64(i) < ColferSizeMax {
		return i, nil
	}
eof:
	if uint64(i) >= ColferSizeMax {
		return 0, ColferMax(fmt.Sprintf("colfer: struct raftpb.Entry size exceeds %d bytes", ColferSizeMax))
	}
	return 0, io.EOF
}

// Unmarshal unmarshals the message instance using the input byte slice.
func (m *Message) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRaft
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: Message: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: Message: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Type", wireType)
			}
			m.Type = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Type |= (MessageType(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field To", wireType)
			}
			m.To = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.To |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field From", wireType)
			}
			m.From = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.From |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ShardId", wireType)
			}
			m.ShardID = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ShardID |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 5:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Term", wireType)
			}
			m.Term = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Term |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 6:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field LogTerm", wireType)
			}
			m.LogTerm = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.LogTerm |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 7:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field LogIndex", wireType)
			}
			m.LogIndex = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.LogIndex |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 8:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Commit", wireType)
			}
			m.Commit = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Commit |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 9:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Reject", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.Reject = bool(v != 0)
		case 10:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Hint", wireType)
			}
			m.Hint = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Hint |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 11:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Entries", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if cap(m.Entries) == 0 {
				count := m.entryCount(dAtA[postIndex:]) + 1
				m.Entries = make([]Entry, 0, count)
			}
			m.Entries = append(m.Entries, Entry{})
			if err := m.Entries[len(m.Entries)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 12:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Snapshot", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if err := m.Snapshot.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 13:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field HintHigh", wireType)
			}
			m.HintHigh = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.HintHigh |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		default:
			iNdEx = preIndex
			skippy, err := skipRaft(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}

func (m *Message) entryCount(dAtA []byte) int {
	l := len(dAtA)
	iNdEx := 0
	count := 0
	for iNdEx < l {
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		if fieldNum == 11 {
			var msglen int
			for shift := uint(0); ; shift += 7 {
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			count++
			postIndex := iNdEx + msglen
			iNdEx = postIndex
		} else {
			return count
		}
	}
	return count
}

func (m *MessageBatch) messageCount(dAtA []byte) int {
	l := len(dAtA)
	iNdEx := 0
	count := 0
	for iNdEx < l {
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		if fieldNum == 1 {
			var msglen int
			for shift := uint(0); ; shift += 7 {
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			count++
			postIndex := iNdEx + msglen
			iNdEx = postIndex
		} else {
			break
		}
	}
	return count
}

// Unmarshal unmarshals the message batch instance using the input byte slice.
func (m *MessageBatch) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRaft
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: MessageBatch: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: MessageBatch: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Requests", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if len(m.Requests) == 0 {
				count := m.messageCount(dAtA[postIndex:]) + 1
				m.Requests = make([]Message, 0, count)
			}
			m.Requests = append(m.Requests, Message{})
			if err := m.Requests[len(m.Requests)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field DeploymentId", wireType)
			}
			m.DeploymentId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.DeploymentId |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field SourceAddress", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= (uint64(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + intStringLen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.SourceAddress = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 4:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field BinVer", wireType)
			}
			m.BinVer = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.BinVer |= (uint32(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		default:
			iNdEx = preIndex
			skippy, err := skipRaft(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}

// SizeUpperLimit returns the upper limit size of the message.
func (m *Message) SizeUpperLimit() int {
	l := 0
	l += (16 * 12)
	l += m.Snapshot.Size()
	if len(m.Entries) > 0 {
		for _, e := range m.Entries {
			l += 16
			l += e.SizeUpperLimit()
		}
	}
	return l
}

// SizeUpperLimit returns the upper limit size of the message batch.
func (m *MessageBatch) SizeUpperLimit() int {
	l := 0
	l += (16 * 3) + len(m.SourceAddress)
	for _, msg := range m.Requests {
		l += 16
		l += msg.SizeUpperLimit()
	}
	return l
}
````

## File: raftpb/raft_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raftpb

import (
	"crypto/rand"
	"math"
	"reflect"
	"testing"
	"unsafe"

	"github.com/lni/dragonboat/v4/client"
	sm "github.com/lni/dragonboat/v4/statemachine"
	"github.com/stretchr/testify/require"
)

func TestStateMachineTypeHaveExpectedValues(t *testing.T) {
	require.Equal(t, sm.Type(0), sm.Type(UnknownStateMachine))
	require.Equal(t, sm.Type(sm.RegularStateMachine),
		sm.Type(RegularStateMachine))
	require.Equal(t, sm.Type(sm.ConcurrentStateMachine),
		sm.Type(ConcurrentStateMachine))
	require.Equal(t, sm.Type(sm.OnDiskStateMachine),
		sm.Type(OnDiskStateMachine))
}

func TestBootstrapValidateHandlesJoiningNode(t *testing.T) {
	bootstrap := Bootstrap{Join: true}
	require.True(t, bootstrap.Validate(nil, true, UnknownStateMachine))
	require.True(t, bootstrap.Validate(nil, false, UnknownStateMachine))

	bootstrap = Bootstrap{Join: false, Addresses: make(map[uint64]string)}
	bootstrap.Addresses[100] = "address1"
	require.False(t, bootstrap.Validate(nil, true, UnknownStateMachine))
}

func TestCorruptedBootstrapValueIsChecked(t *testing.T) {
	bootstrap := Bootstrap{Join: false, Addresses: make(map[uint64]string)}
	require.Panics(t, func() {
		bootstrap.Validate(nil, true, UnknownStateMachine)
	})
}

func TestInconsistentInitialMembersAreCheckedAndReported(t *testing.T) {
	bootstrap := Bootstrap{Join: false, Addresses: make(map[uint64]string)}
	bootstrap.Addresses[100] = "address1"
	bootstrap.Addresses[200] = "address2"
	bootstrap.Addresses[300] = "address3"
	require.True(t, bootstrap.Validate(nil, false, UnknownStateMachine))

	nodes1 := make(map[uint64]string)
	require.True(t, bootstrap.Validate(nodes1, false, UnknownStateMachine))

	nodes1[100] = "address1"
	require.False(t, bootstrap.Validate(nodes1, false, UnknownStateMachine))

	nodes1[200] = "address2"
	nodes1[300] = "address3"
	require.True(t, bootstrap.Validate(nodes1, false, UnknownStateMachine))

	nodes1[300] = "address4"
	require.False(t, bootstrap.Validate(nodes1, false, UnknownStateMachine))
}

func TestInconsistentStateMachineTypeIsDetected(t *testing.T) {
	tests := []struct {
		bt     StateMachineType
		ct     StateMachineType
		result bool
	}{
		{UnknownStateMachine, UnknownStateMachine, true},
		{UnknownStateMachine, RegularStateMachine, true},
		{UnknownStateMachine, ConcurrentStateMachine, true},
		{UnknownStateMachine, OnDiskStateMachine, true},
		{RegularStateMachine, RegularStateMachine, true},
		{RegularStateMachine, ConcurrentStateMachine, false},
		{RegularStateMachine, OnDiskStateMachine, false},
		{ConcurrentStateMachine, RegularStateMachine, false},
		{ConcurrentStateMachine, ConcurrentStateMachine, true},
		{ConcurrentStateMachine, OnDiskStateMachine, false},
		{OnDiskStateMachine, RegularStateMachine, false},
		{OnDiskStateMachine, ConcurrentStateMachine, false},
		{OnDiskStateMachine, OnDiskStateMachine, true},
	}
	for idx, tt := range tests {
		addr := make(map[uint64]string)
		addr[100] = "addr1"
		bs := Bootstrap{Type: tt.bt, Addresses: addr}
		require.Equal(t, tt.result, bs.Validate(nil, false, tt.ct),
			"test case %d failed", idx)
	}
}

func TestIsConfigChange(t *testing.T) {
	e1 := Entry{Type: ConfigChangeEntry}
	e2 := Entry{Type: ApplicationEntry}
	require.True(t, e1.IsConfigChange())
	require.False(t, e2.IsConfigChange())

	e3 := Entry{}
	require.False(t, e3.IsConfigChange())
}

func TestNoOPEntryIsNotUpdateEntry(t *testing.T) {
	e := &Entry{}
	require.False(t, e.IsUpdateEntry())
}

func TestNoOPEntryIsNotSessionManaged(t *testing.T) {
	e := &Entry{}
	require.False(t, e.IsSessionManaged())
}

func TestIsEmpty(t *testing.T) {
	entries := []Entry{
		{Type: ConfigChangeEntry},
		{ClientID: 12345},
		{Cmd: make([]byte, 1)},
	}
	for idx, ent := range entries {
		require.False(t, ent.IsEmpty(), "entry %d should not be empty", idx)
	}

	entries = []Entry{
		{
			Type:     ApplicationEntry,
			ClientID: client.NotSessionManagedClientID,
		},
		{},
	}
	for idx, ent := range entries {
		require.True(t, ent.IsEmpty(), "entry idx %d should be empty", idx)
	}
}

func TestIsSessionManaged(t *testing.T) {
	e1 := Entry{Type: ConfigChangeEntry}
	e2 := Entry{
		Type:     ApplicationEntry,
		ClientID: client.NotSessionManagedClientID,
	}
	e3 := Entry{ClientID: 12345}

	require.False(t, e1.IsSessionManaged())
	require.False(t, e2.IsSessionManaged())
	require.True(t, e3.IsSessionManaged())

	e4 := Entry{}
	require.False(t, e4.IsSessionManaged())
}

func TestIsNoOPSession(t *testing.T) {
	e1 := Entry{SeriesID: client.NoOPSeriesID}
	require.True(t, e1.IsNoOPSession())

	e2 := Entry{SeriesID: client.NoOPSeriesID + 1}
	require.False(t, e2.IsNoOPSession())

	e3 := Entry{}
	require.True(t, e3.IsNoOPSession())
}

func TestIsNewSessionRequest(t *testing.T) {
	entries := []Entry{
		{Type: ConfigChangeEntry},
		{Cmd: make([]byte, 1)},
		{ClientID: client.NotSessionManagedClientID},
		{SeriesID: client.SeriesIDForRegister + 1},
		{},
	}
	for idx, ent := range entries {
		require.False(t, ent.IsNewSessionRequest(),
			"%d is not supposed to be a IsNewSessionRequest %+v", idx, ent)
	}

	ent := Entry{
		Type:     ApplicationEntry,
		ClientID: 123456,
		SeriesID: client.SeriesIDForRegister,
	}
	require.True(t, ent.IsNewSessionRequest())
}

func TestIsEndOfSessionRequest(t *testing.T) {
	entries := []Entry{
		{Type: ConfigChangeEntry},
		{Cmd: make([]byte, 1)},
		{ClientID: client.NotSessionManagedClientID},
		{SeriesID: client.SeriesIDForUnregister - 1},
		{},
	}
	for idx, ent := range entries {
		require.False(t, ent.IsEndOfSessionRequest(),
			"%d is not supposed to be a IsEndOfSessionRequest %+v", idx, ent)
	}

	ent := Entry{
		Type:     ApplicationEntry,
		ClientID: 123456,
		SeriesID: client.SeriesIDForUnregister,
	}
	require.True(t, ent.IsEndOfSessionRequest())
}

func TestEntrySizeUpperLimit(t *testing.T) {
	max64 := uint64(math.MaxUint64)
	e1 := Entry{
		Term:        max64,
		Index:       max64,
		Type:        1,
		Key:         max64,
		ClientID:    max64,
		SeriesID:    max64,
		RespondedTo: max64,
		Cmd:         make([]byte, 1024),
	}
	require.GreaterOrEqual(t, e1.SizeUpperLimit(), e1.Size())

	e1.Cmd = nil
	require.GreaterOrEqual(t, e1.SizeUpperLimit(), e1.Size())

	e2 := Entry{}
	require.GreaterOrEqual(t, e2.SizeUpperLimit(), e2.Size())

	e2.Cmd = make([]byte, 1024)
	require.GreaterOrEqual(t, e2.SizeUpperLimit(), e2.Size())
}

func TestEntryBatchSizeUpperLimit(t *testing.T) {
	max64 := uint64(math.MaxUint64)
	e1 := Entry{
		Term:        max64,
		Index:       max64,
		Type:        1,
		Key:         max64,
		ClientID:    max64,
		SeriesID:    max64,
		RespondedTo: max64,
		Cmd:         make([]byte, 1024),
	}
	eb := EntryBatch{
		Entries: make([]Entry, 0),
	}
	require.LessOrEqual(t, eb.Size(), eb.SizeUpperLimit())

	for i := 0; i < 1024; i++ {
		eb.Entries = append(eb.Entries, e1)
	}
	require.LessOrEqual(t, eb.Size(), eb.SizeUpperLimit())

	e1.Cmd = nil
	eb.Entries = make([]Entry, 0)
	for i := 0; i < 1024; i++ {
		eb.Entries = append(eb.Entries, e1)
	}
	require.LessOrEqual(t, eb.Size(), eb.SizeUpperLimit())

	e2 := Entry{}
	eb.Entries = make([]Entry, 0)
	for i := 0; i < 1024; i++ {
		eb.Entries = append(eb.Entries, e2)
	}
	require.LessOrEqual(t, eb.Size(), eb.SizeUpperLimit())
}

func getMaxSizedMsg() Message {
	max64 := uint64(math.MaxUint64)
	msg := Message{
		Type:     NoOP,
		To:       max64,
		From:     max64,
		ShardID:  max64,
		Term:     max64,
		LogTerm:  max64,
		LogIndex: max64,
		Commit:   max64,
		Reject:   true,
		Hint:     max64,
		HintHigh: max64,
	}
	e1 := Entry{
		Term:        max64,
		Index:       max64,
		Type:        1,
		Key:         max64,
		ClientID:    max64,
		SeriesID:    max64,
		RespondedTo: max64,
		Cmd:         make([]byte, 1024),
	}
	for i := 0; i < 1024; i++ {
		msg.Entries = append(msg.Entries, e1)
	}
	msg.Snapshot.Filepath = "longfilepathisherexxxxxxxxxxxxxxxxx"
	msg.Snapshot.FileSize = max64
	msg.Snapshot.Index = max64
	msg.Snapshot.Term = max64
	return msg
}

func TestMessageSizeUpperLimit(t *testing.T) {
	msg := getMaxSizedMsg()
	require.LessOrEqual(t, msg.Size(), msg.SizeUpperLimit())

	msg2 := Message{}
	require.LessOrEqual(t, msg2.Size(), msg2.SizeUpperLimit())
}

func TestMessageBatchSizeUpperLimit(t *testing.T) {
	max64 := uint64(math.MaxUint64)
	max32 := uint32(math.MaxUint32)
	msg := getMaxSizedMsg()
	mb := MessageBatch{
		DeploymentId:  max64,
		BinVer:        max32,
		SourceAddress: "longaddressisherexxxxxxxxxxxxxxxxxxxxxxxxx",
	}
	for i := 0; i < 1024; i++ {
		mb.Requests = append(mb.Requests, msg)
	}
	require.LessOrEqual(t, mb.Size(), mb.SizeUpperLimit())

	mb2 := MessageBatch{}
	require.LessOrEqual(t, mb2.Size(), mb2.SizeUpperLimit())

	mb2.DeploymentId = max64
	mb2.BinVer = max32
	mb2.SourceAddress = "longaddressisherexxxxxxxxxxxxxxxxxxxxxxxxxx"
	require.LessOrEqual(t, mb2.Size(), mb2.SizeUpperLimit())
}

func TestGetEntrySliceInMemSize(t *testing.T) {
	e0 := Entry{}
	e16 := Entry{Cmd: make([]byte, 16)}
	e64 := Entry{Cmd: make([]byte, 64)}
	tests := []struct {
		ents []Entry
		size uint64
	}{
		{[]Entry{}, 0},
		{[]Entry{e0}, 80},
		{[]Entry{e16}, 96},
		{[]Entry{e64}, 144},
		{[]Entry{e0, e64}, 224},
		{[]Entry{e0, e16, e64}, 320},
	}
	for idx, tt := range tests {
		result := GetEntrySliceInMemSize(tt.ents)
		require.Equal(t, tt.size, result, "test case %d", idx)
	}
}

func TestMetadataEntry(t *testing.T) {
	me := Entry{
		Type:  MetadataEntry,
		Index: 200,
		Term:  5,
	}
	require.True(t, me.IsEmpty())
	require.False(t, me.IsSessionManaged())
	require.True(t, me.IsNoOPSession())
	require.False(t, me.IsNewSessionRequest() || me.IsEndOfSessionRequest())
	require.False(t, me.IsUpdateEntry())
}

func isDifferentInstance(a, b []byte) bool {
	if len(a) == 0 || len(b) == 0 {
		return true // empty slices can't alias each other
	}

	aPtr := unsafe.Pointer(&a[0])
	bPtr := unsafe.Pointer(&b[0])

	return aPtr != bPtr
}

func TestEntryCanBeMarshalledAndUnmarshalled(t *testing.T) {
	cmd := make([]byte, 1024)
	_, err := rand.Read(cmd)
	require.NoError(t, err)
	e := Entry{
		Type:        MetadataEntry,
		Index:       200,
		Term:        5,
		Key:         12345678,
		ClientID:    7654321,
		RespondedTo: 13579,
		Cmd:         cmd,
	}
	m, err := e.Marshal()
	require.NoError(t, err)

	e2 := Entry{}
	err = e2.Unmarshal(m)
	require.NoError(t, err)

	require.True(t, reflect.DeepEqual(&e, &e2))
	require.True(t, isDifferentInstance(e.Cmd, e2.Cmd))
}

func TestRaftDataStatusCanBeMarshaled(t *testing.T) {
	r := &RaftDataStatus{
		Address:             "mydomain.com:12345",
		BinVer:              uint32(123),
		HardHash:            uint64(1234567890),
		LogdbType:           "mytype",
		Hostname:            "myhostname",
		DeploymentId:        uint64(1234567890),
		EntryBatchSize:      uint64(123456789),
		AddressByNodeHostId: true,
	}
	check := func(v *RaftDataStatus) {
		data, err := v.Marshal()
		require.NoError(t, err)

		r2 := &RaftDataStatus{}
		err = r2.Unmarshal(data)
		require.NoError(t, err)

		require.True(t, reflect.DeepEqual(v, r2))
	}
	check(r)
	r.AddressByNodeHostId = false
	check(r)
}

func TestMessageCanDrop(t *testing.T) {
	tests := []struct {
		t       MessageType
		canDrop bool
	}{
		{LocalTick, true},
		{Election, true},
		{LeaderHeartbeat, true},
		{ConfigChangeEvent, true},
		{NoOP, true},
		{Ping, true},
		{Pong, true},
		{Propose, true},
		{SnapshotStatus, false},
		{Unreachable, false},
		{CheckQuorum, true},
		{BatchedReadIndex, true},
		{Replicate, true},
		{ReplicateResp, true},
		{RequestVote, true},
		{RequestVoteResp, true},
		{InstallSnapshot, false},
		{Heartbeat, true},
		{HeartbeatResp, true},
		{ReadIndex, true},
		{ReadIndexResp, true},
		{Quiesce, true},
		{SnapshotReceived, true},
		{LeaderTransfer, true},
		{TimeoutNow, true},
		{RateLimit, true},
	}
	for idx, tt := range tests {
		m := Message{Type: tt.t}
		require.Equal(t, tt.canDrop, m.CanDrop(), "test case %d", idx)
	}
}
````

## File: raftpb/raft.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raftpb

import (
	"fmt"
	"math"
	"strings"
	"unsafe"

	"github.com/lni/goutils/stringutil"

	"github.com/lni/dragonboat/v4/client"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/logger"
)

var (
	plog                = logger.GetLogger("raftpb")
	panicOnSizeMismatch = settings.Soft.PanicOnSizeMismatch
	emptyState          = State{}
)

const (
	// NoNode is the flag used to indicate that the node id field is not set.
	NoNode uint64 = 0
)

// IsEmptyState returns a boolean flag indicating whether the given State is
// empty.
func IsEmptyState(st State) bool {
	return isStateEqual(st, emptyState)
}

// IsEmptySnapshot returns a boolean flag indicating whether the given snapshot
// is and empty dummy record.
func IsEmptySnapshot(s Snapshot) bool {
	return s.Index == 0
}

// IsStateEqual returns whether two input state instances are equal.
func IsStateEqual(a State, b State) bool {
	return isStateEqual(a, b)
}

func isStateEqual(a State, b State) bool {
	return a.Term == b.Term && a.Vote == b.Vote && a.Commit == b.Commit
}

// IsProposal returns a boolean value indicating whether the entry is a
// regular update entry.
func (m *Entry) IsProposal() bool {
	return m.Type == ApplicationEntry ||
		m.Type == EncodedEntry || m.Type == MetadataEntry
}

// IsConfigChange returns a boolean value indicating whether the entry is for
// config change.
func (m *Entry) IsConfigChange() bool {
	return m.Type == ConfigChangeEntry
}

// IsEmpty returns a boolean value indicating whether the entry is Empty.
func (m *Entry) IsEmpty() bool {
	if m.IsConfigChange() {
		return false
	}
	if m.IsSessionManaged() {
		return false
	}
	return len(m.Cmd) == 0
}

// IsSessionManaged returns a boolean value indicating whether the entry is
// session managed.
func (m *Entry) IsSessionManaged() bool {
	if m.IsConfigChange() {
		return false
	}
	if m.ClientID == client.NotSessionManagedClientID {
		return false
	}
	return true
}

// IsNoOPSession returns a boolean value indicating whether the entry is NoOP
// session managed.
func (m *Entry) IsNoOPSession() bool {
	return m.SeriesID == client.NoOPSeriesID
}

// IsNewSessionRequest returns a boolean value indicating whether the entry is
// for reqeusting a new client.
func (m *Entry) IsNewSessionRequest() bool {
	return !m.IsConfigChange() &&
		len(m.Cmd) == 0 &&
		m.ClientID != client.NotSessionManagedClientID &&
		m.SeriesID == client.SeriesIDForRegister
}

// IsEndOfSessionRequest returns a boolean value indicating whether the entry
// is for requesting the session to come to an end.
func (m *Entry) IsEndOfSessionRequest() bool {
	return !m.IsConfigChange() &&
		len(m.Cmd) == 0 &&
		m.ClientID != client.NotSessionManagedClientID &&
		m.SeriesID == client.SeriesIDForUnregister
}

// IsUpdateEntry returns a boolean flag indicating whether the entry is a
// regular application entry not used for session management.
func (m *Entry) IsUpdateEntry() bool {
	return !m.IsConfigChange() && m.IsSessionManaged() &&
		!m.IsNewSessionRequest() && !m.IsEndOfSessionRequest()
}

// NewBootstrapInfo creates and returns a new bootstrap record.
func NewBootstrapInfo(join bool,
	smType StateMachineType, nodes map[uint64]string) Bootstrap {
	bootstrap := Bootstrap{
		Join:      join,
		Addresses: make(map[uint64]string),
		Type:      smType,
	}
	for nid, addr := range nodes {
		bootstrap.Addresses[nid] = stringutil.CleanAddress(addr)
	}
	return bootstrap
}

// Validate checks whether the incoming nodes parameter and the join flag is
// valid given the recorded bootstrap infomration in Log DB.
func (b *Bootstrap) Validate(nodes map[uint64]string,
	join bool, smType StateMachineType) bool {
	if b.Type != UnknownStateMachine && b.Type != smType {
		plog.Errorf("recorded sm type %s, got %s", b.Type, smType)
		return false
	}
	if !b.Join && len(b.Addresses) == 0 {
		panic("invalid non-join bootstrap record with 0 address")
	}
	if b.Join && len(nodes) > 0 {
		plog.Errorf("restarting previously joined node, member list %v", nodes)
		return false
	}
	if join && len(b.Addresses) > 0 {
		plog.Errorf("joining node when it is an initial member")
		return false
	}
	valid := true
	if len(nodes) > 0 {
		if len(nodes) != len(b.Addresses) {
			valid = false
		}
		for nid, addr := range nodes {
			ba, ok := b.Addresses[nid]
			if !ok {
				valid = false
			}
			if strings.Compare(ba, stringutil.CleanAddress(addr)) != 0 {
				valid = false
			}
		}
	}
	if !valid {
		plog.Errorf("inconsistent node list, bootstrap %v, incoming %v",
			b.Addresses, nodes)
	}
	return valid
}

func checkFileSize(path string, size uint64, fs vfs.IFS) {
	var er func(format string, args ...interface{})
	if panicOnSizeMismatch {
		er = plog.Panicf
	} else {
		er = plog.Errorf
	}
	fi, err := fs.Stat(path)
	if err != nil {
		plog.Panicf("failed to access %s", path)
	}
	if size != uint64(fi.Size()) {
		er("file %s size %d, expect %d", path, fi.Size(), size)
	}
}

// Validate validates the snapshot instance.
func (snapshot *Snapshot) Validate(fs vfs.IFS) bool {
	if len(snapshot.Filepath) == 0 || snapshot.FileSize == 0 {
		return false
	}
	checkFileSize(snapshot.Filepath, snapshot.FileSize, fs)
	for _, f := range snapshot.Files {
		if len(f.Filepath) == 0 || f.FileSize == 0 {
			return false
		}
		checkFileSize(f.Filepath, f.FileSize, fs)
	}
	return true
}

// Filename returns the filename of the external snapshot file.
func (m *SnapshotFile) Filename() string {
	return fmt.Sprintf("external-file-%d", m.FileId)
}

// GetEntrySliceSize returns the upper limit of the entry slice size.
func GetEntrySliceSize(ents []Entry) uint64 {
	sz := uint64(0)
	for _, e := range ents {
		sz += uint64(e.SizeUpperLimit())
	}
	return sz
}

// GetEntrySliceInMemSize returns the in memory size of the specified entry
// slice. Size 24 bytes used to hold ents itself is not counted.
func GetEntrySliceInMemSize(ents []Entry) uint64 {
	sz := uint64(0)
	if len(ents) == 0 {
		return 0
	}
	stSz := uint64(unsafe.Sizeof(ents[0]))
	for _, e := range ents {
		sz += uint64(len(e.Cmd))
		sz += stSz
	}
	return sz
}

// IChunkSink is the snapshot chunk sink for handling snapshot chunks being
// streamed.
type IChunkSink interface {
	// return (sent, stopped)
	Receive(chunk Chunk) (bool, bool)
	Close() error
	ShardID() uint64
	ToReplicaID() uint64
}

var (
	// LastChunkCount is the special chunk count value used to indicate that the
	// chunk is the last one.
	LastChunkCount uint64 = math.MaxUint64
	// PoisonChunkCount is the special chunk count value used to indicate that
	// the processing goroutine should return.
	PoisonChunkCount uint64 = math.MaxUint64 - 1
)

// IsLastChunk returns a boolean value indicating whether the chunk is the last
// chunk of a snapshot.
func (m Chunk) IsLastChunk() bool {
	return m.ChunkCount == LastChunkCount || m.ChunkCount == m.ChunkId+1
}

// IsLastFileChunk returns a boolean value indicating whether the chunk is the
// last chunk of a snapshot file.
func (m Chunk) IsLastFileChunk() bool {
	return m.FileChunkId+1 == m.FileChunkCount
}

// IsPoisonChunk returns a boolean value indicating whether the chunk is a
// special poison chunk.
func (m Chunk) IsPoisonChunk() bool {
	return m.ChunkCount == PoisonChunkCount
}

// CanDrop returns a boolean value indicating whether the message can be
// safely dropped.
func (m *Message) CanDrop() bool {
	return m.Type != InstallSnapshot &&
		m.Type != Unreachable && m.Type != SnapshotStatus
}

// Marshaler is the interface for instances that can be marshalled.
type Marshaler interface {
	Marshal() ([]byte, error)
	MarshalTo([]byte) (int, error)
}

// Unmarshaler is the interface for instances that can be unmarshalled.
type Unmarshaler interface {
	Unmarshal([]byte) error
}

// MustMarshal marshals the input object or panic if there is any error.
func MustMarshal(m Marshaler) []byte {
	data, err := m.Marshal()
	if err != nil {
		panic(err)
	}
	return data
}

// MustMarshalTo marshals the input object to the specified buffer or panic
// if there is any error.
func MustMarshalTo(m Marshaler, result []byte) []byte {
	sz, err := m.MarshalTo(result)
	if err != nil {
		panic(err)
	}
	return result[:sz]
}

// MustUnmarshal unmarshals the specified object using the provided marshalled
// data. MustUnmarshal will panic if there is any error.
func MustUnmarshal(m Unmarshaler, data []byte) {
	if err := m.Unmarshal(data); err != nil {
		panic(err)
	}
}
````

## File: raftpb/raftdatastatus.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: raft.proto

package raftpb

import (
	"fmt"
	"io"
)

type RaftDataStatus struct {
	Address             string
	BinVer              uint32
	HardHash            uint64
	LogdbType           string
	Hostname            string
	DeploymentId        uint64
	StepWorkerCount     uint64
	LogdbShardCount     uint64
	MaxSessionCount     uint64
	EntryBatchSize      uint64
	AddressByNodeHostId bool
}

func (m *RaftDataStatus) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *RaftDataStatus) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	dAtA[i] = 0xa
	i++
	i = encodeVarintRaft(dAtA, i, uint64(len(m.Address)))
	i += copy(dAtA[i:], m.Address)
	dAtA[i] = 0x10
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.BinVer))
	dAtA[i] = 0x18
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.HardHash))
	dAtA[i] = 0x22
	i++
	i = encodeVarintRaft(dAtA, i, uint64(len(m.LogdbType)))
	i += copy(dAtA[i:], m.LogdbType)
	dAtA[i] = 0x2a
	i++
	i = encodeVarintRaft(dAtA, i, uint64(len(m.Hostname)))
	i += copy(dAtA[i:], m.Hostname)
	dAtA[i] = 0x30
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.DeploymentId))
	dAtA[i] = 0x38
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.StepWorkerCount))
	dAtA[i] = 0x40
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.LogdbShardCount))
	dAtA[i] = 0x48
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.MaxSessionCount))
	dAtA[i] = 0x50
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.EntryBatchSize))
	dAtA[i] = 0x58
	i++
	if m.AddressByNodeHostId {
		dAtA[i] = 1
	} else {
		dAtA[i] = 0
	}
	i++
	return i, nil
}

func (m *RaftDataStatus) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	l = len(m.Address)
	n += 1 + l + sovRaft(uint64(l))
	n += 1 + sovRaft(uint64(m.BinVer))
	n += 1 + sovRaft(uint64(m.HardHash))
	l = len(m.LogdbType)
	n += 1 + l + sovRaft(uint64(l))
	l = len(m.Hostname)
	n += 1 + l + sovRaft(uint64(l))
	n += 1 + sovRaft(uint64(m.DeploymentId))
	n += 1 + sovRaft(uint64(m.StepWorkerCount))
	n += 1 + sovRaft(uint64(m.LogdbShardCount))
	n += 1 + sovRaft(uint64(m.MaxSessionCount))
	n += 1 + sovRaft(uint64(m.EntryBatchSize))
	n += 2
	return n
}

func (m *RaftDataStatus) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRaft
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: RaftDataStatus: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: RaftDataStatus: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Address", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Address = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field BinVer", wireType)
			}
			m.BinVer = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.BinVer |= uint32(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field HardHash", wireType)
			}
			m.HardHash = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.HardHash |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field LogdbType", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.LogdbType = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 5:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Hostname", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Hostname = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 6:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field DeploymentId", wireType)
			}
			m.DeploymentId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.DeploymentId |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 7:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field StepWorkerCount", wireType)
			}
			m.StepWorkerCount = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.StepWorkerCount |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 8:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field LogdbShardCount", wireType)
			}
			m.LogdbShardCount = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.LogdbShardCount |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 9:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field MaxSessionCount", wireType)
			}
			m.MaxSessionCount = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.MaxSessionCount |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 10:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field EntryBatchSize", wireType)
			}
			m.EntryBatchSize = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.EntryBatchSize |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 11:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field AddressByNodeHostId", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.AddressByNodeHostId = bool(v != 0)
		default:
			iNdEx = preIndex
			skippy, err := skipRaft(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
````

## File: raftpb/snapshot.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: raft.proto

package raftpb

import (
	"fmt"
	"io"
	"sync/atomic"
)

type ICompactor interface {
	Compact(uint64) error
}

type Snapshot struct {
	Filepath    string
	FileSize    uint64
	Index       uint64
	Term        uint64
	Membership  Membership
	Files       []*SnapshotFile
	Checksum    []byte
	Dummy       bool
	ShardID     uint64
	Type        StateMachineType
	Imported    bool
	OnDiskIndex uint64
	Witness     bool
	// refCount will not be marshaled
	refCount  *int32
	compactor ICompactor
}

func (m *Snapshot) Load(c ICompactor) {
	if m.compactor != nil {
		panic("trying to load the snapshot again")
	}
	m.refCount = new(int32)
	m.compactor = c
	m.Ref()
}

func (m *Snapshot) Ref() {
	if m.compactor == nil {
		panic("not loaded")
	}
	atomic.AddInt32(m.refCount, 1)
}

func (m *Snapshot) Unref() error {
	if m.compactor == nil {
		panic("not loaded")
	}
	if atomic.AddInt32(m.refCount, -1) == 0 {
		plog.Infof("going to call compact on %d", m.Index)
		return m.compactor.Compact(m.Index)
	}
	return nil
}

func (m *Snapshot) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *Snapshot) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	dAtA[i] = 0x12
	i++
	i = encodeVarintRaft(dAtA, i, uint64(len(m.Filepath)))
	i += copy(dAtA[i:], m.Filepath)
	dAtA[i] = 0x18
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.FileSize))
	dAtA[i] = 0x20
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Index))
	dAtA[i] = 0x28
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Term))
	dAtA[i] = 0x32
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Membership.Size()))
	n1, err := m.Membership.MarshalTo(dAtA[i:])
	if err != nil {
		return 0, err
	}
	i += n1
	if len(m.Files) > 0 {
		for _, msg := range m.Files {
			dAtA[i] = 0x3a
			i++
			i = encodeVarintRaft(dAtA, i, uint64(msg.Size()))
			n, err := msg.MarshalTo(dAtA[i:])
			if err != nil {
				return 0, err
			}
			i += n
		}
	}
	if m.Checksum != nil {
		dAtA[i] = 0x42
		i++
		i = encodeVarintRaft(dAtA, i, uint64(len(m.Checksum)))
		i += copy(dAtA[i:], m.Checksum)
	}
	dAtA[i] = 0x48
	i++
	if m.Dummy {
		dAtA[i] = 1
	} else {
		dAtA[i] = 0
	}
	i++
	dAtA[i] = 0x50
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.ShardID))
	dAtA[i] = 0x58
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Type))
	dAtA[i] = 0x60
	i++
	if m.Imported {
		dAtA[i] = 1
	} else {
		dAtA[i] = 0
	}
	i++
	dAtA[i] = 0x68
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.OnDiskIndex))
	dAtA[i] = 0x70
	i++
	if m.Witness {
		dAtA[i] = 1
	} else {
		dAtA[i] = 0
	}
	i++
	return i, nil
}

func (m *Snapshot) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	l = len(m.Filepath)
	n += 1 + l + sovRaft(uint64(l))
	n += 1 + sovRaft(uint64(m.FileSize))
	n += 1 + sovRaft(uint64(m.Index))
	n += 1 + sovRaft(uint64(m.Term))
	l = m.Membership.Size()
	n += 1 + l + sovRaft(uint64(l))
	if len(m.Files) > 0 {
		for _, e := range m.Files {
			l = e.Size()
			n += 1 + l + sovRaft(uint64(l))
		}
	}
	if m.Checksum != nil {
		l = len(m.Checksum)
		n += 1 + l + sovRaft(uint64(l))
	}
	n += 2
	n += 1 + sovRaft(uint64(m.ShardID))
	n += 1 + sovRaft(uint64(m.Type))
	n += 2
	n += 1 + sovRaft(uint64(m.OnDiskIndex))
	n += 2
	return n
}

func (m *Snapshot) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRaft
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: Snapshot: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: Snapshot: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Filepath", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Filepath = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field FileSize", wireType)
			}
			m.FileSize = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.FileSize |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Index", wireType)
			}
			m.Index = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Index |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 5:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Term", wireType)
			}
			m.Term = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Term |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 6:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Membership", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if err := m.Membership.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 7:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Files", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Files = append(m.Files, &SnapshotFile{})
			if err := m.Files[len(m.Files)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 8:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Checksum", wireType)
			}
			var byteLen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				byteLen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if byteLen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + byteLen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Checksum = append(m.Checksum[:0], dAtA[iNdEx:postIndex]...)
			if m.Checksum == nil {
				m.Checksum = []byte{}
			}
			iNdEx = postIndex
		case 9:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Dummy", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.Dummy = bool(v != 0)
		case 10:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ShardId", wireType)
			}
			m.ShardID = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ShardID |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 11:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Type", wireType)
			}
			m.Type = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Type |= StateMachineType(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 12:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Imported", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.Imported = bool(v != 0)
		case 13:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field OnDiskIndex", wireType)
			}
			m.OnDiskIndex = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.OnDiskIndex |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 14:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Witness", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.Witness = bool(v != 0)
		default:
			iNdEx = preIndex
			skippy, err := skipRaft(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
````

## File: raftpb/snapshotfile.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: raft.proto

package raftpb

import (
	"fmt"
	"io"
)

type SnapshotFile struct {
	Filepath string
	FileSize uint64
	FileId   uint64
	Metadata []byte
}

func (m *SnapshotFile) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *SnapshotFile) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	dAtA[i] = 0x12
	i++
	i = encodeVarintRaft(dAtA, i, uint64(len(m.Filepath)))
	i += copy(dAtA[i:], m.Filepath)
	dAtA[i] = 0x18
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.FileSize))
	dAtA[i] = 0x20
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.FileId))
	if m.Metadata != nil {
		dAtA[i] = 0x2a
		i++
		i = encodeVarintRaft(dAtA, i, uint64(len(m.Metadata)))
		i += copy(dAtA[i:], m.Metadata)
	}
	return i, nil
}

func (m *SnapshotFile) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	l = len(m.Filepath)
	n += 1 + l + sovRaft(uint64(l))
	n += 1 + sovRaft(uint64(m.FileSize))
	n += 1 + sovRaft(uint64(m.FileId))
	if m.Metadata != nil {
		l = len(m.Metadata)
		n += 1 + l + sovRaft(uint64(l))
	}
	return n
}

func (m *SnapshotFile) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRaft
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: SnapshotFile: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: SnapshotFile: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Filepath", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Filepath = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field FileSize", wireType)
			}
			m.FileSize = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.FileSize |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field FileId", wireType)
			}
			m.FileId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.FileId |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 5:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Metadata", wireType)
			}
			var byteLen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				byteLen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if byteLen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + byteLen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Metadata = append(m.Metadata[:0], dAtA[iNdEx:postIndex]...)
			if m.Metadata == nil {
				m.Metadata = []byte{}
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipRaft(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
````

## File: raftpb/snapshotheader.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: raft.proto

package raftpb

import (
	"fmt"
	"io"
)

type SnapshotHeader struct {
	SessionSize     uint64
	DataStoreSize   uint64
	UnreliableTime  uint64
	GitVersion      string
	HeaderChecksum  []byte
	PayloadChecksum []byte
	ChecksumType    ChecksumType
	Version         uint64
	CompressionType CompressionType
}

func (m *SnapshotHeader) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *SnapshotHeader) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	dAtA[i] = 0x8
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.SessionSize))
	dAtA[i] = 0x10
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.DataStoreSize))
	dAtA[i] = 0x18
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.UnreliableTime))
	dAtA[i] = 0x22
	i++
	i = encodeVarintRaft(dAtA, i, uint64(len(m.GitVersion)))
	i += copy(dAtA[i:], m.GitVersion)
	if m.HeaderChecksum != nil {
		dAtA[i] = 0x2a
		i++
		i = encodeVarintRaft(dAtA, i, uint64(len(m.HeaderChecksum)))
		i += copy(dAtA[i:], m.HeaderChecksum)
	}
	if m.PayloadChecksum != nil {
		dAtA[i] = 0x32
		i++
		i = encodeVarintRaft(dAtA, i, uint64(len(m.PayloadChecksum)))
		i += copy(dAtA[i:], m.PayloadChecksum)
	}
	dAtA[i] = 0x38
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.ChecksumType))
	dAtA[i] = 0x40
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Version))
	dAtA[i] = 0x48
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.CompressionType))
	return i, nil
}

func (m *SnapshotHeader) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	n += 1 + sovRaft(uint64(m.SessionSize))
	n += 1 + sovRaft(uint64(m.DataStoreSize))
	n += 1 + sovRaft(uint64(m.UnreliableTime))
	l = len(m.GitVersion)
	n += 1 + l + sovRaft(uint64(l))
	if m.HeaderChecksum != nil {
		l = len(m.HeaderChecksum)
		n += 1 + l + sovRaft(uint64(l))
	}
	if m.PayloadChecksum != nil {
		l = len(m.PayloadChecksum)
		n += 1 + l + sovRaft(uint64(l))
	}
	n += 1 + sovRaft(uint64(m.ChecksumType))
	n += 1 + sovRaft(uint64(m.Version))
	n += 1 + sovRaft(uint64(m.CompressionType))
	return n
}

func (m *SnapshotHeader) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRaft
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: SnapshotHeader: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: SnapshotHeader: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field SessionSize", wireType)
			}
			m.SessionSize = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.SessionSize |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field DataStoreSize", wireType)
			}
			m.DataStoreSize = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.DataStoreSize |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field UnreliableTime", wireType)
			}
			m.UnreliableTime = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.UnreliableTime |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field GitVersion", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.GitVersion = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 5:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field HeaderChecksum", wireType)
			}
			var byteLen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				byteLen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if byteLen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + byteLen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.HeaderChecksum = append(m.HeaderChecksum[:0], dAtA[iNdEx:postIndex]...)
			if m.HeaderChecksum == nil {
				m.HeaderChecksum = []byte{}
			}
			iNdEx = postIndex
		case 6:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field PayloadChecksum", wireType)
			}
			var byteLen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				byteLen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if byteLen < 0 {
				return ErrInvalidLengthRaft
			}
			postIndex := iNdEx + byteLen
			if postIndex < 0 {
				return ErrInvalidLengthRaft
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.PayloadChecksum = append(m.PayloadChecksum[:0], dAtA[iNdEx:postIndex]...)
			if m.PayloadChecksum == nil {
				m.PayloadChecksum = []byte{}
			}
			iNdEx = postIndex
		case 7:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ChecksumType", wireType)
			}
			m.ChecksumType = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ChecksumType |= ChecksumType(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 8:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Version", wireType)
			}
			m.Version = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Version |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 9:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field CompressionType", wireType)
			}
			m.CompressionType = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.CompressionType |= CompressionType(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		default:
			iNdEx = preIndex
			skippy, err := skipRaft(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
````

## File: raftpb/state.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: raft.proto

package raftpb

import (
	"fmt"
	"io"
)

type State struct {
	Term   uint64
	Vote   uint64
	Commit uint64
}

func (m *State) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *State) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	dAtA[i] = 0x8
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Term))
	dAtA[i] = 0x10
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Vote))
	dAtA[i] = 0x18
	i++
	i = encodeVarintRaft(dAtA, i, uint64(m.Commit))
	return i, nil
}

func (m *State) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	n += 1 + sovRaft(uint64(m.Term))
	n += 1 + sovRaft(uint64(m.Vote))
	n += 1 + sovRaft(uint64(m.Commit))
	return n
}

func (m *State) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRaft
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: State: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: State: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Term", wireType)
			}
			m.Term = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Term |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Vote", wireType)
			}
			m.Vote = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Vote |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Commit", wireType)
			}
			m.Commit = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRaft
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Commit |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		default:
			iNdEx = preIndex
			skippy, err := skipRaft(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) < 0 {
				return ErrInvalidLengthRaft
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
````

## File: raftpb/types.go
````go
// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: raft.proto

package raftpb

type MessageType int32

const (
	LocalTick          MessageType = 0
	Election           MessageType = 1
	LeaderHeartbeat    MessageType = 2
	ConfigChangeEvent  MessageType = 3
	NoOP               MessageType = 4
	Ping               MessageType = 5
	Pong               MessageType = 6
	Propose            MessageType = 7
	SnapshotStatus     MessageType = 8
	Unreachable        MessageType = 9
	CheckQuorum        MessageType = 10
	BatchedReadIndex   MessageType = 11
	Replicate          MessageType = 12
	ReplicateResp      MessageType = 13
	RequestVote        MessageType = 14
	RequestVoteResp    MessageType = 15
	InstallSnapshot    MessageType = 16
	Heartbeat          MessageType = 17
	HeartbeatResp      MessageType = 18
	ReadIndex          MessageType = 19
	ReadIndexResp      MessageType = 20
	Quiesce            MessageType = 21
	SnapshotReceived   MessageType = 22
	LeaderTransfer     MessageType = 23
	TimeoutNow         MessageType = 24
	RateLimit          MessageType = 25
	RequestPreVote     MessageType = 26
	RequestPreVoteResp MessageType = 27
	LogQuery           MessageType = 28
)

var MessageType_name = map[int32]string{
	0:  "LocalTick",
	1:  "Election",
	2:  "LeaderHeartbeat",
	3:  "ConfigChangeEvent",
	4:  "NoOP",
	5:  "Ping",
	6:  "Pong",
	7:  "Propose",
	8:  "SnapshotStatus",
	9:  "Unreachable",
	10: "CheckQuorum",
	11: "BatchedReadIndex",
	12: "Replicate",
	13: "ReplicateResp",
	14: "RequestVote",
	15: "RequestVoteResp",
	16: "InstallSnapshot",
	17: "Heartbeat",
	18: "HeartbeatResp",
	19: "ReadIndex",
	20: "ReadIndexResp",
	21: "Quiesce",
	22: "SnapshotReceived",
	23: "LeaderTransfer",
	24: "TimeoutNow",
	25: "RateLimit",
	26: "RequestPreVote",
	27: "RequestPreVoteResp",
	28: "LogQuery",
}

var MessageType_value = map[string]int32{
	"LocalTick":          0,
	"Election":           1,
	"LeaderHeartbeat":    2,
	"ConfigChangeEvent":  3,
	"NoOP":               4,
	"Ping":               5,
	"Pong":               6,
	"Propose":            7,
	"SnapshotStatus":     8,
	"Unreachable":        9,
	"CheckQuorum":        10,
	"BatchedReadIndex":   11,
	"Replicate":          12,
	"ReplicateResp":      13,
	"RequestVote":        14,
	"RequestVoteResp":    15,
	"InstallSnapshot":    16,
	"Heartbeat":          17,
	"HeartbeatResp":      18,
	"ReadIndex":          19,
	"ReadIndexResp":      20,
	"Quiesce":            21,
	"SnapshotReceived":   22,
	"LeaderTransfer":     23,
	"TimeoutNow":         24,
	"RateLimit":          25,
	"RequestPreVote":     26,
	"RequestPreVoteResp": 27,
	"LogQuery":           28,
}

func (x MessageType) String() string {
	return MessageType_name[int32(x)]
}

type EntryType int32

const (
	ApplicationEntry  EntryType = 0
	ConfigChangeEntry EntryType = 1
	EncodedEntry      EntryType = 2
	MetadataEntry     EntryType = 3
)

var EntryType_name = map[int32]string{
	0: "ApplicationEntry",
	1: "ConfigChangeEntry",
	2: "EncodedEntry",
	3: "MetadataEntry",
}

var EntryType_value = map[string]int32{
	"ApplicationEntry":  0,
	"ConfigChangeEntry": 1,
	"EncodedEntry":      2,
	"MetadataEntry":     3,
}

func (x EntryType) String() string {
	return EntryType_name[int32(x)]
}

type ConfigChangeType int32

const (
	AddNode      ConfigChangeType = 0
	RemoveNode   ConfigChangeType = 1
	AddNonVoting ConfigChangeType = 2
	AddWitness   ConfigChangeType = 3
)

var ConfigChangeType_name = map[int32]string{
	0: "AddNode",
	1: "RemoveNode",
	2: "AddNonVoting",
	3: "AddWitness",
}

var ConfigChangeType_value = map[string]int32{
	"AddNode":      0,
	"RemoveNode":   1,
	"AddNonVoting": 2,
	"AddWitness":   3,
}

func (x ConfigChangeType) String() string {
	return ConfigChangeType_name[int32(x)]
}

type StateMachineType int32

const (
	UnknownStateMachine    StateMachineType = 0
	RegularStateMachine    StateMachineType = 1
	ConcurrentStateMachine StateMachineType = 2
	OnDiskStateMachine     StateMachineType = 3
)

var StateMachineType_name = map[int32]string{
	0: "UnknownStateMachine",
	1: "RegularStateMachine",
	2: "ConcurrentStateMachine",
	3: "OnDiskStateMachine",
}

var StateMachineType_value = map[string]int32{
	"UnknownStateMachine":    0,
	"RegularStateMachine":    1,
	"ConcurrentStateMachine": 2,
	"OnDiskStateMachine":     3,
}

func (x StateMachineType) String() string {
	return StateMachineType_name[int32(x)]
}

type CompressionType int32

const (
	NoCompression CompressionType = 0
	Snappy        CompressionType = 1
)

var CompressionType_name = map[int32]string{
	0: "NoCompression",
	1: "Snappy",
}

var CompressionType_value = map[string]int32{
	"NoCompression": 0,
	"Snappy":        1,
}

func (x CompressionType) String() string {
	return CompressionType_name[int32(x)]
}

type ChecksumType int32

const (
	CRC32IEEE ChecksumType = 0
	HIGHWAY   ChecksumType = 1
)

var ChecksumType_name = map[int32]string{
	0: "CRC32IEEE",
	1: "HIGHWAY",
}

var ChecksumType_value = map[string]int32{
	"CRC32IEEE": 0,
	"HIGHWAY":   1,
}

func (x ChecksumType) String() string {
	return ChecksumType_name[int32(x)]
}
````

## File: raftpb/update_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raftpb

import (
	"testing"

	"github.com/stretchr/testify/require"
)

func TestUpdateMarshalAndUnmarshal(t *testing.T) {
	buf := make([]byte, 1024*16)
	tests := []struct {
		input Update
	}{
		{Update{EntriesToSave: []Entry{{Index: 100, Term: 200, Cmd: []byte("test-data")}, {Index: 200, Term: 300}}}},
		{Update{State: State{100, 200, 300}}},
		{Update{Snapshot: Snapshot{Index: 100, Term: 200}}},
		{Update{State: State{100, 200, 300}, Snapshot: Snapshot{Index: 100, Term: 200}}},
		{Update{State: State{100, 200, 300}, Snapshot: Snapshot{Index: 100, Term: 200}, EntriesToSave: []Entry{{Index: 100, Term: 200, Cmd: []byte("test-data")}, {Index: 200, Term: 300}}}},
	}
	for _, tt := range tests {
		if _, err := tt.input.MarshalTo(buf); err != nil {
			require.NoError(t, err)
		}
		var loaded Update
		require.NoError(t, loaded.Unmarshal(buf))
		require.Equal(t, tt.input, loaded)
	}
}
````

## File: raftpb/update.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package raftpb

import (
	"bytes"
	"encoding/binary"
	"io"
)

// LeaderUpdate describes updated leader
type LeaderUpdate struct {
	LeaderID uint64
	Term     uint64
}

// LogQueryResult is the result of log query.
type LogQueryResult struct {
	FirstIndex uint64
	LastIndex  uint64
	Error      error
	Entries    []Entry
}

// IsEmpty returns a boolean value indicating whether the LogQueryResult is
// empty.
func (r *LogQueryResult) IsEmpty() bool {
	return r.FirstIndex == 0 && r.LastIndex == 0 &&
		r.Error == nil && len(r.Entries) == 0
}

// SystemCtx is used to identify a ReadIndex operation.
type SystemCtx struct {
	Low  uint64
	High uint64
}

// ReadyToRead is used to indicate that a previous batch of ReadIndex requests
// are now ready for read once the entry specified by the Index value is applied in
// the state machine.
type ReadyToRead struct {
	Index     uint64
	SystemCtx SystemCtx
}

// UpdateCommit is used to describe how to commit the Update instance to
// progress the state of raft.
type UpdateCommit struct {
	// the last index known to be pushed to rsm for execution.
	Processed uint64
	// the last index confirmed to be executed.
	LastApplied      uint64
	StableLogTo      uint64
	StableLogTerm    uint64
	StableSnapshotTo uint64
	ReadyToRead      uint64
}

// Update is a collection of state, entries and messages that are expected to be
// processed by raft's upper layer to progress the raft node modelled as state
// machine.
type Update struct {
	ShardID   uint64
	ReplicaID uint64
	// The current persistent state of a raft node. It must be stored onto
	// persistent storage before any non-replication can be sent to other nodes.
	// isStateEqual(emptyState) returns true when the state is empty.
	State
	// whether CommittedEntries can be applied without waiting for the Update
	// to be persisted to disk
	FastApply bool
	// EntriesToSave are entries waiting to be stored onto persistent storage.
	EntriesToSave []Entry
	// CommittedEntries are entries already committed in raft and ready to be
	// applied by dragonboat applications.
	CommittedEntries []Entry
	// Whether there are more committed entries ready to be applied.
	MoreCommittedEntries bool
	// Snapshot is the metadata of the snapshot ready to be applied.
	Snapshot Snapshot
	// ReadyToReads provides a list of ReadIndex requests ready for local read.
	ReadyToReads []ReadyToRead
	// Messages is a list of outgoing messages to be sent to remote nodes.
	// As stated above, replication messages can be immediately sent, all other
	// messages must be sent after the persistent state and entries are saved
	// onto persistent storage.
	Messages []Message
	// LastApplied is the actual last applied index reported by the RSM.
	LastApplied uint64
	// UpdateCommit contains info on how the Update instance can be committed
	// to actually progress the state of raft.
	UpdateCommit UpdateCommit
	// DroppedEntries is a list of entries dropped when no leader is available
	DroppedEntries []Entry
	// DroppedReadIndexes is a list of read index requests  dropped when no leader
	// is available.
	DroppedReadIndexes []SystemCtx
	LogQueryResult     LogQueryResult
	LeaderUpdate       LeaderUpdate
}

// HasUpdate returns a boolean value indicating whether the returned Update
// instance actually has any update to be processed.
func (u *Update) HasUpdate() bool {
	return !IsEmptyState(u.State) ||
		!IsEmptySnapshot(u.Snapshot) ||
		len(u.EntriesToSave) > 0 ||
		len(u.CommittedEntries) > 0 ||
		len(u.Messages) > 0 ||
		len(u.ReadyToReads) > 0 ||
		len(u.DroppedEntries) > 0
}

// MarshalTo encodes the fields that need to be persisted to the specified
// buffer.
func (u *Update) MarshalTo(buf []byte) (int, error) {
	n1 := binary.PutUvarint(buf, u.ShardID)
	n2 := binary.PutUvarint(buf[n1:], u.ReplicaID)
	offset := n1 + n2
	if IsEmptyState(u.State) {
		buf[offset] = 0
		offset++
	} else {
		buf[offset] = 1
		offset++
		n, err := u.State.MarshalTo(buf[offset+4:])
		if err != nil {
			return 0, err
		}
		binary.LittleEndian.PutUint32(buf[offset:], uint32(n))
		offset += (n + 4)
	}
	binary.LittleEndian.PutUint32(buf[offset:], uint32(len(u.EntriesToSave)))
	offset += 4
	for _, e := range u.EntriesToSave {
		n, err := e.MarshalTo(buf[offset+4:])
		if err != nil {
			return 0, err
		}
		binary.LittleEndian.PutUint32(buf[offset:], uint32(n))
		offset += (n + 4)
	}
	if IsEmptySnapshot(u.Snapshot) {
		buf[offset] = 0
		offset++
	} else {
		buf[offset] = 1
		offset++
		n, err := u.Snapshot.MarshalTo(buf[offset+4:])
		if err != nil {
			return 0, err
		}
		binary.LittleEndian.PutUint32(buf[offset:], uint32(n))
		offset += (n + 4)
	}
	return offset, nil
}

type countedByteReader struct {
	reader io.ByteReader
	count  int
}

func (r *countedByteReader) ReadByte() (byte, error) {
	v, err := r.reader.ReadByte()
	r.count++
	return v, err
}

// Unmarshal decodes the Update state from the input buf.
func (u *Update) Unmarshal(buf []byte) error {
	r := &countedByteReader{
		reader: bytes.NewReader(buf),
	}
	var err error
	u.ShardID, err = binary.ReadUvarint(r)
	if err != nil {
		return err
	}
	u.ReplicaID, err = binary.ReadUvarint(r)
	if err != nil {
		return err
	}
	offset := r.count
	if buf[offset] == 0 {
		offset++
	} else {
		offset++
		l := binary.LittleEndian.Uint32(buf[offset:])
		if err := u.State.Unmarshal(buf[offset+4 : offset+4+int(l)]); err != nil {
			return err
		}
		offset += (4 + int(l))
	}
	count := binary.LittleEndian.Uint32(buf[offset:])
	offset += 4
	if count > 0 {
		u.EntriesToSave = make([]Entry, count)
	}
	for i := uint32(0); i < count; i++ {
		l := binary.LittleEndian.Uint32(buf[offset:])
		var entry Entry
		if err := entry.Unmarshal(buf[offset+4 : offset+4+int(l)]); err != nil {
			return err
		}
		u.EntriesToSave[i] = entry
		offset += (4 + int(l))
	}
	if buf[offset] == 1 {
		offset++
		l := binary.LittleEndian.Uint32(buf[offset:])
		if err := u.Snapshot.Unmarshal(buf[offset+4 : offset+4+int(l)]); err != nil {
			return err
		}
	}
	return nil
}

// SizeUpperLimit returns the upper limit of the estimated size of marshalled
// Update instance.
func (u *Update) SizeUpperLimit() int {
	sz := 2 + 4 + 16
	sz += int(GetEntrySliceSize(u.EntriesToSave))
	sz += u.State.SizeUpperLimit()
	if !IsEmptySnapshot(u.Snapshot) {
		sz += u.Snapshot.Size()
	} else {
		sz += 48
	}
	return sz
}
````

## File: statemachine/concurrent.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package statemachine

import (
	"io"
)

// IConcurrentStateMachine is the interface to be implemented by application's
// state machine when concurrent access to the state machine is required. For
// a typical IConcurrentStateMachine type, most of its managed data is expected
// to be fitted into memory, Dragonboat manages its captured snapshots and
// saved Raft Logs to ensure such in-memory state machine state can be restored
// after reboot.
//
// The Update method is always invoked from the same goroutine. The Lookup,
// SaveSnapshot and GetHash methods can be invoked concurrent to the Update
// method. The Lookup method is also allowed to be invoked concurrent to the
// RecoverFromSnapshot method. It is user's responsibility to implement the
// IConcurrentStateMachine instance with such supported concurrency while also
// protecting the state machine data integrity. Invocations to the Update,
// PrepareSnapshot, RecoverFromSnapshot and the Close methods are guarded by
// the system to ensure mutual exclusion.
//
// When created, an IConcurrentStateMachine instance should always start from an
// empty state. Dragonboat will use saved snapshots and Raft Logs to help a
// restarted IConcurrentStateMachine instance to catch up to its previous state.
//
// IConcurrentStateMachine is provided as an alternative option to the
// IStateMachine interface which also keeps state machine data mostly in memory
// but without concurrent access support. Users are recommended to use the
// IStateMachine interface whenever possible.
type IConcurrentStateMachine interface {
	// Update updates the IConcurrentStateMachine instance. The input Entry slice
	// is list of continuous proposed and committed commands from clients, they
	// are provided together as a batch so the IConcurrentStateMachine
	// implementation can choose to batch them and apply together to hide latency.
	//
	// The Update method must be deterministic, meaning that given the same initial
	// state of IConcurrentStateMachine and the same input sequence, it should
	// reach to the same updated state and outputs the same returned results. The
	// input entry slice should be the only input to this method. Reading from
	// the system clock, random number generator or other similar external data
	// sources will violate the deterministic requirement of the Update method.
	//
	// The IConcurrentStateMachine implementation should not keep a reference to
	// the input entry slice after return.
	//
	// Update returns the input entry slice with the Result field of all its
	// members set.
	//
	// Update returns an error when there is unrecoverable error for updating the
	// on disk state machine, e.g. disk failure when trying to update the state
	// machine.
	Update([]Entry) ([]Entry, error)
	// Lookup queries the state of the IConcurrentStateMachine instance and
	// returns the query result as a byte slice. The input byte slice specifies
	// what to query, it is up to the IConcurrentStateMachine implementation to
	// interpret the input byte slice.
	//
	// When an error is returned by the Lookup() method, the error will be passed
	// to the caller of NodeHost's ReadLocalNode() or SyncRead() methods to be
	// handled. A typical scenario for returning an error is that the state
	// machine has already been closed or aborted from a RecoverFromSnapshot
	// procedure when Lookup is being handled.
	//
	// The IConcurrentStateMachine implementation should not keep a reference of
	// the input byte slice after return.
	//
	// The Lookup() method is a read only method, it should never change the state
	// of the IConcurrentStateMachine instance.
	Lookup(interface{}) (interface{}, error)
	// PrepareSnapshot prepares the snapshot to be concurrently captured and saved.
	// PrepareSnapshot is invoked before SaveSnapshot is called and it is invoked
	// with mutual exclusion protection from the Update method.
	//
	// PrepareSnapshot in general saves a state identifier of the current state,
	// such state identifier is usually a version number, a sequence number, a
	// change ID or some other in memory small data structure used for describing
	// the point in time state of the state machine. The state identifier is
	// returned as an interface{} and it is provided to the SaveSnapshot() method
	// so the state machine state at that identified point in time can be saved
	// when SaveSnapshot is invoked.
	//
	// PrepareSnapshot returns an error when there is unrecoverable error for
	// preparing the snapshot.
	PrepareSnapshot() (interface{}, error)
	// SaveSnapshot saves the point in time state of the IConcurrentStateMachine
	// identified by the input state identifier to the provided io.Writer backed
	// by a file on disk and the provided ISnapshotFileCollection instance. This
	// is a read only method that should never change the state of the
	// IConcurrentStateMachine instance.
	//
	// It is important to understand that SaveSnapshot should never save the
	// current latest state. The point in time state identified by the input state
	// identifier is what suppose to be saved, the latest state might be different
	// from such specified point in time state as the state machine might have
	// already been updated by the Update() method after the completion of
	// the call to PrepareSnapshot.
	//
	// It is SaveSnapshot's responsibility to free the resources owned by the
	// input state identifier when it is done.
	//
	// The ISnapshotFileCollection instance is used to record finalized external
	// files that should also be included as a part of the snapshot. All other
	// state data should be saved into the io.Writer backed by snapshot file on
	// disk. It is application's responsibility to save the complete state so
	// that the recovered IConcurrentStateMachine state is considered as
	// identical to the original state.
	//
	// The provided read-only chan struct{} is to notify the SaveSnapshot method
	// that the associated Raft node is being closed so the IConcurrentStateMachine
	// can choose to abort the SaveSnapshot procedure and return
	// ErrSnapshotStopped immediately.
	//
	// SaveSnapshot is allowed to abort the snapshotting operation at any time by
	// returning ErrSnapshotAborted.
	//
	// SaveSnapshot returns the encountered error when generating the snapshot.
	// Other than the above mentioned ErrSnapshotStopped and ErrSnapshotAborted
	// errors, the IConcurrentStateMachine implementation should only return a
	// non-nil error when the system need to be immediately halted for critical
	// errors, e.g. disk error preventing you from saving the snapshot.
	SaveSnapshot(interface{},
		io.Writer, ISnapshotFileCollection, <-chan struct{}) error
	// RecoverFromSnapshot recovers the state of the IConcurrentStateMachine
	// instance from a previously saved snapshot captured by the SaveSnapshot()
	// method. The saved snapshot is provided as an io.Reader backed by a file
	// on disk together with a list of files previously recorded into the
	// ISnapshotFileCollection in SaveSnapshot().
	//
	// Dragonboat ensures that Update() and Close() will not be invoked when
	// RecoverFromSnapshot() is in progress.
	//
	// The provided read-only chan struct{} is provided to notify the
	// RecoverFromSnapshot() method that the associated Raft node is being closed.
	// On receiving such notification, RecoverFromSnapshot() can choose to
	// abort recovering from the snapshot and return an ErrSnapshotStopped error
	// immediately. Other than ErrSnapshotStopped, IConcurrentStateMachine should
	// only return a non-nil error when the system need to be immediately halted
	// for non-recoverable error, e.g. disk error preventing you from reading the
	// complete saved snapshot.
	//
	// RecoverFromSnapshot is invoked when restarting from a previously saved
	// state or when the Raft node is significantly behind its leader.
	RecoverFromSnapshot(io.Reader, []SnapshotFile, <-chan struct{}) error
	// Close closes the IConcurrentStateMachine instance.
	//
	// The Close method is not allowed to update the state of the
	// IConcurrentStateMachine visible to the Lookup() method.
	//
	// Close allows the application to finalize resources to a state easier to
	// be re-opened and used in the future. It is important to understand that
	// Close is not guaranteed to be always called, e.g. node might crash at any
	// time. IConcurrentStateMachine should be designed in a way that the
	// safety and integrity of its managed data doesn't rely on whether the Close
	// method is called or not.
	//
	// Other than setting up some internal flags to indicate that the
	// IConcurrentStateMachine instance has been closed, the Close method is not
	// allowed to update the state of IConcurrentStateMachine visible to the
	// Lookup method.
	Close() error
}

// CreateConcurrentStateMachineFunc is a factory function type for creating an
// IConcurrentStateMachine instance.
type CreateConcurrentStateMachineFunc func(uint64, uint64) IConcurrentStateMachine
````

## File: statemachine/disk.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package statemachine

import (
	"io"

	"github.com/cockroachdb/errors"
)

var (
	// ErrSnapshotStreaming is the error returned when the snapshot data being
	// generated can not be streamed to its intended destination.
	ErrSnapshotStreaming = errors.New("failed to stream the snapshot")
	// ErrOpenStopped is the error returned by the Open method of an
	// IOnDiskStateMachine type when it chooses to abort from its Open method.
	ErrOpenStopped = errors.New("open method did not complete")
)

// IOnDiskStateMachine is the interface to be implemented by application's
// state machine when the state machine state is always persisted on disks.
// IOnDiskStateMachine basically matches the state machine type described
// in the section 5.2 of the Raft thesis.
//
// For IOnDiskStateMachine types, concurrent access to the state machine is
// supported. An IOnDiskStateMachine type allows its Update method to be
// concurrently invoked when there are ongoing calls to the Lookup or the
// SaveSnapshot method. Lookup is also allowed when the RecoverFromSnapshot or
// the Close methods are being invoked. Invocations to the Update, Sync,
// PrepareSnapshot, RecoverFromSnapshot and Close methods are guarded by the
// system to ensure mutual exclusion.
//
// Once created, the Open method is immediately invoked to open and use the
// persisted state on disk. This makes IOnDiskStateMachine different from
// IStateMachine types which require the state machine state to be fully
// reconstructed from saved snapshots and Raft logs.
//
// Applications that implement IOnDiskStateMachine are recommended to setup
// periodic snapshotting with relatively short time intervals, that triggers
// the state machine's metadata, usually only a few KBytes each, to be
// periodically snapshotted and thus causes negligible overheads for the system.
// It also provides opportunities for the system to signal Raft Log compactions
// to free up disk spaces.
type IOnDiskStateMachine interface {
	// Open opens the existing on disk state machine to be used or it creates a
	// new state machine with empty state if it does not exist. Open returns the
	// most recent index value of the Raft log that has been persisted, or it
	// returns 0 when the state machine is a new one.
	//
	// The provided read only chan struct{} channel is used to notify the Open
	// method that the node has been stopped and the Open method can choose to
	// abort by returning an ErrOpenStopped error.
	//
	// Open is called shortly after the Raft node is started. The Update method
	// and the Lookup method will not be called before the completion of the Open
	// method.
	Open(stopc <-chan struct{}) (uint64, error)
	// Update updates the IOnDiskStateMachine instance. The input Entry slice
	// is a list of continuous proposed and committed commands from clients, they
	// are provided together as a batch so the IOnDiskStateMachine implementation
	// can choose to batch them and apply together to hide latency. Update returns
	// the input entry slice with the Result field of all its members set.
	//
	// The read only Index field of each input Entry instance is the Raft log
	// index of each entry, it is IOnDiskStateMachine's responsibility to
	// atomically persist the Index value together with the corresponding state
	// update.
	//
	// The Update method can choose to synchronize all of its in-core state with
	// that on disk. This can minimize the number of committed Raft entries that
	// need to be re-applied after reboot. Update can also choose to postpone such
	// synchronization until the Sync method is invoked, this approach produces
	// higher throughput during fault free running at the cost that some of the
	// most recent Raft entries not synchronized onto disks will have to be
	// re-applied after reboot.
	//
	// When the Update method does not synchronize its in-core state with that on
	// disk, the implementation must ensure that after a reboot there is no
	// applied entry in the State Machine more recent than any entry that was
	// lost during reboot. For example, consider a state machine with 3 applied
	// entries, let's assume their index values to be 1, 2 and 3. Once they have
	// been applied into the state machine without synchronizing the in-core state
	// with that on disk, it is okay to lose the data associated with the applied
	// entry 3, but it is strictly forbidden to have the data associated with the
	// applied entry 3 available in the state machine while the one with index
	// value 2 got lost during reboot.
	//
	// The Update method must be deterministic, meaning given the same initial
	// state of IOnDiskStateMachine and the same input sequence, it should reach
	// to the same updated state and outputs the same results. The input entry
	// slice should be the only input to this method. Reading from the system
	// clock, random number generator or other similar external data sources will
	// likely violate the deterministic requirement of the Update method.
	//
	// Concurrent calls to the Lookup method and the SaveSnapshot method are not
	// blocked when the state machine is being updated by the Update method.
	//
	// The IOnDiskStateMachine implementation should not keep a reference to the
	// input entry slice after return.
	//
	// Update returns an error when there is unrecoverable error when updating the
	// on disk state machine.
	Update([]Entry) ([]Entry, error)
	// Lookup queries the state of the IOnDiskStateMachine instance and returns
	// the query result as an interface{}. The input interface{} specifies what to
	// query, it is up to the IOnDiskStateMachine implementation to interpret such
	// input. The returned interface{} contains the query result.
	//
	// When an error is returned by the Lookup method, the error will be passed
	// to the caller to be handled. A typical scenario for returning an error is
	// that the state machine has already been closed or aborted from a
	// RecoverFromSnapshot procedure before Lookup is called.
	//
	// Concurrent calls to the Update and RecoverFromSnapshot method are not
	// blocked when calls to the Lookup method are being processed.
	//
	// The IOnDiskStateMachine implementation should not keep any reference of
	// the input interface{} after return.
	//
	// The Lookup method is a read only method, it should never change the state
	// of IOnDiskStateMachine.
	Lookup(interface{}) (interface{}, error)
	// Sync synchronizes all in-core state of the state machine to persisted
	// storage so the state machine can continue from its latest state after
	// reboot.
	//
	// Sync is always invoked with mutual exclusion protection from the Update,
	// PrepareSnapshot, RecoverFromSnapshot and Close methods.
	//
	// Sync returns an error when there is unrecoverable error for synchronizing
	// the in-core state.
	Sync() error
	// PrepareSnapshot prepares the snapshot to be concurrently captured and
	// streamed. PrepareSnapshot is invoked before SaveSnapshot is called and it
	// is always invoked with mutual exclusion protection from the Update, Sync,
	// RecoverFromSnapshot and Close methods.
	//
	// PrepareSnapshot in general saves a state identifier of the current state,
	// such state identifier can be a version number, a sequence number, a change
	// ID or some other small in memory data structure used for describing the
	// point in time state of the state machine. The state identifier is returned
	// as an interface{} before being passed to the SaveSnapshot() method.
	//
	// PrepareSnapshot returns an error when there is unrecoverable error for
	// preparing the snapshot.
	PrepareSnapshot() (interface{}, error)
	// SaveSnapshot saves the point in time state of the IOnDiskStateMachine
	// instance identified by the input state identifier, which is usually not
	// the latest state of the IOnDiskStateMachine instance, to the provided
	// io.Writer.
	//
	// It is application's responsibility to save the complete state to the
	// provided io.Writer in a deterministic manner. That is for the same state
	// machine state, when SaveSnapshot is invoked multiple times with the same
	// input state identifier, the content written to the provided io.Writer
	// should always be the same.
	//
	// When there is any connectivity error between the local node and the remote
	// node, an ErrSnapshotStreaming will be returned by io.Writer's Write method.
	// The SaveSnapshot method should return ErrSnapshotStreaming to abort its
	// operation.
	//
	// It is SaveSnapshot's responsibility to free the resources owned by the
	// input state identifier when it is done.
	//
	// The provided read-only chan struct{} is provided to notify the SaveSnapshot
	// method that the associated Raft node is being closed so the
	// IOnDiskStateMachine can choose to abort the SaveSnapshot procedure and
	// return ErrSnapshotStopped immediately.
	//
	// SaveSnapshot is allowed to abort the snapshotting operation at any time by
	// returning ErrSnapshotAborted.
	//
	// The SaveSnapshot method is allowed to be invoked when there is concurrent
	// call to the Update method. SaveSnapshot is a read-only method, it should
	// never change the state of the IOnDiskStateMachine.
	//
	// SaveSnapshot returns the encountered error when generating the snapshot.
	// Other than the above mentioned ErrSnapshotStopped and ErrSnapshotAborted
	// errors, the IOnDiskStateMachine implementation should only return a non-nil
	// error when the system need to be immediately halted for critical errors,
	// e.g. disk error preventing you from saving the snapshot.
	SaveSnapshot(interface{}, io.Writer, <-chan struct{}) error
	// RecoverFromSnapshot recovers the state of the IOnDiskStateMachine instance
	// from a snapshot captured by the SaveSnapshot() method on a remote node. The
	// saved snapshot is provided as an io.Reader backed by a file stored on disk.
	//
	// Dragonboat ensures that the Update, Sync, PrepareSnapshot, SaveSnapshot and
	// Close methods will not be invoked when RecoverFromSnapshot() is in
	// progress.
	//
	// The provided read-only chan struct{} is provided to notify the
	// RecoverFromSnapshot method that the associated Raft node has been closed.
	// On receiving such notification, RecoverFromSnapshot() can choose to
	// abort recovering from the snapshot and return an ErrSnapshotStopped error
	// immediately. Other than ErrSnapshotStopped, IOnDiskStateMachine should
	// only return a non-nil error when the system need to be immediately halted
	// for non-recoverable error.
	//
	// RecoverFromSnapshot is not required to synchronize its recovered in-core
	// state with that on disk.
	RecoverFromSnapshot(io.Reader, <-chan struct{}) error
	// Close closes the IOnDiskStateMachine instance. Close is invoked when the
	// state machine is in a ready-to-exit state in which there will be no further
	// call to the Update, Sync, PrepareSnapshot, SaveSnapshot and the
	// RecoverFromSnapshot method. It is possible to have concurrent Lookup calls,
	// Lookup can also be called after the return of Close.
	//
	// Close allows the application to finalize resources to a state easier to
	// be re-opened and restarted in the future. It is important to understand
	// that Close is not guaranteed to be always invoked, e.g. node can crash at
	// any time without calling the Close method. IOnDiskStateMachine should be
	// designed in a way that the safety and integrity of its on disk data
	// doesn't rely on whether Close is eventually called or not.
	//
	// Other than setting up some internal flags to indicate that the
	// IOnDiskStateMachine instance has been closed, the Close method is not
	// allowed to update the state of IOnDiskStateMachine visible to the outside.
	Close() error
}

// CreateOnDiskStateMachineFunc is a factory function type for creating
// IOnDiskStateMachine instances.
type CreateOnDiskStateMachineFunc func(shardID uint64, replicaID uint64) IOnDiskStateMachine
````

## File: statemachine/extension.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package statemachine

import (
	"github.com/cockroachdb/errors"
)

var (
	// ErrNotImplemented indicates that the requested optional feature is not
	// implemented by the state machine.
	ErrNotImplemented = errors.New("requested feature not implemented")
)

// IHash is an optional interface to be implemented by a user state machine type
// when the ability to generate state machine hash is required.
type IHash interface {
	// GetHash returns a uint64 value used to represent the current state of the
	// state machine. The hash should be generated in a deterministic manner
	// which means nodes from the same Raft shard are suppose to return the
	// same hash result when they have the same Raft Log entries applied.
	//
	// GetHash is a read-only operation.
	GetHash() (uint64, error)
}

// IExtended is an optional interface to be implemented by a user state machine
// type, most of its member methods are for performance optimization purposes.
type IExtended interface {
	// NALookup is similar to user state machine's Lookup method, it tries to
	// minimize extra heap allocation by taking a byte slice as the input and the
	// returned query result is also provided as a byte slice. The input byte
	// slice specifies what to query, it is up to the implementation to interpret
	// the input byte slice.
	//
	// Lookup method is a read-only method, it should never change state machine's
	// state.
	NALookup([]byte) ([]byte, error)
}
````

## File: statemachine/rsm.go
````go
// Copyright 2018-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

/*
Package statemachine contains the definitions of the IStateMachine and
IOnDiskStateMachine interfaces for supporting the replicated state machine
approach.

User services are provided with fault tolerance when they are implemented
as IStateMachine or IOnDiskStateMachine instances. The Update method is
used for update operations for the services, the Lookup method is used for
handling read only queries to the services, snapshot related methods,
including PrepareSnapshot, SaveSnapshot and RecoverFromSnapshot, are used
to update service state based on snapshots.
*/
package statemachine

import (
	"io"

	"github.com/cockroachdb/errors"
)

var (
	// ErrSnapshotStopped is returned by state machine's SaveSnapshot and
	// RecoverFromSnapshot methods to indicate that those two snapshot operations
	// have been aborted as the associated raft node is being closed.
	ErrSnapshotStopped = errors.New("snapshot stopped")
	// ErrSnapshotAborted is returned by state machine's SaveSnapshot method to
	// indicate that the SaveSnapshot operation is aborted by the user.
	ErrSnapshotAborted = errors.New("snapshot aborted")
)

// Type is the state machine type.
type Type uint64

const (
	// RegularStateMachine is the state machine type that implements
	// IStateMachine.
	RegularStateMachine = 1
	// ConcurrentStateMachine is the state machine type that implements
	// IConcurrentStateMachine
	ConcurrentStateMachine = 2
	// OnDiskStateMachine is the state machine type that implements
	// IOnDiskStateMachine
	OnDiskStateMachine = 3
)

// SnapshotFile is the struct used to describe external files included in a
// snapshot.
type SnapshotFile struct {
	// FileID is the ID of the file provided to ISnapshotFileCollection.AddFile().
	FileID uint64
	// Filepath is the current full path of the file.
	Filepath string
	// Metadata is the metadata provided to ISnapshotFileCollection.AddFile().
	Metadata []byte
}

// ISnapshotFileCollection is the interface used by the
// IStateMachine.SaveSnapshot() method for recording external files that should
// be included as a part of the snapshot being created.
//
// For example, consider you have a IStateMachine implementation internally
// backed by a NoSQL DB, when creating a snapshot of the IStateMachine instance,
// the state of the NoSQL DB need to be captured and saved as well. When the
// NoSQL has its own built-in feature to snapshot the current state into some
// on-disk files, these files should be included in IStateMachine's snapshot so
// they can be used to reconstruct the current state of the NoSQL DB when the
// IStateMachine instance recovers from the saved snapshot.
type ISnapshotFileCollection interface {
	// AddFile adds an external file to the snapshot being currently generated.
	// The file must have been finalized meaning its content will not change in
	// the future. It is your application's responsibility to make sure that the
	// file being added is accessible from the current process and it is
	// possible to create a hard link to it from the NodeHostDir directory
	// specified in NodeHost's NodeHostConfig. The file to be added is identified
	// by the specified fileID. The metadata byte slice is the metadata of the
	// file being added. It can be the checksum of the file, file type, file name,
	// other file hierarchy information, or a serialized combination of such
	// metadata.
	AddFile(fileID uint64, path string, metadata []byte)
}

// Result is the result type generated and returned by the Update method in
// IStateMachine and IOnDiskStateMachine types.
type Result struct {
	// Value is a 64 bits integer value used to indicate the outcome of the
	// update operation.
	Value uint64
	// Data is an optional byte slice created and returned by the update
	// operation. It is useful for CompareAndSwap style updates in which an
	// arbitrary length of bytes need to be returned.
	// Users are strongly recommended to use the query methods supported by
	// NodeHost to query the state of their IStateMachine and IOnDiskStateMachine
	// types, proposal based queries are known to work but are not recommended.
	Data []byte
}

// Entry represents a Raft log entry that is going to be provided to the Update
// method of an IConcurrentStateMachine or IOnDiskStateMachine instance.
type Entry struct {
	// Index is the Raft log index of the entry. The field is set by the
	// Dragonboat library and it is strictly read-only.
	Index uint64
	// Cmd is the proposed command. This field is strictly read-only.
	Cmd []byte
	// Result is the result value obtained from the Update method of an
	// IConcurrentStateMachine or IOnDiskStateMachine instance.
	Result Result
}

// IStateMachine is the interface to be implemented by application's state
// machine when most of the state machine data is stored in memory. It is the
// basic state machine type described in the Raft thesis.
//
// A sync.RWMutex is used internally by dragonboat as a reader/writer mutual
// exclusion lock to guard the IStateMachine instance when accessing
// IStateMachine's member methods. The Update, RecoverFromSnapshot and Close
// methods are invoked when the write lock is acquired, other methods are
// invoked when the shared read lock is acquired.
//
// As the state is mostly in memory, snapshots are usually periodically captured
// to save its state to disk. After each reboot, IStateMachine state must be
// reconstructed from the empty state based on the latest snapshot and saved
// Raft logs.
//
// When created, an IStateMachine instance should always start from an empty
// state. Saved snapshots and Raft logs will be used to help a rebooted
// IStateMachine instance to catch up to its previous state.
type IStateMachine interface {
	// Update updates the IStateMachine instance. The input data slice is the
	// proposed command from client, it is up to the IStateMachine implementation
	// to interpret this byte slice and update the IStateMachine instance
	// accordingly.
	//
	// The Update method must be deterministic, meaning given the same initial
	// state of IStateMachine and the same input, it should always reach to the
	// same updated state and outputs the same returned value. This requires the
	// input byte slice should be the only input to this method. Reading from
	// system clock, random number generator or other similar data sources will
	// likely violate the deterministic requirement of the Update method.
	//
	// The IStateMachine implementation should not keep a reference to the input
	// byte slice after return.
	//
	// Update returns a Result value used to indicate the outcome of the update
	// operation. An error is returned when there is unrecoverable error, such
	// error will cause the program to panic.
	Update(Entry) (Result, error)
	// Lookup queries the state of the IStateMachine instance. The input
	// interface{} specifies what to query, it is up to the IStateMachine
	// implementation to interpret such input interface{}. The returned
	// interface{} is the query result provided by the IStateMachine
	// implementation.
	//
	// The IStateMachine implementation should not keep a reference of the input
	// interface{} after return. The Lookup method is a read only method, it
	// should never change the state of IStateMachine.
	//
	// When an error is returned by the Lookup method, it will be passed to the
	// user client.
	Lookup(interface{}) (interface{}, error)
	// SaveSnapshot saves the current state of the IStateMachine instance to the
	// provided io.Writer backed by an on disk file. SaveSnapshot is a read only
	// operation.
	//
	// The data saved into the io.Writer is usually the in-memory data, while the
	// ISnapshotFileCollection instance is used to record immutable files that
	// should also be included as a part of the snapshot. It is application's
	// responsibility to save the complete state so that the reconstructed
	// IStateMachine state based on such saved snapshot will be considered as
	// identical to the original state.
	//
	// The provided read-only chan struct{} is used to notify that the associated
	// Raft node is being closed so the implementation can choose to abort the
	// SaveSnapshot procedure and return ErrSnapshotStopped immediately.
	//
	// SaveSnapshot is allowed to abort the snapshotting operation at any time by
	// returning ErrSnapshotAborted.
	//
	// Other than the above mentioned ErrSnapshotStopped and ErrSnapshotAborted
	// errors, the IStateMachine implementation should only return a non-nil error
	// when the system need to be immediately halted for critical errors, e.g.
	// disk error preventing you from saving the snapshot.
	SaveSnapshot(io.Writer, ISnapshotFileCollection, <-chan struct{}) error
	// RecoverFromSnapshot recovers the state of an IStateMachine instance from a
	// previously saved snapshot captured by the SaveSnapshot method. The saved
	// snapshot is provided as an io.Reader backed by an on disk file and
	// a list of immutable files previously recorded into the
	// ISnapshotFileCollection by the SaveSnapshot method.
	//
	// The provided read-only chan struct{} is used to notify the
	// RecoverFromSnapshot() method that the associated Raft node is being closed.
	// On receiving such notification, RecoverFromSnapshot() can choose to
	// abort and return ErrSnapshotStopped immediately. Other than
	// ErrSnapshotStopped, IStateMachine should only return a non-nil error when
	// the system must be immediately halted for non-recoverable error, e.g. disk
	// error preventing you from reading the complete saved snapshot.
	RecoverFromSnapshot(io.Reader, []SnapshotFile, <-chan struct{}) error
	// Close closes the IStateMachine instance.
	//
	// The Close method is not allowed to update the state of the IStateMachine
	// visible to IStateMachine's Lookup method.
	//
	// This allows the application to finalize resources to a state easier to be
	// re-opened and used in the future. It is important to understand that Close
	// is not guaranteed to be always called, e.g. node might crash at any time.
	// IStateMachine should be designed in a way that the safety and integrity of
	// its managed data doesn't rely on whether the Close method is called or not.
	Close() error
}

// CreateStateMachineFunc is a factory function type for creating IStateMachine
// instances.
type CreateStateMachineFunc func(shardID uint64, replicaID uint64) IStateMachine
````

## File: tools/fsync/main.go
````go
package main

import (
	"flag"
	"fmt"
	"os"
	"sync"
)

const (
	// BATCH is the number of write operations to repeat
	BATCH = 10000
	// BUCKETS is the number of parallel write jobs
	BUCKETS = 16
)

var data []byte
var fs []*os.File

var parallel = flag.Bool("parallel", false, "whether to use parallel writes")
var size = flag.Int("size", 64*1024, "size of each write in bytes")

func init() {
	flag.Parse()
	data = make([]byte, *size)
	if *parallel {
		fs = make([]*os.File, BUCKETS)
		for i := 0; i < len(fs); i++ {
			f, err := os.Create(fmt.Sprintf("%v.dat", i))
			if err != nil {
				panic(err)
			}
			fs[i] = f
		}
	} else {
		fs = make([]*os.File, 1)
		f, err := os.Create("test.dat")
		if err != nil {
			panic(err)
		}
		fs[0] = f
	}
}

func main() {
	if *parallel {
		parallelWrite()
	} else {
		sequentialWrite()
	}
	for _, f := range fs {
		if err := f.Close(); err != nil {
			panic(err)
		}
	}
}

func sequentialWrite() {
	for i := 0; i < BATCH; i++ {
		for j := 0; j < BUCKETS; j++ {
			if _, err := fs[0].Write(data); err != nil {
				panic(err)
			}
		}
		if err := fs[0].Sync(); err != nil {
			panic(err)
		}
	}
}

func parallelWrite() {
	var wg sync.WaitGroup
	if len(fs) != BUCKETS {
		panic("unexpected file count")
	}
	for _, f := range fs {
		wg.Add(1)
		gf := f
		go func() {
			for i := 0; i < BATCH; i++ {
				if _, err := gf.Write(data); err != nil {
					panic(err)
				}
				if err := gf.Sync(); err != nil {
					panic(err)
				}
			}
			wg.Done()
		}()
	}
	wg.Wait()
}
````

## File: tools/fsync/README.md
````markdown
### ST4000NM0033-9ZM170 4T, SATA HDD ###

size=64KBytes, parallel=false

real	5m18.625s
user	0m2.499s
sys	  0m20.135s

size=64KBytes, parallel=true

real	13m35.931s
user	0m3.107s
sys	  1m8.695s

size=16KBytes, parallel=false

real	4m10.861s
user	0m1.416s
sys	  0m7.387s

size=16KBytes, parallel=true

real	11m11.082s
user	0m3.175s
sys	  0m41.429s

size=4KBytes, parallel=false

real	3m54.008s
user	0m1.070s
sys	  0m5.136s

size=4KBytes, parallel=true

real	11m4.150s
user	0m3.135s
sys	  0m35.322s

### Intel S3710 800G, SATA SSD ###

size=64KBytes, parallel=false

real	0m38.138s
user	0m1.255s
sys	  0m15.931s

size=64KBytes, parallel=true

real	0m27.761s
user	0m1.959s
sys	  1m2.902s

size=16KBytes, parallel=false

real	0m13.636s
user	0m0.657s
sys	  0m5.547s

size=16KBytes, parallel=true

real	0m13.811s
user	0m2.025s
sys	  0m37.480s

size=4KBytes, parallel=false

real	0m5.410s
user	0m0.456s
sys	  0m1.930s

size=4KBytes, parallel=true

real	0m8.291s
user	0m1.809s
sys	  0m26.752s

### Intel P4510 2T, NVME SSD ###

size=64KBytes, parallel=false

real	0m11.746s
user	0m0.566s
sys	  0m8.133s

size=64KBytes, parallel=true

real	0m6.425s
user	0m0.785s
sys	  0m24.773s

size=16KBytes, parallel=false

real	0m4.323s
user	0m0.289s
sys	  0m2.813s

size=16KBytes, parallel=true

real	0m4.610s
user	0m0.561s
sys	  0m16.516s

size=4KBytes, parallel=false

real	0m1.697s
user	0m0.171s
sys	  0m0.903s

size=4KBytes, parallel=true

real	0m2.695s
user	0m0.338s
sys	  0m10.404s
````

## File: tools/upgrade310/upgrade.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package upgrade310

import (
	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/logdb"
	"github.com/lni/dragonboat/v4/internal/rsm"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/utils"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

var firstError = utils.FirstError

// CanUpgradeToV310 determines whether your production dataset is safe to use
// the v3.0.3 or higher version of Dragonboat. You need to stop your NodeHost
// before invoking CanUpgradeToV310.
//
// CanUpgradeToV310 checks whether there is any snapshot that has already been
// streamed or imported but has not been fully applied into user state machine
// yet.
//
// The input parameter nhConfig should be the same NodeHostConfig instance you
// use to initiate your NodeHost object. CanUpgradeToV310 returns a boolean flag
// indicating whether it is safe to upgrade. If it returns false, you can
// restart your NodeHost using the existing version of Dragonboat, e.g. v3.0.2,
// to allow pending snapshots to be fully applied. Repeat the above steps until
// CanUpgradeToV310 returns true.
//
// Note that for the vast majority cases, CanUpgradeToV310 is expected to
// return true after its first run, which means it is safe to go ahead and
// upgrade the Dragonboat version.
func CanUpgradeToV310(nhConfig config.NodeHostConfig) (result bool, err error) {
	if nhConfig.DeploymentID == 0 {
		nhConfig.DeploymentID = 1
	}
	if err := nhConfig.Prepare(); err != nil {
		return false, err
	}
	fs := nhConfig.Expert.FS
	env, err := server.NewEnv(nhConfig, fs)
	if err != nil {
		return false, err
	}
	defer func() {
		err = firstError(err, env.Close())
	}()
	if err := env.LockNodeHostDir(); err != nil {
		return false, err
	}
	nhDir, walDir := env.GetLogDBDirs(nhConfig.DeploymentID)
	var ldb raftio.ILogDB
	if nhConfig.Expert.LogDBFactory == nil {
		ldb, err = logdb.NewDefaultLogDB(nhConfig,
			nil, []string{nhDir}, []string{walDir})
	} else {
		ldb, err = nhConfig.Expert.LogDBFactory.Create(nhConfig,
			nil, []string{nhDir}, []string{walDir})
	}
	if err != nil {
		return false, err
	}
	defer func() {
		err = firstError(err, ldb.Close())
	}()
	niList, err := ldb.ListNodeInfo()
	if err != nil {
		return false, err
	}
	for _, ni := range niList {
		ss, err := ldb.GetSnapshot(ni.ShardID, ni.ReplicaID)
		if err != nil {
			return false, err
		}
		if ss.Type == pb.OnDiskStateMachine && ss.OnDiskIndex == 0 {
			shrunk, err := rsm.IsShrunkSnapshotFile(ss.Filepath, fs)
			if err != nil {
				return false, err
			}
			if !shrunk {
				return false, nil
			}
		}
	}
	return true, nil
}
````

## File: tools/import_test.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tools

import (
	"bytes"
	"crypto/rand"
	"fmt"
	"io"
	"testing"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/vfs"
	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/stretchr/testify/require"
)

var (
	testDataDir    = "import_test_safe_to_delete"
	testDstDataDir = "import_test_dst_safe_to_delete"
)

func TestCheckImportSettings(t *testing.T) {
	members := make(map[uint64]string)
	err := checkImportSettings(config.NodeHostConfig{}, members, 1)
	require.Equal(t, ErrInvalidMembers, err, "invalid members not reported")

	members[1] = "a1"
	err = checkImportSettings(config.NodeHostConfig{RaftAddress: "a2"},
		members, 1)
	require.Equal(t, ErrInvalidMembers, err, "invalid member address not reported")

	err = checkImportSettings(config.NodeHostConfig{RaftAddress: "a1"},
		members, 1)
	require.NoError(t, err)
}

func TestGetSnapshotFilenames(t *testing.T) {
	fs := vfs.GetTestFS()
	require.NoError(t, fs.RemoveAll(testDataDir))
	require.NoError(t, fs.MkdirAll(testDataDir, 0755))

	defer func() {
		require.NoError(t, fs.RemoveAll(testDataDir))
	}()

	for i := 0; i < 16; i++ {
		fn := fmt.Sprintf("%d.%s", i, server.SnapshotFileSuffix)
		dst := fs.PathJoin(testDataDir, fn)
		f, err := fs.Create(dst)
		require.NoError(t, err, "failed to create file")
		require.NoError(t, f.Close(), "failed to close file")
	}

	fns, err := getSnapshotFilenames(testDataDir, fs)
	require.NoError(t, err, "failed to get filenames")
	require.Equal(t, 16, len(fns), "failed to return all filenames")

	fps, err := getSnapshotFiles(testDataDir, fs)
	require.NoError(t, err, "failed to get filenames")
	require.Equal(t, 16, len(fps), "failed to return all filenames")

	_, err = getSnapshotFilepath(testDataDir, fs)
	require.Equal(t, ErrIncompleteSnapshot, err,
		"failed to report ErrIncompleteSnapshot")
}

func TestSnapshotFilepath(t *testing.T) {
	fs := vfs.GetTestFS()
	require.NoError(t, fs.RemoveAll(testDataDir))
	require.NoError(t, fs.MkdirAll(testDataDir, 0755))

	defer func() {
		require.NoError(t, fs.RemoveAll(testDataDir))
	}()

	fn := fmt.Sprintf("testdata.%s", server.SnapshotFileSuffix)
	dst := fs.PathJoin(testDataDir, fn)
	f, err := fs.Create(dst)
	require.NoError(t, err, "failed to create file")
	require.NoError(t, f.Close(), "failed to close file")

	fp, err := getSnapshotFilepath(testDataDir, fs)
	require.NoError(t, err, "failed to get snapshot file path")
	require.Equal(t, fs.PathJoin(testDataDir, fn), fp, "unexpected fp")
}

func TestCheckMembers(t *testing.T) {
	membership := pb.Membership{
		Addresses:  map[uint64]string{1: "a1", 2: "a2", 3: "a3"},
		NonVotings: map[uint64]string{4: "a4"},
		Removed:    map[uint64]bool{5: true},
	}
	tests := []struct {
		members map[uint64]string
		ok      bool
	}{
		{map[uint64]string{1: "a2"}, false},
		{map[uint64]string{4: "a4"}, false},
		{map[uint64]string{4: "a5"}, false},
		{map[uint64]string{5: "a5"}, false},
		{map[uint64]string{6: "a6"}, true},
	}
	for idx, tt := range tests {
		err := checkMembers(membership, tt.members)
		if tt.ok {
			require.NoError(t, err, "test case %d failed", idx)
		} else {
			require.Error(t, err, "test case %d should have failed", idx)
		}
	}
}

func createTestDataFile(path string, sz uint64, fs vfs.IFS) error {
	f, err := fs.Create(path)
	if err != nil {
		return err
	}
	data := make([]byte, sz)
	if _, err := rand.Read(data); err != nil {
		panic(err)
	}
	_, err = f.Write(data)
	if err != nil {
		return err
	}
	return f.Close()
}

func TestCopySnapshot(t *testing.T) {
	fs := vfs.GetTestFS()
	require.NoError(t, fs.RemoveAll(testDataDir))
	require.NoError(t, fs.RemoveAll(testDstDataDir))
	require.NoError(t, fs.MkdirAll(testDataDir, 0755))
	require.NoError(t, fs.MkdirAll(testDstDataDir, 0755))

	defer func() {
		require.NoError(t, fs.RemoveAll(testDataDir))
	}()
	defer func() {
		require.NoError(t, fs.RemoveAll(testDstDataDir))
	}()

	src := fs.PathJoin(testDataDir, "test.gbsnap")
	require.NoError(t, createTestDataFile(src, 1024, fs),
		"failed to create test file")

	extsrc := fs.PathJoin(testDataDir, "external-1")
	require.NoError(t, createTestDataFile(extsrc, 2048, fs),
		"failed to create external test file")

	ss := pb.Snapshot{
		Filepath: src,
		Files:    []*pb.SnapshotFile{{Filepath: extsrc}},
	}
	require.NoError(t, copySnapshot(ss, testDataDir, testDstDataDir, fs),
		"failed to copy snapshot files")

	exp := fs.PathJoin(testDstDataDir, "test.gbsnap")
	fi, err := fs.Stat(exp)
	require.NoError(t, err, "failed to get file stat")
	require.Equal(t, int64(1024), fi.Size(), "failed to copy the file")

	exp = fs.PathJoin(testDstDataDir, "external-1")
	fi, err = fs.Stat(exp)
	require.NoError(t, err, "failed to get file stat")
	require.Equal(t, int64(2048), fi.Size(), "failed to copy the file")
}

func TestCopySnapshotFile(t *testing.T) {
	fs := vfs.GetTestFS()
	require.NoError(t, fs.RemoveAll(testDataDir))
	require.NoError(t, fs.MkdirAll(testDataDir, 0755))

	defer func() {
		require.NoError(t, fs.RemoveAll(testDataDir))
	}()

	src := fs.PathJoin(testDataDir, "test.data")
	dst := fs.PathJoin(testDataDir, "test.data.copied")
	f, err := fs.Create(src)
	require.NoError(t, err, "failed to create test file")

	data := make([]byte, 125)
	_, err = rand.Read(data)
	require.NoError(t, err)
	_, err = f.Write(data)
	require.NoError(t, err, "failed to write test data")
	require.NoError(t, f.Close(), "failed to close file")

	require.NoError(t, copyFile(src, dst, fs), "failed to copy file")

	buf := &bytes.Buffer{}
	dstf, err := fs.Open(dst)
	require.NoError(t, err, "failed to open")
	defer func() {
		require.NoError(t, dstf.Close())
	}()

	_, err = io.Copy(buf, dstf)
	require.NoError(t, err)
	require.True(t, bytes.Equal(buf.Bytes(), data), "content changed")
}

func TestMissingMetadataFileIsReported(t *testing.T) {
	fs := vfs.GetTestFS()
	require.NoError(t, fs.RemoveAll(testDataDir))
	require.NoError(t, fs.MkdirAll(testDataDir, 0755))

	defer func() {
		require.NoError(t, fs.RemoveAll(testDataDir))
	}()

	_, err := getSnapshotRecord(testDataDir, "test.data", fs)
	require.Error(t, err, "failed to report error")
}

func TestGetProcessedSnapshotRecord(t *testing.T) {
	fs := vfs.GetTestFS()
	ss := pb.Snapshot{
		Filepath: "/original_dir/test.gbsnap",
		FileSize: 123,
		Index:    1023,
		Term:     10,
		Checksum: make([]byte, 8),
		Dummy:    false,
		Membership: pb.Membership{
			Removed:    make(map[uint64]bool),
			NonVotings: make(map[uint64]string),
			Addresses:  make(map[uint64]string),
		},
		Type:    pb.OnDiskStateMachine,
		ShardID: 345,
		Files:   make([]*pb.SnapshotFile, 0),
	}
	ss.Membership.Addresses[1] = "a1"
	ss.Membership.Addresses[2] = "a2"
	ss.Membership.Removed[3] = true
	ss.Membership.NonVotings[4] = "a4"

	f1 := &pb.SnapshotFile{
		Filepath: "/original_dir/external-1",
		FileSize: 1,
		FileId:   1,
		Metadata: make([]byte, 8),
	}
	f2 := &pb.SnapshotFile{
		Filepath: "/original_dir/external-2",
		FileSize: 2,
		FileId:   2,
		Metadata: make([]byte, 8),
	}
	ss.Files = append(ss.Files, f1)
	ss.Files = append(ss.Files, f2)

	members := make(map[uint64]string)
	members[1] = "a1"
	members[5] = "a5"
	finalDir := "final_data"

	newss := getProcessedSnapshotRecord(finalDir, ss, members, fs)
	require.Equal(t, ss.Index, newss.Index, "index/term not copied")
	require.Equal(t, ss.Term, newss.Term, "index/term not copied")
	require.Equal(t, ss.Dummy, newss.Dummy, "dummy/ShardId/Type fields not copied")
	require.Equal(t, ss.ShardID, newss.ShardID, "dummy/ShardId/Type fields not copied")
	require.Equal(t, ss.Type, newss.Type, "dummy/ShardId/Type fields not copied")
	require.Equal(t, finalDir, fs.PathDir(newss.Filepath),
		"filepath not processed")

	for _, file := range newss.Files {
		require.Equal(t, finalDir, fs.PathDir(file.Filepath),
			"filepath in files not processed")
	}

	v, ok := newss.Membership.Addresses[1]
	require.True(t, ok, "node 1 not in new ss")
	require.Equal(t, "a1", v, "node 1 not in new ss")

	_, ok = newss.Membership.Addresses[2]
	require.False(t, ok, "node 2 not removed from new ss")

	v, ok = newss.Membership.Addresses[5]
	require.True(t, ok, "node 5 not in new ss")
	require.Equal(t, "a5", v, "node 5 not in new ss")

	require.Equal(t, 2, len(newss.Membership.Addresses),
		"unexpected member count")
	require.Equal(t, 0, len(newss.Membership.NonVotings),
		"NonVotings not empty")
	require.Equal(t, 3, len(newss.Membership.Removed),
		"unexpected removed count")

	_, ok1 := newss.Membership.Removed[2]
	_, ok2 := newss.Membership.Removed[3]
	_, ok3 := newss.Membership.Removed[4]
	require.True(t, ok1, "unexpected removed content")
	require.True(t, ok2, "unexpected removed content")
	require.True(t, ok3, "unexpected removed content")
}
````

## File: tools/import.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package tools

/*
Package tools provides functions and types typically used to construct DevOps
tools for managing Dragonboat based applications.
*/

import (
	"bytes"
	"io"
	"os"
	"runtime"
	"strings"

	"github.com/cockroachdb/errors"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/logdb"
	"github.com/lni/dragonboat/v4/internal/rsm"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/utils"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/logger"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

var (
	plog = logger.GetLogger("tools")
)

var (
	unmanagedDeploymentID = settings.UnmanagedDeploymentID
	// ErrInvalidMembers indicates that the provided member nodes is invalid.
	ErrInvalidMembers = errors.New("invalid members")
	// ErrPathNotExist indicates that the specified exported snapshot directory
	// do not exist.
	ErrPathNotExist = errors.New("path does not exist")
	// ErrIncompleteSnapshot indicates that the specified exported snapshot
	// directory does not contain a complete snapshot.
	ErrIncompleteSnapshot = errors.New("snapshot is incomplete")
)

var firstError = utils.FirstError

// ImportSnapshot is used to repair the Raft shard already has its quorum
// nodes permanently lost or damaged. Such repair is only required when the
// Raft shard permanently lose its quorum. You are not suppose to use this
// function when the shard still have its majority nodes running or when
// the node failures are not permanent. In our experience, a well monitored
// and managed Dragonboat system can usually avoid using the ImportSnapshot
// tool by always replace permanently dead nodes with available ones in time.
//
// ImportSnapshot imports the exported snapshot available in the specified
// srcDir directory to the system and rewrites the history of node replicaID so
// the node owns the imported snapshot and the membership of the Raft shard
// is rewritten to the details specified in memberNodes.
//
// ImportSnapshot is typically invoked by a DevOps tool separated from the
// Dragonboat based application. The NodeHost instance must be stopped on that
// host when invoking the function ImportSnapshot.
//
// As an example, consider a Raft shard with three nodes with the ReplicaID
// values being 1, 2 and 3, they run on three distributed hostss each with a
// running NodeHost instance and the RaftAddress values are m1, m2 and
// m3. The ShardID value of the Raft shard is 100. Let's say hosts
// identified by m2 and m3 suddenly become permanently gone and thus cause the
// Raft shard to lose its quorum nodes. To repair the shard, we can use the
// ImportSnapshot function to overwrite the state and membership of the Raft
// shard.
//
// Assuming we have two other running hosts identified as m4 and m5, we want to
// have two new nodes with ReplicaID 4 and 5 to replace the permanently lost ndoes
// 2 and 3. In this case, the memberNodes map should contain the following
// content:
//
//	memberNodes: map[uint64]string{
//	  {1: "m1"}, {4: "m4"}, {5: "m5"},
//	}
//
// we first shutdown NodeHost instances on all involved hosts and call the
// ImportSnapshot function from the DevOps tool. Assuming the directory
// /backup/shard100 contains the exported snapshot we previously saved by using
// NodeHost's ExportSnapshot method, then -
//
// on m1, we call -
// ImportSnapshot(nhConfig1, "/backup/shard100", memberNodes, 1)
//
// on m4 -
// ImportSnapshot(nhConfig4, "/backup/shard100", memberNodes, 4)
//
// on m5 -
// ImportSnapshot(nhConfig5, "/backup/shard100", memberNodes, 5)
//
// The nhConfig* value used above should be the same as the one used to start
// your NodeHost instances, they are suppose to be slightly different on m1, m4
// and m5 to reflect the differences between these hosts, e.g. the RaftAddress
// values. srcDir values are all set to "/backup/shard100", that directory
// should contain the exact same snapshot. The memberNodes value should be the
// same across all three hosts.
//
// Once ImportSnapshot is called on all three of those hosts, we end up having
// the history of the Raft shard overwritten to the state in which -
//   - there are 3 nodes in the Raft shard, the ReplicaID values are 1, 4 and 5.
//     they run on hosts m1, m4 and m5.
//   - nodes 2 and 3 are permanently removed from the shard. you should never
//     restart any of them as both hosts m2 and m3 are suppose to be permanently
//     unavailable.
//   - the state captured in the snapshot became the state of the shard. all
//     proposals more recent than the state of the snapshot are lost.
//
// Once the NodeHost instances are restarted on m1, m4 and m5, nodes 1, 4 and 5
// of the Raft shard 100 can be restarted in the same way as after rebooting
// the hosts m1, m4 and m5.
//
// It is your applications's responsibility to let m4 and m5 to be aware that
// node 4 and 5 are now running there.
func ImportSnapshot(nhConfig config.NodeHostConfig,
	srcDir string, memberNodes map[uint64]string, replicaID uint64) (err error) {
	if nhConfig.DeploymentID == 0 {
		plog.Infof("NodeHostConfig.DeploymentID not set, default to %d",
			unmanagedDeploymentID)
		nhConfig.DeploymentID = unmanagedDeploymentID
	}
	if nhConfig.Expert.FS == nil {
		nhConfig.Expert.FS = vfs.DefaultFS
	}
	if err := nhConfig.Prepare(); err != nil {
		return err
	}
	fs := nhConfig.Expert.FS
	if err := checkImportSettings(nhConfig, memberNodes, replicaID); err != nil {
		return err
	}
	ssfp, err := getSnapshotFilepath(srcDir, fs)
	if err != nil {
		return err
	}
	oldss, err := getSnapshotRecord(srcDir, server.MetadataFilename, fs)
	if err != nil {
		return err
	}
	ok, err := isCompleteSnapshotImage(ssfp, oldss, fs)
	if err != nil {
		return err
	}
	if !ok {
		return ErrIncompleteSnapshot
	}
	if err := checkMembers(oldss.Membership, memberNodes); err != nil {
		return err
	}
	env, err := server.NewEnv(nhConfig, fs)
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, env.Close())
	}()
	if _, _, err := env.CreateNodeHostDir(nhConfig.DeploymentID); err != nil {
		return err
	}
	logdb, err := getLogDB(*env, nhConfig)
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, logdb.Close())
	}()
	if err := env.CheckNodeHostDir(nhConfig,
		logdb.BinaryFormat(), logdb.Name()); err != nil {
		return err
	}
	ssDir := env.GetSnapshotDir(nhConfig.DeploymentID,
		oldss.ShardID, replicaID)
	exist, err := fileutil.Exist(ssDir, fs)
	if err != nil {
		return err
	}
	if exist {
		if err := cleanupSnapshotDir(ssDir, fs); err != nil {
			return err
		}
	} else {
		if err := env.CreateSnapshotDir(nhConfig.DeploymentID,
			oldss.ShardID, replicaID); err != nil {
			return err
		}
	}
	getSnapshotDir := func(cid uint64, nid uint64) string {
		return env.GetSnapshotDir(nhConfig.DeploymentID, cid, nid)
	}
	ssEnv := server.NewSSEnv(getSnapshotDir,
		oldss.ShardID, replicaID, oldss.Index, replicaID, server.SnapshotMode, fs)
	if err := ssEnv.CreateTempDir(); err != nil {
		return err
	}
	dstDir := ssEnv.GetTempDir()
	finalDir := ssEnv.GetFinalDir()
	ss := getProcessedSnapshotRecord(finalDir, oldss, memberNodes, fs)
	if err := copySnapshot(oldss, srcDir, dstDir, fs); err != nil {
		return err
	}
	if err := ssEnv.FinalizeSnapshot(&ss); err != nil {
		return err
	}
	return logdb.ImportSnapshot(ss, replicaID)
}

func cleanupSnapshotDir(dir string, fs vfs.IFS) error {
	files, err := fs.List(dir)
	if err != nil {
		return err
	}
	for _, v := range files {
		fi, err := fs.Stat(fs.PathJoin(dir, v))
		if err != nil {
			return err
		}
		if !fi.IsDir() {
			continue
		}
		name := []byte(fi.Name())
		if server.SnapshotDirNameRe.Match(name) ||
			server.GenSnapshotDirNameRe.Match(name) ||
			server.RecvSnapshotDirNameRe.Match(name) {
			ssdir := fs.PathJoin(dir, fi.Name())
			if err := fs.RemoveAll(ssdir); err != nil {
				return err
			}
		}
	}
	return fileutil.SyncDir(dir, fs)
}

func checkImportSettings(nhConfig config.NodeHostConfig,
	memberNodes map[uint64]string, replicaID uint64) error {
	addr, ok := memberNodes[replicaID]
	if !ok {
		plog.Errorf("node ID not found in the memberNode map")
		return ErrInvalidMembers
	}
	if nhConfig.RaftAddress != addr {
		plog.Errorf("node address in NodeHostConfig %s, in members %s",
			nhConfig.RaftAddress, addr)
		return ErrInvalidMembers
	}
	return nil
}

func isCompleteSnapshotImage(ssfp string,
	ss pb.Snapshot, fs vfs.IFS) (bool, error) {
	checksum, err := rsm.GetV2PayloadChecksum(ssfp, fs)
	if err != nil {
		return false, err
	}
	return bytes.Equal(checksum, ss.Checksum), nil
}

func getSnapshotFilepath(dir string, fs vfs.IFS) (string, error) {
	exist, err := fileutil.Exist(dir, fs)
	if err != nil {
		return "", err
	}
	if !exist {
		return "", ErrPathNotExist
	}
	files, err := getSnapshotFiles(dir, fs)
	if err != nil {
		return "", err
	}
	if len(files) != 1 {
		return "", ErrIncompleteSnapshot
	}
	return files[0], nil
}

func getSnapshotFiles(path string, fs vfs.IFS) ([]string, error) {
	names, err := getSnapshotFilenames(path, fs)
	if err != nil {
		return nil, err
	}
	results := make([]string, 0)
	for _, name := range names {
		results = append(results, fs.PathJoin(path, name))
	}
	return results, nil
}

func getSnapshotFilenames(path string, fs vfs.IFS) ([]string, error) {
	files, err := fs.List(path)
	if err != nil {
		return nil, err
	}
	results := make([]string, 0)
	for _, v := range files {
		file, err := fs.Stat(fs.PathJoin(path, v))
		if err != nil {
			return nil, err
		}
		if file.IsDir() {
			continue
		}
		if strings.HasSuffix(file.Name(), server.SnapshotFileSuffix) {
			results = append(results, file.Name())
		}
	}
	return results, nil
}

func getSnapshotRecord(dir string,
	filename string, fs vfs.IFS) (pb.Snapshot, error) {
	var ss pb.Snapshot
	if err := fileutil.GetFlagFileContent(dir, filename, &ss, fs); err != nil {
		return pb.Snapshot{}, err
	}
	return ss, nil
}

func checkMembers(old pb.Membership, members map[uint64]string) error {
	for replicaID, addr := range members {
		v, ok := old.Addresses[replicaID]
		if ok && v != addr {
			return errors.New("node address changed")
		}
		v, ok = old.NonVotings[replicaID]
		if ok && v != addr {
			return errors.New("node address changed")
		}
		if ok {
			return errors.New("adding an nonVoting as regular node")
		}
		v, ok = old.Witnesses[replicaID]
		if ok && v != addr {
			return errors.New("node address changed")
		}
		if ok {
			return errors.New("adding a witness as regular node")
		}
		_, ok = old.Removed[replicaID]
		if ok {
			return errors.New("adding a removed node")
		}
	}
	return nil
}

func getProcessedSnapshotRecord(dstDir string,
	old pb.Snapshot, members map[uint64]string, fs vfs.IFS) pb.Snapshot {
	for _, file := range old.Files {
		file.Filepath = fs.PathJoin(dstDir, fs.PathBase(file.Filepath))
	}
	ss := pb.Snapshot{
		Filepath: fs.PathJoin(dstDir, fs.PathBase(old.Filepath)),
		FileSize: old.FileSize,
		Index:    old.Index,
		Term:     old.Term,
		Checksum: old.Checksum,
		Dummy:    old.Dummy,
		Membership: pb.Membership{
			ConfigChangeId: old.Index,
			Removed:        make(map[uint64]bool),
			NonVotings:     make(map[uint64]string),
			Addresses:      make(map[uint64]string),
			Witnesses:      make(map[uint64]string),
		},
		Files:    old.Files,
		Type:     old.Type,
		ShardID:  old.ShardID,
		Imported: true,
	}
	for nid := range old.Membership.Addresses {
		_, ok := members[nid]
		if !ok {
			ss.Membership.Removed[nid] = true
		}
	}
	for nid := range old.Membership.NonVotings {
		_, ok := members[nid]
		if !ok {
			ss.Membership.Removed[nid] = true
		}
	}
	for nid := range old.Membership.Witnesses {
		_, ok := members[nid]
		if !ok {
			ss.Membership.Removed[nid] = true
		}
	}
	for nid := range old.Membership.Removed {
		ss.Membership.Removed[nid] = true
	}
	for nid, addr := range members {
		ss.Membership.Addresses[nid] = addr
	}
	return ss
}

func copySnapshot(ss pb.Snapshot,
	srcDir string, dstDir string, fs vfs.IFS) error {
	fp, err := getSnapshotFilepath(srcDir, fs)
	if err != nil {
		return err
	}
	dstfp := fs.PathJoin(dstDir, fs.PathBase(fp))
	if err := copyFile(fp, dstfp, fs); err != nil {
		return err
	}
	for _, file := range ss.Files {
		fname := fs.PathBase(file.Filepath)
		if err := copyFile(fs.PathJoin(srcDir, fname),
			fs.PathJoin(dstDir, fname), fs); err != nil {
			return err
		}
	}
	return nil
}

func copyFile(src string, dst string, fs vfs.IFS) (err error) {
	in, err := fs.Open(src)
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, in.Close())
	}()
	fi, err := in.Stat()
	if err != nil {
		return err
	}
	out, err := fs.Create(dst)
	if err != nil {
		return err
	}
	defer func() {
		err = firstError(err, out.Close())
	}()
	if runtime.GOOS != "windows" {
		of, ok := out.(*os.File)
		if ok {
			if err := of.Chmod(fi.Mode()); err != nil {
				return err
			}
		}
	}
	if _, err = io.Copy(out, in); err != nil {
		return err
	}
	if err := out.Sync(); err != nil {
		return err
	}
	return fileutil.SyncDir(fs.PathDir(dst), fs)
}

func getLogDB(env server.Env,
	nhConfig config.NodeHostConfig) (raftio.ILogDB, error) {
	nhDir, walDir := env.GetLogDBDirs(nhConfig.DeploymentID)
	if nhConfig.Expert.LogDBFactory != nil {
		return nhConfig.Expert.LogDBFactory.Create(nhConfig,
			nil, []string{nhDir}, []string{walDir})
	}
	return logdb.NewDefaultLogDB(nhConfig, nil, []string{nhDir}, []string{walDir})
}
````

## File: .codecov.yml
````yaml
coverage:
  precision: 2
  round: up
  range: "70...100"

  status:
    project: no
    patch: no
    changes: no
````

## File: .gitignore
````
/build
**/checkdisk
/multiraft-monkey-testing
*safe_to_delete*
*safe-to-delete*
out.txt
coverage.out
coverage.txt
*.pprof
*.swp
*.tmp
*.o
*.a
*-fuzz.zip
*.bin
*.win
gitversion.go
drummer-data
nodehost-data
````

## File: AUTHORS
````
Dragonboat was originally created by Lei Ni (nilei81@gmail.com) in 2017. 

Below is a list of authors who contributed on major features and improvements.

JasonYuchen (jasonyuchen@foxmail.com)
Boyang Chen (bchen11@outlook.com)

Below is an inevitably incomplete list of other contributors who also helped to
make Dragonboat better (alphabetically sorted by names):

Boyang Chen (bchen11@outlook.com)
dcosmos 
gensmusic (gensmusic@163.com)
JasonYuchen (jasonyuchen@foxmail.com)
shijiayun
stffab (stffabi@pm.me)
````

## File: benchmark_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"crypto/rand"
	mathrand "math/rand"
	"os"
	"sync"
	"sync/atomic"
	"testing"

	"github.com/golang/snappy"
	"github.com/lni/goutils/random"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/client"
	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/logdb"
	"github.com/lni/dragonboat/v4/internal/registry"
	"github.com/lni/dragonboat/v4/internal/rsm"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/tests"
	"github.com/lni/dragonboat/v4/internal/transport"
	"github.com/lni/dragonboat/v4/internal/utils/dio"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/logger"
	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
)

func benchmarkAllocs(b *testing.B, sz uint64) {
	b.ReportAllocs()
	b.RunParallel(func(pb *testing.PB) {
		for pb.Next() {
			m := make([]byte, sz)
			b.SetBytes(int64(sz))
			if uint64(len(m)) < sz {
				b.Errorf("len(m) < %d", sz)
			}
		}
	})
}

func BenchmarkAllocs16Bytes(b *testing.B) {
	benchmarkAllocs(b, 16)
}

func BenchmarkAllocs512Bytes(b *testing.B) {
	benchmarkAllocs(b, 512)
}

func BenchmarkAllocs4096Bytes(b *testing.B) {
	benchmarkAllocs(b, 4096)
}

func benchmarkEncodedPayload(b *testing.B, ct dio.CompressionType, sz uint64) {
	b.ReportAllocs()
	b.SetBytes(int64(sz))
	input := make([]byte, sz)
	_, err := rand.Read(input)
	require.NoError(b, err)
	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		rsm.GetEncoded(ct, input, nil)
	}
}

func BenchmarkSnappyEncodedPayload16Bytes(b *testing.B) {
	benchmarkEncodedPayload(b, dio.Snappy, 16)
}

func BenchmarkSnappyEncodedPayload512Bytes(b *testing.B) {
	benchmarkEncodedPayload(b, dio.Snappy, 512)
}

func BenchmarkSnappyEncodedPayload4096Bytes(b *testing.B) {
	benchmarkEncodedPayload(b, dio.Snappy, 4096)
}

func BenchmarkNoCompressionEncodedPayload16Bytes(b *testing.B) {
	benchmarkEncodedPayload(b, dio.NoCompression, 16)
}

func BenchmarkNoCompressionEncodedPayload512Bytes(b *testing.B) {
	benchmarkEncodedPayload(b, dio.NoCompression, 512)
}

func BenchmarkNoCompressionEncodedPayload4096Bytes(b *testing.B) {
	benchmarkEncodedPayload(b, dio.NoCompression, 4096)
}

func BenchmarkAddToEntryQueue(b *testing.B) {
	b.ReportAllocs()
	q := newEntryQueue(1000000, 0)
	total := uint32(0)
	entry := pb.Entry{}
	b.RunParallel(func(pb *testing.PB) {
		for pb.Next() {
			t := atomic.AddUint32(&total, 1)
			if t%2048 == 0 {
				atomic.StoreUint32(&total, 0)
				q.get(false)
			} else {
				q.add(entry)
			}
		}
	})
}

func benchmarkProposeN(b *testing.B, sz int) {
	b.ReportAllocs()
	data := make([]byte, sz)
	p := &sync.Pool{}
	p.New = func() interface{} {
		obj := &RequestState{}
		obj.CompletedC = make(chan RequestResult, 1)
		obj.pool = p
		return obj
	}
	total := uint32(0)
	q := newEntryQueue(2048, 0)
	cfg := config.Config{ShardID: 1, ReplicaID: 1}
	pp := newPendingProposal(cfg, false, p, q)
	session := client.NewNoOPSession(1, random.LockGuardedRand)
	b.RunParallel(func(pb *testing.PB) {
		for pb.Next() {
			v := atomic.AddUint32(&total, 1)
			b.SetBytes(int64(sz))
			rs, err := pp.propose(session, data, 100)
			if err != nil {
				b.Errorf("%v", err)
			}
			if v%128 == 0 {
				atomic.StoreUint32(&total, 0)
				q.get(false)
			}
			pp.applied(rs.key, rs.clientID, rs.seriesID, sm.Result{Value: 1}, false)
			rs.readyToRelease.set()
			rs.Release()
		}
	})
}

func BenchmarkPropose16(b *testing.B) {
	benchmarkProposeN(b, 16)
}

func BenchmarkPropose128(b *testing.B) {
	benchmarkProposeN(b, 128)
}

func BenchmarkPropose1024(b *testing.B) {
	benchmarkProposeN(b, 1024)
}

func BenchmarkPendingProposalNextKey(b *testing.B) {
	b.ReportAllocs()
	p := &sync.Pool{}
	p.New = func() interface{} {
		obj := &RequestState{}
		obj.CompletedC = make(chan RequestResult, 1)
		obj.pool = p
		return obj
	}
	q := newEntryQueue(2048, 0)
	cfg := config.Config{ShardID: 1, ReplicaID: 1}
	pp := newPendingProposal(cfg, false, p, q)
	b.RunParallel(func(pb *testing.PB) {
		clientID := mathrand.Uint64()
		for pb.Next() {
			pp.nextKey(clientID)
		}
	})
}

func BenchmarkReadIndexRead(b *testing.B) {
	b.ReportAllocs()
	p := &sync.Pool{}
	p.New = func() interface{} {
		obj := &RequestState{}
		obj.CompletedC = make(chan RequestResult, 1)
		obj.pool = p
		return obj
	}
	total := uint32(0)
	q := newReadIndexQueue(2048)
	pri := newPendingReadIndex(p, q)
	b.RunParallel(func(pb *testing.PB) {
		for pb.Next() {
			v := atomic.AddUint32(&total, 1)
			rs, err := pri.read(100)
			if err != nil {
				b.Errorf("%v", err)
			}
			if v%128 == 0 {
				atomic.StoreUint32(&total, 0)
				q.get()
			}
			rs.readyToRelease.set()
			rs.Release()
		}
	})
}

func benchmarkMarshalEntryN(b *testing.B, sz int) {
	b.ReportAllocs()
	e := pb.Entry{
		Index:       12843560,
		Term:        123,
		Type:        pb.ApplicationEntry,
		Key:         13563799145,
		ClientID:    234926831800,
		SeriesID:    12843560,
		RespondedTo: 12843550,
		Cmd:         make([]byte, sz),
	}
	data := make([]byte, e.Size())
	for i := 0; i < b.N; i++ {
		n, err := e.MarshalTo(data)
		if n > len(data) {
			b.Errorf("n > len(data)")
		}
		b.SetBytes(int64(n))
		if err != nil {
			b.Errorf("%v", err)
		}
	}
}

func BenchmarkMarshalEntry16(b *testing.B) {
	benchmarkMarshalEntryN(b, 16)
}

func BenchmarkMarshalEntry128(b *testing.B) {
	benchmarkMarshalEntryN(b, 128)
}

func BenchmarkMarshalEntry1024(b *testing.B) {
	benchmarkMarshalEntryN(b, 1024)
}

func BenchmarkWorkerReady(b *testing.B) {
	b.ReportAllocs()
	rc := newWorkReady(1)
	b.RunParallel(func(pbt *testing.PB) {
		for pbt.Next() {
			rc.shardReady(1)
		}
	})
}

func BenchmarkReadyShard(b *testing.B) {
	b.ReportAllocs()
	rc := newReadyShard()
	b.RunParallel(func(pbt *testing.PB) {
		for pbt.Next() {
			rc.setShardReady(1)
		}
	})
}

func BenchmarkFSyncLatency(b *testing.B) {
	b.StopTimer()
	l := logger.GetLogger("logdb")
	l.SetLevel(logger.WARNING)
	db := getNewTestDB("db", "lldb", vfs.DefaultFS)
	defer func() {
		require.NoError(b, os.RemoveAll(rdbTestDirectory))
	}()
	defer func() {
		require.NoError(b, db.Close())
	}()
	e := pb.Entry{
		Index:       12843560,
		Term:        123,
		Type:        pb.ApplicationEntry,
		Key:         13563799145,
		ClientID:    234926831800,
		SeriesID:    12843560,
		RespondedTo: 12843550,
		Cmd:         make([]byte, 8*1024),
	}
	u := pb.Update{
		ShardID:       1,
		ReplicaID:     1,
		EntriesToSave: []pb.Entry{e},
	}
	b.StartTimer()
	for n := 0; n < b.N; n++ {
		if err := db.SaveRaftState([]pb.Update{u}, 1); err != nil {
			b.Fatalf("%v", err)
		}
	}
}

func benchmarkSaveRaftState(b *testing.B, sz int) {
	b.ReportAllocs()
	b.StopTimer()
	l := logger.GetLogger("logdb")
	l.SetLevel(logger.WARNING)
	db := getNewTestDB("db", "lldb", vfs.DefaultFS)
	defer func() {
		require.NoError(b, os.RemoveAll(rdbTestDirectory))
	}()
	defer func() {
		require.NoError(b, db.Close())
	}()
	shardID := uint32(1)
	b.StartTimer()
	b.RunParallel(func(pbt *testing.PB) {
		ldb, ok := db.(*logdb.ShardedDB)
		if !ok {
			b.Fatalf("not a logdb.ShardedDB instance")
		}
		rdbctx := ldb.GetLogDBThreadContext()
		e := pb.Entry{
			Index:       12843560,
			Term:        123,
			Type:        pb.ApplicationEntry,
			Key:         13563799145,
			ClientID:    234926831800,
			SeriesID:    12843560,
			RespondedTo: 12843550,
			Cmd:         make([]byte, sz),
		}
		cid := uint64(atomic.AddUint32(&shardID, 1))
		bytes := e.Size() * 128
		u := pb.Update{
			ShardID:   cid,
			ReplicaID: 1,
		}
		iidx := e.Index
		for i := uint64(0); i < 128; i++ {
			e.Index = iidx + i
			u.EntriesToSave = append(u.EntriesToSave, e)
		}
		for pbt.Next() {
			rdbctx.Reset()
			if err := ldb.SaveRaftStateCtx([]pb.Update{u}, rdbctx); err != nil {
				b.Errorf("%v", err)
			}
			b.SetBytes(int64(bytes))
		}
	})
}

func BenchmarkSaveRaftState16(b *testing.B) {
	benchmarkSaveRaftState(b, 16)
}

func BenchmarkSaveRaftState128(b *testing.B) {
	benchmarkSaveRaftState(b, 128)
}

func BenchmarkSaveRaftState1024(b *testing.B) {
	benchmarkSaveRaftState(b, 1024)
}

type benchmarkMessageHandler struct {
	expected uint64
	count    uint64
	ch       chan struct{}
}

func (h *benchmarkMessageHandler) wait() {
	<-h.ch
}

func (h *benchmarkMessageHandler) reset() {
	atomic.StoreUint64(&h.count, 0)
}

func (h *benchmarkMessageHandler) HandleMessageBatch(batch pb.MessageBatch) (uint64, uint64) {
	v := atomic.AddUint64(&h.count, uint64(len(batch.Requests)))
	if v >= h.expected {
		h.ch <- struct{}{}
	}
	return 0, 0
}

func (h *benchmarkMessageHandler) HandleUnreachable(shardID uint64, replicaID uint64) {
}

func (h *benchmarkMessageHandler) HandleSnapshotStatus(shardID uint64,
	replicaID uint64, rejected bool) {
}

func (h *benchmarkMessageHandler) HandleSnapshot(shardID uint64,
	replicaID uint64, from uint64) {
}

type dummyTransportEvent struct{}

func (d *dummyTransportEvent) ConnectionEstablished(addr string, snapshot bool) {}
func (d *dummyTransportEvent) ConnectionFailed(addr string, snapshot bool)      {}

func benchmarkTransport(b *testing.B, sz int) {
	b.ReportAllocs()
	b.StopTimer()
	l := logger.GetLogger("transport")
	l.SetLevel(logger.ERROR)
	l = logger.GetLogger("grpc")
	l.SetLevel(logger.ERROR)
	addr1 := "localhost:43567"
	addr2 := "localhost:43568"
	nhc1 := config.NodeHostConfig{
		RaftAddress: addr1,
		Expert: config.ExpertConfig{
			FS: vfs.DefaultFS,
		},
	}
	env1, err := server.NewEnv(nhc1, vfs.DefaultFS)
	if err != nil {
		b.Fatalf("failed to new context %v", err)
	}
	nhc2 := config.NodeHostConfig{
		RaftAddress: addr2,
		Expert: config.ExpertConfig{
			FS: vfs.DefaultFS,
		},
	}
	env2, err := server.NewEnv(nhc2, vfs.DefaultFS)
	if err != nil {
		b.Fatalf("failed to new context %v", err)
	}
	nodes1 := registry.NewNodeRegistry(settings.Soft.StreamConnections, nil)
	nodes2 := registry.NewNodeRegistry(settings.Soft.StreamConnections, nil)
	nodes1.Add(1, 2, addr2)
	handler1 := &benchmarkMessageHandler{
		ch:       make(chan struct{}, 1),
		expected: 128,
	}
	handler2 := &benchmarkMessageHandler{
		ch:       make(chan struct{}, 1),
		expected: 128,
	}
	t1, err := transport.NewTransport(nhc1,
		handler1, env1, nodes1, nil, &dummyTransportEvent{}, vfs.DefaultFS)
	if err != nil {
		b.Fatalf("failed to create transport %v", err)
	}
	t2, err := transport.NewTransport(nhc2,
		handler2, env2, nodes2, nil, &dummyTransportEvent{}, vfs.DefaultFS)
	if err != nil {
		b.Fatalf("failed to create transport %v", err)
	}
	defer func() {
		if err := t2.Close(); err != nil {
			b.Fatalf("failed to stop the transport module %v", err)
		}
	}()
	defer func() {
		if err := t1.Close(); err != nil {
			b.Fatalf("failed to stop the transport module %v", err)
		}
	}()
	msgs := make([]pb.Message, 0)
	e := pb.Entry{
		Index:       12843560,
		Term:        123,
		Type:        pb.ApplicationEntry,
		Key:         13563799145,
		ClientID:    234926831800,
		SeriesID:    12843560,
		RespondedTo: 12843550,
		Cmd:         make([]byte, sz),
	}
	for i := 0; i < 128; i++ {
		m := pb.Message{
			Type:     pb.Replicate,
			To:       2,
			From:     1,
			ShardID:  1,
			Term:     100,
			LogTerm:  100,
			LogIndex: 123456789,
			Commit:   123456789,
		}
		for j := 0; j < 64; j++ {
			m.Entries = append(m.Entries, e)
		}
		msgs = append(msgs, m)
	}
	b.StartTimer()
	for i := 0; i < b.N; i++ {
		handler1.reset()
		for _, msg := range msgs {
			t1.Send(msg)
		}
		handler1.wait()
	}
}

func BenchmarkTransport16(b *testing.B) {
	benchmarkTransport(b, 16)
}

func BenchmarkTransport128(b *testing.B) {
	benchmarkTransport(b, 128)
}

func BenchmarkTransport1024(b *testing.B) {
	benchmarkTransport(b, 1024)
}

func BenchmarkLookup(b *testing.B) {
	b.ReportAllocs()
	b.StopTimer()
	ds := &tests.NoOP{}
	done := make(chan struct{})
	config := config.Config{ShardID: 1, ReplicaID: 1}
	nds := rsm.NewNativeSM(config, rsm.NewInMemStateMachine(ds), done)
	input := make([]byte, 1)
	b.StartTimer()
	for i := 0; i < b.N; i++ {
		result, err := nds.Lookup(input)
		if err != nil {
			b.Fatalf("lookup failed %v", err)
		}
		if result == nil || len(result.([]byte)) != 1 {
			b.Fatalf("unexpected result")
		}
	}
}

func BenchmarkNALookup(b *testing.B) {
	b.ReportAllocs()
	b.StopTimer()
	ds := &tests.NoOP{}
	done := make(chan struct{})
	config := config.Config{ShardID: 1, ReplicaID: 1}
	nds := rsm.NewNativeSM(config, rsm.NewInMemStateMachine(ds), done)
	input := make([]byte, 1)
	b.StartTimer()
	for i := 0; i < b.N; i++ {
		result, err := nds.NALookup(input)
		if err != nil {
			b.Fatalf("lookup failed %v", err)
		}
		if len(result) != 1 {
			b.Fatalf("unexpected result")
		}
	}
}

func benchmarkStateMachineStep(b *testing.B, sz int, noopSession bool) {
	b.ReportAllocs()
	b.StopTimer()
	ds := &tests.NoOP{NoAlloc: true}
	done := make(chan struct{})
	config := config.Config{ShardID: 1, ReplicaID: 1}
	nds := rsm.NewNativeSM(config, rsm.NewInMemStateMachine(ds), done)
	smo := rsm.NewStateMachine(nds, nil, config, &testDummyNodeProxy{}, vfs.DefaultFS)
	idx := uint64(0)
	var s *client.Session
	if noopSession {
		s = client.NewNoOPSession(1, random.LockGuardedRand)
	} else {
		s = &client.Session{
			ShardID:  1,
			ClientID: 1234576,
		}
	}
	e := pb.Entry{
		Term:        123,
		Type:        pb.ApplicationEntry,
		Key:         13563799145,
		ClientID:    s.ClientID,
		SeriesID:    s.SeriesID,
		RespondedTo: s.RespondedTo,
		Cmd:         make([]byte, sz),
	}
	entries := make([]pb.Entry, 0)
	batch := make([]rsm.Task, 0, 100000)
	smEntries := make([]sm.Entry, 0)
	task := rsm.Task{Recover: false}
	if !noopSession {
		idx++
		e.Index = idx
		e.SeriesID = client.SeriesIDForRegister
		entries = append(entries, e)
		task.Entries = entries
		smo.TaskQ().Add(task)
		if _, err := smo.Handle(batch, smEntries); err != nil {
			b.Fatalf("handle failed %v", err)
		}
	}
	b.StartTimer()
	for x := 0; x < b.N; x++ {
		entries = entries[:0]
		for i := uint64(0); i < 128; i++ {
			idx++
			e.Index = idx
			entries = append(entries, e)
		}
		task.Entries = entries
		smo.TaskQ().Add(task)
		if _, err := smo.Handle(batch, smEntries); err != nil {
			b.Fatalf("handle failed %v", err)
		}
	}
}

func BenchmarkStateMachineStepNoOPSession16(b *testing.B) {
	benchmarkStateMachineStep(b, 16, true)
}

func BenchmarkStateMachineStepNoOPSession128(b *testing.B) {
	benchmarkStateMachineStep(b, 128, true)
}

func BenchmarkStateMachineStepNoOPSession1024(b *testing.B) {
	benchmarkStateMachineStep(b, 1024, true)
}

func BenchmarkStateMachineStep16(b *testing.B) {
	benchmarkStateMachineStep(b, 16, false)
}

func BenchmarkStateMachineStep128(b *testing.B) {
	benchmarkStateMachineStep(b, 128, false)
}

func BenchmarkStateMachineStep1024(b *testing.B) {
	benchmarkStateMachineStep(b, 1024, false)
}

type noopSink struct{}

func (n *noopSink) Receive(pb.Chunk) (bool, bool) { return true, false }
func (n *noopSink) Close() error                  { return nil }
func (n *noopSink) ShardID() uint64               { return 1 }
func (n *noopSink) ToReplicaID() uint64           { return 1 }

func BenchmarkChunkWriter(b *testing.B) {
	sink := &noopSink{}
	meta := rsm.SSMeta{}
	cw := rsm.NewChunkWriter(sink, meta)
	sz := int64(1024 * 256)
	data := make([]byte, sz)
	_, err := rand.Read(data)
	require.NoError(b, err)
	b.ReportAllocs()
	b.SetBytes(sz)
	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		if _, err := cw.Write(data); err != nil {
			b.Fatalf("failed to write %v", err)
		}
	}
}

func BenchmarkSnappyCompressedChunkWriter(b *testing.B) {
	sink := &noopSink{}
	meta := rsm.SSMeta{}
	cw := rsm.NewChunkWriter(sink, meta)
	w := snappy.NewBufferedWriter(cw)
	sz := int64(1024 * 256)
	data := make([]byte, sz)
	_, err := rand.Read(data)
	require.NoError(b, err)
	b.ReportAllocs()
	b.SetBytes(sz)
	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		if _, err := w.Write(data); err != nil {
			b.Fatalf("failed to write %v", err)
		}
	}
}

type marshaler interface {
	Marshal() ([]byte, error)
}

func mustMarshal(m marshaler) []byte {
	result, err := m.Marshal()
	if err != nil {
		panic(err)
	}
	return result
}

func marshalData(e pb.Entry) {
	_, err := e.Marshal()
	if err != nil {
		panic(err)
	}
}

func mustMarshalData(e pb.Entry) {
	mustMarshal(&e)
}

func BenchmarkMarshal(b *testing.B) {
	b.ReportAllocs()
	e := pb.Entry{}
	for i := 0; i < b.N; i++ {
		marshalData(e)
	}
}

func BenchmarkMustMarshal(b *testing.B) {
	b.ReportAllocs()
	e := pb.Entry{}
	for i := 0; i < b.N; i++ {
		mustMarshalData(e)
	}
}
````

## File: CHANGELOG.md
````markdown
## v4.0 (TBD)

Dragonboat v4.0 is a major release with new features, improvements and API changes.

### New features

- Experimental Raft Pre-Vote support.
- Experimental LogDB implementation called tan, it is significantly faster than Key-Value store based approach.

### Improvements

- Better error handling.
- Simplified snapshot compaction.
- Removed dependency on protobuf.
- Fixed snapshot notification.
- Fixed unreachable notification.
- Upgraded to a more recent version of pebble.
- Non-voting node (used to be called observer node) support has been marked as production ready.
- Made the experimental gossip feature a first class citizen of the library.

### Other changes

- Raft observer node has been renamed as non-voting node.
- RocksDB support has been removed as Pebble provides bidirectional compatible with RocksDB. 
- ClusterID/NodeID has been renamed as ShardID/ReplicaID.
- Deprecated v3.x features and APIs have been removed. 

## v3.3 (2021-01-20)

Dragonboat v3.3 is a major release that comes with new features and improvements. All v3.2.x users are recommended to upgrade.

### New features

- Pebble, which is bidirectional compatible with RocksDB, has been made the default engine for storing Raft Logs. RocksDB and CGO are no longer required.
- Added the ability to slow down incoming proposals when the Raft Logs engine is highly loaded.
- Added the option to get notified when proposals and config changes are committed.
- Added an experimental gossip service to allow NodeHosts to use dynamically assigned IP addresses as RaftAddress.
- Added the ability to better control memory footprint.
- Added ARM64/Linux as a new targeted platform.

Note that Pebble provides bidirectional compatibility with RocksDB v6.2.1. Existing Dragonboat applications can upgrade to v3.3 without any conversion unless a newer version of RocksDB was used. RocksDB v6.4.x has been briefly tested and it seems to be compatible with Pebble as well. 

### Improvements

- Optimized the read index implementation.
- Reduced LogDB restart delays.
- Made LogDB configurations accessible programmatically.
- Added protobuf workaround to allow Dragonboat and etcd to be used in the same project.
- Fixed a few data race issues.
- Fixed a potential Raft election deadlock issue when nodes are highly loaded.
- Allow incoming proposals to be rate limited when LogDB is busy.
- Simplified many aspects of the library.

### Breaking changes

- The signature of config.LogDBFactoryFunc has been changed. Your application is not affected unless it uses a custom LogDB implementation.
- Due to lack of user interests, C++ binding is no longer supported.
- LevelDB based LogDB is no longer supported.
- NodeHostConfig's FS and SystemTickerPrecision fields have been moved into NodeHostConfig.Expert.

## v3.2 (2020-03-05)

Dragonboat v3.2 comes with new features and improvements. All v3.1.x users are recommended to upgrade. 

### New features

- Added snappy compression support for Raft entries and snapshots.
- Added experimental witness support.
- Added new API to allow LogDB compaction to be manually triggered.
- Added event listener support to allow users to be notified for certain Raft events.
- Added system event listener support to allow users to be notified for certain system events.
- Added Raft related metrics to exported.
- Added rate limit support to control the maximum bandwidth used for snapshot streaming.
- Updated the C++ binding to cover all v3.1 features. Thanks JasonYuchen for working on that.
- Added a virtual filesystem layer to support more filesystem related tests.
- Added experimental Windows and FreeBSD support.

### Improvements

- Removed the restriction on max proposal payload size.
- Re-enabled the range delete support in LogDB.
- Better handling of concurrent snapshot streams.
- Extensive testing have been done on a high performance native Go KV store called Pebble.
- TolerateCorruptedTailRecords is now the default WAL recovery mode in the RocksDB based LogDB.

### Breaking changes

There is no breaking change for regular users. However, 

 - If you have built customized transport module implementing the raftio.IRaftRPC interface, there is minor change to the config.RaftRPCFactoryFunc type. See github.com/lni/dragoboat/config/config.go for details.
 - The default transport module has been updated, it is no longer compatible with earlier versions of dragonboat. 
 - The default LogDB data format is no longer backward compatible with v3.1 or earlier. 

### Other changes

 - LevelDB support has been marked as depreciated. It will be removed from dragonboat in the next major release. 

## v3.1 (2019-07-04)

Dragonboat v3.1 is a maintenance release with breaking change. All v3.0.x users are recommended to upgrade. Please make sure to carefully read the CHANGELOG below before upgrading.

### Bug fixes

- Fixed ImportSnapshot. 

### New features

- Added NodeHostConfig.RaftEventListener to allow user applications to be notified for certain Raft events.

### Improvements

- Made restarting an existing node faster.
- Immediately return ErrClusterNotReady when requests are dropped for not having a leader.

### Breaking changes

- When upgrading to v3.1.x from v3.0.x, dragonboat requires all streamed or imported snapshots to have been applied. github.com/lni/dragonboat/tools/upgrade310 is provided to check that. See the godoc in github.com/lni/dragonboat/tools/upgrade310 for more details. For users who use NodeHost.RequestSnapshot to export snapshots for backup purposes, we recommend to re-generate all exported snapshots once upgraded to v3.1.

## v3.0 (2019-06-21)

Dragonboat v3.0 is a major release with breaking changes. Please make sure to carefully read the CHANGELOG below before upgrading.

### New features

- Added on disk state machine (statemachine.IOnDiskStateMachine) support. The on disk state machine is close to the concept described in the section 5.2 of Diego Ongaro's Raft thesis. 
- Added new API for requesting a snapshot to be created or exported.
- Added the ability to use exported snapshot to repair permanently damaged cluster that no longer has majority quorum.
- Added new API for cleaning up data and release disk spaces after a node is removed from its Raft cluster.
- Added the ability to limit peak memory usage when disk or network is slow.
- Added Go module support. Go 1.12 is required.

### Improvements

- Further improved self checking on configurations.
- Added snapshot binary format version 2 with block base checksum.
- Synchronous variants have been provided for all asynchronous user request APIs in NodeHost.

### Breaking changes

- The Drummer package has been made invisible from user applications.
- The statemachine.IStateMachine interface has been upgraded to reflect the fact that not all state machine data is stored in memory ([#46](https://github.com/lni/dragonboat/issues/46)).

## v2.1 (2019-02-20)

### New features

- Added support to store Raft Logs in LevelDB.

## v2.0 (2019-01-04)

Initial open source release.
````

## File: CONTRIBUTING.md
````markdown
# Contributing to Dragonboat

* Please read the included LICENSE file to understand the licensing and copyright arrangements.
* It is always good to raise an issue or draft pull request to discuss your proposed changes first.
* Please prepare some automated tests to cover your changes, Go code should be formatted using gofmt.
* When reporting a bug, please provide detailed steps on how it can be reproduced, full execution log is also recommended. Reported bugs without enough supporting information will be closed.
````

## File: engine_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"testing"

	"github.com/stretchr/testify/require"
)

func TestBitmapAdd(t *testing.T) {
	var b bitmap
	for i := uint64(0); i < 64; i++ {
		require.False(t, b.contains(i),
			"unexpectedly contains value %d", i)
		b.add(i)
		require.True(t, b.contains(i),
			"failed to add value %d", i)
	}
}

func TestBitmapContains(t *testing.T) {
	var b bitmap
	b.add(1)
	b.add(3)
	require.True(t, b.contains(1), "contains 1 failed")
	require.True(t, b.contains(3), "contains 3 failed")
	require.False(t, b.contains(2), "contains 2 failed")
}

func TestWorkReadyCanBeCreated(t *testing.T) {
	wr := newWorkReady(4)
	require.Equal(t, 4, len(wr.maps), "unexpected ready list len")
	require.Equal(t, 4, len(wr.channels), "unexpected ready list len")
	require.Equal(t, uint64(4), wr.count, "unexpected count value")
}

func TestPartitionerWorksAsExpected(t *testing.T) {
	wr := newWorkReady(4)
	p := wr.getPartitioner()
	vals := make(map[uint64]struct{})
	for i := uint64(0); i < uint64(128); i++ {
		idx := p.GetPartitionID(i)
		vals[idx] = struct{}{}
	}
	require.Equal(t, 4, len(vals), "unexpected partitioner outcome")
}

func TestAllShardsReady(t *testing.T) {
	wr := newWorkReady(4)
	nodes := make([]*node, 0)
	for i := uint64(0); i < uint64(4); i++ {
		nodes = append(nodes, &node{shardID: i})
	}
	wr.allShardsReady(nodes)
	for i := uint64(0); i < uint64(4); i++ {
		ch := wr.channels[i]
		select {
		case <-ch:
		default:
			require.Fail(t, "channel not ready")
		}
		rc := wr.maps[i]
		m := rc.getReadyShards()
		require.Equal(t, 1, len(m), "unexpected map size")
		_, ok := m[i]
		require.True(t, ok, "shard not set")
	}
	nodes = nodes[:0]
	nodes = append(nodes, []*node{{shardID: 0}, {shardID: 2},
		{shardID: 3}}...)
	wr.allShardsReady(nodes)
	ch := wr.channels[1]
	select {
	case <-ch:
		require.Fail(t, "channel unexpectedly set as ready")
	default:
	}
	rc := wr.maps[1]
	m := rc.getReadyShards()
	require.Equal(t, 0, len(m), "shard map unexpected set")
}

func TestWorkCanBeSetAsReady(t *testing.T) {
	wr := newWorkReady(4)
	select {
	case <-wr.waitCh(1):
		require.Fail(t, "ready signaled")
	case <-wr.waitCh(2):
		require.Fail(t, "ready signaled")
	case <-wr.waitCh(3):
		require.Fail(t, "ready signaled")
	case <-wr.waitCh(4):
		require.Fail(t, "ready signaled")
	default:
	}
	wr.shardReady(0)
	select {
	case <-wr.waitCh(1):
	case <-wr.waitCh(2):
		require.Fail(t, "ready signaled")
	case <-wr.waitCh(3):
		require.Fail(t, "ready signaled")
	case <-wr.waitCh(4):
		require.Fail(t, "ready signaled")
	default:
		require.Fail(t, "ready not signaled")
	}
	wr.shardReady(9)
	select {
	case <-wr.waitCh(1):
		require.Fail(t, "ready signaled")
	case <-wr.waitCh(2):
	case <-wr.waitCh(3):
		require.Fail(t, "ready signaled")
	case <-wr.waitCh(4):
		require.Fail(t, "ready signaled")
	default:
		require.Fail(t, "ready not signaled")
	}
}

func TestReturnedReadyMapContainsReadyShardID(t *testing.T) {
	wr := newWorkReady(4)
	wr.shardReady(0)
	wr.shardReady(4)
	wr.shardReady(129)
	ready := wr.getReadyMap(1)
	require.Equal(t, 2, len(ready),
		"unexpected ready map size, sz: %d", len(ready))
	_, ok := ready[0]
	_, ok2 := ready[4]
	require.True(t, ok && ok2, "missing shard id")
	ready = wr.getReadyMap(2)
	require.Equal(t, 1, len(ready), "unexpected ready map size")
	_, ok = ready[129]
	require.True(t, ok, "missing shard id")
	ready = wr.getReadyMap(3)
	require.Equal(t, 0, len(ready), "unexpected ready map size")
}

func TestLoadedNodes(t *testing.T) {
	lns := newLoadedNodes()
	require.Nil(t, lns.get(2, 3), "unexpectedly returned true")
	nodes := make(map[uint64]*node)
	n := &node{}
	n.replicaID = 3
	nodes[2] = n
	lns.update(1, fromStepWorker, nodes)
	require.NotNil(t, lns.get(2, 3), "unexpectedly returned false")
	n.replicaID = 4
	lns.update(1, fromStepWorker, nodes)
	require.Nil(t, lns.get(2, 3), "unexpectedly returned true")
	nodes = make(map[uint64]*node)
	nodes[5] = n
	n.replicaID = 3
	lns.update(1, fromStepWorker, nodes)
	require.Nil(t, lns.get(2, 3), "unexpectedly returned true")
}

func TestBusyMapKeyIsIgnoredWhenUpdatingLoadedNodes(t *testing.T) {
	m := make(map[uint64]*node)
	m[1] = &node{shardID: 100, replicaID: 100}
	m[2] = &node{shardID: 200, replicaID: 200}
	l := newLoadedNodes()
	l.updateFromBusySSNodes(m)
	nm := l.nodes[nodeType{workerID: 0, from: fromWorker}]
	require.Equal(t, 2, len(nm), "unexpected map len")
	n, ok := nm[100]
	require.True(t, ok, "failed to locate the node")
	require.Equal(t, uint64(100), n.shardID, "failed to locate the node")
	n, ok = nm[200]
	require.True(t, ok, "failed to locate the node")
	require.Equal(t, uint64(200), n.shardID, "failed to locate the node")
}
````

## File: engine.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"reflect"
	"sync"
	"time"

	"github.com/lni/goutils/syncutil"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/rsm"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
)

var (
	reloadTime           = settings.Soft.NodeReloadMillisecond
	timedCloseWaitSecond = settings.Soft.CloseWorkerTimedWaitSecond
	timedCloseWait       = time.Second * time.Duration(timedCloseWaitSecond)
	nodeReloadInterval   = time.Millisecond * time.Duration(reloadTime)
	taskBatchSize        = settings.Soft.TaskBatchSize
)

type bitmap struct {
	v uint64
}

func (b *bitmap) contains(v uint64) bool {
	if v >= 64 {
		panic("invalid v")
	}
	return b.v&(1<<v) > 0
}

func (b *bitmap) add(v uint64) {
	if v >= 64 {
		panic("invalid v")
	}
	b.v = b.v | (1 << v)
}

type from uint64

const (
	fromStepWorker from = iota
	fromCommitWorker
	fromApplyWorker
	fromWorkerPool
	fromWorker
)

type nodeLoader interface {
	describe() string
	getShardSetIndex() uint64
	forEachShard(f func(uint64, *node) bool) uint64
}

type nodeType struct {
	workerID uint64
	from     from
}

type loadedNodes struct {
	nodes map[nodeType]map[uint64]*node
	mu    sync.Mutex
}

func newLoadedNodes() *loadedNodes {
	return &loadedNodes{
		nodes: make(map[nodeType]map[uint64]*node),
	}
}

func (l *loadedNodes) get(shardID uint64, replicaID uint64) *node {
	l.mu.Lock()
	defer l.mu.Unlock()
	for _, m := range l.nodes {
		if n, ok := m[shardID]; ok && n.replicaID == replicaID {
			return n
		}
	}
	return nil
}

func (l *loadedNodes) update(workerID uint64,
	from from, nodes map[uint64]*node) {
	l.mu.Lock()
	defer l.mu.Unlock()
	nt := nodeType{workerID: workerID, from: from}
	l.nodes[nt] = nodes
}

// nodes is a map of workerID -> *node
func (l *loadedNodes) updateFromBusySSNodes(nodes map[uint64]*node) {
	l.updateFromLoadedSSNodes(fromWorker, nodes)
}

// nodes is a map of shardID -> *node
func (l *loadedNodes) updateFromLoadedSSNodes(from from,
	nodes map[uint64]*node) {
	l.mu.Lock()
	defer l.mu.Unlock()
	nt := nodeType{workerID: 0, from: from}
	nm := make(map[uint64]*node, len(nodes))
	for _, n := range nodes {
		nm[n.shardID] = n
	}
	l.nodes[nt] = nm
}

type workReady struct {
	partitioner server.IPartitioner
	maps        []*readyShard
	channels    []chan struct{}
	count       uint64
}

func newWorkReady(count uint64) *workReady {
	wr := &workReady{
		partitioner: server.NewFixedPartitioner(count),
		count:       count,
		maps:        make([]*readyShard, count),
		channels:    make([]chan struct{}, count),
	}
	for i := uint64(0); i < count; i++ {
		wr.channels[i] = make(chan struct{}, 1)
		wr.maps[i] = newReadyShard()
	}
	return wr
}

func (wr *workReady) getPartitioner() server.IPartitioner {
	return wr.partitioner
}

func (wr *workReady) notify(idx uint64) {
	select {
	case wr.channels[idx] <- struct{}{}:
	default:
	}
}

func (wr *workReady) shardReadyByUpdates(updates []pb.Update) {
	var notified bitmap
	for _, ud := range updates {
		if len(ud.CommittedEntries) > 0 {
			idx := wr.partitioner.GetPartitionID(ud.ShardID)
			readyMap := wr.maps[idx]
			readyMap.setShardReady(ud.ShardID)
		}
	}
	for _, ud := range updates {
		if len(ud.CommittedEntries) > 0 {
			idx := wr.partitioner.GetPartitionID(ud.ShardID)
			if !notified.contains(idx) {
				notified.add(idx)
				wr.notify(idx)
			}
		}
	}
}

func (wr *workReady) shardReadyByMessageBatch(mb pb.MessageBatch) {
	var notified bitmap
	for _, req := range mb.Requests {
		idx := wr.partitioner.GetPartitionID(req.ShardID)
		readyMap := wr.maps[idx]
		readyMap.setShardReady(req.ShardID)
	}
	for _, req := range mb.Requests {
		idx := wr.partitioner.GetPartitionID(req.ShardID)
		if !notified.contains(idx) {
			notified.add(idx)
			wr.notify(idx)
		}
	}
}

func (wr *workReady) allShardsReady(nodes []*node) {
	var notified bitmap
	for _, n := range nodes {
		idx := wr.partitioner.GetPartitionID(n.shardID)
		readyMap := wr.maps[idx]
		readyMap.setShardReady(n.shardID)
	}
	for _, n := range nodes {
		idx := wr.partitioner.GetPartitionID(n.shardID)
		if !notified.contains(idx) {
			notified.add(idx)
			wr.notify(idx)
		}
	}
}

func (wr *workReady) shardReady(shardID uint64) {
	idx := wr.partitioner.GetPartitionID(shardID)
	readyMap := wr.maps[idx]
	readyMap.setShardReady(shardID)
	wr.notify(idx)
}

func (wr *workReady) waitCh(workerID uint64) chan struct{} {
	return wr.channels[workerID-1]
}

func (wr *workReady) getReadyMap(workerID uint64) map[uint64]struct{} {
	readyMap := wr.maps[workerID-1]
	return readyMap.getReadyShards()
}

type job struct {
	node       *node
	sink       getSink
	task       rsm.Task
	instanceID uint64
	shardID    uint64
}

type ssWorker struct {
	stopper    *syncutil.Stopper
	requestC   chan job
	completedC chan struct{}
	workerID   uint64
}

func newSSWorker(workerID uint64, stopper *syncutil.Stopper) *ssWorker {
	w := &ssWorker{
		workerID:   workerID,
		stopper:    stopper,
		requestC:   make(chan job, 1),
		completedC: make(chan struct{}, 1),
	}
	stopper.RunWorker(func() {
		w.workerMain()
	})
	return w
}

func (w *ssWorker) workerMain() {
	for {
		select {
		case <-w.stopper.ShouldStop():
			return
		case job := <-w.requestC:
			if job.node == nil {
				panic("req.node == nil")
			}
			if err := w.handle(job); err != nil {
				panicNow(err)
			}
			w.completed()
		}
	}
}

func (w *ssWorker) completed() {
	w.completedC <- struct{}{}
}

func (w *ssWorker) handle(j job) error {
	if j.task.Recover {
		return w.recover(j)
	} else if j.task.Save {
		return w.save(j)
	} else if j.task.Stream {
		return w.stream(j)
	}
	panic("unknown snapshot task type")
}

func (w *ssWorker) recover(j job) error {
	var err error
	var index uint64
	if index, err = j.node.recover(j.task); err != nil {
		return err
	}
	j.node.recoverDone(index)
	return nil
}

func (w *ssWorker) save(j job) error {
	if err := j.node.save(j.task); err != nil {
		return err
	}
	j.node.saveDone()
	return nil
}

func (w *ssWorker) stream(j job) error {
	if err := j.node.stream(j.sink()); err != nil {
		return err
	}
	j.node.streamDone()
	return nil
}

type workerPool struct {
	nh            nodeLoader
	saving        map[uint64]struct{}
	cciReady      *workReady
	saveReady     *workReady
	recoverReady  *workReady
	streamReady   *workReady
	workerStopper *syncutil.Stopper
	busy          map[uint64]*node
	loaded        *loadedNodes
	recovering    map[uint64]struct{}
	streaming     map[uint64]uint64
	nodes         map[uint64]*node
	poolStopper   *syncutil.Stopper
	pending       []job
	workers       []*ssWorker
	cci           uint64
}

func newWorkerPool(nh nodeLoader,
	snapshotWorkerCount uint64, loaded *loadedNodes) *workerPool {
	w := &workerPool{
		nh:            nh,
		loaded:        loaded,
		cciReady:      newWorkReady(1),
		saveReady:     newWorkReady(1),
		recoverReady:  newWorkReady(1),
		streamReady:   newWorkReady(1),
		nodes:         make(map[uint64]*node),
		workers:       make([]*ssWorker, snapshotWorkerCount),
		busy:          make(map[uint64]*node, snapshotWorkerCount),
		saving:        make(map[uint64]struct{}, snapshotWorkerCount),
		recovering:    make(map[uint64]struct{}, snapshotWorkerCount),
		streaming:     make(map[uint64]uint64, snapshotWorkerCount),
		pending:       make([]job, 0),
		workerStopper: syncutil.NewStopper(),
		poolStopper:   syncutil.NewStopper(),
	}
	for workerID := uint64(0); workerID < snapshotWorkerCount; workerID++ {
		w.workers[workerID] = newSSWorker(workerID, w.workerStopper)
	}
	w.poolStopper.RunWorker(func() {
		w.workerPoolMain()
	})
	return w
}

func (p *workerPool) close() error {
	p.poolStopper.Stop()
	return nil
}

func (p *workerPool) getWorker() *ssWorker {
	for _, w := range p.workers {
		if _, busy := p.busy[w.workerID]; !busy {
			return w
		}
	}
	return nil
}

func (p *workerPool) workerPoolMain() {
	ticker := time.NewTicker(200 * time.Millisecond)
	defer ticker.Stop()
	cases := make([]reflect.SelectCase, len(p.workers)+6)
	for {
		toSchedule := false
		// 0 - pool stopper stopc
		// 1 - p.saveReady.waitCh(1)
		// 2 - p.recoverReady.waitCh(1)
		// 3 - p.streamReady.waitCh(1)
		// 4 - p.cciReady.waitCh(1)
		// 5 - worker completedC
		// 5 + len(workers) - ticker.C
		cases[0] = reflect.SelectCase{
			Dir:  reflect.SelectRecv,
			Chan: reflect.ValueOf(p.poolStopper.ShouldStop()),
		}
		cases[1] = reflect.SelectCase{
			Dir:  reflect.SelectRecv,
			Chan: reflect.ValueOf(p.saveReady.waitCh(1)),
		}
		cases[2] = reflect.SelectCase{
			Dir:  reflect.SelectRecv,
			Chan: reflect.ValueOf(p.recoverReady.waitCh(1)),
		}
		cases[3] = reflect.SelectCase{
			Dir:  reflect.SelectRecv,
			Chan: reflect.ValueOf(p.streamReady.waitCh(1)),
		}
		cases[4] = reflect.SelectCase{
			Dir:  reflect.SelectRecv,
			Chan: reflect.ValueOf(p.cciReady.waitCh(1)),
		}
		for idx, w := range p.workers {
			cases[5+idx] = reflect.SelectCase{
				Dir:  reflect.SelectRecv,
				Chan: reflect.ValueOf(w.completedC),
			}
		}
		cases[5+len(p.workers)] = reflect.SelectCase{
			Dir:  reflect.SelectRecv,
			Chan: reflect.ValueOf(ticker.C),
		}
		chosen, _, _ := reflect.Select(cases)
		if chosen == 0 {
			p.workerStopper.Stop()
			p.unloadNodes()
			return
		} else if chosen == 1 {
			shards := p.saveReady.getReadyMap(1)
			p.loadNodes()
			for cid := range shards {
				if j, ok := p.getSaveJob(cid); ok {
					plog.Debugf("%s saveRequested for %d", p.nh.describe(), cid)
					p.pending = append(p.pending, j)
					toSchedule = true
				}
			}
		} else if chosen == 2 {
			shards := p.recoverReady.getReadyMap(1)
			p.loadNodes()
			for cid := range shards {
				if j, ok := p.getRecoverJob(cid); ok {
					plog.Debugf("%s recoverRequested for %d", p.nh.describe(), cid)
					p.pending = append(p.pending, j)
					toSchedule = true
				}
			}
		} else if chosen == 3 {
			shards := p.streamReady.getReadyMap(1)
			p.loadNodes()
			for cid := range shards {
				if j, ok := p.getStreamJob(cid); ok {
					plog.Debugf("%s streamRequested for %d", p.nh.describe(), cid)
					p.pending = append(p.pending, j)
					toSchedule = true
				}
			}
		} else if chosen == 4 {
			p.loadNodes()
		} else if chosen >= 5 && chosen <= 5+len(p.workers)-1 {
			workerID := uint64(chosen - 5)
			p.completed(workerID)
			toSchedule = true
		} else if chosen == len(cases)-1 {
			p.loadNodes()
		} else {
			plog.Panicf("chosen %d, unexpected case", chosen)
		}
		if toSchedule {
			p.loadNodes()
			p.schedule()
		}
	}
}

func (p *workerPool) unloadNodes() {
	for _, n := range p.nodes {
		n.offloaded()
	}
	for _, n := range p.busy {
		n.offloaded()
	}
}

func (p *workerPool) updateLoadedBusyNodes() {
	p.loaded.updateFromBusySSNodes(p.busy)
}

func (p *workerPool) loadNodes() {
	if p.nh.getShardSetIndex() != p.cci {
		newNodes := make(map[uint64]*node)
		loaded := make([]*node, 0)
		p.cci = p.nh.forEachShard(func(cid uint64, n *node) bool {
			if on, ok := p.nodes[cid]; ok {
				if on.instanceID != n.instanceID {
					plog.Panicf("%s from two incarnations found", n.id())
				}
				newNodes[cid] = on
			} else {
				loaded = append(loaded, n)
				newNodes[cid] = n
			}
			return true
		})
		p.loaded.updateFromLoadedSSNodes(fromWorkerPool, newNodes)
		for cid, n := range p.nodes {
			if _, ok := newNodes[cid]; !ok {
				n.offloaded()
			}
		}
		for _, n := range loaded {
			n.loaded()
		}
		p.nodes = newNodes
	}
}

func (p *workerPool) completed(workerID uint64) {
	count := 0
	n, ok := p.busy[workerID]
	if !ok {
		plog.Panicf("worker %d is not busy", workerID)
	}
	if _, ok := p.saving[n.shardID]; ok {
		plog.Debugf("%s completed saveRequested", n.id())
		delete(p.saving, n.shardID)
		count++
	}
	if _, ok := p.recovering[n.shardID]; ok {
		plog.Debugf("%s completed recoverRequested", n.id())
		delete(p.recovering, n.shardID)
		count++
	}
	if sc, ok := p.streaming[n.shardID]; ok {
		plog.Debugf("%s completed streamRequested", n.id())
		switch sc {
		case 0:
			plog.Panicf("node completed streaming when not streaming")
		case 1:
			delete(p.streaming, n.shardID)
		default:
			p.streaming[n.shardID] = sc - 1
		}
		count++
	}
	if count == 0 {
		plog.Panicf("not sure what got completed")
	}
	if count > 1 {
		plog.Panicf("completed more than one type of snapshot op")
	}
	p.setIdle(workerID)
}

func (p *workerPool) inProgress(shardID uint64) bool {
	_, ok1 := p.saving[shardID]
	_, ok2 := p.recovering[shardID]
	_, ok3 := p.streaming[shardID]
	return ok1 || ok2 || ok3
}

func (p *workerPool) canStream(shardID uint64) bool {
	if _, ok := p.saving[shardID]; ok {
		return false
	}
	_, ok := p.recovering[shardID]
	return !ok
}

func (p *workerPool) canSave(shardID uint64) bool {
	return !p.inProgress(shardID)
}

func (p *workerPool) canRecover(shardID uint64) bool {
	return !p.inProgress(shardID)
}

func (p *workerPool) canSchedule(j job) bool {
	if j.task.Recover {
		return p.canRecover(j.shardID)
	} else if j.task.Save {
		return p.canSave(j.shardID)
	} else if j.task.Stream {
		return p.canStream(j.shardID)
	} else {
		plog.Panicf("unknown task type %+v", j.task)
	}
	panic("not suppose to reach here")
}

func (p *workerPool) setIdle(workerID uint64) {
	n, ok := p.busy[workerID]
	if !ok {
		plog.Panicf("worker %d is not busy", workerID)
	}
	delete(p.busy, workerID)
	p.updateLoadedBusyNodes()
	n.offloaded()
}

func (p *workerPool) setBusy(n *node, workerID uint64) {
	if _, ok := p.busy[workerID]; ok {
		plog.Panicf("trying to use a busy worker")
	}
	n.loaded()
	p.busy[workerID] = n
	p.updateLoadedBusyNodes()
}

func (p *workerPool) startStreaming(n *node) {
	if count, ok := p.streaming[n.shardID]; !ok {
		p.streaming[n.shardID] = 1
	} else {
		p.streaming[n.shardID] = count + 1
	}
}

func (p *workerPool) startSaving(n *node) {
	if _, ok := p.saving[n.shardID]; ok {
		plog.Panicf("%s trying to start saving again", n.id())
	}
	p.saving[n.shardID] = struct{}{}
}

func (p *workerPool) startRecovering(n *node) {
	if _, ok := p.recovering[n.shardID]; ok {
		plog.Panicf("%s trying to start recovering again", n.id())
	}
	p.recovering[n.shardID] = struct{}{}
}

func (p *workerPool) start(j job, n *node, workerID uint64) {
	p.setBusy(n, workerID)
	if j.task.Recover {
		p.startRecovering(n)
	} else if j.task.Save {
		p.startSaving(n)
	} else if j.task.Stream {
		p.startStreaming(n)
	} else {
		plog.Panicf("unknown task type %+v", j.task)
	}
}

func (p *workerPool) schedule() {
	for {
		if !p.scheduleWorker() {
			return
		}
	}
}

func (p *workerPool) scheduleWorker() bool {
	if len(p.pending) == 0 {
		return false
	}
	w := p.getWorker()
	if w == nil {
		plog.Debugf("%s no more worker", p.nh.describe())
		return false
	}
	for idx, j := range p.pending {
		n, ok := p.nodes[j.shardID]
		if !ok {
			p.removeFromPending(idx)
			return true
		}
		if p.canSchedule(j) {
			p.scheduleTask(j, n, w)
			p.removeFromPending(idx)
			return true
		}
	}
	return false
}

func (p *workerPool) removeFromPending(idx int) {
	sz := len(p.pending)
	copy(p.pending[idx:], p.pending[idx+1:])
	p.pending = p.pending[:sz-1]
}

func (p *workerPool) getSaveJob(shardID uint64) (job, bool) {
	n, ok := p.nodes[shardID]
	if !ok {
		return job{}, false
	}
	req, ok := n.ss.getSaveReq()
	if !ok {
		return job{}, false
	}
	return job{
		task:       req,
		node:       n,
		instanceID: n.instanceID,
		shardID:    shardID,
	}, true
}

func (p *workerPool) getRecoverJob(shardID uint64) (job, bool) {
	n, ok := p.nodes[shardID]
	if !ok {
		return job{}, false
	}
	req, ok := n.ss.getRecoverReq()
	if !ok {
		return job{}, false
	}
	return job{
		task:       req,
		node:       n,
		instanceID: n.instanceID,
		shardID:    shardID,
	}, true
}

func (p *workerPool) getStreamJob(shardID uint64) (job, bool) {
	n, ok := p.nodes[shardID]
	if !ok {
		return job{}, false
	}
	req, sinkFn, ok := n.ss.getStreamReq()
	if !ok {
		return job{}, false
	}
	return job{
		task:       req,
		node:       n,
		sink:       sinkFn,
		instanceID: n.instanceID,
		shardID:    shardID,
	}, true
}

func (p *workerPool) scheduleTask(j job, n *node, w *ssWorker) {
	if n.instanceID == j.instanceID {
		p.start(j, n, w.workerID)
		select {
		case w.requestC <- j:
		default:
			panic("worker received multiple jobs")
		}
	}
}

type closeReq struct {
	node *node
}

type closeWorker struct {
	stopper    *syncutil.Stopper
	requestC   chan closeReq
	completedC chan struct{}
	workerID   uint64
}

func newCloseWorker(workerID uint64, stopper *syncutil.Stopper) *closeWorker {
	w := &closeWorker{
		workerID:   workerID,
		stopper:    stopper,
		requestC:   make(chan closeReq, 1),
		completedC: make(chan struct{}, 1),
	}
	stopper.RunWorker(func() {
		w.workerMain()
	})
	return w
}

func (w *closeWorker) workerMain() {
	for {
		select {
		case <-w.stopper.ShouldStop():
			return
		case req := <-w.requestC:
			if err := w.handle(req); err != nil {
				panicNow(err)
			}
			w.completed()
		}
	}
}

func (w *closeWorker) completed() {
	w.completedC <- struct{}{}
}

func (w *closeWorker) handle(req closeReq) error {
	if req.node.destroyed() {
		return nil
	}
	return req.node.destroy()
}

type closeWorkerPool struct {
	ready         chan closeReq
	busy          map[uint64]uint64
	processing    map[uint64]struct{}
	workerStopper *syncutil.Stopper
	poolStopper   *syncutil.Stopper
	workers       []*closeWorker
	pending       []*node
}

func newCloseWorkerPool(closeWorkerCount uint64) *closeWorkerPool {
	w := &closeWorkerPool{
		workers:       make([]*closeWorker, closeWorkerCount),
		ready:         make(chan closeReq, 1),
		busy:          make(map[uint64]uint64, closeWorkerCount),
		processing:    make(map[uint64]struct{}, closeWorkerCount),
		pending:       make([]*node, 0),
		workerStopper: syncutil.NewStopper(),
		poolStopper:   syncutil.NewStopper(),
	}

	for workerID := uint64(0); workerID < closeWorkerCount; workerID++ {
		w.workers[workerID] = newCloseWorker(workerID, w.workerStopper)
	}
	w.poolStopper.RunWorker(func() {
		w.workerPoolMain()
	})
	return w
}

func (p *closeWorkerPool) close() error {
	p.poolStopper.Stop()
	return nil
}

func (p *closeWorkerPool) workerPoolMain() {
	cases := make([]reflect.SelectCase, len(p.workers)+2)
	for {
		// 0 - pool stopper stopc
		// 1 - node ready for destroy
		cases[0] = reflect.SelectCase{
			Dir:  reflect.SelectRecv,
			Chan: reflect.ValueOf(p.poolStopper.ShouldStop()),
		}
		cases[1] = reflect.SelectCase{
			Dir:  reflect.SelectRecv,
			Chan: reflect.ValueOf(p.ready),
		}
		for idx, w := range p.workers {
			cases[2+idx] = reflect.SelectCase{
				Dir:  reflect.SelectRecv,
				Chan: reflect.ValueOf(w.completedC),
			}
		}
		chosen, v, _ := reflect.Select(cases)
		if chosen == 0 {
			p.timedWait()
			return
		} else if chosen == 1 {
			node := v.Interface().(closeReq).node
			p.pending = append(p.pending, node)
		} else if chosen > 1 && chosen < len(p.workers)+2 {
			workerID := uint64(chosen - 2)
			p.completed(workerID)
		} else {
			plog.Panicf("chosen %d, unknown case", chosen)
		}
		p.schedule()
	}
}

func (p *closeWorkerPool) timedWait() {
	timer := time.NewTimer(timedCloseWait)
	timeout := false
	defer timer.Stop()
	defer p.workerStopper.Stop()
	defer func() {
		if timeout {
			plog.Infof("timedWait ready to exit, busy %d, pending %d",
				len(p.busy), len(p.pending))
		}
	}()
	// p.ready is buffered, don't ignore that buffered close req
	select {
	case v := <-p.ready:
		p.pending = append(p.pending, v.node)
	default:
	}
	p.schedule()
	cases := make([]reflect.SelectCase, len(p.workers)+1)
	for !p.isIdle() {
		cases[0] = reflect.SelectCase{
			Dir:  reflect.SelectRecv,
			Chan: reflect.ValueOf(timer.C),
		}
		for idx, w := range p.workers {
			cases[1+idx] = reflect.SelectCase{
				Dir:  reflect.SelectRecv,
				Chan: reflect.ValueOf(w.completedC),
			}
		}
		chosen, _, _ := reflect.Select(cases)
		if chosen == 0 {
			timeout = true
			return
		} else if chosen > 0 && chosen < len(p.workers)+1 {
			select {
			case <-timer.C:
				timeout = true
				return
			default:
			}
			workerID := uint64(chosen - 1)
			p.completed(workerID)
			p.schedule()
		} else {
			plog.Panicf("chosen %d, unknown case", chosen)
		}
	}
}

func (p *closeWorkerPool) isIdle() bool {
	return len(p.busy) == 0 && len(p.pending) == 0
}

func (p *closeWorkerPool) completed(workerID uint64) {
	shardID, ok := p.busy[workerID]
	if !ok {
		plog.Panicf("close worker %d is not in busy state", workerID)
	}
	if _, ok := p.processing[shardID]; !ok {
		plog.Panicf("shard %d is not being processed", shardID)
	}
	delete(p.processing, shardID)
	delete(p.busy, workerID)
}

func (p *closeWorkerPool) setBusy(workerID uint64, shardID uint64) {
	p.processing[shardID] = struct{}{}
	p.busy[workerID] = shardID
}

func (p *closeWorkerPool) getWorker() *closeWorker {
	for _, w := range p.workers {
		if _, busy := p.busy[w.workerID]; !busy {
			return w
		}
	}
	return nil
}

func (p *closeWorkerPool) schedule() {
	for {
		if !p.scheduleWorker() {
			return
		}
	}
}

func (p *closeWorkerPool) canSchedule(n *node) bool {
	_, ok := p.processing[n.shardID]
	return !ok
}

func (p *closeWorkerPool) scheduleWorker() bool {
	w := p.getWorker()
	if w == nil {
		return false
	}

	for i := 0; i < len(p.pending); i++ {
		node := p.pending[0]
		p.removeFromPending(0)
		if p.canSchedule(node) {
			p.scheduleReq(node, w)
			return true
		} else {
			p.pending = append(p.pending, node)
		}
	}

	return false
}

func (p *closeWorkerPool) scheduleReq(n *node, w *closeWorker) {
	p.setBusy(w.workerID, n.shardID)
	select {
	case w.requestC <- closeReq{node: n}:
	default:
		panic("worker received multiple jobs")
	}
}

func (p *closeWorkerPool) removeFromPending(idx int) {
	sz := len(p.pending)
	copy(p.pending[idx:], p.pending[idx+1:])
	p.pending = p.pending[:sz-1]
}

type engine struct {
	nodeStopper     *syncutil.Stopper
	commitStopper   *syncutil.Stopper
	taskStopper     *syncutil.Stopper
	nh              nodeLoader
	loaded          *loadedNodes
	env             *server.Env
	logdb           raftio.ILogDB
	stepWorkReady   *workReady
	stepCCIReady    *workReady
	commitWorkReady *workReady
	commitCCIReady  *workReady
	applyWorkReady  *workReady
	applyCCIReady   *workReady
	wp              *workerPool
	cp              *closeWorkerPool
	ec              chan error
	notifyCommit    bool
}

func newExecEngine(nh nodeLoader, cfg config.EngineConfig, notifyCommit bool,
	errorInjection bool, env *server.Env, logdb raftio.ILogDB) *engine {
	if cfg.ExecShards == 0 {
		panic("ExecShards == 0")
	}
	loaded := newLoadedNodes()
	s := &engine{
		nh:              nh,
		env:             env,
		logdb:           logdb,
		loaded:          loaded,
		nodeStopper:     syncutil.NewStopper(),
		commitStopper:   syncutil.NewStopper(),
		taskStopper:     syncutil.NewStopper(),
		stepWorkReady:   newWorkReady(cfg.ExecShards),
		stepCCIReady:    newWorkReady(cfg.ExecShards),
		commitWorkReady: newWorkReady(cfg.CommitShards),
		commitCCIReady:  newWorkReady(cfg.CommitShards),
		applyWorkReady:  newWorkReady(cfg.ApplyShards),
		applyCCIReady:   newWorkReady(cfg.ApplyShards),
		wp:              newWorkerPool(nh, cfg.SnapshotShards, loaded),
		cp:              newCloseWorkerPool(cfg.CloseShards),
		notifyCommit:    notifyCommit,
	}
	if errorInjection {
		s.ec = make(chan error, 1)
	}
	for i := uint64(1); i <= cfg.ExecShards; i++ {
		workerID := i
		s.nodeStopper.RunWorker(func() {
			if errorInjection {
				defer func() {
					if r := recover(); r != nil {
						if ce, ok := r.(error); ok {
							s.crash(ce)
						}
					}
				}()
			}
			s.stepWorkerMain(workerID)
		})
	}
	if notifyCommit {
		for i := uint64(1); i <= cfg.CommitShards; i++ {
			commitWorkerID := i
			s.commitStopper.RunWorker(func() {
				s.commitWorkerMain(commitWorkerID)
			})
		}
	}
	for i := uint64(1); i <= cfg.ApplyShards; i++ {
		applyWorkerID := i
		s.taskStopper.RunWorker(func() {
			s.applyWorkerMain(applyWorkerID)
		})
	}
	return s
}

func (e *engine) crash(err error) {
	select {
	case e.ec <- err:
	default:
	}
}

func (e *engine) close() error {
	e.nodeStopper.Stop()
	e.commitStopper.Stop()
	e.taskStopper.Stop()
	var err error
	err = firstError(err, e.wp.close())
	return firstError(err, e.cp.close())
}

func (e *engine) nodeLoaded(shardID uint64, replicaID uint64) bool {
	return e.loaded.get(shardID, replicaID) != nil
}

func (e *engine) destroyedC(shardID uint64, replicaID uint64) <-chan struct{} {
	if n := e.loaded.get(shardID, replicaID); n != nil {
		return n.sm.DestroyedC()
	}
	return nil
}

func (e *engine) load(workerID uint64,
	cci uint64, nodes map[uint64]*node,
	from from, ready *workReady) (map[uint64]*node, uint64) {
	result, offloaded, cci := e.loadBucketNodes(workerID, cci, nodes,
		ready.getPartitioner(), from)
	e.loaded.update(workerID, from, result)
	for _, n := range offloaded {
		n.offloaded()
	}
	return result, cci
}

func (e *engine) commitWorkerMain(workerID uint64) {
	nodes := make(map[uint64]*node)
	ticker := time.NewTicker(nodeReloadInterval)
	defer ticker.Stop()
	cci := uint64(0)
	for {
		select {
		case <-e.commitStopper.ShouldStop():
			e.offloadNodeMap(nodes)
			return
		case <-ticker.C:
			nodes, cci = e.loadCommitNodes(workerID, cci, nodes)
			e.processCommits(make(map[uint64]struct{}), nodes)
		case <-e.commitCCIReady.waitCh(workerID):
			nodes, cci = e.loadCommitNodes(workerID, cci, nodes)
		case <-e.commitWorkReady.waitCh(workerID):
			if cci == 0 || len(nodes) == 0 {
				nodes, cci = e.loadCommitNodes(workerID, cci, nodes)
			}
			active := e.commitWorkReady.getReadyMap(workerID)
			e.processCommits(active, nodes)
		}
	}
}

func (e *engine) loadCommitNodes(workerID uint64, cci uint64,
	nodes map[uint64]*node) (map[uint64]*node, uint64) {
	return e.load(workerID, cci, nodes, fromCommitWorker, e.commitWorkReady)
}

func (e *engine) processCommits(idmap map[uint64]struct{},
	nodes map[uint64]*node) {
	if len(idmap) == 0 {
		for k := range nodes {
			idmap[k] = struct{}{}
		}
	}
	for shardID := range idmap {
		node, ok := nodes[shardID]
		if !ok || node.stopped() {
			continue
		}
		node.notifyCommittedEntries()
	}
}

func (e *engine) applyWorkerMain(workerID uint64) {
	nodes := make(map[uint64]*node)
	ticker := time.NewTicker(nodeReloadInterval)
	defer ticker.Stop()
	batch := make([]rsm.Task, 0, taskBatchSize)
	entries := make([]sm.Entry, 0, taskBatchSize)
	cci := uint64(0)
	count := uint64(0)
	for {
		select {
		case <-e.taskStopper.ShouldStop():
			e.offloadNodeMap(nodes)
			return
		case <-ticker.C:
			nodes, cci = e.loadApplyNodes(workerID, cci, nodes)
			a := make(map[uint64]struct{})
			if err := e.processApplies(a, nodes, batch, entries); err != nil {
				panicNow(err)
			}
			count++
			if count%200 == 0 {
				batch = make([]rsm.Task, 0, taskBatchSize)
				entries = make([]sm.Entry, 0, taskBatchSize)
			}
		case <-e.applyCCIReady.waitCh(workerID):
			nodes, cci = e.loadApplyNodes(workerID, cci, nodes)
		case <-e.applyWorkReady.waitCh(workerID):
			if cci == 0 || len(nodes) == 0 {
				nodes, cci = e.loadApplyNodes(workerID, cci, nodes)
			}
			a := e.applyWorkReady.getReadyMap(workerID)
			if err := e.processApplies(a, nodes, batch, entries); err != nil {
				panicNow(err)
			}
		}
	}
}

func (e *engine) loadApplyNodes(workerID uint64, cci uint64,
	nodes map[uint64]*node) (map[uint64]*node, uint64) {
	return e.load(workerID, cci, nodes, fromApplyWorker, e.applyWorkReady)
}

// S: save snapshot
// R: recover from snapshot
// existing op, new op, action
// S, S, ignore the new op
// S, R, R is queued as node state, will be handled when S is done
// R, R, won't happen, when in R state, processApplies will not process the node
// R, S, won't happen, when in R state, processApplies will not process the node

func (e *engine) processApplies(idmap map[uint64]struct{},
	nodes map[uint64]*node, batch []rsm.Task, entries []sm.Entry) error {
	if len(idmap) == 0 {
		for k := range nodes {
			idmap[k] = struct{}{}
		}
	}
	for shardID := range idmap {
		node, ok := nodes[shardID]
		if !ok || node.stopped() {
			continue
		}
		if node.processStatusTransition() {
			continue
		}
		task, err := node.handleTask(batch, entries)
		if err != nil {
			return err
		}
		if task.IsSnapshotTask() {
			node.handleSnapshotTask(task)
		}
	}
	return nil
}

func (e *engine) stepWorkerMain(workerID uint64) {
	nodes := make(map[uint64]*node)
	ticker := time.NewTicker(nodeReloadInterval)
	defer ticker.Stop()
	cci := uint64(0)
	stopC := e.nodeStopper.ShouldStop()
	updates := make([]pb.Update, 0)
	for {
		select {
		case <-stopC:
			e.offloadNodeMap(nodes)
			return
		case <-ticker.C:
			nodes, cci = e.loadStepNodes(workerID, cci, nodes)
			a := make(map[uint64]struct{})
			if err := e.processSteps(workerID, a, nodes, updates, stopC); err != nil {
				panicNow(err)
			}
		case <-e.stepCCIReady.waitCh(workerID):
			nodes, cci = e.loadStepNodes(workerID, cci, nodes)
		case <-e.stepWorkReady.waitCh(workerID):
			if cci == 0 || len(nodes) == 0 {
				nodes, cci = e.loadStepNodes(workerID, cci, nodes)
			}
			a := e.stepWorkReady.getReadyMap(workerID)
			if err := e.processSteps(workerID, a, nodes, updates, stopC); err != nil {
				panicNow(err)
			}
		}
	}
}

func (e *engine) loadStepNodes(workerID uint64,
	cci uint64, nodes map[uint64]*node) (map[uint64]*node, uint64) {
	return e.load(workerID, cci, nodes, fromStepWorker, e.stepWorkReady)
}

func (e *engine) loadBucketNodes(workerID uint64,
	csi uint64, nodes map[uint64]*node, partitioner server.IPartitioner,
	from from) (map[uint64]*node, []*node, uint64) {
	bucket := workerID - 1
	newCSI := e.nh.getShardSetIndex()
	var offloaded []*node
	if newCSI != csi {
		newNodes := make(map[uint64]*node)
		loaded := make([]*node, 0)
		newCSI = e.nh.forEachShard(func(cid uint64, v *node) bool {
			if n, ok := nodes[cid]; ok {
				if n.instanceID != v.instanceID {
					plog.Panicf("%s from two incarnations found", n.id())
				}
			} else {
				if partitioner.GetPartitionID(cid) == bucket {
					loaded = append(loaded, v)
				}
			}
			if partitioner.GetPartitionID(cid) == bucket {
				newNodes[cid] = v
			}
			return true
		})
		for cid, node := range nodes {
			if _, ok := newNodes[cid]; !ok {
				offloaded = append(offloaded, node)
			}
		}
		for _, n := range loaded {
			n.loaded()
		}
		return newNodes, offloaded, newCSI
	}
	return nodes, offloaded, csi
}

func (e *engine) processSteps(workerID uint64,
	active map[uint64]struct{},
	nodes map[uint64]*node, nodeUpdates []pb.Update, stopC chan struct{}) error {
	if len(nodes) == 0 {
		return nil
	}
	if len(active) == 0 {
		for cid := range nodes {
			active[cid] = struct{}{}
		}
	}
	nodeUpdates = nodeUpdates[:0]
	for cid := range active {
		node, ok := nodes[cid]
		if !ok || node.stopped() {
			continue
		}
		ud, hasUpdate, err := node.stepNode()
		if err != nil {
			return err
		}
		if hasUpdate {
			nodeUpdates = append(nodeUpdates, ud)
		}
	}
	if err := e.applySnapshotAndUpdate(nodeUpdates, nodes, true); err != nil {
		return err
	}
	// see raft thesis section 10.2.1 on details why we send Replicate message
	// before those entries are persisted to disk
	for _, ud := range nodeUpdates {
		node := nodes[ud.ShardID]
		node.sendReplicateMessages(ud)
		node.processReadyToRead(ud)
		node.processDroppedEntries(ud)
		node.processDroppedReadIndexes(ud)
		node.processLogQuery(ud.LogQueryResult)
		node.processLeaderUpdate(ud.LeaderUpdate)
	}
	if err := e.logdb.SaveRaftState(nodeUpdates, workerID); err != nil {
		return err
	}
	if err := e.onSnapshotSaved(nodeUpdates, nodes); err != nil {
		return err
	}
	if err := e.applySnapshotAndUpdate(nodeUpdates, nodes, false); err != nil {
		return err
	}
	for _, ud := range nodeUpdates {
		node := nodes[ud.ShardID]
		if err := node.processRaftUpdate(ud); err != nil {
			return err
		}
		e.processMoreCommittedEntries(ud)
		node.commitRaftUpdate(ud)
	}
	if lazyFreeCycle > 0 {
		resetNodeUpdate(nodeUpdates)
	}
	return nil
}

func resetNodeUpdate(nodeUpdates []pb.Update) {
	for i := range nodeUpdates {
		nodeUpdates[i].EntriesToSave = nil
		nodeUpdates[i].CommittedEntries = nil
		for j := range nodeUpdates[i].Messages {
			nodeUpdates[i].Messages[j].Entries = nil
		}
	}
}

func (e *engine) processMoreCommittedEntries(ud pb.Update) {
	if ud.MoreCommittedEntries {
		e.setStepReady(ud.ShardID)
	}
}

func (e *engine) applySnapshotAndUpdate(updates []pb.Update,
	nodes map[uint64]*node, fastApply bool) error {
	notifyCommit := false
	for _, ud := range updates {
		if ud.FastApply != fastApply {
			continue
		}
		node := nodes[ud.ShardID]
		if node.notifyCommit {
			notifyCommit = true
		}
		if err := node.processSnapshot(ud); err != nil {
			return err
		}
		node.applyRaftUpdates(ud)
	}
	if !notifyCommit {
		e.setApplyReadyByUpdates(updates)
	} else {
		e.setCommitReadyByUpdates(updates)
	}
	return nil
}

func (e *engine) onSnapshotSaved(updates []pb.Update,
	nodes map[uint64]*node) error {
	for _, ud := range updates {
		if !pb.IsEmptySnapshot(ud.Snapshot) {
			node := nodes[ud.ShardID]
			if err := node.removeSnapshotFlagFile(ud.Snapshot.Index); err != nil {
				return err
			}
		}
	}
	return nil
}

func (e *engine) setCloseReady(n *node) {
	e.cp.ready <- closeReq{node: n}
}

func (e *engine) setStepReadyByMessageBatch(mb pb.MessageBatch) {
	e.stepWorkReady.shardReadyByMessageBatch(mb)
}

func (e *engine) setAllStepReady(nodes []*node) {
	e.stepWorkReady.allShardsReady(nodes)
}

func (e *engine) setStepReady(shardID uint64) {
	e.stepWorkReady.shardReady(shardID)
}

func (e *engine) setCommitReadyByUpdates(updates []pb.Update) {
	e.commitWorkReady.shardReadyByUpdates(updates)
}

func (e *engine) setCommitReady(shardID uint64) {
	e.commitWorkReady.shardReady(shardID)
}

func (e *engine) setApplyReadyByUpdates(updates []pb.Update) {
	e.applyWorkReady.shardReadyByUpdates(updates)
}

func (e *engine) setApplyReady(shardID uint64) {
	e.applyWorkReady.shardReady(shardID)
}

func (e *engine) setStreamReady(shardID uint64) {
	e.wp.streamReady.shardReady(shardID)
}

func (e *engine) setSaveReady(shardID uint64) {
	e.wp.saveReady.shardReady(shardID)
}

func (e *engine) setRecoverReady(shardID uint64) {
	e.wp.recoverReady.shardReady(shardID)
}

func (e *engine) setCCIReady(shardID uint64) {
	e.stepCCIReady.shardReady(shardID)
	e.commitCCIReady.shardReady(shardID)
	e.applyCCIReady.shardReady(shardID)
	e.wp.cciReady.shardReady(shardID)
}

func (e *engine) offloadNodeMap(nodes map[uint64]*node) {
	for _, node := range nodes {
		node.offloaded()
	}
}
````

## File: event.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"fmt"
	"io"
	"sync/atomic"

	"github.com/VictoriaMetrics/metrics"

	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/raftio"
)

// WriteHealthMetrics writes all health metrics in Prometheus format to the
// specified writer. This function is typically called by the metrics http
// handler.
func WriteHealthMetrics(w io.Writer) {
	metrics.WritePrometheus(w, false)
}

type raftEventListener struct {
	readIndexDropped    *metrics.Counter
	proposalDropped     *metrics.Counter
	replicationRejected *metrics.Counter
	snapshotRejected    *metrics.Counter
	queue               *leaderInfoQueue
	hasLeader           *metrics.Gauge
	term                *metrics.Gauge
	campaignLaunched    *metrics.Counter
	campaignSkipped     *metrics.Counter
	leaderID            uint64
	termValue           uint64
	replicaID           uint64
	shardID             uint64
	metrics             bool
}

var _ server.IRaftEventListener = (*raftEventListener)(nil)

func newRaftEventListener(shardID uint64, replicaID uint64,
	useMetrics bool, queue *leaderInfoQueue) *raftEventListener {
	el := &raftEventListener{
		shardID:   shardID,
		replicaID: replicaID,
		metrics:   useMetrics,
		queue:     queue,
	}
	if useMetrics {
		label := fmt.Sprintf(`{shardid="%d",replicaid="%d"}`, shardID, replicaID)
		name := fmt.Sprintf(`dragonboat_raftnode_campaign_launched_total%s`, label)
		el.campaignLaunched = metrics.GetOrCreateCounter(name)
		name = fmt.Sprintf(`dragonboat_raftnode_campaign_skipped_total%s`, label)
		el.campaignSkipped = metrics.GetOrCreateCounter(name)
		name = fmt.Sprintf(`dragonboat_raftnode_snapshot_rejected_total%s`, label)
		el.snapshotRejected = metrics.GetOrCreateCounter(name)
		name = fmt.Sprintf(`dragonboat_raftnode_replication_rejected_total%s`, label)
		el.replicationRejected = metrics.GetOrCreateCounter(name)
		name = fmt.Sprintf(`dragonboat_raftnode_proposal_dropped_total%s`, label)
		el.proposalDropped = metrics.GetOrCreateCounter(name)
		name = fmt.Sprintf(`dragonboat_raftnode_read_index_dropped_total%s`, label)
		el.readIndexDropped = metrics.GetOrCreateCounter(name)
		name = fmt.Sprintf(`dragonboat_raftnode_has_leader%s`, label)
		el.hasLeader = metrics.GetOrCreateGauge(name, func() float64 {
			if atomic.LoadUint64(&el.leaderID) == raftio.NoLeader {
				return 0.0
			}
			return 1.0
		})
		name = fmt.Sprintf(`dragonboat_raftnode_term%s`, label)
		el.term = metrics.GetOrCreateGauge(name, func() float64 {
			return float64(atomic.LoadUint64(&el.termValue))
		})
	}
	return el
}

func (e *raftEventListener) close() {
}

func (e *raftEventListener) LeaderUpdated(info server.LeaderInfo) {
	atomic.StoreUint64(&e.leaderID, info.LeaderID)
	atomic.StoreUint64(&e.termValue, info.Term)
	if e.queue != nil {
		ui := raftio.LeaderInfo{
			ShardID:   info.ShardID,
			ReplicaID: info.ReplicaID,
			Term:      info.Term,
			LeaderID:  info.LeaderID,
		}
		e.queue.addLeaderInfo(ui)
	}
}

func (e *raftEventListener) CampaignLaunched(info server.CampaignInfo) {
	if e.metrics {
		e.campaignLaunched.Add(1)
	}
}

func (e *raftEventListener) CampaignSkipped(info server.CampaignInfo) {
	if e.metrics {
		e.campaignSkipped.Add(1)
	}
}

func (e *raftEventListener) SnapshotRejected(info server.SnapshotInfo) {
	if e.metrics {
		e.snapshotRejected.Add(1)
	}
}

func (e *raftEventListener) ReplicationRejected(info server.ReplicationInfo) {
	if e.metrics {
		e.replicationRejected.Add(1)
	}
}

func (e *raftEventListener) ProposalDropped(info server.ProposalInfo) {
	if e.metrics {
		e.proposalDropped.Add(len(info.Entries))
	}
}

func (e *raftEventListener) ReadIndexDropped(info server.ReadIndexInfo) {
	if e.metrics {
		e.readIndexDropped.Add(1)
	}
}

type sysEventListener struct {
	stopc  chan struct{}
	events chan server.SystemEvent
	ul     raftio.ISystemEventListener
}

func newSysEventListener(l raftio.ISystemEventListener,
	stopc chan struct{}) *sysEventListener {
	return &sysEventListener{
		stopc:  stopc,
		events: make(chan server.SystemEvent),
		ul:     l,
	}
}

func (l *sysEventListener) Publish(e server.SystemEvent) {
	if l.ul == nil {
		return
	}
	select {
	case l.events <- e:
	case <-l.stopc:
		return
	}
}

func (l *sysEventListener) handle(e server.SystemEvent) {
	if l.ul == nil {
		return
	}
	switch e.Type {
	case server.NodeHostShuttingDown:
		l.ul.NodeHostShuttingDown()
	case server.NodeReady:
		l.ul.NodeReady(getNodeInfo(e))
	case server.NodeUnloaded:
		l.ul.NodeUnloaded(getNodeInfo(e))
	case server.NodeDeleted:
		l.ul.NodeDeleted(getNodeInfo(e))
	case server.MembershipChanged:
		l.ul.MembershipChanged(getNodeInfo(e))
	case server.ConnectionEstablished:
		l.ul.ConnectionEstablished(getConnectionInfo(e))
	case server.ConnectionFailed:
		l.ul.ConnectionFailed(getConnectionInfo(e))
	case server.SendSnapshotStarted:
		l.ul.SendSnapshotStarted(getSnapshotInfo(e))
	case server.SendSnapshotCompleted:
		l.ul.SendSnapshotCompleted(getSnapshotInfo(e))
	case server.SendSnapshotAborted:
		l.ul.SendSnapshotAborted(getSnapshotInfo(e))
	case server.SnapshotReceived:
		l.ul.SnapshotReceived(getSnapshotInfo(e))
	case server.SnapshotRecovered:
		l.ul.SnapshotRecovered(getSnapshotInfo(e))
	case server.SnapshotCreated:
		l.ul.SnapshotCreated(getSnapshotInfo(e))
	case server.SnapshotCompacted:
		l.ul.SnapshotCompacted(getSnapshotInfo(e))
	case server.LogCompacted:
		l.ul.LogCompacted(getEntryInfo(e))
	case server.LogDBCompacted:
		l.ul.LogDBCompacted(getEntryInfo(e))
	default:
		panic("unknown event type")
	}
}

func getSnapshotInfo(e server.SystemEvent) raftio.SnapshotInfo {
	return raftio.SnapshotInfo{
		ShardID:   e.ShardID,
		ReplicaID: e.ReplicaID,
		From:      e.From,
		Index:     e.Index,
	}
}

func getNodeInfo(e server.SystemEvent) raftio.NodeInfo {
	return raftio.NodeInfo{
		ShardID:   e.ShardID,
		ReplicaID: e.ReplicaID,
	}
}

func getEntryInfo(e server.SystemEvent) raftio.EntryInfo {
	return raftio.EntryInfo{
		ShardID:   e.ShardID,
		ReplicaID: e.ReplicaID,
		Index:     e.Index,
	}
}

func getConnectionInfo(e server.SystemEvent) raftio.ConnectionInfo {
	return raftio.ConnectionInfo{
		Address:            e.Address,
		SnapshotConnection: e.SnapshotConnection,
	}
}
````

## File: go.mod
````
module github.com/lni/dragonboat/v4

require (
	github.com/VictoriaMetrics/metrics v1.18.1
	github.com/cespare/xxhash/v2 v2.1.2
	github.com/cockroachdb/errors v1.9.0
	github.com/cockroachdb/pebble v0.0.0-20221207173255-0f086d933dac
	github.com/golang/snappy v0.0.4
	github.com/google/uuid v1.3.0
	github.com/hashicorp/memberlist v0.3.1
	github.com/kr/pretty v0.3.0
	github.com/lni/goutils v1.4.0
	github.com/lni/vfs v0.2.1-0.20220616104132-8852fd867376
	github.com/pierrec/lz4/v4 v4.1.14
	github.com/stretchr/testify v1.7.0
	golang.org/x/exp v0.0.0-20200513190911-00229845015e
	golang.org/x/sys v0.34.0
)

require (
	github.com/DataDog/zstd v1.4.5 // indirect
	github.com/HdrHistogram/hdrhistogram-go v1.1.2 // indirect
	github.com/armon/go-metrics v0.0.0-20180917152333-f0300d1749da // indirect
	github.com/cockroachdb/logtags v0.0.0-20211118104740-dabe8e521a4f // indirect
	github.com/cockroachdb/redact v1.1.3 // indirect
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/getsentry/sentry-go v0.12.0 // indirect
	github.com/gogo/protobuf v1.3.2 // indirect
	github.com/google/btree v1.0.0 // indirect
	github.com/hashicorp/errwrap v1.0.0 // indirect
	github.com/hashicorp/go-immutable-radix v1.0.0 // indirect
	github.com/hashicorp/go-msgpack v0.5.3 // indirect
	github.com/hashicorp/go-multierror v1.0.0 // indirect
	github.com/hashicorp/go-sockaddr v1.0.0 // indirect
	github.com/hashicorp/golang-lru v0.5.1 // indirect
	github.com/klauspost/compress v1.11.13 // indirect
	github.com/kr/text v0.2.0 // indirect
	github.com/miekg/dns v1.1.26 // indirect
	github.com/pkg/errors v0.9.1 // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	github.com/rogpeppe/go-internal v1.8.1 // indirect
	github.com/sean-/seed v0.0.0-20170313163322-e2103e2c3529 // indirect
	github.com/valyala/fastrand v1.1.0 // indirect
	github.com/valyala/histogram v1.2.0 // indirect
	golang.org/x/crypto v0.40.0 // indirect
	golang.org/x/net v0.41.0 // indirect
	golang.org/x/sync v0.1.0 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)

go 1.23.0
````

## File: issue_template.md
````markdown
> Note: for reported bugs, please fill in the following details. bug reports without detailed steps on how to reproduce will be automatically closed. 

### Dragonboat version

### Expected behavior

### Actual behavior

### Steps to reproduce the behavior
````

## File: LICENSE
````
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
````

## File: Makefile
````
# Copyright 2018-2021 Lei Ni (nilei81@gmail.com) and other contributors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

GOEXEC ?= go
# Dragonboat is known to work on - 
# Linux AMD64, Linux ARM64, MacOS, Windows/MinGW and FreeBSD AMD64
# only Linux AMD64 is officially supported
OS := $(shell uname)
# the location of this Makefile
PKGROOT=$(shell dirname $(realpath $(lastword $(MAKEFILE_LIST))))
# name of the package
PKGNAME=$(shell go list)

ifeq ($(DRAGONBOAT_LOGDB),rocksdb)
$(error rocksdb is no longer supported)
else ifeq ($(DRAGONBOAT_LOGDB),)
ifneq ($(MEMFS_TEST),)
$(info using memfs based pebble)
LOGDB_TAG=dragonboat_memfs_test
else
ifneq ($(MEMFS_TEST_TO_RUN),)
$(info using memfs based pebble)
LOGDB_TAG=dragonboat_memfs_test
else
$(info using pebble based log storage)
endif
endif
else
$(error LOGDB type $(DRAGONBOAT_LOGDB) not supported)
endif

# verbosity, use -v to see details of go build
VERBOSE ?= -v
ifeq ($(VERBOSE),)
GO=@$(GOEXEC)
else
GO=$(GOEXEC)
endif

ifeq ($(RACE),1)
RACE_DETECTOR_FLAG=-race
$(warning "data race detector enabled")
endif

ifeq ($(COVER),1)
COVER_FLAG=-coverprofile=coverage.out
$(warning "coverage enabled, `go tool cover -html=coverage.out` to see results")
endif

ifneq ($(TEST_TO_RUN),)
$(info Running selected tests $(TEST_TO_RUN))
SELECTED_TEST_OPTION=-run $(TEST_TO_RUN)
endif

ifneq ($(MEMFS_TEST_TO_RUN),)
$(info Running selected tests $(MEMFS_TEST_TO_RUN))
SELECTED_TEST_OPTION=-run $(MEMFS_TEST_TO_RUN)
endif

ifneq ($(BENCHMARK_TO_RUN),)
$(info Running selected benchmarks $(BENCHMARK_TO_RUN))
SELECTED_BENCH_OPTION=-run ^$$ -bench=$(BENCHMARK_TO_RUN)
else
SELECTED_BENCH_OPTION=-run ^$$ -bench=.
endif

# go build tags
GOBUILDTAGVALS+=$(LOGDB_TAG)
GOBUILDTAGS="$(GOBUILDTAGVALS)"
TESTTAGVALS+=$(GOBUILDTAGVALS)
TESTTAGS="$(TESTTAGVALS)"
EXTNAME=linux

.PHONY: all
all: unit-test-bin
.PHONY: rebuild-all
rebuild-all: clean unit-test-bin

###############################################################################
# tests
###############################################################################
ifneq ($(TESTTAGS),"")
GOCMDTAGS=-tags=$(TESTTAGS)
endif

TEST_OPTIONS=test $(GOCMDTAGS) -timeout=2400s -count=1 $(VERBOSE) \
  $(RACE_DETECTOR_FLAG) $(COVER_FLAG) $(SELECTED_TEST_OPTION)
.PHONY: dragonboat-test
dragonboat-test: test-raft test-raftpb test-rsm test-logdb test-transport    \
	test-multiraft test-config test-client test-server test-tools test-fs   	 \
	test-id test-utils test-tan test-registry
.PHONY: ci-test
ci-test: test-raft test-raftpb test-rsm test-logdb test-transport 		       \
  test-config test-client test-server test-tests test-tools test-fs 				 \
	test-id test-utils test-tan test-registry
.PHONY: test
test: dragonboat-test test-tests
.PHONY: dev-test
dev-test: test
.PHONY: actions-test
actions-test: ci-test test-cov

###############################################################################
# build unit tests
###############################################################################
.PHONY: unit-test-bin
unit-test-bin: TEST_OPTIONS=test -c -o $@.bin -tags=$(TESTTAGS) 						 \
	-count=1 $(VERBOSE) $(RACE_DETECTOR_FLAG) $(SELECTED_TEST_OPTION) 
.PHONY: unit-test-bin
unit-test-bin: test-raft test-raftpb test-rsm test-logdb test-transport 		 \
  test-multiraft test-config test-client test-server test-tools \
	test-tests test-fs test-id test-utils test-tan test-registry

###############################################################################
# fast tests executed for every git push
###############################################################################
.PHONY: benchmark
benchmark:
	$(GOTEST) $(SELECTED_BENCH_OPTION)
.PHONY: benchmark-tan
benchmark-tan:
	$(GOTEST) $(SELECTED_BENCH_OPTION) $(PKGNAME)/internal/tan
.PHONY: benchmark-fsync
benchmark-fsync:
	$(GOTEST)	-run ^$$ -bench=BenchmarkFSyncLatency

GOTEST=$(GO) $(TEST_OPTIONS)
.PHONY: slow-test
slow-test:
	SLOW_TEST=1 $(GOTEST) $(PKGNAME)
.PHONY: test-server
test-server:
	$(GOTEST) $(PKGNAME)/internal/server
.PHONY: test-config
test-config:
	$(GOTEST) $(PKGNAME)/config
.PHONY: test-client
test-client:
	$(GOTEST) $(PKGNAME)/client
.PHONY: test-raft
test-raft:
	$(GOTEST) $(PKGNAME)/internal/raft
.PHONY: test-raftpb
test-raftpb:
	$(GOTEST) $(PKGNAME)/raftpb
.PHONY: test-rsm
test-rsm:
	$(GOTEST) $(PKGNAME)/internal/rsm
.PHONY: test-logdb
test-logdb:
	$(GOTEST) $(PKGNAME)/internal/logdb
.PHONY: test-transport
test-transport:
	$(GOTEST) $(PKGNAME)/internal/transport
.PHONY: test-multiraft
test-multiraft:
	$(GOTEST) $(PKGNAME)
.PHONY: test-tests
test-tests:
	$(GOTEST) $(PKGNAME)/internal/tests
.PHONY: test-fs
test-fs:
	$(GOTEST) $(PKGNAME)/internal/fileutil
.PHONY: test-tools
test-tools:
	$(GOTEST) $(PKGNAME)/tools
.PHONY: test-id
test-id:
	$(GOTEST) $(PKGNAME)/internal/id
.PHONY: test-utils
test-utils:
	$(GOTEST) $(PKGNAME)/internal/utils/dio
.PHONY: test-tan
test-tan:
	$(GOTEST) $(PKGNAME)/internal/tan
.PHONY: test-registry
test-registry:
	$(GOTEST) $(PKGNAME)/internal/registry
.PHONY: test-cov
test-cov:
	$(GOTEST) -coverprofile=coverage.txt -covermode=atomic

###############################################################################
# tools
###############################################################################
.PHONY: tools
tools: tools-checkdisk

.PHONY: tools-checkdisk
tools-checkdisk:
	$(GO) build $(PKGNAME)/tools/checkdisk

###############################################################################
# static checks
###############################################################################
GOLANGCI_LINT_VERSION=v2.1.6
.PHONY: install-static-check-tools
install-static-check-tools:
	@go install github.com/golangci/golangci-lint/v2/cmd/golangci-lint@$(GOLANGCI_LINT_VERSION)

EXTRA_LINTERS=-E misspell -E rowserrcheck -E unconvert -E prealloc
.PHONY: static-check
static-check:
	golangci-lint run --timeout 3m $(EXTRA_LINTERS)

extra-static-check: override EXTRA_LINTERS :=-E dupl
extra-static-check: static-check

###############################################################################
# clean
###############################################################################
.PHONY: clean
clean:
	@find . -type d -name "*safe_to_delete" -print | xargs rm -rf
	@rm -f gitversion.go 
	@rm -f test-*.*
	@rm -f checkdisk
	@$(GO) clean -i -testcache $(PKG)
````

## File: monkey.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//go:build dragonboat_monkeytest
// +build dragonboat_monkeytest

package dragonboat

import (
	"sync/atomic"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/transport"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/raftio"
)

func ApplyMonkeySettings() {
	transport.ApplyMonkeySettings()
}

//
// code here is used in testing only.
//

// MemFS is a in memory vfs intended to be used in testing. User applications
// can usually ignore such vfs related types and fields.
type MemFS = vfs.MemFS

// GetTestFS returns a vfs instance that can be used in testing. User
// applications can usually ignore such vfs related types and fields.
func GetTestFS() config.IFS {
	return vfs.GetTestFS()
}

// Shards returns a list of raft nodes managed by the nodehost instance.
func (nh *NodeHost) Shards() []*node {
	result := make([]*node, 0)
	nh.mu.RLock()
	nh.mu.shards.Range(func(k, v interface{}) bool {
		result = append(result, v.(*node))
		return true
	})
	nh.mu.RUnlock()
	return result
}

func SetPendingProposalShards(sz uint64) {
	pendingProposalShards = sz
}

func SetTaskBatchSize(sz uint64) {
	taskBatchSize = sz
}

func SetIncomingProposalsMaxLen(sz uint64) {
	incomingProposalsMaxLen = sz
}

func SetIncomingReadIndexMaxLen(sz uint64) {
	incomingReadIndexMaxLen = sz
}

func SetReceiveQueueLen(v uint64) {
	receiveQueueLen = v
}

func (nh *NodeHost) Stopped() bool {
	return atomic.LoadInt32(&nh.closed) != 0
}

func (nh *NodeHost) SetTransportDropBatchHook(f transport.SendMessageBatchFunc) {
	nh.transport.(*transport.Transport).SetPreSendBatchHook(f)
}

func (nh *NodeHost) SetPreStreamChunkSendHook(f transport.StreamChunkSendFunc) {
	nh.transport.(*transport.Transport).SetPreStreamChunkSendHook(f)
}

func (nh *NodeHost) GetLogDB() raftio.ILogDB {
	return nh.mu.logdb
}

func (n *node) GetLastApplied() uint64 {
	return n.sm.GetLastApplied()
}

func (n *node) DumpRaftInfoToLog() {
	n.raftMu.Lock()
	defer n.raftMu.Unlock()
	n.dumpRaftInfoToLog()
}

func (n *node) IsLeader() bool {
	return n.isLeader()
}

func (n *node) IsFollower() bool {
	return n.isFollower()
}

func (n *node) GetStateMachineHash() uint64 {
	return n.getStateMachineHash()
}

func (n *node) GetSessionHash() uint64 {
	return n.getSessionHash()
}

func (n *node) GetMembershipHash() uint64 {
	return n.getMembershipHash()
}

func (n *node) GetRateLimiter() *server.InMemRateLimiter {
	return n.p.GetRateLimiter()
}

func (n *node) GetInMemLogSize() uint64 {
	n.raftMu.Lock()
	defer n.raftMu.Unlock()
	return n.p.GetInMemLogSize()
}

func (n *node) getStateMachineHash() uint64 {
	if v, err := n.sm.GetHash(); err != nil {
		panic(err)
	} else {
		return v
	}
}

func (n *node) getSessionHash() uint64 {
	return n.sm.GetSessionHash()
}

func (n *node) getMembershipHash() uint64 {
	return n.sm.GetMembershipHash()
}

func (n *node) dumpRaftInfoToLog() {
	addrMap := make(map[uint64]string)
	m := n.sm.GetMembership()
	for replicaID := range m.Addresses {
		if replicaID == n.replicaID {
			addrMap[replicaID] = n.getRaftAddress()
		} else {
			v, _, err := n.nodeRegistry.Resolve(n.shardID, replicaID)
			if err == nil {
				addrMap[replicaID] = v
			}
		}
	}
	n.p.DumpRaftInfoToLog(addrMap)
}

// PartitionNode puts the node into test partition mode. All connectivity to
// the outside world should be stopped.
func (nh *NodeHost) PartitionNode() {
	plog.Infof("entered partition test mode")
	atomic.StoreInt32(&nh.partitioned, 1)
}

// RestorePartitionedNode removes the node from test partition mode. No other
// change is going to be made on the local node. It is up to the local node it
// self to repair/restore any other state.
func (nh *NodeHost) RestorePartitionedNode() {
	plog.Infof("restored from partition test mode")
	atomic.StoreInt32(&nh.partitioned, 0)
}

// IsPartitioned indicates whether the local node is in partitioned mode. This
// function is only implemented in the monkey test build mode. It always
// returns false in a regular build.
func (nh *NodeHost) IsPartitioned() bool {
	return nh.isPartitioned()
}

// IsPartitioned indicates whether the local node is in partitioned mode.
func (nh *NodeHost) isPartitioned() bool {
	return atomic.LoadInt32(&nh.partitioned) == 1
}
````

## File: monkeynoop.go
````go
// Copyright 2017-2019 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//go:build !dragonboat_monkeytest
// +build !dragonboat_monkeytest

package dragonboat

import (
	"sync/atomic"
)

func (nh *NodeHost) isPartitioned() bool {
	return atomic.LoadInt32(&nh.partitioned) == 1
}
````

## File: node_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"context"
	"fmt"
	"math/rand"
	"sort"
	"strings"
	"sync"
	"testing"
	"time"

	"github.com/lni/goutils/leaktest"
	"github.com/lni/goutils/random"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/client"
	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/logdb"
	"github.com/lni/dragonboat/v4/internal/raft"
	"github.com/lni/dragonboat/v4/internal/registry"
	"github.com/lni/dragonboat/v4/internal/rsm"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/tests"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
)

const (
	raftTestTopDir            = "raft_node_test_safe_to_delete"
	logdbDir                  = "logdb_test_dir_safe_to_delete"
	lowLatencyLogDBDir        = "logdb_ll_test_dir_safe_to_delete"
	snapDir                   = "snap_test_dir_safe_to_delete/snap-%d-%d"
	testShardID        uint64 = 1100
	tickMillisecond    uint64 = 50
)

func getMemberNodes(r *rsm.StateMachine) []uint64 {
	m := r.GetMembership()
	n := make([]uint64, 0)
	for nid := range m.Addresses {
		n = append(n, nid)
	}
	return n
}

func mustComplete(rs *RequestState, t *testing.T) {
	select {
	case v := <-rs.ResultC():
		require.True(t, v.Completed(), "got %v, want %v", v.code, requestCompleted)
	default:
		require.Fail(t, "failed to complete the proposal")
	}
}

func mustReject(rs *RequestState, t *testing.T) {
	select {
	case v := <-rs.ResultC():
		assert.True(t, v.Rejected(), "got %v, want %d", v, requestRejected)
	default:
		assert.Fail(t, "failed to complete the add node request")
	}
}

func mustHasLeaderNode(nodes []*node, t *testing.T) *node {
	for _, node := range nodes {
		if node.isLeader() {
			return node
		}
	}
	require.Fail(t, "no leader")
	return nil
}

type testRouter struct {
	shardID  uint64
	qm       map[uint64]*server.MessageQueue
	dropRate uint8
}

func newTestRouter(shardID uint64, replicaIDList []uint64) *testRouter {
	m := make(map[uint64]*server.MessageQueue)
	for _, replicaID := range replicaIDList {
		m[replicaID] = server.NewMessageQueue(1000, false, 0, 1024*1024*256)
	}
	return &testRouter{qm: m, shardID: shardID}
}

func (r *testRouter) shouldDrop(msg pb.Message) bool {
	if raft.IsLocalMessageType(msg.Type) {
		return false
	}
	if r.dropRate == 0 {
		return false
	}
	if rand.Uint32()%100 < uint32(r.dropRate) {
		return true
	}
	return false
}

func (r *testRouter) send(msg pb.Message) {
	if msg.ShardID != r.shardID {
		panic("shard id does not match")
	}
	if r.shouldDrop(msg) {
		return
	}
	if q, ok := r.qm[msg.To]; ok {
		q.Add(msg)
	}
}

func (r *testRouter) getQ(shardID uint64,
	replicaID uint64) *server.MessageQueue {
	if shardID != r.shardID {
		panic("shard id does not match")
	}
	q, ok := r.qm[replicaID]
	if !ok {
		panic("node id not found in the test msg router")
	}
	return q
}

func (r *testRouter) addQ(replicaID uint64, q *server.MessageQueue) {
	r.qm[replicaID] = q
}

func cleanupTestDir(fs vfs.IFS) {
	if err := fs.RemoveAll(raftTestTopDir); err != nil {
		panic(err)
	}
}

func getTestRaftNodes(count int, ordered bool,
	fs vfs.IFS) ([]*node, []*rsm.StateMachine, *testRouter, raftio.ILogDB) {
	return doGetTestRaftNodes(1, count, ordered, nil, fs)
}

type dummyEngine struct {
}

func (d *dummyEngine) setCloseReady(n *node)          {}
func (d *dummyEngine) setStepReady(shardID uint64)    {}
func (d *dummyEngine) setCommitReady(shardID uint64)  {}
func (d *dummyEngine) setApplyReady(shardID uint64)   {}
func (d *dummyEngine) setStreamReady(shardID uint64)  {}
func (d *dummyEngine) setSaveReady(shardID uint64)    {}
func (d *dummyEngine) setRecoverReady(shardID uint64) {}

func doGetTestRaftNodes(startID uint64, count int, ordered bool,
	ldb raftio.ILogDB, fs vfs.IFS) ([]*node, []*rsm.StateMachine,
	*testRouter, raftio.ILogDB) {
	nodes := make([]*node, 0)
	smList := make([]*rsm.StateMachine, 0)
	replicaIDList := make([]uint64, 0)
	// peers map
	peers := make(map[uint64]string)
	endID := startID + uint64(count-1)
	for i := startID; i <= endID; i++ {
		replicaIDList = append(replicaIDList, i)
		peers[i] = fmt.Sprintf("peer:%d", 12345+i)
	}
	// pools
	requestStatePool := &sync.Pool{}
	requestStatePool.New = func() interface{} {
		obj := &RequestState{}
		obj.CompletedC = make(chan RequestResult, 1)
		obj.pool = requestStatePool
		return obj
	}
	var err error
	if ldb == nil {
		nodeLogDir := fs.PathJoin(raftTestTopDir, logdbDir)
		nodeLowLatencyLogDir := fs.PathJoin(raftTestTopDir, lowLatencyLogDBDir)
		if err := fs.MkdirAll(nodeLogDir, 0755); err != nil {
			panic(err)
		}
		if err := fs.MkdirAll(nodeLowLatencyLogDir, 0755); err != nil {
			panic(err)
		}
		cfg := config.NodeHostConfig{
			Expert: config.GetDefaultExpertConfig(),
		}
		cfg.Expert.LogDB.Shards = 2
		cfg.Expert.FS = fs
		ldb, err = logdb.NewDefaultLogDB(cfg,
			nil, []string{nodeLogDir}, []string{nodeLowLatencyLogDir})
		if err != nil {
			plog.Panicf("failed to open logdb, %+v", err)
		}
	}
	// message router
	router := newTestRouter(testShardID, replicaIDList)
	for i := startID; i <= endID; i++ {
		// create the snapshotter object
		nodeSnapDir := fmt.Sprintf(snapDir, testShardID, i)
		snapdir := fs.PathJoin(raftTestTopDir, nodeSnapDir)
		if err := fs.MkdirAll(snapdir, 0755); err != nil {
			panic(err)
		}
		rootDirFunc := func(cid uint64, nid uint64) string {
			return snapdir
		}
		lr := logdb.NewLogReader(testShardID, i, ldb)
		snapshotter := newSnapshotter(testShardID, i, rootDirFunc, ldb, lr, fs)
		lr.SetCompactor(snapshotter)
		// create the sm
		noopSM := &tests.NoOP{}
		cfg := config.Config{
			ReplicaID:           i,
			ShardID:             testShardID,
			ElectionRTT:         20,
			HeartbeatRTT:        2,
			CheckQuorum:         true,
			SnapshotEntries:     10,
			CompactionOverhead:  10,
			OrderedConfigChange: ordered,
		}
		create := func(shardID uint64, replicaID uint64,
			done <-chan struct{}) rsm.IManagedStateMachine {
			return rsm.NewNativeSM(cfg, rsm.NewInMemStateMachine(noopSM), done)
		}
		// node registry
		nr := registry.NewNodeRegistry(settings.Soft.StreamConnections, nil)
		ch := router.getQ(testShardID, i)
		nhConfig := config.NodeHostConfig{RTTMillisecond: tickMillisecond}
		node, err := newNode(peers,
			true,
			cfg,
			nhConfig,
			create,
			snapshotter,
			lr,
			&dummyEngine{},
			nil,
			nil,
			nil,
			router.send,
			nr,
			requestStatePool,
			ldb,
			nil,
			newSysEventListener(nil, nil))
		if err != nil {
			panic(err)
		}
		node.mq = ch
		nodes = append(nodes, node)
		smList = append(smList, node.sm)
	}
	return nodes, smList, router, ldb
}

func step(nodes []*node) bool {
	hasEvent := false
	nodeUpdates := make([]pb.Update, 0)
	activeNodes := make([]*node, 0)
	// step the events, collect all ready structs
	for _, node := range nodes {
		if !node.initialized() {
			commit := rsm.Task{Initial: true}
			ss, _ := node.sm.Recover(commit)
			node.setInitialStatus(ss.Index)
		}
		if node.initialized() {
			hasEvent, err := node.handleEvents()
			if err != nil {
				panic(err)
			}
			if hasEvent {
				ud, ok, err := node.getUpdate()
				if err != nil {
					panic(err)
				}
				if ok {
					nodeUpdates = append(nodeUpdates, ud)
					activeNodes = append(activeNodes, node)
				}
				// quiesce state
				if node.qs.newQuiesceState() {
					node.sendEnterQuiesceMessages()
				}
			}
		}
	}
	// batch the snapshot records together and store them into the logdb
	if err := nodes[0].logdb.SaveSnapshots(nodeUpdates); err != nil {
		panic(err)
	}
	for idx, ud := range nodeUpdates {
		node := activeNodes[idx]
		if err := node.processSnapshot(ud); err != nil {
			panic(err)
		}
		node.applyRaftUpdates(ud)
		node.sendReplicateMessages(ud)
		node.processReadyToRead(ud)
		node.processLeaderUpdate(ud.LeaderUpdate)
	}
	// persistent state and entries are saved first
	// then the snapshot. order can not be changed.
	if err := nodes[0].logdb.SaveRaftState(nodeUpdates, 1); err != nil {
		panic(err)
	}
	for idx, ud := range nodeUpdates {
		node := activeNodes[idx]
		if err := node.processRaftUpdate(ud); err != nil {
			panic(err)
		}
		node.commitRaftUpdate(ud)
		if ud.LastApplied-node.ss.getReqIndex() > node.config.SnapshotEntries {
			if err := node.save(rsm.Task{}); err != nil {
				panic(err)
			}
		}
		rec, err := node.sm.Handle(make([]rsm.Task, 0), nil)
		if err != nil {
			panic(err)
		}
		if rec.IsSnapshotTask() {
			if rec.Recover || rec.Initial {
				if _, err := node.sm.Recover(rec); err != nil {
					panic(err)
				}
			} else if rec.Save {
				if err := node.save(rsm.Task{}); err != nil {
					panic(err)
				}
			}
		}
	}
	return hasEvent
}

func singleStepNodes(nodes []*node, smList []*rsm.StateMachine,
	r *testRouter) {
	for _, node := range nodes {
		tick := node.pendingReadIndexes.getTick() + 1
		tickMsg := pb.Message{Type: pb.LocalTick, To: node.replicaID, Hint: tick}
		tickMsg.ShardID = testShardID
		r.send(tickMsg)
	}
	step(nodes)
}

func stepNodes(nodes []*node, smList []*rsm.StateMachine,
	r *testRouter, ticks uint64) {
	s := ticks + 10
	for i := uint64(0); i < s; i++ {
		for _, node := range nodes {
			tick := node.pendingReadIndexes.getTick() + 1
			tickMsg := pb.Message{
				Type:    pb.LocalTick,
				To:      node.replicaID,
				ShardID: testShardID,
				Hint:    tick,
			}
			r.send(tickMsg)
		}
		step(nodes)
	}
}

func stepNodesUntilThereIsLeader(nodes []*node, smList []*rsm.StateMachine,
	r *testRouter) {
	count := 0
	for {
		stepNodes(nodes, smList, r, 1)
		count++
		if isStableGroup(nodes) {
			stepNodes(nodes, smList, r, 10)
			if isStableGroup(nodes) {
				return
			}
		}
		if count > 200 {
			panic("failed to has any leader after 200 second")
		}
	}
}

func isStableGroup(nodes []*node) bool {
	hasLeader := false
	inElection := false
	for _, node := range nodes {
		if node.isLeader() {
			hasLeader = true
			continue
		}
		if !node.isFollower() {
			inElection = true
		}
	}
	return hasLeader && !inElection
}

func stopNodes(nodes []*node) {
	for _, node := range nodes {
		node.close()
	}
}

func TestNodeCanBeCreatedAndStarted(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	defer cleanupTestDir(fs)
	nodes, smList, router, ldb := getTestRaftNodes(3, false, fs)
	assert.Len(t, nodes, 3)
	assert.Len(t, smList, 3)
	defer stopNodes(nodes)
	defer func() {
		require.NoError(t, ldb.Close())
	}()
	stepNodesUntilThereIsLeader(nodes, smList, router)
}

func getMaxLastApplied(smList []*rsm.StateMachine) uint64 {
	maxLastApplied := uint64(0)
	for _, sm := range smList {
		la := sm.GetLastApplied()
		if la > maxLastApplied {
			maxLastApplied = la
		}
	}
	return maxLastApplied
}

func getProposalTestClient(n *node,
	nodes []*node, smList []*rsm.StateMachine,
	router *testRouter) (*client.Session, bool) {
	cs := client.NewSession(n.shardID, random.NewLockedRand())
	cs.PrepareForRegister()
	rs, err := n.pendingProposals.propose(cs, nil, 50)
	if err != nil {
		plog.Errorf("error: %v", err)
		return nil, false
	}
	stepNodes(nodes, smList, router, 50)
	select {
	case v := <-rs.ResultC():
		if v.Completed() && v.GetResult().Value == cs.ClientID {
			cs.PrepareForPropose()
			return cs, true
		}
		plog.Infof("unknown result/code: %v", v)
	case <-n.stopC:
		plog.Errorf("stopc triggered")
		return nil, false
	}
	plog.Errorf("failed get test client")
	return nil, false
}

func closeProposalTestClient(n *node,
	nodes []*node, smList []*rsm.StateMachine,
	router *testRouter, session *client.Session) {
	session.PrepareForUnregister()
	rs, err := n.pendingProposals.propose(session, nil, 50)
	if err != nil {
		return
	}
	stepNodes(nodes, smList, router, 50)
	select {
	case v := <-rs.ResultC():
		if v.Completed() && v.GetResult().Value == session.ClientID {
			return
		}
	case <-n.stopC:
		return
	}
}

func makeCheckedTestProposal(t *testing.T, session *client.Session,
	data []byte, timeoutInMillisecond uint64,
	nodes []*node, smList []*rsm.StateMachine, router *testRouter,
	expectedCode RequestResultCode, checkResult bool, expectedResult uint64) {
	n := mustHasLeaderNode(nodes, t)
	tick := uint64(50)
	rs, err := n.propose(session, data, tick)
	require.NoError(t, err, "failed to make proposal")

	stepNodes(nodes, smList, router, tick)
	select {
	case v := <-rs.ResultC():
		assert.Equal(t, expectedCode, v.code)
		if checkResult {
			assert.Equal(t, expectedResult, v.GetResult().Value)
		}
	default:
		assert.Fail(t, "failed to complete the proposal")
	}
}

func runRaftNodeTest(t *testing.T, quiesce bool, ordered bool,
	tf func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB), fs vfs.IFS) {
	defer leaktest.AfterTest(t)()
	defer cleanupTestDir(fs)
	nodes, smList, router, ldb := getTestRaftNodes(3, ordered, fs)
	if quiesce {
		for idx := range nodes {
			(nodes[idx]).qs.enabled = true
		}
		for _, node := range nodes {
			assert.False(t, node.qs.quiesced(), "node quiesced on startup")
		}
	}
	stepNodesUntilThereIsLeader(nodes, smList, router)
	require.Len(t, nodes, 3, "failed to get 3 nodes")

	defer stopNodes(nodes)
	defer func() {
		require.NoError(t, ldb.Close())
	}()
	tf(t, nodes, smList, router, ldb)
}

func TestLastAppliedValueCanBeReturned(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		sm := smList[0]
		for i := uint64(5); i <= 100; i++ {
			sm.SetLastApplied(i)
			hasEvent, err := n.handleEvents()
			require.NoError(t, err, "unexpected error %v", err)
			assert.True(t, hasEvent, "handle events reported no event")

			ud, ok, err := n.getUpdate()
			require.NoError(t, err, "unexpected error %v", err)
			if assert.True(t, ok, "no update") {
				assert.Equal(t, i, ud.LastApplied, "last applied value not returned")
			}
			ud.UpdateCommit.LastApplied = 0
			n.p.Commit(ud)
		}
		hasEvents, err := n.handleEvents()
		require.NoError(t, err, "unexpected error %v", err)
		assert.False(t, hasEvents, "unexpected event")

		ud, ok, err := n.getUpdate()
		require.NoError(t, err, "unexpected error %v", err)
		assert.False(t, ok, "unexpected update, %+v", ud)
	}
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestLastAppliedValueIsAlwaysOneWayIncreasing(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		require.Panics(t, func() {
			n := nodes[0]
			sm := smList[0]
			// this will panic because last applied is already > 1 after test setup
			sm.SetLastApplied(1)
			_, err := n.handleEvents()
			require.NoError(t, err)
			_, _, err = n.getUpdate()
			require.NoError(t, err)
		})
	}
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestProposalCanBeMadeWithMessageDrops(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		router.dropRate = 3
		n := mustHasLeaderNode(nodes, t)
		var ok bool
		var session *client.Session
		for i := 0; i < 3; i++ {
			session, ok = getProposalTestClient(n, nodes, smList, router)
			if ok {
				break
			}
		}
		require.NotNil(t, session, "failed to get session")
		for i := 0; i < 20; i++ {
			maxLastApplied := getMaxLastApplied(smList)
			makeCheckedTestProposal(t, session, []byte("test-data"), 4000,
				nodes, smList, router, requestCompleted, false, 0)
			session.ProposalCompleted()
			assert.Equal(t, maxLastApplied+1, getMaxLastApplied(smList), "didn't move the last applied value in smList")
		}
		closeProposalTestClient(n, nodes, smList, router, session)
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestLeaderIDCanBeQueried(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		v, _, ok := n.getLeaderID()
		assert.True(t, ok, "failed to get leader id")
		assert.GreaterOrEqual(t, v, uint64(1), "unexpected leader id %d", v)
		assert.LessOrEqual(t, v, uint64(3), "unexpected leader id %d", v)
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestMembershipCanBeLocallyRead(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		m := n.sm.GetMembership()
		v := m.Addresses
		assert.Len(t, v, 3, "unexpected member count")

		addr1, ok := v[1]
		assert.True(t, ok)
		assert.Equal(t, "peer:12346", addr1)

		addr2, ok := v[2]
		assert.True(t, ok)
		assert.Equal(t, "peer:12347", addr2)

		addr3, ok := v[3]
		assert.True(t, ok)
		assert.Equal(t, "peer:12348", addr3)
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestConfigChangeOnWitnessWillBeRejected(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		n.config.IsWitness = true
		_, err := n.requestConfigChange(pb.AddNode, 100, "noidea:9090", 0, 10)
		assert.Equal(t, ErrInvalidOperation, err, "config change not rejected")
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestReadOnWitnessWillBeRejected(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		n.config.IsWitness = true
		_, err := n.read(10)
		assert.Equal(t, ErrInvalidOperation, err, "read not rejected")
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestMakingProposalOnWitnessNodeWillBeRejected(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		n.config.IsWitness = true
		cs := client.NewNoOPSession(n.shardID, random.NewLockedRand())
		_, err := n.propose(cs, make([]byte, 1), 10)
		assert.Equal(t, ErrInvalidOperation, err, "making proposal not rejected")
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestProposingSessionOnWitnessNodeWillBeRejected(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		n.config.IsWitness = true
		_, err := n.proposeSession(nil, 10)
		assert.Equal(t, ErrInvalidOperation, err, "proposing session not rejected")
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestRequestingSnapshotOnWitnessWillBeRejected(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		n.config.IsWitness = true
		_, err := n.requestSnapshot(SnapshotOption{}, 10)
		assert.Equal(t, ErrInvalidOperation, err, "requesting snapshot not rejected")
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestProposalWithClientSessionCanBeMade(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		session, ok := getProposalTestClient(n, nodes, smList, router)
		require.True(t, ok, "failed to get session")

		data := []byte("test-data")
		maxLastApplied := getMaxLastApplied(smList)
		makeCheckedTestProposal(t, session, data, 4000,
			nodes, smList, router, requestCompleted, true, uint64(len(data)))

		assert.Equal(t, maxLastApplied+1, getMaxLastApplied(smList), "didn't move the last applied value in smList")
		closeProposalTestClient(n, nodes, smList, router, session)
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestProposalWithNotRegisteredClientWillBeRejected(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		session, ok := getProposalTestClient(n, nodes, smList, router)
		require.True(t, ok, "failed to get session")

		session.ClientID = 123456789
		data := []byte("test-data")
		maxLastApplied := getMaxLastApplied(smList)
		makeCheckedTestProposal(t, session, data, 2000,
			nodes, smList, router, requestRejected, true, 0)
		assert.Equal(t, maxLastApplied+1, getMaxLastApplied(smList), "didn't move the last applied value in smList")
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestDuplicatedProposalReturnsTheSameResult(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		session, ok := getProposalTestClient(n, nodes, smList, router)
		require.True(t, ok, "failed to get session")

		data := []byte("test-data")
		maxLastApplied := getMaxLastApplied(smList)
		makeCheckedTestProposal(t, session, data, 2000,
			nodes, smList, router, requestCompleted, true, uint64(len(data)))
		assert.Equal(t, maxLastApplied+1, getMaxLastApplied(smList), "didn't move the last applied value in smList")

		data = []byte("test-data-2")
		maxLastApplied = getMaxLastApplied(smList)
		makeCheckedTestProposal(t, session, data, 2000,
			nodes, smList, router, requestCompleted, true, uint64(len(data)-2))
		assert.Equal(t, maxLastApplied+1, getMaxLastApplied(smList), "didn't move the last applied value in smList")
		closeProposalTestClient(n, nodes, smList, router, session)
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestReproposeRespondedDataWillTimeout(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		session, ok := getProposalTestClient(n, nodes, smList, router)
		require.True(t, ok, "failed to get session")

		data := []byte("test-data")
		maxLastApplied := getMaxLastApplied(smList)
		_, err := n.propose(session, data, 10)
		require.NoError(t, err, "failed to make proposal")

		stepNodes(nodes, smList, router, 10)
		assert.Equal(t, maxLastApplied+1, getMaxLastApplied(smList), "didn't move the last applied value in smList")

		respondedSeriesID := session.SeriesID
		session.ProposalCompleted()
		for i := 0; i < 3; i++ {
			makeCheckedTestProposal(t, session, data, 2000,
				nodes, smList, router, requestCompleted, true, uint64(len(data)))
			session.ProposalCompleted()
			respondedSeriesID = session.RespondedTo
		}
		session.SeriesID = respondedSeriesID
		plog.Infof("series id %d, responded to %d",
			session.SeriesID, session.RespondedTo)
		rs, _ := n.propose(session, data, 10)
		stepNodes(nodes, smList, router, 10)
		select {
		case v := <-rs.ResultC():
			assert.True(t, v.Timeout(), "didn't timeout, v: %d", v.code)
		default:
			assert.Fail(t, "failed to complete the proposal")
		}
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestProposalsWithIllFormedSessionAreChecked(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		s1 := client.NewSession(n.shardID, random.NewLockedRand())
		s1.SeriesID = client.SeriesIDForRegister
		_, err := n.propose(s1, nil, 10)
		assert.Equal(t, ErrInvalidSession, err)

		s1 = client.NewSession(n.shardID, random.NewLockedRand())
		s1.SeriesID = client.SeriesIDForUnregister
		_, err = n.propose(s1, nil, 10)
		assert.Equal(t, ErrInvalidSession, err)

		s1 = client.NewSession(n.shardID, random.NewLockedRand())
		s1.SeriesID = 100
		s1.ShardID = 123456
		_, err = n.propose(s1, nil, 10)
		assert.Equal(t, ErrInvalidSession, err)

		s1 = client.NewSession(n.shardID, random.NewLockedRand())
		s1.SeriesID = 1
		s1.ClientID = 0
		_, err = n.propose(s1, nil, 10)
		assert.Equal(t, ErrInvalidSession, err)
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestProposalsWithCorruptedSessionWillPanic(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		s1 := client.NewSession(n.shardID, random.NewLockedRand())
		s1.SeriesID = 100
		s1.RespondedTo = 200
		require.Panics(t, func() {
			_, err := n.propose(s1, nil, 10)
			require.NoError(t, err, "failed to make proposal %v", err)
		})
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestRaftNodeQuiesceCanBeDisabled(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	// quiesce is disabled by default
	defer cleanupTestDir(fs)
	nodes, smList, router, ldb := getTestRaftNodes(3, false, fs)
	require.Len(t, nodes, 3, "failed to get 3 nodes")

	for _, node := range nodes {
		assert.False(t, node.qs.quiesced(), "node quiesced on startup")
	}
	stepNodesUntilThereIsLeader(nodes, smList, router)
	defer stopNodes(nodes)
	defer func() {
		require.NoError(t, ldb.Close())
	}()
	// need to step more than quiesce.threshold() as the startup
	// config change messages are going to be recorded as activities
	for i := uint64(0); i <= nodes[0].qs.threshold()*2; i++ {
		singleStepNodes(nodes, smList, router)
	}
	for _, node := range nodes {
		assert.False(t, node.qs.quiesced(), "node is quiesced when quiesce is not enabled")
	}
}

func TestNodesCanEnterQuiesce(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		// need to step more than quiesce.threshold() as the startup
		// config change messages are going to be recorded as activities
		for i := uint64(0); i <= nodes[0].qs.threshold()*2; i++ {
			singleStepNodes(nodes, smList, router)
		}
		for _, node := range nodes {
			assert.True(t, node.qs.quiesced(), "node failed to enter quiesced")
		}
		// step more, nodes should stay in quiesce state.
		for i := uint64(0); i <= nodes[0].qs.threshold()*3; i++ {
			singleStepNodes(nodes, smList, router)
		}
		for _, node := range nodes {
			assert.True(t, node.qs.quiesced(), "node failed to enter quiesced")
		}
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, true, false, tf, fs)
}

func TestNodesCanExitQuiesceByMakingProposal(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		// need to step more than quiesce.threshold() as the startup
		// config change messages are going to be recorded as activities
		for i := uint64(0); i <= nodes[0].qs.threshold()*2; i++ {
			singleStepNodes(nodes, smList, router)
		}
		for _, node := range nodes {
			assert.True(t, node.qs.quiesced(), "node failed to enter quiesced")
		}
		n := nodes[0]
		done := false
		for i := 0; i < 5; i++ {
			_, ok := getProposalTestClient(n, nodes, smList, router)
			if ok {
				done = true
				break
			}
		}
		assert.True(t, done, "failed to get proposal client -- didn't exit from quiesce?")
		for i := uint64(0); i <= 3; i++ {
			singleStepNodes(nodes, smList, router)
		}
		for _, node := range nodes {
			assert.False(t, node.qs.quiesced(), "node failed to exit from quiesced")
		}
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, true, false, tf, fs)
}

func TestNodesCanExitQuiesceByReadIndex(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		// need to step more than quiesce.threshold() as the startup
		// config change messages are going to be recorded as activities
		for i := uint64(0); i <= nodes[0].qs.threshold()*2; i++ {
			singleStepNodes(nodes, smList, router)
		}
		for _, node := range nodes {
			assert.True(t, node.qs.quiesced(), "node failed to enter quiesced")
		}
		n := nodes[0]
		rs, err := n.read(10)
		require.NoError(t, err, "failed to read")

		var done bool
		for i := uint64(0); i <= 5; i++ {
			singleStepNodes(nodes, smList, router)
			select {
			case <-rs.ResultC():
				done = true
			default:
			}
			if done {
				break
			}
		}
		for _, node := range nodes {
			assert.False(t, node.qs.quiesced(), "node failed to exit from quiesced")
		}
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, true, false, tf, fs)
}

func TestNodesCanExitQuiesceByConfigChange(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		// need to step more than quiesce.threshold() as the startup
		// config change messages are going to be recorded as activities
		for i := uint64(0); i <= nodes[0].qs.threshold()*2; i++ {
			singleStepNodes(nodes, smList, router)
		}
		for _, node := range nodes {
			assert.True(t, node.qs.quiesced(), "node failed to enter quiesced")
		}
		n := nodes[0]
		done := false
		for i := 0; i < 5; i++ {
			rs, err := n.requestAddNodeWithOrderID(24680, "localhost:12345", 0, 10)
			require.NoError(t, err, "request to add node failed, %v", err)

			hasResp := false
			for i := uint64(0); i < 25; i++ {
				singleStepNodes(nodes, smList, router)
				select {
				case v := <-rs.ResultC():
					if v.Completed() {
						done = true
					}
					hasResp = true
				default:
					continue
				}
			}
			if !assert.True(t, hasResp, "config change timeout not fired") {
				return
			}
			if done {
				break
			}
		}
		for i := uint64(0); i < 20; i++ {
			singleStepNodes(nodes, smList, router)
		}
		for _, node := range nodes {
			assert.False(t, node.qs.quiesced(), "node failed to exit from quiesced")
		}
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, true, false, tf, fs)
}

func TestLinearizableReadCanBeMade(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		session, ok := getProposalTestClient(n, nodes, smList, router)
		require.True(t, ok, "failed to get session")

		rs, err := n.propose(session, []byte("test-data"), 10)
		require.NoError(t, err, "failed to make proposal")
		stepNodes(nodes, smList, router, 10)
		mustComplete(rs, t)
		closeProposalTestClient(n, nodes, smList, router, session)

		rs, err = n.read(10)
		require.NoError(t, err)
		require.NotNil(t, rs.node, "rs.node not set")

		stepNodes(nodes, smList, router, 10)
		mustComplete(rs, t)
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func testNodeCanBeAdded(t *testing.T, fs vfs.IFS) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		router.dropRate = 3
		n := mustHasLeaderNode(nodes, t)
		rs, err := n.requestAddNodeWithOrderID(4, "a4:4", 0, 10)
		require.NoError(t, err, "request to delete node failed")
		stepNodes(nodes, smList, router, 10)
		mustComplete(rs, t)
		for _, node := range nodes {
			assert.True(t, sliceEqual([]uint64{1, 2, 3, 4}, getMemberNodes(node.sm)),
				"failed to delete the node, %v", getMemberNodes(node.sm))
		}
	}
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestNodeCanBeAddedWithMessageDrops(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	for i := 0; i < 10; i++ {
		testNodeCanBeAdded(t, fs)
	}
}

func TestNodeCanBeDeleted(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		rs, err := n.requestDeleteNodeWithOrderID(2, 0, 10)
		require.NoError(t, err, "request to delete node failed")
		stepNodes(nodes, smList, router, 10)
		mustComplete(rs, t)
		assert.False(t, nodes[0].stopped(), "node id 1 is not suppose to be in stopped state")
		assert.True(t, nodes[1].stopped(), "node is not stopped")
		assert.False(t, nodes[2].stopped(), "node id 3 is not suppose to be in stopped state")
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func sliceEqual(s1 []uint64, s2 []uint64) bool {
	if len(s1) != len(s2) {
		return false
	}
	sort.Slice(s1, func(i, j int) bool { return s1[i] < s1[j] })
	sort.Slice(s2, func(i, j int) bool { return s2[i] < s2[j] })
	for idx, v := range s1 {
		if v != s2[idx] {
			return false
		}
	}
	return true
}

func TestNodeCanBeAdded2(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		fs := vfs.GetTestFS()
		n := nodes[0]
		session, ok := getProposalTestClient(n, nodes, smList, router)
		require.True(t, ok, "failed to get session")

		for i := 0; i < 5; i++ {
			rs, err := n.propose(session, []byte("test-data"), 10)
			require.NoError(t, err)
			stepNodes(nodes, smList, router, 10)
			mustComplete(rs, t)
			session.ProposalCompleted()
		}
		closeProposalTestClient(n, nodes, smList, router, session)

		rs, err := n.requestAddNodeWithOrderID(4, "a4:4", 0, 10)
		require.NoError(t, err, "request to add node failed")
		stepNodes(nodes, smList, router, 10)
		mustComplete(rs, t)
		for _, node := range nodes {
			assert.False(t, node.stopped(), "node %d is stopped, this is unexpected", node.replicaID)
			assert.True(t, sliceEqual([]uint64{1, 2, 3, 4}, getMemberNodes(node.sm)), "node members not expected: %v", getMemberNodes(node.sm))
		}
		// now bring the node 5 online
		newNodes, newSMList, newRouter, _ := doGetTestRaftNodes(4, 1, true, ldb, fs)
		require.Len(t, newNodes, 1, "failed to get 1 nodes")

		router.addQ(4, newRouter.qm[4])
		nodes = append(nodes, newNodes[0])
		smList = append(smList, newSMList[0])
		nodes[3].sendRaftMessage = router.send
		stepNodes(nodes, smList, router, 100)
		assert.Equal(t, smList[0].GetLastApplied(), newSMList[0].GetLastApplied(), "last applied not equal")
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestNodeCanBeAddedWhenOrderIsEnforced(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		rs, err := n.requestAddNodeWithOrderID(5, "a5:5", 0, 10)
		require.NoError(t, err, "request to add node failed")
		stepNodes(nodes, smList, router, 10)
		mustReject(rs, t)
		for _, node := range nodes {
			assert.False(t, node.stopped(), "node %d is stopped, this is unexpected", node.replicaID)
			assert.True(t, sliceEqual([]uint64{1, 2, 3}, getMemberNodes(node.sm)), "node members not expected: %v", getMemberNodes(node.sm))
		}

		m := n.sm.GetMembership()
		ccid := m.ConfigChangeId
		rs, err = n.requestAddNodeWithOrderID(5, "a5:5", ccid, 10)
		require.NoError(t, err, "request to add node failed")
		stepNodes(nodes, smList, router, 10)
		mustComplete(rs, t)
		for _, node := range nodes {
			assert.False(t, node.stopped(), "node %d is stopped, this is unexpected", node.replicaID)
			assert.True(t, sliceEqual([]uint64{1, 2, 3, 5}, getMemberNodes(node.sm)), "node members not expected: %v", getMemberNodes(node.sm))
		}
	}
	runRaftNodeTest(t, false, true, tf, fs)
}

func TestNodeCanBeDeletedWhenOrderIsEnforced(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		rs, err := n.requestDeleteNodeWithOrderID(2, 0, 10)
		require.NoError(t, err, "request to delete node failed")
		stepNodes(nodes, smList, router, 10)
		mustReject(rs, t)
		for _, node := range nodes {
			assert.False(t, node.stopped(), "node %d is stopped, this is unexpected", node.replicaID)
			assert.True(t, sliceEqual([]uint64{1, 2, 3}, getMemberNodes(node.sm)), "node members not expected: %v", getMemberNodes(node.sm))
		}
		m := n.sm.GetMembership()
		ccid := m.ConfigChangeId
		rs, err = n.requestDeleteNodeWithOrderID(2, ccid, 10)
		require.NoError(t, err, "request to add node failed")
		stepNodes(nodes, smList, router, 10)
		mustComplete(rs, t)
		for _, node := range nodes {
			if node.replicaID == 2 {
				continue
			}
			assert.True(t, sliceEqual([]uint64{1, 3}, getMemberNodes(node.sm)), "node members not expected: %v", getMemberNodes(node.sm))
		}
	}
	runRaftNodeTest(t, false, true, tf, fs)
}

func getSnapshotFileCount(dir string, fs vfs.IFS) (int, error) {
	fiList, err := fs.List(dir)
	if err != nil {
		return 0, err
	}
	count := 0
	for _, fn := range fiList {
		fi, err := fs.Stat(fs.PathJoin(dir, fn))
		if err != nil {
			return 0, err
		}
		if !fi.IsDir() {
			continue
		}
		if strings.HasPrefix(fi.Name(), "snapshot-") {
			count++
		}
	}
	return count, nil
}

func TestSnapshotCanBeMade(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		session, ok := getProposalTestClient(n, nodes, smList, router)
		require.True(t, ok, "failed to get session")

		maxLastApplied := getMaxLastApplied(smList)
		proposalCount := 50
		for i := 0; i < proposalCount; i++ {
			data := fmt.Sprintf("test-data-%d", i)
			rs, err := n.propose(session, []byte(data), 10)
			require.NoError(t, err, "failed to make proposal")
			stepNodes(nodes, smList, router, 10)
			mustComplete(rs, t)
			session.ProposalCompleted()
		}
		assert.Equal(t, maxLastApplied+uint64(proposalCount), getMaxLastApplied(smList), "not all %d proposals applied", proposalCount)
		closeProposalTestClient(n, nodes, smList, router, session)
		// check we do have snapshots saved on disk
		for _, node := range nodes {
			sd := fmt.Sprintf(snapDir, testShardID, node.replicaID)
			dir := fs.PathJoin(raftTestTopDir, sd)
			count, err := getSnapshotFileCount(dir, fs)
			require.NoError(t, err, "failed to get snapshot count")
			assert.NotZero(t, count, "no snapshot image")
		}
	}
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestSnapshotCanBeMadeTwice(t *testing.T) {
	tf := func(t *testing.T, nodes []*node,
		smList []*rsm.StateMachine, router *testRouter, ldb raftio.ILogDB) {
		n := nodes[0]
		session, ok := getProposalTestClient(n, nodes, smList, router)
		require.True(t, ok, "failed to get session")

		maxLastApplied := getMaxLastApplied(smList)
		proposalCount := 50
		for i := 0; i < proposalCount; i++ {
			data := fmt.Sprintf("test-data-%d", i)
			rs, err := n.propose(session, []byte(data), 10)
			require.NoError(t, err, "failed to make proposal")
			stepNodes(nodes, smList, router, 10)
			mustComplete(rs, t)
			session.ProposalCompleted()
		}
		assert.Equal(t, maxLastApplied+uint64(proposalCount), getMaxLastApplied(smList), "not all %d proposals applied", proposalCount)
		closeProposalTestClient(n, nodes, smList, router, session)
		// check we do have snapshots saved on disk
		for _, node := range nodes {
			require.NoError(t, node.save(rsm.Task{}))
			require.NoError(t, node.save(rsm.Task{}))
		}
	}
	fs := vfs.GetTestFS()
	runRaftNodeTest(t, false, false, tf, fs)
}

func TestNodesCanBeRestarted(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	defer cleanupTestDir(fs)
	nodes, smList, router, ldb := getTestRaftNodes(3, false, fs)
	require.Len(t, nodes, 3, "failed to get 3 nodes")

	stepNodesUntilThereIsLeader(nodes, smList, router)
	n := mustHasLeaderNode(nodes, t)
	session, ok := getProposalTestClient(n, nodes, smList, router)
	require.True(t, ok, "failed to get session")

	maxLastApplied := getMaxLastApplied(smList)
	for i := 0; i < 25; i++ {
		rs, err := n.propose(session, []byte("test-data"), 10)
		require.NoError(t, err)
		stepNodes(nodes, smList, router, 10)
		mustComplete(rs, t)
		session.ProposalCompleted()
	}
	assert.Equal(t, maxLastApplied+25, getMaxLastApplied(smList), "not all 25 proposals applied")
	closeProposalTestClient(n, nodes, smList, router, session)
	for _, node := range nodes {
		sd := fmt.Sprintf(snapDir, testShardID, node.replicaID)
		dir := fs.PathJoin(raftTestTopDir, sd)
		count, err := getSnapshotFileCount(dir, fs)
		require.NoError(t, err, "failed to get snapshot count")
		require.NotZero(t, count, "no snapshot available, count: %d", count)
	}
	// stop the whole thing
	for _, node := range nodes {
		node.close()
	}
	require.NoError(t, ldb.Close())
	// restart
	nodes, smList, router, ldb = getTestRaftNodes(3, false, fs)
	defer stopNodes(nodes)
	defer func() {
		require.NoError(t, ldb.Close())
	}()
	require.Len(t, nodes, 3, "failed to get 3 nodes")
	stepNodesUntilThereIsLeader(nodes, smList, router)
	stepNodes(nodes, smList, router, 100)
	assert.GreaterOrEqual(t, getMaxLastApplied(smList), maxLastApplied+5,
		"not recovered from snapshot, got %d, marker %d", getMaxLastApplied(smList), maxLastApplied+5)
}

func TestGetTimeoutMillisecondFromContext(t *testing.T) {
	defer leaktest.AfterTest(t)()
	_, err := getTimeoutFromContext(context.Background())
	assert.Equal(t, ErrDeadlineNotSet, err)

	d := time.Now()
	time.Sleep(100 * time.Millisecond)
	ctx, cancel := context.WithDeadline(context.Background(), d)
	defer cancel()
	_, err = getTimeoutFromContext(ctx)
	assert.Equal(t, ErrInvalidDeadline, err)

	ctx, cancel = context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()
	v, err := getTimeoutFromContext(ctx)
	require.NoError(t, err)

	timeout := v.Milliseconds()
	assert.Greater(t, timeout, int64(4500), "v %d, want (4500,5000]", timeout)
	assert.LessOrEqual(t, timeout, int64(5000), "v %d, want (4500,5000]", timeout)
}

func TestPayloadTooBig(t *testing.T) {
	tests := []struct {
		maxInMemLogSize uint64
		payloadSize     uint64
		tooBig          bool
	}{
		{0, 1, false},
		{0, 1024 * 1024 * 1024, false},
		{settings.EntryNonCmdFieldsSize + 1, 1, false},
		{settings.EntryNonCmdFieldsSize + 1, 2, true},
		{settings.EntryNonCmdFieldsSize * 2, settings.EntryNonCmdFieldsSize, false},
		{settings.EntryNonCmdFieldsSize * 2, settings.EntryNonCmdFieldsSize + 1, true},
	}
	for idx, tt := range tests {
		cfg := config.Config{
			ReplicaID:       1,
			HeartbeatRTT:    1,
			ElectionRTT:     10,
			MaxInMemLogSize: tt.maxInMemLogSize,
		}
		require.NoError(t, cfg.Validate(), "invalid cfg for test %d", idx)
		n := node{config: cfg}
		assert.Equal(t, tt.tooBig, n.payloadTooBig(int(tt.payloadSize)), "test index %d", idx)
	}
}

//
// node states
//

type dummyPipeline struct{}

func (d *dummyPipeline) setCloseReady(*node)            {}
func (d *dummyPipeline) setStepReady(shardID uint64)    {}
func (d *dummyPipeline) setCommitReady(shardID uint64)  {}
func (d *dummyPipeline) setApplyReady(shardID uint64)   {}
func (d *dummyPipeline) setStreamReady(shardID uint64)  {}
func (d *dummyPipeline) setSaveReady(shardID uint64)    {}
func (d *dummyPipeline) setRecoverReady(shardID uint64) {}

func TestProcessUninitilizedNode(t *testing.T) {
	n := &node{ss: snapshotState{}, pipeline: &dummyPipeline{}}
	assert.True(t, n.processUninitializedNodeStatus(), "failed to return the recover request")
	assert.True(t, n.ss.recovering(), "not in recovering mode")

	req, ok := n.ss.getRecoverReq()
	require.True(t, ok, "failed to set recover req")
	assert.True(t, req.Initial)
	assert.True(t, req.Recover)

	n2 := &node{ss: snapshotState{}, initializedC: make(chan struct{})}
	n2.setInitialized()
	assert.False(t, n2.processUninitializedNodeStatus(), "unexpected recover from snapshot request")
}

func TestProcessRecoveringNodeCanBeSkipped(t *testing.T) {
	n := &node{ss: snapshotState{}}
	assert.False(t, n.processRecoverStatus(), "processRecoveringNode not skipped")
}

func TestProcessTakingSnapshotNodeCanBeSkipped(t *testing.T) {
	n := &node{ss: snapshotState{}}
	assert.False(t, n.processSaveStatus(), "processTakingSnapshotNode not skipped")
}

func TestRecoveringFromSnapshotNodeCanComplete(t *testing.T) {
	n := &node{
		ss:           snapshotState{},
		sysEvents:    newSysEventListener(nil, nil),
		initializedC: make(chan struct{}),
	}
	n.ss.setRecovering()
	n.ss.notifySnapshotStatus(false, true, false, true, 100)
	assert.False(t, n.processRecoverStatus(), "node unexpectedly skipped")
	assert.False(t, n.ss.recovering(), "still recovering")
	assert.True(t, n.initialized(), "not marked as initialized")
	assert.Equal(t, uint64(100), n.ss.snapshotIndex, "unexpected snapshot index")
}

func TestNotReadyRecoveringFromSnapshotNode(t *testing.T) {
	n := &node{ss: snapshotState{}, sysEvents: newSysEventListener(nil, nil)}
	n.ss.setRecovering()
	assert.True(t, n.processRecoverStatus(), "not skipped")
}

func TestTakingSnapshotNodeCanComplete(t *testing.T) {
	n := &node{ss: snapshotState{}, initializedC: make(chan struct{})}
	n.ss.setSaving()
	n.ss.notifySnapshotStatus(true, false, false, false, 0)
	n.setInitialized()
	assert.False(t, n.processSaveStatus(), "node unexpectedly skipped")
	assert.False(t, n.ss.saving(), "still taking snapshot")
}

func TestTakingSnapshotOnUninitializedNodeWillPanic(t *testing.T) {
	require.Panics(t, func() {
		n := &node{ss: snapshotState{}}
		n.ss.setSaving()
		n.ss.notifySnapshotStatus(true, false, false, false, 0)
		n.processSaveStatus()
	})
}

func TestGetCompactionIndex(t *testing.T) {
	cfg := config.Config{
		CompactionOverhead: 234,
	}
	n := node{config: cfg}
	req1 := rsm.SSRequest{
		OverrideCompaction: true,
		CompactionOverhead: 123,
	}
	req2 := rsm.SSRequest{
		OverrideCompaction: false,
		CompactionOverhead: 456,
	}
	req3 := rsm.SSRequest{
		OverrideCompaction: true,
		CompactionIndex:    300,
	}

	v1, _ := n.getCompactionIndex(req1, 200)
	assert.Equal(t, uint64(77), v1, "snapshot overhead override not applied")

	v2, _ := n.getCompactionIndex(req2, 500)
	assert.Equal(t, uint64(266), v2, "snapshot overhead override unexpectedly applied")

	v3, _ := n.getCompactionIndex(req3, 500)
	assert.Equal(t, uint64(300), v3, "snapshot index not correctly set")

	v4, ok := n.getCompactionIndex(req3, 299)
	assert.False(t, ok)
	assert.Zero(t, v4, "snapshot index unexpectedly set")
}

type testDummyNodeProxy struct{}

func (np *testDummyNodeProxy) StepReady()                                            {}
func (np *testDummyNodeProxy) RestoreRemotes(pb.Snapshot) error                      { return nil }
func (np *testDummyNodeProxy) ApplyUpdate(pb.Entry, sm.Result, bool, bool, bool)     {}
func (np *testDummyNodeProxy) ApplyConfigChange(pb.ConfigChange, uint64, bool) error { return nil }
func (np *testDummyNodeProxy) ReplicaID() uint64                                     { return 1 }
func (np *testDummyNodeProxy) ShardID() uint64                                       { return 1 }
func (np *testDummyNodeProxy) ShouldStop() <-chan struct{}                           { return nil }

func TestNotReadyTakingSnapshotNodeIsSkippedWhenConcurrencyIsNotSupported(t *testing.T) {
	fs := vfs.GetTestFS()
	n := &node{ss: snapshotState{}, initializedC: make(chan struct{})}
	config := config.Config{ShardID: 1, ReplicaID: 1}
	n.sm = rsm.NewStateMachine(
		rsm.NewNativeSM(config, &rsm.InMemStateMachine{}, nil),
		nil, config, &testDummyNodeProxy{}, fs)

	assert.False(t, n.concurrentSnapshot(), "concurrency not suppose to be supported")
	n.ss.setSaving()
	n.setInitialized()
	require.True(t, n.processSaveStatus(), "node not skipped")
}

func TestNotReadyTakingSnapshotConcurrentNodeIsNotSkipped(t *testing.T) {
	fs := vfs.GetTestFS()
	n := &node{ss: snapshotState{}, initializedC: make(chan struct{})}
	config := config.Config{ShardID: 1, ReplicaID: 1}
	n.sm = rsm.NewStateMachine(
		rsm.NewNativeSM(config, &rsm.ConcurrentStateMachine{}, nil),
		nil, config, &testDummyNodeProxy{}, fs)

	assert.True(t, n.concurrentSnapshot(), "concurrency not supported")
	n.ss.setSaving()
	n.setInitialized()
	assert.False(t, n.processSaveStatus(), "node unexpectedly skipped")
}

func TestIsWitnessNode(t *testing.T) {
	n1 := node{config: config.Config{}}
	assert.False(t, n1.isWitness(), "not expect to be witness")
	n2 := node{config: config.Config{IsWitness: true}}
	assert.True(t, n2.isWitness(), "not reported as witness")
}

func TestSaveSnapshotAborted(t *testing.T) {
	tests := []struct {
		err     error
		aborted bool
	}{
		{sm.ErrSnapshotStopped, true},
		{sm.ErrSnapshotAborted, true},
		{nil, false},
		{sm.ErrSnapshotStreaming, false},
	}

	for idx, tt := range tests {
		assert.Equal(t, tt.aborted, saveAborted(tt.err), "test index %d", idx)
	}
}

func TestLogDBMetrics(t *testing.T) {
	l := logDBMetrics{}
	l.update(true)
	assert.True(t, l.isBusy())
	l.update(false)
	assert.False(t, l.isBusy())
}

func TestUninitializedNodeNotAllowedToMakeRequests(t *testing.T) {
	n := node{}
	require.False(t, n.initialized())

	_, err := n.propose(nil, nil, 1)
	assert.Equal(t, ErrShardNotReady, err)

	_, err = n.proposeSession(nil, 1)
	assert.Equal(t, ErrShardNotReady, err)

	_, err = n.read(1)
	assert.Equal(t, ErrShardNotReady, err)

	err = n.requestLeaderTransfer(1)
	assert.Equal(t, ErrShardNotReady, err)

	_, err = n.requestSnapshot(SnapshotOption{}, 1)
	assert.Equal(t, ErrShardNotReady, err)

	_, err = n.requestConfigChange(pb.ConfigChangeType(0),
		1, "localhost:1", 1, 1)
	assert.Equal(t, ErrShardNotReady, err)
}

func TestEntriesToApply(t *testing.T) {
	tests := []struct {
		inputIndex   uint64
		inputLength  uint64
		crash        bool
		resultIndex  uint64
		resultLength uint64
	}{
		{1, 5, true, 0, 0},
		{1, 10, true, 0, 0},
		{1, 11, false, 11, 1},
		{1, 20, false, 11, 10},
		{10, 6, false, 11, 5},
		{11, 5, false, 11, 5},
		{12, 5, true, 0, 0},
	}
	for idx, tt := range tests {
		t.Run(fmt.Sprintf("test-%d", idx), func(t *testing.T) {
			testFunc := func() {
				inputs := make([]pb.Entry, 0)
				for i := tt.inputIndex; i < tt.inputIndex+tt.inputLength; i++ {
					inputs = append(inputs, pb.Entry{Index: i})
				}
				n := &node{pushedIndex: 10}
				results := pb.EntriesToApply(inputs, n.pushedIndex, true)

				assert.Len(t, results, int(tt.resultLength))
				if len(results) > 0 {
					assert.Equal(t, tt.resultIndex, results[0].Index)
				}
			}

			if tt.crash {
				require.Panics(t, testFunc)
			} else {
				require.NotPanics(t, testFunc)
			}
		})
	}
}
````

## File: node.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"sync"
	"sync/atomic"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/logutil"

	"github.com/lni/dragonboat/v4/client"
	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/logdb"
	"github.com/lni/dragonboat/v4/internal/raft"
	"github.com/lni/dragonboat/v4/internal/rsm"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/transport"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
)

var (
	incomingProposalsMaxLen = settings.Soft.IncomingProposalQueueLength
	incomingReadIndexMaxLen = settings.Soft.IncomingReadIndexQueueLength
	syncTaskInterval        = settings.Soft.SyncTaskInterval
	lazyFreeCycle           = settings.Soft.LazyFreeCycle
)

type pipeline interface {
	setCloseReady(*node)
	setStepReady(shardID uint64)
	setCommitReady(shardID uint64)
	setApplyReady(shardID uint64)
	setStreamReady(shardID uint64)
	setSaveReady(shardID uint64)
	setRecoverReady(shardID uint64)
}

type logDBMetrics struct {
	busy int32
}

func (l *logDBMetrics) update(busy bool) {
	v := int32(0)
	if busy {
		v = int32(1)
	}
	atomic.StoreInt32(&l.busy, v)
}

func (l *logDBMetrics) isBusy() bool {
	return atomic.LoadInt32(&l.busy) != 0
}

type leaderInfo struct {
	leaderID uint64
	term     uint64
}

type node struct {
	shardInfo             atomic.Value
	leaderInfo            atomic.Value
	nodeRegistry          raftio.INodeRegistry
	logdb                 raftio.ILogDB
	pipeline              pipeline
	getStreamSink         func(uint64, uint64) *transport.Sink
	ss                    snapshotState
	configChangeC         <-chan configChangeRequest
	snapshotC             <-chan rsm.SSRequest
	toApplyQ              *rsm.TaskQueue
	toCommitQ             *rsm.TaskQueue
	syncTask              task
	metrics               *logDBMetrics
	stopC                 chan struct{}
	sysEvents             *sysEventListener
	raftEvents            *raftEventListener
	handleSnapshotStatus  func(uint64, uint64, bool)
	sendRaftMessage       func(pb.Message)
	validateTarget        func(string) bool
	sm                    *rsm.StateMachine
	incomingReadIndexes   *readIndexQueue
	incomingProposals     *entryQueue
	snapshotLock          sync.Mutex
	pendingProposals      pendingProposal
	pendingReadIndexes    pendingReadIndex
	pendingConfigChange   pendingConfigChange
	pendingSnapshot       pendingSnapshot
	pendingLeaderTransfer pendingLeaderTransfer
	pendingRaftLogQuery   pendingRaftLogQuery
	initializedC          chan struct{}
	p                     raft.Peer
	logReader             *logdb.LogReader
	snapshotter           *snapshotter
	mq                    *server.MessageQueue
	qs                    *quiesceState
	raftAddress           string
	config                config.Config
	currentTick           uint64
	gcTick                uint64
	appliedIndex          uint64
	pushedIndex           uint64
	confirmedIndex        uint64
	tickMillisecond       uint64
	shardID               uint64
	replicaID             uint64
	instanceID            uint64
	initializedFlag       uint64
	closeOnce             sync.Once
	raftMu                sync.Mutex
	new                   bool
	logDBLimited          bool
	rateLimited           bool
	notifyCommit          bool
}

var _ rsm.INode = (*node)(nil)

var instanceID uint64

func newNode(peers map[uint64]string,
	initialMember bool,
	config config.Config,
	nhConfig config.NodeHostConfig,
	createSM rsm.ManagedStateMachineFactory,
	snapshotter *snapshotter,
	logReader *logdb.LogReader,
	pipeline pipeline,
	liQueue *leaderInfoQueue,
	getStreamSink func(uint64, uint64) *transport.Sink,
	handleSnapshotStatus func(uint64, uint64, bool),
	sendMessage func(pb.Message),
	nodeRegistry raftio.INodeRegistry,
	pool *sync.Pool,
	ldb raftio.ILogDB,
	metrics *logDBMetrics,
	sysEvents *sysEventListener) (*node, error) {
	notifyCommit := nhConfig.NotifyCommit
	proposals := newEntryQueue(incomingProposalsMaxLen, lazyFreeCycle)
	readIndexes := newReadIndexQueue(incomingReadIndexMaxLen)
	configChangeC := make(chan configChangeRequest, 1)
	snapshotC := make(chan rsm.SSRequest, 1)
	stopC := make(chan struct{})
	mq := server.NewMessageQueue(receiveQueueLen,
		false, lazyFreeCycle, nhConfig.MaxReceiveQueueSize)
	rn := &node{
		shardID:               config.ShardID,
		replicaID:             config.ReplicaID,
		raftAddress:           nhConfig.RaftAddress,
		instanceID:            atomic.AddUint64(&instanceID, 1),
		tickMillisecond:       nhConfig.RTTMillisecond,
		config:                config,
		incomingProposals:     proposals,
		incomingReadIndexes:   readIndexes,
		configChangeC:         configChangeC,
		snapshotC:             snapshotC,
		pipeline:              pipeline,
		getStreamSink:         getStreamSink,
		handleSnapshotStatus:  handleSnapshotStatus,
		stopC:                 stopC,
		pendingProposals:      newPendingProposal(config, notifyCommit, pool, proposals),
		pendingReadIndexes:    newPendingReadIndex(pool, readIndexes),
		pendingConfigChange:   newPendingConfigChange(configChangeC, notifyCommit),
		pendingSnapshot:       newPendingSnapshot(snapshotC),
		pendingLeaderTransfer: newPendingLeaderTransfer(),
		pendingRaftLogQuery:   newPendingRaftLogQuery(),
		nodeRegistry:          nodeRegistry,
		snapshotter:           snapshotter,
		logReader:             logReader,
		sendRaftMessage:       sendMessage,
		mq:                    mq,
		logdb:                 ldb,
		syncTask:              newTask(syncTaskInterval),
		sysEvents:             sysEvents,
		notifyCommit:          notifyCommit,
		metrics:               metrics,
		initializedC:          make(chan struct{}),
		ss:                    snapshotState{},
		validateTarget:        nhConfig.GetTargetValidator(),
		qs: &quiesceState{
			electionTick: config.ElectionRTT * 2,
			enabled:      config.Quiesce,
			shardID:      config.ShardID,
			replicaID:    config.ReplicaID,
		},
	}
	ds := createSM(config.ShardID, config.ReplicaID, stopC)
	sm := rsm.NewStateMachine(ds, snapshotter, config, rn, snapshotter.fs)
	if notifyCommit {
		rn.toCommitQ = rsm.NewTaskQueue()
	}
	rn.toApplyQ = sm.TaskQ()
	rn.sm = sm
	rn.raftEvents = newRaftEventListener(config.ShardID,
		config.ReplicaID, nhConfig.EnableMetrics, liQueue)
	new, err := rn.startRaft(config, peers, initialMember)
	if err != nil {
		return nil, err
	}
	rn.new = new
	return rn, nil
}

func (n *node) ReplicaID() uint64 {
	return n.replicaID
}

func (n *node) ShardID() uint64 {
	return n.shardID
}

func (n *node) ShouldStop() <-chan struct{} {
	return n.stopC
}

func (n *node) StepReady() {
	n.pipeline.setStepReady(n.shardID)
}

func (n *node) applyReady() {
	n.pipeline.setApplyReady(n.shardID)
}

func (n *node) commitReady() {
	n.pipeline.setCommitReady(n.shardID)
}

func (n *node) ApplyUpdate(e pb.Entry,
	result sm.Result, rejected bool, ignored bool, notifyRead bool) {
	if n.isWitness() {
		return
	}
	if notifyRead {
		n.pendingReadIndexes.applied(e.Index)
	}
	if !ignored {
		if e.Key == 0 {
			plog.Panicf("key is 0")
		}
		n.pendingProposals.applied(e.ClientID, e.SeriesID, e.Key, result, rejected)
	}
}

func (n *node) ApplyConfigChange(cc pb.ConfigChange,
	key uint64, rejected bool) error {
	n.raftMu.Lock()
	defer n.raftMu.Unlock()
	if !rejected {
		if err := n.applyConfigChange(cc); err != nil {
			return err
		}
	}
	return n.configChangeProcessed(key, rejected)
}

func (n *node) applyConfigChange(cc pb.ConfigChange) error {
	if err := n.p.ApplyConfigChange(cc); err != nil {
		return err
	}
	switch cc.Type {
	case pb.AddNode, pb.AddNonVoting, pb.AddWitness:
		n.nodeRegistry.Add(n.shardID, cc.ReplicaID, cc.Address)
	case pb.RemoveNode:
		if cc.ReplicaID == n.replicaID {
			plog.Infof("%s applied ConfChange Remove for itself", n.id())
			n.nodeRegistry.RemoveShard(n.shardID)
			n.requestRemoval()
			n.notifySelfRemove()
		} else {
			n.nodeRegistry.Remove(n.shardID, cc.ReplicaID)
		}
	default:
		plog.Panicf("unknown config change type, %s", cc.Type)
	}
	return nil
}

func (n *node) configChangeProcessed(key uint64, rejected bool) error {
	if n.isWitness() {
		return nil
	}
	if rejected {
		if err := n.p.RejectConfigChange(); err != nil {
			return err
		}
	} else {
		n.notifyConfigChange()
	}
	n.pendingConfigChange.apply(key, rejected)
	return nil
}

func (n *node) processLeaderUpdate(u pb.LeaderUpdate) {
	if u.Term == 0 {
		return
	}
	leaderInfo := &leaderInfo{
		leaderID: u.LeaderID,
		term:     u.Term,
	}
	n.leaderInfo.Store(leaderInfo)
}

func (n *node) processLogQuery(r pb.LogQueryResult) {
	if r.IsEmpty() {
		return
	}
	outOfRange := false
	if r.Error != nil {
		if errors.Is(r.Error, raft.ErrCompacted) {
			outOfRange = true
		} else {
			panic(r.Error)
		}
	}
	n.pendingRaftLogQuery.returned(outOfRange,
		LogRange{FirstIndex: r.FirstIndex, LastIndex: r.LastIndex}, r.Entries)
}

func (n *node) RestoreRemotes(snapshot pb.Snapshot) error {
	if snapshot.Membership.ConfigChangeId == 0 {
		plog.Panicf("invalid ConfChangeId")
	}
	n.raftMu.Lock()
	defer n.raftMu.Unlock()
	for nid, addr := range snapshot.Membership.Addresses {
		n.nodeRegistry.Add(n.shardID, nid, addr)
	}
	for nid, addr := range snapshot.Membership.NonVotings {
		n.nodeRegistry.Add(n.shardID, nid, addr)
	}
	for nid, addr := range snapshot.Membership.Witnesses {
		n.nodeRegistry.Add(n.shardID, nid, addr)
	}
	for nid := range snapshot.Membership.Removed {
		if nid == n.replicaID {
			n.nodeRegistry.RemoveShard(n.shardID)
			n.requestRemoval()
			n.notifySelfRemove()
		}
	}
	plog.Debugf("%s is restoring remotes", n.id())
	if err := n.p.RestoreRemotes(snapshot); err != nil {
		return err
	}
	n.notifyConfigChange()
	return nil
}

func (n *node) startRaft(cfg config.Config,
	peers map[uint64]string, initial bool) (bool, error) {
	newNode, err := n.replayLog(cfg.ShardID, cfg.ReplicaID)
	if err != nil {
		return false, err
	}
	pas := make([]raft.PeerAddress, 0)
	for k, v := range peers {
		pas = append(pas, raft.PeerAddress{ReplicaID: k, Address: v})
	}
	n.p = raft.Launch(cfg, n.logReader, n.raftEvents, pas, initial, newNode)
	return newNode, nil
}

func (n *node) close() {
	n.requestRemoval()
	n.raftEvents.close()
	n.mq.Close()
	n.pendingReadIndexes.close()
	n.pendingProposals.close()
	n.pendingConfigChange.close()
	n.pendingSnapshot.close()
	n.pendingRaftLogQuery.close()
}

func (n *node) stopped() bool {
	select {
	case <-n.stopC:
		return true
	default:
	}
	return false
}

func (n *node) requestRemoval() {
	n.closeOnce.Do(func() {
		close(n.stopC)
	})
	plog.Debugf("%s called requestRemoval()", n.id())
}

func (n *node) concurrentSnapshot() bool {
	return n.sm.Concurrent()
}

func (n *node) supportClientSession() bool {
	return !n.OnDiskStateMachine() && !n.isWitness()
}

func (n *node) isWitness() bool {
	return n.config.IsWitness
}

func (n *node) OnDiskStateMachine() bool {
	return n.sm.OnDiskStateMachine()
}

func (n *node) proposeSession(session *client.Session,
	timeout uint64) (*RequestState, error) {
	if !n.initialized() {
		return nil, ErrShardNotReady
	}
	if n.isWitness() {
		return nil, ErrInvalidOperation
	}
	if !session.ValidForSessionOp(n.shardID) {
		return nil, ErrInvalidSession
	}
	return n.pendingProposals.propose(session, nil, timeout)
}

func (n *node) payloadTooBig(sz int) bool {
	if n.config.MaxInMemLogSize == 0 {
		return false
	}
	return uint64(sz+settings.EntryNonCmdFieldsSize) > n.config.MaxInMemLogSize
}

func (n *node) propose(session *client.Session,
	cmd []byte, timeout uint64) (*RequestState, error) {
	if !n.initialized() {
		return nil, ErrShardNotReady
	}
	if n.isWitness() {
		return nil, ErrInvalidOperation
	}
	if !session.ValidForProposal(n.shardID) {
		return nil, ErrInvalidSession
	}
	if n.payloadTooBig(len(cmd)) {
		return nil, ErrPayloadTooBig
	}
	return n.pendingProposals.propose(session, cmd, timeout)
}

func (n *node) read(timeout uint64) (*RequestState, error) {
	if !n.initialized() {
		return nil, ErrShardNotReady
	}
	if n.isWitness() {
		return nil, ErrInvalidOperation
	}
	rs, err := n.pendingReadIndexes.read(timeout)
	if err == nil {
		rs.node = n
	}
	return rs, err
}

func (n *node) requestLeaderTransfer(replicaID uint64) error {
	if !n.initialized() {
		return ErrShardNotReady
	}
	if n.isWitness() {
		return ErrInvalidOperation
	}
	return n.pendingLeaderTransfer.request(replicaID)
}

func (n *node) requestSnapshot(opt SnapshotOption,
	timeout uint64) (*RequestState, error) {
	if !n.initialized() {
		return nil, ErrShardNotReady
	}
	if n.isWitness() {
		return nil, ErrInvalidOperation
	}
	st := rsm.UserRequested
	if opt.Exported {
		plog.Debugf("%s called export snapshot", n.id())
		st = rsm.Exported
		exist, err := fileutil.Exist(opt.ExportPath, n.snapshotter.fs)
		if err != nil {
			return nil, err
		}
		if !exist {
			return nil, ErrDirNotExist
		}
	} else {
		if len(opt.ExportPath) > 0 {
			plog.Warningf("opt.ExportPath set when not exporting a snapshot")
			opt.ExportPath = ""
		}
	}
	return n.pendingSnapshot.request(st,
		opt.ExportPath,
		opt.OverrideCompactionOverhead,
		opt.CompactionOverhead,
		opt.CompactionIndex,
		timeout)
}

func (n *node) queryRaftLog(firstIndex uint64,
	lastIndex uint64, maxSize uint64) (*RequestState, error) {
	if !n.initialized() {
		return nil, ErrShardNotReady
	}
	if n.isWitness() {
		return nil, ErrInvalidOperation
	}
	return n.pendingRaftLogQuery.add(firstIndex, lastIndex, maxSize)
}

func (n *node) reportIgnoredSnapshotRequest(key uint64) {
	n.pendingSnapshot.apply(key, true, false, 0)
}

func (n *node) requestConfigChange(cct pb.ConfigChangeType,
	replicaID uint64, target string, orderID uint64,
	timeout uint64) (*RequestState, error) {
	if !n.initialized() {
		return nil, ErrShardNotReady
	}
	if n.isWitness() {
		return nil, ErrInvalidOperation
	}
	if cct != pb.RemoveNode && !n.validateTarget(target) {
		return nil, ErrInvalidAddress
	}
	cc := pb.ConfigChange{
		Type:           cct,
		ReplicaID:      replicaID,
		ConfigChangeId: orderID,
		Address:        target,
	}
	return n.pendingConfigChange.request(cc, timeout)
}

func (n *node) requestDeleteNodeWithOrderID(replicaID uint64,
	order uint64, timeout uint64) (*RequestState, error) {
	return n.requestConfigChange(pb.RemoveNode, replicaID, "", order, timeout)
}

func (n *node) requestAddNodeWithOrderID(replicaID uint64,
	target string, order uint64, timeout uint64) (*RequestState, error) {
	return n.requestConfigChange(pb.AddNode, replicaID, target, order, timeout)
}

func (n *node) requestAddNonVotingWithOrderID(replicaID uint64,
	target string, order uint64, timeout uint64) (*RequestState, error) {
	return n.requestConfigChange(pb.AddNonVoting, replicaID, target, order, timeout)
}

func (n *node) requestAddWitnessWithOrderID(replicaID uint64,
	target string, order uint64, timeout uint64) (*RequestState, error) {
	return n.requestConfigChange(pb.AddWitness, replicaID, target, order, timeout)
}

func (n *node) getLeaderID() (uint64, uint64, bool) {
	lv := n.leaderInfo.Load()
	if lv == nil {
		return 0, 0, false
	}
	leaderInfo := lv.(*leaderInfo)
	return leaderInfo.leaderID, leaderInfo.term, leaderInfo.leaderID != raft.NoLeader
}

func (n *node) destroy() error {
	return n.sm.Close()
}

func (n *node) destroyed() bool {
	select {
	case <-n.sm.DestroyedC():
		return true
	default:
	}

	return false
}

func (n *node) offloaded() {
	if n.sm.Offloaded() {
		n.pipeline.setCloseReady(n)
		n.sysEvents.Publish(server.SystemEvent{
			Type:      server.NodeUnloaded,
			ShardID:   n.shardID,
			ReplicaID: n.replicaID,
		})
	}
}

func (n *node) loaded() {
	n.sm.Loaded()
}

func (n *node) pushTask(rec rsm.Task, notify bool) {
	if n.notifyCommit {
		n.toCommitQ.Add(rec)
		if notify {
			n.commitReady()
		}
	} else {
		n.toApplyQ.Add(rec)
		if notify {
			n.applyReady()
		}
	}
}

func (n *node) pushEntries(ents []pb.Entry) {
	if len(ents) == 0 {
		return
	}
	n.pushTask(rsm.Task{Entries: ents}, false)
	n.pushedIndex = ents[len(ents)-1].Index
}

func (n *node) pushStreamSnapshotRequest(shardID uint64, replicaID uint64) {
	n.pushTask(rsm.Task{
		ShardID:   shardID,
		ReplicaID: replicaID,
		Stream:    true,
	}, true)
}

func (n *node) pushTakeSnapshotRequest(req rsm.SSRequest) {
	n.pushTask(rsm.Task{
		Save:      true,
		SSRequest: req,
	}, true)
}

func (n *node) pushSnapshot(ss pb.Snapshot, applied uint64) {
	if pb.IsEmptySnapshot(ss) {
		return
	}
	if ss.Index < n.pushedIndex ||
		ss.Index < n.ss.getIndex() ||
		ss.Index < applied {
		plog.Panicf("out of date snapshot, index %d, pushed %d, applied %d, ss %d",
			ss.Index, n.pushedIndex, applied, n.ss.getIndex())
	}
	n.pushTask(rsm.Task{
		Recover: true,
		Index:   ss.Index,
	}, true)
	n.ss.setIndex(ss.Index)
	n.pushedIndex = ss.Index
}

func (n *node) replayLog(shardID uint64, replicaID uint64) (bool, error) {
	plog.Infof("%s replaying raft logs", n.id())
	ss, err := n.snapshotter.GetSnapshotFromLogDB()
	if err != nil && !n.snapshotter.IsNoSnapshotError(err) {
		return false, errors.Wrapf(err, "%s failed to get latest snapshot", n.id())
	}
	if !pb.IsEmptySnapshot(ss) {
		if err = n.logReader.ApplySnapshot(ss); err != nil {
			return false, errors.Wrapf(err, "%s failed to apply snapshot", n.id())
		}
	}
	rs, err := n.logdb.ReadRaftState(shardID, replicaID, ss.Index)
	if errors.Is(err, raftio.ErrNoSavedLog) {
		return true, nil
	}
	if err != nil {
		return false, errors.Wrapf(err, "%s ReadRaftState failed", n.id())
	}
	hasRaftState := !pb.IsEmptyState(rs.State)
	if hasRaftState {
		plog.Infof("%s logdb first entry %d size %d commit %d term %d",
			n.id(), rs.FirstIndex, rs.EntryCount, rs.State.Commit, rs.State.Term)
		n.logReader.SetState(rs.State)
	}
	n.logReader.SetRange(rs.FirstIndex, rs.EntryCount)
	return ss.Index <= 0 && rs.EntryCount <= 0 && !hasRaftState, nil
}

func (n *node) saveSnapshotRequired(applied uint64) bool {
	if n.config.SnapshotEntries == 0 {
		return false
	}
	index := n.ss.getIndex()
	if n.pushedIndex <= n.config.SnapshotEntries+index ||
		applied <= n.config.SnapshotEntries+index ||
		applied <= n.config.SnapshotEntries+n.ss.getReqIndex() {
		return false
	}
	if n.isBusySnapshotting() {
		return false
	}
	plog.Debugf("%s requested to create %s", n.id(), n.ssid(applied))
	n.ss.setReqIndex(applied)
	return true
}

func isSoftSnapshotError(err error) bool {
	return errors.Is(err, raft.ErrCompacted) ||
		errors.Is(err, raft.ErrSnapshotOutOfDate)
}

func saveAborted(err error) bool {
	return errors.Is(err, sm.ErrSnapshotStopped) ||
		errors.Is(err, sm.ErrSnapshotAborted)
}

func snapshotCommitAborted(err error) bool {
	return errors.Is(err, errSnapshotOutOfDate)
}

func streamAborted(err error) bool {
	return saveAborted(err) || errors.Is(err, sm.ErrSnapshotStreaming)
}

func openAborted(err error) bool {
	return errors.Is(err, sm.ErrOpenStopped)
}

func recoverAborted(err error) bool {
	return errors.Is(err, sm.ErrSnapshotStopped) ||
		errors.Is(err, raft.ErrSnapshotOutOfDate)
}

func (n *node) save(rec rsm.Task) error {
	index, err := n.doSave(rec.SSRequest)
	if err != nil {
		return err
	}
	n.pendingSnapshot.apply(rec.SSRequest.Key, index == 0, false, index)
	n.sysEvents.Publish(server.SystemEvent{
		Type:      server.SnapshotCreated,
		ShardID:   n.shardID,
		ReplicaID: n.replicaID,
	})
	return nil
}

func (n *node) doSave(req rsm.SSRequest) (uint64, error) {
	n.snapshotLock.Lock()
	defer n.snapshotLock.Unlock()
	if !req.Exported() && n.sm.GetLastApplied() <= n.ss.getIndex() {
		// a snapshot has been pushed to the sm but not applied yet
		// or the snapshot has been applied and there is no further progress
		return 0, nil
	}
	ss, ssenv, err := n.sm.Save(req)
	if err != nil {
		if saveAborted(err) {
			plog.Warningf("%s save snapshot aborted, %v", n.id(), err)
			ssenv.MustRemoveTempDir()
			n.pendingSnapshot.apply(req.Key, false, true, 0)
			return 0, nil
		} else if isSoftSnapshotError(err) {
			// e.g. trying to save a snapshot at the same index twice
			return 0, nil
		}
		return 0, errors.Wrapf(err, "%s save snapshot failed", n.id())
	}
	plog.Infof("%s saved %s, term %d, file count %d",
		n.id(), n.ssid(ss.Index), ss.Term, len(ss.Files))
	if err := n.snapshotter.Commit(ss, req); err != nil {
		if snapshotCommitAborted(err) || saveAborted(err) {
			// saveAborted() will only be true in monkey test
			// commit abort happens when the final dir already exists, probably due to
			// incoming snapshot
			ssenv.MustRemoveTempDir()
			return 0, nil
		}
		return 0, errors.Wrapf(err, "%s commit snapshot failed", n.id())
	}
	if req.Exported() {
		return ss.Index, nil
	}
	if !ss.Validate(n.snapshotter.fs) {
		plog.Panicf("%s generated invalid snapshot %v", n.id(), ss)
	}
	if err = n.logReader.CreateSnapshot(ss); err != nil {
		if isSoftSnapshotError(err) {
			return 0, nil
		}
		return 0, errors.Wrapf(err, "%s create snapshot failed", n.id())
	}
	n.compactLog(req, ss.Index)
	n.ss.setIndex(ss.Index)
	return ss.Index, nil
}

func (n *node) compactLog(req rsm.SSRequest, index uint64) {
	if compactionIndex, ok := n.getCompactionIndex(req, index); ok {
		n.ss.setCompactLogTo(compactionIndex)
	}
}

func (n *node) getCompactionIndex(req rsm.SSRequest, index uint64) (uint64, bool) {
	if req.OverrideCompaction {
		if req.CompactionIndex > 0 {
			if index >= req.CompactionIndex+1 {
				return req.CompactionIndex, true
			}
			return 0, false
		}
		if index > req.CompactionOverhead {
			return index - req.CompactionOverhead, true
		}
		return 0, false
	}
	if index > n.config.CompactionOverhead {
		return index - n.config.CompactionOverhead, true
	}

	return 0, false
}

func (n *node) stream(sink pb.IChunkSink) error {
	if sink != nil {
		plog.Infof("%s requested to stream to %d", n.id(), sink.ToReplicaID())
		if err := n.sm.Stream(sink); err != nil {
			if !streamAborted(err) {
				return errors.Wrapf(err, "%s stream failed", n.id())
			}
		}
	}
	return nil
}

func (n *node) recover(rec rsm.Task) (_ uint64, err error) {
	n.snapshotLock.Lock()
	defer n.snapshotLock.Unlock()
	if rec.Initial && n.OnDiskStateMachine() {
		plog.Debugf("%s on disk SM is beng initialized", n.id())
		idx, err := n.sm.OpenOnDiskStateMachine()
		if err != nil {
			if openAborted(err) {
				plog.Warningf("%s aborted OpenOnDiskStateMachine", n.id())
				return 0, nil
			}
			return 0, errors.Wrapf(err, "%s OpenOnDiskStateMachine failed", n.id())
		}
		if idx > 0 && rec.NewNode {
			plog.Panicf("%s new node at non-zero index %d", n.id(), idx)
		}
	}
	ss, err := n.sm.Recover(rec)
	if err != nil {
		if recoverAborted(err) {
			plog.Warningf("%s aborted recovery", n.id())
			return 0, nil
		}
		return 0, errors.Wrapf(err, "%s recover failed", n.id())
	}
	if !pb.IsEmptySnapshot(ss) {
		defer func() {
			err = firstError(err, ss.Unref())
		}()
		plog.Infof("%s recovered from %s", n.id(), n.ssid(ss.Index))
		if n.OnDiskStateMachine() {
			if err := n.sm.Sync(); err != nil {
				return 0, errors.Wrapf(err, "%s sync failed", n.id())
			}
			if err := n.snapshotter.Shrink(ss.Index); err != nil {
				return 0, errors.Wrapf(err, "%s shrink failed", n.id())
			}
		}
		n.compactLog(rsm.DefaultSSRequest, ss.Index)
	}
	n.sysEvents.Publish(server.SystemEvent{
		Type:      server.SnapshotRecovered,
		ShardID:   n.shardID,
		ReplicaID: n.replicaID,
		Index:     ss.Index,
	})
	return ss.Index, nil
}

func (n *node) streamDone() {
	n.ss.notifySnapshotStatus(false, false, true, false, 0)
	n.applyReady()
}

func (n *node) saveDone() {
	n.ss.notifySnapshotStatus(true, false, false, false, 0)
	n.applyReady()
}

func (n *node) recoverDone(index uint64) {
	if !n.initialized() {
		n.initialSnapshotDone(index)
	} else {
		n.recoverFromSnapshotDone()
	}
}

func (n *node) initialSnapshotDone(index uint64) {
	n.ss.notifySnapshotStatus(false, true, false, true, index)
	n.applyReady()
}

func (n *node) recoverFromSnapshotDone() {
	n.ss.notifySnapshotStatus(false, true, false, false, 0)
	n.applyReady()
}

func (n *node) handleTask(ts []rsm.Task, es []sm.Entry) (rsm.Task, error) {
	return n.sm.Handle(ts, es)
}

func (n *node) removeSnapshotFlagFile(index uint64) error {
	return n.snapshotter.removeFlagFile(index)
}

func (n *node) runSyncTask() {
	if !n.sm.OnDiskStateMachine() {
		return
	}
	if !n.syncTask.timeToRun(n.millisecondSinceStart()) {
		return
	}
	if !n.sm.TaskChanBusy() {
		n.pushTask(rsm.Task{PeriodicSync: true}, true)
	}
}

func (n *node) removeLog() error {
	if n.ss.hasCompactLogTo() {
		compactTo := n.ss.getCompactLogTo()
		if compactTo == 0 {
			panic("racy compact log to value?")
		}
		if err := n.logReader.Compact(compactTo); err != nil {
			if err != raft.ErrCompacted {
				return err
			}
		}
		if err := n.logdb.RemoveEntriesTo(n.shardID,
			n.replicaID, compactTo); err != nil {
			return err
		}
		plog.Infof("%s compacted log up to index %d", n.id(), compactTo)
		n.ss.setCompactedTo(compactTo)
		n.sysEvents.Publish(server.SystemEvent{
			Type:      server.LogCompacted,
			ShardID:   n.shardID,
			ReplicaID: n.replicaID,
			Index:     compactTo,
		})
		if !n.config.DisableAutoCompactions {
			if _, err := n.requestCompaction(); err != nil {
				if err != ErrRejected {
					return errors.Wrapf(err, "%s failed to request compaction", n.id())
				}
			}
		}
	}
	return nil
}

func (n *node) requestCompaction() (*SysOpState, error) {
	if compactTo := n.ss.getCompactedTo(); compactTo > 0 {
		done, err := n.logdb.CompactEntriesTo(n.shardID, n.replicaID, compactTo)
		if err != nil {
			return nil, err
		}
		n.sysEvents.Publish(server.SystemEvent{
			Type:      server.LogDBCompacted,
			ShardID:   n.shardID,
			ReplicaID: n.replicaID,
			Index:     compactTo,
		})
		return &SysOpState{completedC: done}, nil
	}
	return nil, ErrRejected
}

func isFreeOrderMessage(m pb.Message) bool {
	return m.Type == pb.Replicate || m.Type == pb.Ping
}

func (n *node) sendEnterQuiesceMessages() {
	for replicaID := range n.sm.GetMembership().Addresses {
		if replicaID != n.replicaID {
			msg := pb.Message{
				Type:    pb.Quiesce,
				From:    n.replicaID,
				To:      replicaID,
				ShardID: n.shardID,
			}
			n.sendRaftMessage(msg)
		}
	}
}

func (n *node) sendMessages(msgs []pb.Message) {
	for _, msg := range msgs {
		if !isFreeOrderMessage(msg) {
			msg.ShardID = n.shardID
			n.sendRaftMessage(msg)
		}
	}
}

func (n *node) sendReplicateMessages(ud pb.Update) {
	for _, msg := range ud.Messages {
		if isFreeOrderMessage(msg) {
			msg.ShardID = n.shardID
			n.sendRaftMessage(msg)
		}
	}
}

func (n *node) getUpdate() (pb.Update, bool, error) {
	moreEntries := n.moreEntriesToApply()
	if n.p.HasUpdate(moreEntries) ||
		n.confirmedIndex != n.appliedIndex ||
		n.ss.hasCompactLogTo() || n.ss.hasCompactedTo() {
		if n.appliedIndex < n.confirmedIndex {
			plog.Panicf("applied index moving backwards, %d, now %d",
				n.confirmedIndex, n.appliedIndex)
		}
		ud, err := n.p.GetUpdate(moreEntries, n.appliedIndex)
		if err != nil {
			return pb.Update{}, false, err
		}
		n.confirmedIndex = n.appliedIndex
		return ud, true, nil
	}
	return pb.Update{}, false, nil
}

func (n *node) processDroppedReadIndexes(ud pb.Update) {
	for _, sysctx := range ud.DroppedReadIndexes {
		n.pendingReadIndexes.dropped(sysctx)
	}
}

func (n *node) processDroppedEntries(ud pb.Update) {
	for _, e := range ud.DroppedEntries {
		if e.IsProposal() {
			n.pendingProposals.dropped(e.ClientID, e.SeriesID, e.Key)
		} else if e.Type == pb.ConfigChangeEntry {
			n.pendingConfigChange.dropped(e.Key)
		} else {
			plog.Panicf("unknown entry type %s", e.Type)
		}
	}
}

func (n *node) notifyCommittedEntries() {
	tasks := n.toCommitQ.GetAll()
	for _, t := range tasks {
		for _, e := range t.Entries {
			if e.IsProposal() {
				n.pendingProposals.committed(e.ClientID, e.SeriesID, e.Key)
			} else if e.Type == pb.ConfigChangeEntry {
				n.pendingConfigChange.committed(e.Key)
			} else {
				plog.Panicf("unknown entry type %s", e.Type)
			}
		}
		n.toApplyQ.Add(t)
	}
	if len(tasks) > 0 {
		n.applyReady()
	}
}

func (n *node) processReadyToRead(ud pb.Update) {
	if len(ud.ReadyToReads) > 0 {
		n.pendingReadIndexes.addReady(ud.ReadyToReads)
		n.pendingReadIndexes.applied(ud.LastApplied)
	}
}

func (n *node) processSnapshot(ud pb.Update) error {
	if !pb.IsEmptySnapshot(ud.Snapshot) {
		err := n.logReader.ApplySnapshot(ud.Snapshot)
		if err != nil && !isSoftSnapshotError(err) {
			return errors.Wrapf(err, "%s failed to apply snapshot", n.id())
		}
		plog.Debugf("%s, push snapshot %d", n.id(), ud.Snapshot.Index)
		n.pushSnapshot(ud.Snapshot, ud.LastApplied)
	}
	return nil
}

func (n *node) applyRaftUpdates(ud pb.Update) {
	n.pushEntries(pb.EntriesToApply(ud.CommittedEntries, n.pushedIndex, true))
}

func (n *node) processRaftUpdate(ud pb.Update) error {
	if err := n.logReader.Append(ud.EntriesToSave); err != nil {
		return err
	}
	n.sendMessages(ud.Messages)
	if err := n.removeLog(); err != nil {
		return err
	}
	n.runSyncTask()
	if n.saveSnapshotRequired(ud.LastApplied) {
		n.pushTakeSnapshotRequest(rsm.SSRequest{})
	}
	return nil
}

func (n *node) commitRaftUpdate(ud pb.Update) {
	n.raftMu.Lock()
	n.p.Commit(ud)
	n.raftMu.Unlock()
}

func (n *node) moreEntriesToApply() bool {
	return n.toApplyQ.MoreEntryToApply()
}

func (n *node) hasEntryToApply() bool {
	return n.p.HasEntryToApply()
}

func (n *node) updateAppliedIndex() uint64 {
	n.appliedIndex = n.sm.GetLastApplied()
	n.p.NotifyRaftLastApplied(n.appliedIndex)
	return n.appliedIndex
}

func (n *node) stepNode() (pb.Update, bool, error) {
	n.raftMu.Lock()
	defer n.raftMu.Unlock()
	if n.initialized() {
		hasEvent, err := n.handleEvents()
		if err != nil {
			return pb.Update{}, false, err
		}
		if hasEvent {
			if n.qs.newQuiesceState() {
				n.sendEnterQuiesceMessages()
			}
			ud, hasUpdate, err := n.getUpdate()
			if err != nil {
				return pb.Update{}, false, err
			}
			return ud, hasUpdate, nil
		}
	}
	return pb.Update{}, false, nil
}

func (n *node) handleEvents() (bool, error) {
	hasEvent := false
	lastApplied := n.updateAppliedIndex()
	if lastApplied != n.confirmedIndex {
		hasEvent = true
	}
	if n.hasEntryToApply() {
		hasEvent = true
	}
	event, err := n.handleReadIndex()
	if err != nil {
		return false, err
	}
	if event {
		hasEvent = true
	}
	event, err = n.handleReceivedMessages()
	if err != nil {
		return false, err
	}
	if event {
		hasEvent = true
	}
	event, err = n.handleConfigChange()
	if err != nil {
		return false, err
	}
	if event {
		hasEvent = true
	}
	event, err = n.handleProposals()
	if err != nil {
		return false, err
	}
	if event {
		hasEvent = true
	}
	event, err = n.handleLeaderTransfer()
	if err != nil {
		return false, err
	}
	if event {
		hasEvent = true
	}
	if n.handleSnapshot(lastApplied) {
		hasEvent = true
	}
	if n.handleCompaction() {
		hasEvent = true
	}
	event, err = n.handleLogQuery()
	if err != nil {
		return false, err
	}
	if event {
		hasEvent = true
	}
	n.gc()
	if hasEvent {
		n.pendingReadIndexes.applied(lastApplied)
	}
	return hasEvent, nil
}

func (n *node) gc() {
	if n.gcTick != n.currentTick {
		n.pendingProposals.gc()
		n.pendingConfigChange.gc()
		n.pendingSnapshot.gc()
		n.gcTick = n.currentTick
	}
}

func (n *node) handleCompaction() bool {
	return n.ss.hasCompactedTo() || n.ss.hasCompactLogTo()
}

func (n *node) handleLogQuery() (bool, error) {
	if req := n.pendingRaftLogQuery.get(); req != nil {
		if err := n.p.QueryRaftLog(req.logRange.FirstIndex,
			req.logRange.LastIndex, req.maxSize); err != nil {
			return false, err
		}
		return true, nil
	}
	return false, nil
}

func (n *node) handleLeaderTransfer() (bool, error) {
	target, ok := n.pendingLeaderTransfer.get()
	if ok {
		if err := n.p.RequestLeaderTransfer(target); err != nil {
			return false, err
		}
	}
	return ok, nil
}

func (n *node) handleSnapshot(lastApplied uint64) bool {
	var req rsm.SSRequest
	select {
	case req = <-n.snapshotC:
	default:
		return false
	}
	if !req.Exported() && lastApplied == n.ss.getReqIndex() {
		n.reportIgnoredSnapshotRequest(req.Key)
		return false
	}
	n.ss.setReqIndex(lastApplied)
	n.pushTakeSnapshotRequest(req)
	return true
}

func (n *node) handleProposals() (bool, error) {
	rateLimited := n.p.RateLimited()
	if n.rateLimited != rateLimited {
		n.rateLimited = rateLimited
		plog.Infof("%s new rate limit state is %t", n.id(), rateLimited)
	}
	logDBBusy := n.logDBBusy()
	if n.logDBLimited != logDBBusy {
		n.logDBLimited = logDBBusy
		plog.Infof("%s new LogDB busy state is %t", n.id(), logDBBusy)
	}
	paused := logDBBusy || n.rateLimited
	if entries := n.incomingProposals.get(paused); len(entries) > 0 {
		if err := n.p.ProposeEntries(entries); err != nil {
			return false, err
		}
		return true, nil
	}
	return false, nil
}

func (n *node) handleReadIndex() (bool, error) {
	if reqs := n.incomingReadIndexes.get(); len(reqs) > 0 {
		n.qs.record(pb.ReadIndex)
		ctx := n.pendingReadIndexes.nextCtx()
		n.pendingReadIndexes.add(ctx, reqs)
		if err := n.p.ReadIndex(ctx); err != nil {
			return false, err
		}
		return true, nil
	}
	return false, nil
}

func (n *node) handleConfigChange() (bool, error) {
	if len(n.configChangeC) == 0 {
		return false, nil
	}
	select {
	case req, ok := <-n.configChangeC:
		if !ok {
			n.configChangeC = nil
		} else {
			n.qs.record(pb.ConfigChangeEvent)
			var cc pb.ConfigChange
			pb.MustUnmarshal(&cc, req.data)
			if err := n.p.ProposeConfigChange(cc, req.key); err != nil {
				return false, err
			}
		}
	default:
		return false, nil
	}
	return true, nil
}

func (n *node) isBusySnapshotting() bool {
	snapshotting := n.ss.recovering()
	if !n.concurrentSnapshot() {
		snapshotting = snapshotting || n.ss.saving()
	}
	return snapshotting && n.sm.TaskChanBusy()
}

func (n *node) recordMessage(m pb.Message) {
	if (m.Type == pb.Heartbeat || m.Type == pb.HeartbeatResp) && m.Hint > 0 {
		n.qs.record(pb.ReadIndex)
	} else {
		n.qs.record(m.Type)
	}
}

func (n *node) handleReceivedMessages() (bool, error) {
	count := uint64(0)
	busy := n.isBusySnapshotting()
	msgs := n.mq.Get()
	for _, m := range msgs {
		if m.Type == pb.LocalTick {
			count++
		} else if m.Type == pb.Replicate && busy {
			continue
		}
		done, err := n.handleMessage(m)
		if err != nil {
			return false, err
		}
		if !done {
			n.recordMessage(m)
			if err := n.p.Handle(m); err != nil {
				return false, err
			}
		}
	}
	if count > n.config.ElectionRTT/2 {
		plog.Warningf("%s had %d LocalTick msgs in one batch", n.id(), count)
	}
	if lazyFreeCycle > 0 {
		for i := range msgs {
			msgs[i].Entries = nil
		}
	}
	return len(msgs) > 0, nil
}

func (n *node) handleMessage(m pb.Message) (bool, error) {
	switch m.Type {
	case pb.LocalTick:
		if err := n.tick(m.Hint); err != nil {
			return false, err
		}
	case pb.Quiesce:
		n.qs.tryEnterQuiesce()
	case pb.SnapshotStatus:
		plog.Debugf("%s got ReportSnapshot from %d, rejected %t",
			n.id(), m.From, m.Reject)
		if err := n.p.ReportSnapshotStatus(m.From, m.Reject); err != nil {
			return false, err
		}
	case pb.Unreachable:
		if err := n.p.ReportUnreachableNode(m.From); err != nil {
			return false, err
		}
	default:
		return false, nil
	}
	return true, nil
}

func (n *node) setInitialStatus(index uint64) {
	if n.initialized() {
		plog.Panicf("setInitialStatus called twice")
	}
	plog.Infof("%s initial index set to %d", n.id(), index)
	n.ss.setIndex(index)
	n.pushedIndex = index
	n.setInitialized()
}

func (n *node) handleSnapshotTask(task rsm.Task) {
	if n.ss.recovering() {
		plog.Panicf("%s recovering from snapshot again on %s",
			n.id(), n.getRaftAddress())
	}
	if task.Recover {
		n.reportRecoverSnapshot(task)
	} else if task.Save {
		if n.ss.saving() {
			plog.Warningf("%s taking snapshot, ignored new snapshot req", n.id())
			n.reportIgnoredSnapshotRequest(task.SSRequest.Key)
			return
		}
		n.reportSaveSnapshot(task)
	} else if task.Stream {
		if !n.canStream() {
			n.reportSnapshotStatus(task.ShardID, task.ReplicaID, true)
			return
		}
		n.reportStreamSnapshot(task)
	} else {
		plog.Panicf("unknown task type %+v", task)
	}
}

func (n *node) reportSnapshotStatus(shardID uint64,
	replicaID uint64, failed bool) {
	n.handleSnapshotStatus(shardID, replicaID, failed)
}

func (n *node) reportStreamSnapshot(rec rsm.Task) {
	n.ss.setStreaming()
	getSinkFn := func() pb.IChunkSink {
		conn := n.getStreamSink(rec.ShardID, rec.ReplicaID)
		if conn == nil {
			plog.Errorf("failed to connect to %s", dn(rec.ShardID, rec.ReplicaID))
			return nil
		}
		return conn
	}
	n.ss.setStreamReq(rec, getSinkFn)
	n.pipeline.setStreamReady(n.shardID)
}

func (n *node) canStream() bool {
	if n.ss.streaming() {
		plog.Warningf("%s ignored task.StreamSnapshot", n.id())
		return false
	}
	if !n.sm.ReadyToStream() {
		plog.Warningf("%s is not ready to stream snapshot", n.id())
		return false
	}
	return true
}

func (n *node) reportSaveSnapshot(rec rsm.Task) {
	n.ss.setSaving()
	n.ss.setSaveReq(rec)
	n.pipeline.setSaveReady(n.shardID)
}

func (n *node) reportRecoverSnapshot(rec rsm.Task) {
	n.ss.setRecovering()
	n.ss.setRecoverReq(rec)
	n.pipeline.setRecoverReady(n.shardID)
}

// returns a boolean flag indicating whether to skip task handling for the
// current node
func (n *node) processStatusTransition() bool {
	if n.processSaveStatus() {
		return true
	}
	if n.processStreamStatus() {
		return true
	}
	if n.processRecoverStatus() {
		return true
	}
	if n.processUninitializedNodeStatus() {
		return true
	}
	return false
}

func (n *node) processUninitializedNodeStatus() bool {
	if !n.initialized() {
		plog.Debugf("%s checking initial snapshot", n.id())
		n.reportRecoverSnapshot(rsm.Task{
			Recover: true,
			Initial: true,
			NewNode: n.new,
		})
		return true
	}
	return false
}

func (n *node) processRecoverStatus() bool {
	if n.ss.recovering() {
		rec, ok := n.ss.getRecoverCompleted()
		if !ok {
			return true
		}
		if rec.Save {
			plog.Panicf("got a completed.SnapshotRequested")
		}
		if rec.Initial {
			plog.Infof("%s initialized using %s", n.id(), n.ssid(rec.Index))
			n.setInitialStatus(rec.Index)
			n.sysEvents.Publish(server.SystemEvent{
				Type:      server.NodeReady,
				ShardID:   n.shardID,
				ReplicaID: n.replicaID,
			})
		}
		n.ss.clearRecovering()
	}
	return false
}

func (n *node) processSaveStatus() bool {
	if n.ss.saving() {
		rec, ok := n.ss.getSaveCompleted()
		if !ok {
			return !n.concurrentSnapshot()
		}
		if rec.Save && !n.initialized() {
			plog.Panicf("%s taking snapshot when uninitialized", n.id())
		}
		n.ss.clearSaving()
	}
	return false
}

func (n *node) processStreamStatus() bool {
	if n.ss.streaming() {
		if !n.OnDiskStateMachine() {
			plog.Panicf("non-on disk sm is streaming snapshot")
		}
		if _, ok := n.ss.getStreamCompleted(); !ok {
			return false
		}
		n.ss.clearStreaming()
	}
	return false
}

func (n *node) tick(tick uint64) error {
	n.currentTick++
	n.qs.tick()
	if n.qs.quiesced() {
		if err := n.p.QuiescedTick(); err != nil {
			return err
		}
	} else {
		if err := n.p.Tick(); err != nil {
			return err
		}
	}
	n.pendingSnapshot.tick(tick)
	n.pendingProposals.tick(tick)
	n.pendingReadIndexes.tick(tick)
	n.pendingConfigChange.tick(tick)
	return nil
}

func (n *node) notifySelfRemove() {
	n.sysEvents.Publish(server.SystemEvent{
		Type:      server.NodeDeleted,
		ShardID:   n.shardID,
		ReplicaID: n.replicaID,
	})
}

func (n *node) notifyConfigChange() {
	m := n.sm.GetMembership()
	if len(m.Addresses) == 0 {
		plog.Panicf("empty nodes %s", n.id())
	}
	_, isNonVoting := m.NonVotings[n.replicaID]
	_, isWitness := m.Witnesses[n.replicaID]
	ci := &ShardInfo{
		ShardID:           n.shardID,
		ReplicaID:         n.replicaID,
		IsNonVoting:       isNonVoting,
		IsWitness:         isWitness,
		ConfigChangeIndex: m.ConfigChangeId,
		Replicas:          m.Addresses,
	}
	n.shardInfo.Store(ci)
	n.sysEvents.Publish(server.SystemEvent{
		Type:      server.MembershipChanged,
		ShardID:   n.shardID,
		ReplicaID: n.replicaID,
	})
}

func (n *node) getShardInfo() ShardInfo {
	v := n.shardInfo.Load()
	if v == nil {
		return ShardInfo{
			ShardID:          n.shardID,
			ReplicaID:        n.replicaID,
			Pending:          true,
			StateMachineType: sm.Type(n.sm.Type()),
		}
	}
	info := v.(*ShardInfo)

	leaderID := uint64(0)
	term := uint64(0)
	lv := n.leaderInfo.Load()
	if lv != nil {
		leaderInfo := lv.(*leaderInfo)
		leaderID = leaderInfo.leaderID
		term = leaderInfo.term
	}

	return ShardInfo{
		ShardID:           info.ShardID,
		ReplicaID:         info.ReplicaID,
		LeaderID:          leaderID,
		Term:              term,
		IsNonVoting:       info.IsNonVoting,
		ConfigChangeIndex: info.ConfigChangeIndex,
		Replicas:          info.Replicas,
		StateMachineType:  sm.Type(n.sm.Type()),
	}
}

func (n *node) logDBBusy() bool {
	if n.metrics == nil {
		// only happens in tests
		return false
	}
	return n.metrics.isBusy()
}

func (n *node) id() string {
	return dn(n.shardID, n.replicaID)
}

func (n *node) ssid(index uint64) string {
	return logutil.DescribeSS(n.shardID, n.replicaID, index)
}

func (n *node) isLeader() bool {
	v := n.leaderInfo.Load()
	if v == nil {
		return false
	}
	leaderInfo := v.(*leaderInfo)
	if leaderInfo.term == 0 {
		return false
	}
	return n.replicaID == leaderInfo.leaderID
}

func (n *node) isFollower() bool {
	v := n.leaderInfo.Load()
	if v == nil {
		return false
	}
	leaderInfo := v.(*leaderInfo)
	if leaderInfo.term == 0 {
		return false
	}
	return n.replicaID != leaderInfo.leaderID
}

func (n *node) initialized() bool {
	if atomic.LoadUint64(&n.initializedFlag) != 0 {
		return true
	}
	select {
	case <-n.initializedC:
		atomic.StoreUint64(&n.initializedFlag, 1)
		return true
	default:
	}
	return false
}

func (n *node) setInitialized() {
	close(n.initializedC)
}

func (n *node) millisecondSinceStart() uint64 {
	return n.tickMillisecond * n.currentTick
}

func (n *node) getRaftAddress() string {
	return n.raftAddress
}
````

## File: nodehost_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//	  http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"bytes"
	"context"
	"crypto/rand"
	"encoding/binary"
	"flag"
	"fmt"
	"io"
	"math"
	mathrand "math/rand"
	"os"
	"os/exec"
	"runtime"
	"strconv"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/leaktest"
	"github.com/lni/goutils/random"
	"github.com/lni/goutils/syncutil"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/client"
	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/id"
	"github.com/lni/dragonboat/v4/internal/invariants"
	"github.com/lni/dragonboat/v4/internal/logdb"
	"github.com/lni/dragonboat/v4/internal/registry"
	"github.com/lni/dragonboat/v4/internal/rsm"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/tests"
	"github.com/lni/dragonboat/v4/internal/transport"
	"github.com/lni/dragonboat/v4/internal/vfs"
	chantrans "github.com/lni/dragonboat/v4/plugin/chan"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
	"github.com/lni/dragonboat/v4/tools"
	"github.com/lni/dragonboat/v4/tools/upgrade310"
)

const (
	defaultTestPort = 26001
	testNodeHostID1 = "123e4567-e89b-12d3-a456-426614174000"
	testNodeHostID2 = "123e4567-e89b-12d3-a456-426614174001"
)

func getTestPort() int {
	pv := os.Getenv("DRAGONBOAT_TEST_PORT")
	if len(pv) > 0 {
		port, err := strconv.Atoi(pv)
		if err != nil {
			panic(err)
		}
		return port
	}
	return defaultTestPort
}

var rttMillisecond uint64
var mu sync.Mutex

var rttValues = []uint64{10, 20, 30, 50, 100, 200, 500}

func getRTTMillisecond(fs vfs.IFS, dir string) uint64 {
	mu.Lock()
	defer mu.Unlock()
	if rttMillisecond > 0 {
		return rttMillisecond
	}
	rttMillisecond = calcRTTMillisecond(fs, dir)
	return rttMillisecond
}

func calcRTTMillisecond(fs vfs.IFS, dir string) uint64 {
	testFile := fs.PathJoin(dir, ".dragonboat_test_file_safe_to_delete")
	defer func() {
		_ = fs.RemoveAll(testFile)
	}()
	_ = fs.MkdirAll(dir, 0755)
	f, err := fs.Create(testFile)
	if err != nil {
		panic(err)
	}
	defer func() {
		if err := f.Close(); err != nil {
			panic(err)
		}
	}()
	data := make([]byte, 512)
	total := uint64(0)
	repeat := 5
	for i := 0; i < repeat; i++ {
		if _, err := f.Write(data); err != nil {
			panic(err)
		}
		start := time.Now()
		if err := f.Sync(); err != nil {
			panic(err)
		}
		total += uint64(time.Since(start).Milliseconds())
	}
	rtt := total / uint64(repeat)
	for i := range rttValues {
		if rttValues[i] > rtt {
			if i == 0 {
				return rttValues[0]
			}
			return rttValues[i-1]
		}
	}
	return rttValues[len(rttValues)-1]
}

// typical proposal timeout
func pto(nh *NodeHost) time.Duration {
	rtt := nh.NodeHostConfig().RTTMillisecond
	if invariants.Race {
		return 5 * time.Second
	}
	return time.Duration(rtt*45) * time.Millisecond
}

func lpto(nh *NodeHost) time.Duration {
	rtt := nh.NodeHostConfig().RTTMillisecond
	if invariants.Race {
		return 30 * time.Second
	}
	return time.Duration(rtt*100) * time.Millisecond
}

func getTestExpertConfig(fs vfs.IFS) config.ExpertConfig {
	cfg := config.GetDefaultExpertConfig()
	cfg.LogDB.Shards = 4
	cfg.FS = fs
	return cfg
}

func reportLeakedFD(fs vfs.IFS, t *testing.T) {
	vfs.ReportLeakedFD(fs, t)
}

func getTestNodeHostConfig(fs vfs.IFS) *config.NodeHostConfig {
	cfg := &config.NodeHostConfig{
		WALDir:              singleNodeHostTestDir,
		NodeHostDir:         singleNodeHostTestDir,
		RTTMillisecond:      getRTTMillisecond(fs, singleNodeHostTestDir),
		RaftAddress:         singleNodeHostTestAddr,
		Expert:              getTestExpertConfig(fs),
		SystemEventListener: &testSysEventListener{},
	}
	return cfg
}

func getTestConfig() *config.Config {
	return &config.Config{
		ReplicaID:    1,
		ShardID:      1,
		ElectionRTT:  3,
		HeartbeatRTT: 1,
		CheckQuorum:  true,
	}
}

func waitNodeInfoEvent(t *testing.T, f func() []raftio.NodeInfo, count int) {
	for i := 0; i < 1000; i++ {
		if len(f()) == count {
			return
		}
		time.Sleep(10 * time.Millisecond)
	}
	require.Fail(t, "failed to get node info event")
}

func waitSnapshotInfoEvent(t *testing.T,
	f func() []raftio.SnapshotInfo,
	count int) {
	for i := 0; i < 1000; i++ {
		if len(f()) == count {
			return
		}
		time.Sleep(10 * time.Millisecond)
	}
	require.Fail(t, "failed to get snapshot info event")
}

type testSysEventListener struct {
	mu                    sync.Mutex
	nodeHostShuttingdown  uint64
	nodeUnloaded          []raftio.NodeInfo
	nodeReady             []raftio.NodeInfo
	membershipChanged     []raftio.NodeInfo
	snapshotCreated       []raftio.SnapshotInfo
	snapshotRecovered     []raftio.SnapshotInfo
	snapshotReceived      []raftio.SnapshotInfo
	sendSnapshotStarted   []raftio.SnapshotInfo
	sendSnapshotCompleted []raftio.SnapshotInfo
	snapshotCompacted     []raftio.SnapshotInfo
	logCompacted          []raftio.EntryInfo
	logdbCompacted        []raftio.EntryInfo
	connectionEstablished uint64
}

func copyNodeInfo(info []raftio.NodeInfo) []raftio.NodeInfo {
	return append([]raftio.NodeInfo{}, info...)
}

func (t *testSysEventListener) NodeHostShuttingDown() {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.nodeHostShuttingdown++
}

func (t *testSysEventListener) NodeReady(info raftio.NodeInfo) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.nodeReady = append(t.nodeReady, info)
}

func (t *testSysEventListener) getNodeReady() []raftio.NodeInfo {
	t.mu.Lock()
	defer t.mu.Unlock()
	return copyNodeInfo(t.nodeReady)
}

func (t *testSysEventListener) NodeUnloaded(info raftio.NodeInfo) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.nodeUnloaded = append(t.nodeUnloaded, info)
}

func (t *testSysEventListener) getNodeUnloaded() []raftio.NodeInfo {
	t.mu.Lock()
	defer t.mu.Unlock()
	return copyNodeInfo(t.nodeUnloaded)
}

func (t *testSysEventListener) NodeDeleted(info raftio.NodeInfo) {}

func (t *testSysEventListener) MembershipChanged(info raftio.NodeInfo) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.membershipChanged = append(t.membershipChanged, info)
}

func (t *testSysEventListener) getMembershipChanged() []raftio.NodeInfo {
	t.mu.Lock()
	defer t.mu.Unlock()
	return copyNodeInfo(t.membershipChanged)
}

func (t *testSysEventListener) ConnectionEstablished(
	info raftio.ConnectionInfo,
) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.connectionEstablished++
}

func (t *testSysEventListener) getConnectionEstablished() uint64 {
	t.mu.Lock()
	defer t.mu.Unlock()
	return t.connectionEstablished
}

func (t *testSysEventListener) ConnectionFailed(info raftio.ConnectionInfo) {}

func copySnapshotInfo(info []raftio.SnapshotInfo) []raftio.SnapshotInfo {
	return append([]raftio.SnapshotInfo{}, info...)
}

func (t *testSysEventListener) SendSnapshotStarted(info raftio.SnapshotInfo) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.sendSnapshotStarted = append(t.sendSnapshotStarted, info)
}

func (t *testSysEventListener) getSendSnapshotStarted() []raftio.SnapshotInfo {
	t.mu.Lock()
	defer t.mu.Unlock()
	return copySnapshotInfo(t.sendSnapshotStarted)
}

func (t *testSysEventListener) SendSnapshotCompleted(
	info raftio.SnapshotInfo,
) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.sendSnapshotCompleted = append(t.sendSnapshotCompleted, info)
}

func (t *testSysEventListener) getSendSnapshotCompleted() []raftio.SnapshotInfo {
	t.mu.Lock()
	defer t.mu.Unlock()
	return copySnapshotInfo(t.sendSnapshotCompleted)
}

func (t *testSysEventListener) SendSnapshotAborted(info raftio.SnapshotInfo) {}
func (t *testSysEventListener) SnapshotReceived(info raftio.SnapshotInfo) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.snapshotReceived = append(t.snapshotReceived, info)
}

func (t *testSysEventListener) getSnapshotReceived() []raftio.SnapshotInfo {
	t.mu.Lock()
	defer t.mu.Unlock()
	return copySnapshotInfo(t.snapshotReceived)
}

func (t *testSysEventListener) SnapshotRecovered(info raftio.SnapshotInfo) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.snapshotRecovered = append(t.snapshotRecovered, info)
}

func (t *testSysEventListener) getSnapshotRecovered() []raftio.SnapshotInfo {
	t.mu.Lock()
	defer t.mu.Unlock()
	return copySnapshotInfo(t.snapshotRecovered)
}

func (t *testSysEventListener) SnapshotCreated(info raftio.SnapshotInfo) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.snapshotCreated = append(t.snapshotCreated, info)
}

func (t *testSysEventListener) getSnapshotCreated() []raftio.SnapshotInfo {
	t.mu.Lock()
	defer t.mu.Unlock()
	return copySnapshotInfo(t.snapshotCreated)
}

func (t *testSysEventListener) SnapshotCompacted(info raftio.SnapshotInfo) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.snapshotCompacted = append(t.snapshotCompacted, info)
}

func copyEntryInfo(info []raftio.EntryInfo) []raftio.EntryInfo {
	return append([]raftio.EntryInfo{}, info...)
}

func (t *testSysEventListener) LogCompacted(info raftio.EntryInfo) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.logCompacted = append(t.logCompacted, info)
}

func (t *testSysEventListener) getLogCompacted() []raftio.EntryInfo {
	t.mu.Lock()
	defer t.mu.Unlock()
	return copyEntryInfo(t.logCompacted)
}

func (t *testSysEventListener) LogDBCompacted(info raftio.EntryInfo) {
	t.mu.Lock()
	defer t.mu.Unlock()
	t.logdbCompacted = append(t.logdbCompacted, info)
}

type TimeoutStateMachine struct {
	updateDelay   uint64
	lookupDelay   uint64
	snapshotDelay uint64
	closed        bool
}

func (t *TimeoutStateMachine) Update(e sm.Entry) (sm.Result, error) {
	if t.updateDelay > 0 {
		time.Sleep(time.Duration(t.updateDelay) * time.Millisecond)
	}
	return sm.Result{}, nil
}

func (t *TimeoutStateMachine) Lookup(data interface{}) (interface{}, error) {
	if t.lookupDelay > 0 {
		time.Sleep(time.Duration(t.lookupDelay) * time.Millisecond)
	}
	return data, nil
}

func (t *TimeoutStateMachine) SaveSnapshot(w io.Writer,
	fc sm.ISnapshotFileCollection,
	stopc <-chan struct{}) error {
	if t.snapshotDelay > 0 {
		time.Sleep(time.Duration(t.snapshotDelay) * time.Millisecond)
	}
	_, err := w.Write([]byte("done"))
	return err
}

func (t *TimeoutStateMachine) RecoverFromSnapshot(r io.Reader,
	fc []sm.SnapshotFile,
	stopc <-chan struct{}) error {
	return nil
}

func (t *TimeoutStateMachine) Close() error {
	t.closed = true
	return nil
}

type noopLogDB struct {
}

func (n *noopLogDB) BinaryFormat() uint32 { return 0 }
func (n *noopLogDB) Name() string         { return "noopLogDB" }
func (n *noopLogDB) Close() error         { return nil }
func (n *noopLogDB) HasNodeInfo(shardID uint64,
	replicaID uint64) (bool, error) {
	return true, nil
}
func (n *noopLogDB) CreateNodeInfo(shardID uint64, replicaID uint64) error {
	return nil
}
func (n *noopLogDB) ListNodeInfo() ([]raftio.NodeInfo, error) { return nil, nil }
func (n *noopLogDB) SaveBootstrapInfo(shardID uint64,
	replicaID uint64,
	bs pb.Bootstrap) error {
	return nil
}
func (n *noopLogDB) GetBootstrapInfo(shardID uint64,
	replicaID uint64) (pb.Bootstrap, error) {
	return pb.Bootstrap{}, nil
}
func (n *noopLogDB) SaveRaftState(updates []pb.Update,
	workerID uint64) error {
	return nil
}
func (n *noopLogDB) IterateEntries(ents []pb.Entry,
	size uint64,
	shardID uint64,
	replicaID uint64,
	low uint64,
	high uint64,
	maxSize uint64) ([]pb.Entry, uint64, error) {
	return nil, 0, nil
}
func (n *noopLogDB) ReadRaftState(shardID uint64,
	replicaID uint64,
	lastIndex uint64) (raftio.RaftState, error) {
	return raftio.RaftState{}, nil
}
func (n *noopLogDB) RemoveEntriesTo(shardID uint64,
	replicaID uint64,
	index uint64) error {
	return nil
}
func (n *noopLogDB) CompactEntriesTo(shardID uint64,
	replicaID uint64,
	index uint64) (<-chan struct{}, error) {
	return nil, nil
}
func (n *noopLogDB) RemoveNodeData(shardID uint64, replicaID uint64) error {
	return nil
}
func (n *noopLogDB) SaveSnapshots([]pb.Update) error { return nil }
func (n *noopLogDB) GetSnapshot(shardID uint64,
	replicaID uint64) (pb.Snapshot, error) {
	return pb.Snapshot{}, nil
}
func (n *noopLogDB) ImportSnapshot(snapshot pb.Snapshot,
	replicaID uint64) error {
	return nil
}

type updateConfig func(*config.Config) *config.Config
type updateNodeHostConfig func(*config.NodeHostConfig) *config.NodeHostConfig
type testFunc func(*NodeHost)
type beforeTest func()
type afterTest func(*NodeHost)

type testOption struct {
	updateConfig         updateConfig
	updateNodeHostConfig updateNodeHostConfig
	tf                   testFunc
	rf                   testFunc
	bt                   beforeTest
	at                   afterTest
	defaultTestNode      bool
	fakeDiskNode         bool
	fakeDiskInitialIndex uint64
	createSM             sm.CreateStateMachineFunc
	createConcurrentSM   sm.CreateConcurrentStateMachineFunc
	createOnDiskSM       sm.CreateOnDiskStateMachineFunc
	join                 bool
	newNodeHostToFail    bool
	restartNodeHost      bool
	noElection           bool
	compressed           bool
	fsErrorInjection     bool
}

func createSingleTestNode(t *testing.T, to *testOption, nh *NodeHost) {
	if to.createSM == nil &&
		to.createConcurrentSM == nil &&
		to.createOnDiskSM == nil && !to.defaultTestNode && !to.fakeDiskNode {
		return
	}
	if to.defaultTestNode {
		to.createSM = func(uint64, uint64) sm.IStateMachine {
			return &PST{}
		}
	}
	if to.fakeDiskNode {
		to.createOnDiskSM = func(uint64, uint64) sm.IOnDiskStateMachine {
			return tests.NewFakeDiskSM(to.fakeDiskInitialIndex)
		}
	}
	cfg := getTestConfig()
	if to.updateConfig != nil {
		cfg = to.updateConfig(cfg)
	}
	if to.compressed {
		cfg.SnapshotCompressionType = config.Snappy
		cfg.EntryCompressionType = config.Snappy
	}
	peers := make(map[uint64]string)
	if !to.join {
		if !nh.nhConfig.DefaultNodeRegistryEnabled {
			peers[cfg.ShardID] = nh.RaftAddress()
		} else {
			peers[cfg.ShardID] = nh.ID()
		}
	}
	if to.createSM != nil {
		err := nh.StartReplica(peers, to.join, to.createSM, *cfg)
		require.NoError(t, err, "start shard failed")
	} else if to.createConcurrentSM != nil {
		err := nh.StartConcurrentReplica(peers,
			to.join, to.createConcurrentSM, *cfg)
		require.NoError(t, err, "start concurrent shard failed")
	} else if to.createOnDiskSM != nil {
		err := nh.StartOnDiskReplica(peers,
			to.join, to.createOnDiskSM, *cfg)
		require.NoError(t, err, "start on disk shard fail")
	} else {
		require.Fail(t, "?!?")
	}
}

func runNodeHostTest(t *testing.T, to *testOption, fs vfs.IFS) {
	func() {
		if !to.fsErrorInjection {
			defer leaktest.AfterTest(t)()
		}
		// FIXME:
		// the following RemoveAll call will fail on windows after running error
		// injection tests as some pebble log files are not closed
		_ = fs.RemoveAll(singleNodeHostTestDir)
		if to.bt != nil {
			to.bt()
		}
		nhc := getTestNodeHostConfig(fs)
		if to.updateNodeHostConfig != nil {
			nhc = to.updateNodeHostConfig(nhc)
		}
		nh, err := NewNodeHost(*nhc)
		if err != nil && !to.newNodeHostToFail {
			require.NoError(t, err, "failed to create nodehost")
		}
		if err != nil && to.newNodeHostToFail {
			return
		}
		if err == nil && to.newNodeHostToFail {
			require.Fail(t, "NewNodeHost didn't fail as expected")
		}
		if !to.restartNodeHost {
			defer func() {
				require.NotPanics(t, func() {
					if to.fsErrorInjection {
						defer func() {
							if r := recover(); r != nil {
								return
							}
						}()
					}
					nh.Close()
				})
				if to.at != nil {
					to.at(nh)
				}
			}()
		}
		createSingleTestNode(t, to, nh)
		if !to.noElection {
			waitForLeaderToBeElected(t, nh, 1)
		}
		if to.tf != nil {
			to.tf(nh)
		}
		if to.restartNodeHost {
			nh.Close()
			nh, err = NewNodeHost(*nhc)
			require.NoError(t, err, "failed to create nodehost")
			defer func() {
				nh.Close()
				if to.at != nil {
					to.at(nh)
				}
			}()
			createSingleTestNode(t, to, nh)
			if to.rf != nil {
				to.rf(nh)
			}
		}
	}()
	reportLeakedFD(fs, t)
}

func createProposalsToTriggerSnapshot(t *testing.T,
	nh *NodeHost,
	count uint64,
	timeoutExpected bool) {
	for i := uint64(0); i < count; i++ {
		pto := lpto(nh)
		ctx, cancel := context.WithTimeout(context.Background(), pto)
		cs, err := nh.SyncGetSession(ctx, 1)
		if err != nil {
			if err == ErrTimeout {
				cancel()
				return
			}
			require.NoError(t, err, "unexpected error")
		}
		//time.Sleep(100 * time.Millisecond)
		err = nh.SyncCloseSession(ctx, cs)
		if err != nil {
			if err == ErrTimeout {
				cancel()
				return
			}
			require.NoError(t, err, "failed to close client session")
		}
		cancel()
	}
	if timeoutExpected {
		require.Fail(t, "failed to trigger ")
	}
}

func runNodeHostTestDC(t *testing.T,
	f func(),
	removeDir bool,
	fs vfs.IFS) {
	defer leaktest.AfterTest(t)()
	defer func() {
		require.NoError(t, fs.RemoveAll(singleNodeHostTestDir))
	}()
	if removeDir {
		require.NoError(t, fs.RemoveAll(singleNodeHostTestDir))
	}
	f()
	reportLeakedFD(fs, t)
}

type testLogDBFactory struct {
	ldb raftio.ILogDB
}

func (t *testLogDBFactory) Create(cfg config.NodeHostConfig,
	cb config.LogDBCallback,
	dirs []string,
	wals []string) (raftio.ILogDB, error) {
	return t.ldb, nil
}

func (t *testLogDBFactory) Name() string {
	return t.ldb.Name()
}

func TestLogDBCanBeExtended(t *testing.T) {
	fs := vfs.GetTestFS()
	ldb := &noopLogDB{}
	to := &testOption{
		updateNodeHostConfig: func(
			nhc *config.NodeHostConfig) *config.NodeHostConfig {
			nhc.Expert.LogDBFactory = &testLogDBFactory{ldb: ldb}
			return nhc
		},

		tf: func(nh *NodeHost) {
			assert.Equal(t, ldb.Name(), nh.mu.logdb.Name())
		},
		noElection: true,
	}
	runNodeHostTest(t, to, fs)
}

func TestTCPTransportIsUsedByDefault(t *testing.T) {
	if vfs.GetTestFS() != vfs.DefaultFS {
		t.Skip("memfs test mode, skipped")
	}
	fs := vfs.GetTestFS()
	to := &testOption{
		tf: func(nh *NodeHost) {
			tt := nh.transport.(*transport.Transport)
			assert.Equal(t, transport.TCPTransportName, tt.GetTrans().Name())
		},
		noElection: true,
	}
	runNodeHostTest(t, to, fs)
}

type noopTransportFactory struct{}

func (noopTransportFactory) Create(cfg config.NodeHostConfig,
	h raftio.MessageHandler,
	ch raftio.ChunkHandler) raftio.ITransport {
	return transport.NewNOOPTransport(cfg, h, ch)
}

func (noopTransportFactory) Validate(string) bool {
	return true
}

func TestTransportFactoryIsStillHonored(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		updateNodeHostConfig: func(
			nhc *config.NodeHostConfig) *config.NodeHostConfig {
			nhc.Expert.TransportFactory = noopTransportFactory{}
			return nhc
		},
		tf: func(nh *NodeHost) {
			tt := nh.transport.(*transport.Transport)
			assert.Equal(t, transport.NOOPRaftName, tt.GetTrans().Name())
		},
		noElection: true,
	}
	runNodeHostTest(t, to, fs)
}

func TestTransportFactoryCanBeSet(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		updateNodeHostConfig: func(
			nhc *config.NodeHostConfig) *config.NodeHostConfig {
			nhc.Expert.TransportFactory = &transport.NOOPTransportFactory{}
			return nhc
		},
		tf: func(nh *NodeHost) {
			tt := nh.transport.(*transport.Transport)
			assert.Equal(t, transport.NOOPRaftName, tt.GetTrans().Name())
		},
		noElection: true,
	}
	runNodeHostTest(t, to, fs)
}

type validatorTestModule struct {
}

func (tm *validatorTestModule) Create(nhConfig config.NodeHostConfig,
	handler raftio.MessageHandler,
	chunkHandler raftio.ChunkHandler) raftio.ITransport {
	return transport.NewNOOPTransport(nhConfig, handler, chunkHandler)
}

func (tm *validatorTestModule) Validate(addr string) bool {
	return addr == "localhost:12346" || addr == "localhost:26001"
}

func TestAddressValidatorCanBeSet(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		updateNodeHostConfig: func(
			nhc *config.NodeHostConfig) *config.NodeHostConfig {
			nhc.Expert.TransportFactory = &validatorTestModule{}
			return nhc
		},
		tf: func(nh *NodeHost) {
			pto := lpto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			err := nh.SyncRequestAddReplica(ctx, 1, 100, "localhost:12345", 0)
			cancel()
			require.ErrorIs(t, err, ErrInvalidAddress)
			ctx, cancel = context.WithTimeout(context.Background(), pto)
			err = nh.SyncRequestAddReplica(ctx, 1, 100, "localhost:12346", 0)
			cancel()
			require.NoError(t, err, "failed to add node")
		},
	}
	runNodeHostTest(t, to, fs)
}

type chanTransportFactory struct{}

func (*chanTransportFactory) Create(nhConfig config.NodeHostConfig,
	handler raftio.MessageHandler,
	chunkHandler raftio.ChunkHandler) raftio.ITransport {
	return chantrans.NewChanTransport(nhConfig, handler, chunkHandler)
}

func (tm *chanTransportFactory) Validate(addr string) bool {
	return addr == nodeHostTestAddr1 || addr == nodeHostTestAddr2
}

func TestGossip(t *testing.T) {
	testDefaultNodeRegistryEnabled(t, true, nil)
}

func TestMediumSizedClusterGossip(t *testing.T) {
	if os.Getenv("LONG_TEST") == "" {
		t.Skip("Skipping long test")
	}
	defer leaktest.AfterTest(t)()
	for i := 0; i < 16; i++ {
		fs := vfs.GetTestFS()
		dir := fs.PathJoin(singleNodeHostTestDir, fmt.Sprintf("nh%d", i))
		cfg := config.NodeHostConfig{
			NodeHostDir:                dir,
			RTTMillisecond:             getRTTMillisecond(fs, dir),
			RaftAddress:                fmt.Sprintf("127.0.0.1:%d", 25000+i*10),
			DefaultNodeRegistryEnabled: true,
			Expert: config.ExpertConfig{
				FS:                      fs,
				TestGossipProbeInterval: 50 * time.Millisecond,
			},
			Gossip: config.GossipConfig{
				BindAddress:      fmt.Sprintf("127.0.0.1:%d", 25000+i*10+1),
				AdvertiseAddress: fmt.Sprintf("127.0.0.1:%d", 25000+i*10+1),
				Seed:             []string{"127.0.0.1:25001", "127.0.0.1:25011"},
			},
		}
		nh, err := NewNodeHost(cfg)
		require.NoError(t, err, "failed to create nh")
		defer nh.Close()
	}
}

func TestCustomTransportCanUseNodeHostID(t *testing.T) {
	factory := &chanTransportFactory{}
	testDefaultNodeRegistryEnabled(t, true, factory)
}

func TestCustomTransportCanGoWithoutNodeHostID(t *testing.T) {
	factory := &chanTransportFactory{}
	testDefaultNodeRegistryEnabled(t, false, factory)
}

func testDefaultNodeRegistryEnabled(t *testing.T,
	addressByNodeHostID bool,
	factory config.TransportFactory) {
	fs := vfs.GetTestFS()
	datadir1 := fs.PathJoin(singleNodeHostTestDir, "nh1")
	datadir2 := fs.PathJoin(singleNodeHostTestDir, "nh2")
	require.NoError(t, os.RemoveAll(singleNodeHostTestDir))
	defer func() {
		require.NoError(t, os.RemoveAll(singleNodeHostTestDir))
	}()
	addr1 := nodeHostTestAddr1
	addr2 := nodeHostTestAddr2
	nhc1 := config.NodeHostConfig{
		NodeHostDir:                datadir1,
		RTTMillisecond:             getRTTMillisecond(fs, datadir1),
		RaftAddress:                addr1,
		DefaultNodeRegistryEnabled: addressByNodeHostID,
		Expert: config.ExpertConfig{
			FS:                      fs,
			TestGossipProbeInterval: 50 * time.Millisecond,
		},
	}
	if addressByNodeHostID {
		nhc1.Gossip = config.GossipConfig{
			BindAddress:      "127.0.0.1:25001",
			AdvertiseAddress: "127.0.0.1:25001",
			Seed:             []string{"127.0.0.1:25002"},
		}
	}
	nhc2 := config.NodeHostConfig{
		NodeHostDir:                datadir2,
		RTTMillisecond:             getRTTMillisecond(fs, datadir2),
		RaftAddress:                addr2,
		DefaultNodeRegistryEnabled: addressByNodeHostID,
		Expert: config.ExpertConfig{
			FS:                      fs,
			TestGossipProbeInterval: 50 * time.Millisecond,
		},
	}
	if addressByNodeHostID {
		nhc2.Gossip = config.GossipConfig{
			BindAddress:      "127.0.0.1:25002",
			AdvertiseAddress: "127.0.0.1:25002",
			Seed:             []string{"127.0.0.1:25001"},
		}
	}
	nhid1, err := id.NewUUID(testNodeHostID1)
	require.NoError(t, err, "failed to parse nhid")
	nhc1.NodeHostID = nhid1.String()
	nhid2, err := id.NewUUID(testNodeHostID2)
	require.NoError(t, err, "failed to parse nhid")
	nhc2.NodeHostID = nhid2.String()
	nhc1.Expert.TransportFactory = factory
	nhc2.Expert.TransportFactory = factory
	nh1, err := NewNodeHost(nhc1)
	require.NoError(t, err, "failed to create nh")
	defer nh1.Close()
	nh2, err := NewNodeHost(nhc2)
	require.NoError(t, err, "failed to create nh2")
	defer nh2.Close()
	peers := make(map[uint64]string)
	if addressByNodeHostID {
		peers[1] = testNodeHostID1
		peers[2] = testNodeHostID2
	} else {
		peers[1] = addr1
		peers[2] = addr2
	}
	createSM := func(uint64, uint64) sm.IStateMachine {
		return &PST{}
	}
	rc := config.Config{
		ShardID:         1,
		ReplicaID:       1,
		ElectionRTT:     10,
		HeartbeatRTT:    1,
		SnapshotEntries: 0,
	}
	require.NoError(t, nh1.StartReplica(peers, false, createSM, rc))
	rc.ReplicaID = 2
	require.NoError(t, nh2.StartReplica(peers, false, createSM, rc))
	waitForLeaderToBeElected(t, nh1, 1)
	waitForLeaderToBeElected(t, nh2, 1)
	pto := lpto(nh1)
	session := nh1.GetNoOPSession(1)
	for i := 0; i < 1000; i++ {
		ctx, cancel := context.WithTimeout(context.Background(), pto)
		if _, err := nh1.SyncPropose(ctx, session, make([]byte, 0)); err == nil {
			cancel()
			return
		}
		cancel()
		time.Sleep(100 * time.Millisecond)
	}
	require.Fail(t, "failed to make proposal")
}

func TestNodeHostRegistry(t *testing.T) {
	fs := vfs.GetTestFS()
	datadir1 := fs.PathJoin(singleNodeHostTestDir, "nh1")
	datadir2 := fs.PathJoin(singleNodeHostTestDir, "nh2")
	require.NoError(t, os.RemoveAll(singleNodeHostTestDir))
	defer func() {
		require.NoError(t, os.RemoveAll(singleNodeHostTestDir))
	}()
	addr1 := nodeHostTestAddr1
	addr2 := nodeHostTestAddr2
	nhc1 := config.NodeHostConfig{
		NodeHostDir:                datadir1,
		RTTMillisecond:             getRTTMillisecond(fs, datadir1),
		RaftAddress:                addr1,
		DefaultNodeRegistryEnabled: true,
		Expert: config.ExpertConfig{
			FS:                      fs,
			TestGossipProbeInterval: 50 * time.Millisecond,
		},
	}
	nhc1.Gossip = config.GossipConfig{
		BindAddress:      "127.0.0.1:25001",
		AdvertiseAddress: "127.0.0.1:25001",
		Seed:             []string{"127.0.0.1:25002"},
	}
	nhc2 := config.NodeHostConfig{
		NodeHostDir:                datadir2,
		RTTMillisecond:             getRTTMillisecond(fs, datadir2),
		RaftAddress:                addr2,
		DefaultNodeRegistryEnabled: true,
		Expert: config.ExpertConfig{
			FS:                      fs,
			TestGossipProbeInterval: 50 * time.Millisecond,
		},
	}
	nhc2.Gossip = config.GossipConfig{
		BindAddress:      "127.0.0.1:25002",
		AdvertiseAddress: "127.0.0.1:25002",
		Seed:             []string{"127.0.0.1:25001"},
	}
	nhid1, err := id.NewUUID(testNodeHostID1)
	require.NoError(t, err, "failed to parse nhid")
	nhc1.NodeHostID = nhid1.String()
	nhc1.Gossip.Meta = []byte(testNodeHostID1)
	nhid2, err := id.NewUUID(testNodeHostID2)
	require.NoError(t, err, "failed to parse nhid")
	nhc2.NodeHostID = nhid2.String()
	nhc2.Gossip.Meta = []byte(testNodeHostID2)
	nh1, err := NewNodeHost(nhc1)
	require.NoError(t, err, "failed to create nh")
	defer nh1.Close()
	nh2, err := NewNodeHost(nhc2)
	require.NoError(t, err, "failed to create nh2")
	defer nh2.Close()
	peers := make(map[uint64]string)
	peers[1] = testNodeHostID1
	createSM := func(uint64, uint64) sm.IStateMachine {
		return &PST{}
	}
	rc := config.Config{
		ShardID:         1,
		ReplicaID:       1,
		ElectionRTT:     10,
		HeartbeatRTT:    1,
		SnapshotEntries: 0,
	}
	require.NoError(t, nh1.StartReplica(peers, false, createSM, rc))
	rc.ShardID = 2
	peers[1] = testNodeHostID2
	require.NoError(t, nh2.StartReplica(peers, false, createSM, rc))
	rc.ShardID = 3
	require.NoError(t, nh2.StartReplica(peers, false, createSM, rc))
	waitForLeaderToBeElected(t, nh1, 1)
	waitForLeaderToBeElected(t, nh2, 2)
	waitForLeaderToBeElected(t, nh2, 3)
	good := false
	for i := 0; i < 1000; i++ {
		r1, ok := nh1.GetNodeHostRegistry()
		assert.True(t, ok)
		r2, ok := nh2.GetNodeHostRegistry()
		assert.True(t, ok)
		if r1.NumOfShards() != 3 || r2.NumOfShards() != 3 {
			time.Sleep(10 * time.Millisecond)
		} else {
			good = true
			break
		}
	}
	require.True(t, good, "registry failed to report the expected num of shards")
	rc.ShardID = 100
	require.NoError(t, nh2.StartReplica(peers, false, createSM, rc))
	waitForLeaderToBeElected(t, nh2, 100)
	for i := 0; i < 1000; i++ {
		r1, ok := nh1.GetNodeHostRegistry()
		assert.True(t, ok)
		r2, ok := nh2.GetNodeHostRegistry()
		assert.True(t, ok)
		if r1.NumOfShards() != 4 || r2.NumOfShards() != 4 {
			time.Sleep(10 * time.Millisecond)
		} else {
			v1, ok := r1.GetMeta(testNodeHostID1)
			assert.True(t, ok)
			assert.Equal(t, testNodeHostID1, string(v1))
			v2, ok := r1.GetMeta(testNodeHostID2)
			assert.True(t, ok)
			assert.Equal(t, testNodeHostID2, string(v2))
			return
		}
	}
	require.Fail(t, "failed to report the expected num of shards")
}

func TestGossipCanHandleDynamicRaftAddress(t *testing.T) {
	fs := vfs.GetTestFS()
	datadir1 := fs.PathJoin(singleNodeHostTestDir, "nh1")
	datadir2 := fs.PathJoin(singleNodeHostTestDir, "nh2")
	require.NoError(t, os.RemoveAll(singleNodeHostTestDir))
	defer func() {
		require.NoError(t, os.RemoveAll(singleNodeHostTestDir))
	}()
	addr1 := nodeHostTestAddr1
	addr2 := nodeHostTestAddr2
	nhc1 := config.NodeHostConfig{
		NodeHostDir:                datadir1,
		RTTMillisecond:             getRTTMillisecond(fs, datadir1),
		RaftAddress:                addr1,
		DefaultNodeRegistryEnabled: true,
		Expert: config.ExpertConfig{
			FS:                      fs,
			TestGossipProbeInterval: 50 * time.Millisecond,
		},
	}
	nhc2 := config.NodeHostConfig{
		NodeHostDir:                datadir2,
		RTTMillisecond:             getRTTMillisecond(fs, datadir2),
		RaftAddress:                addr2,
		DefaultNodeRegistryEnabled: true,
		Expert: config.ExpertConfig{
			FS:                      fs,
			TestGossipProbeInterval: 50 * time.Millisecond,
		},
	}
	nhid1, err := id.NewUUID(testNodeHostID1)
	require.NoError(t, err, "failed to parse nhid")
	nhc1.NodeHostID = nhid1.String()
	nhid2, err := id.NewUUID(testNodeHostID2)
	require.NoError(t, err, "failed to parse nhid")
	nhc2.NodeHostID = nhid2.String()
	nhc1.Gossip = config.GossipConfig{
		BindAddress:      "127.0.0.1:25001",
		AdvertiseAddress: "127.0.0.1:25001",
		Seed:             []string{"127.0.0.1:25002"},
	}
	nhc2.Gossip = config.GossipConfig{
		BindAddress:      "127.0.0.1:25002",
		AdvertiseAddress: "127.0.0.1:25002",
		Seed:             []string{"127.0.0.1:25001"},
	}
	nh1, err := NewNodeHost(nhc1)
	require.NoError(t, err, "failed to create nh")
	defer nh1.Close()
	nh2, err := NewNodeHost(nhc2)
	require.NoError(t, err, "failed to create nh2")
	nh2NodeHostID := nh2.ID()
	peers := make(map[uint64]string)
	peers[1] = testNodeHostID1
	peers[2] = testNodeHostID2
	createSM := func(uint64, uint64) sm.IStateMachine {
		return &PST{}
	}
	rc := config.Config{
		ShardID:         1,
		ReplicaID:       1,
		ElectionRTT:     3,
		HeartbeatRTT:    1,
		SnapshotEntries: 0,
	}
	require.NoError(t, nh1.StartReplica(peers, false, createSM, rc))
	rc.ReplicaID = 2
	require.NoError(t, nh2.StartReplica(peers, false, createSM, rc))
	waitForLeaderToBeElected(t, nh1, 1)
	waitForLeaderToBeElected(t, nh2, 1)
	pto := lpto(nh1)
	session := nh1.GetNoOPSession(1)
	testProposal := func() {
		done := false
		for i := 0; i < 100; i++ {
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			_, err := nh1.SyncPropose(ctx, session, make([]byte, 0))
			cancel()
			if err != nil {
				time.Sleep(100 * time.Millisecond)
				continue
			}
			done = true
			break
		}
		require.True(t, done, "failed to make proposal")
	}
	testProposal()
	nh2.Close()
	nhc2.RaftAddress = nodeHostTestAddr3
	nh2, err = NewNodeHost(nhc2)
	require.NoError(t, err, "failed to restart nh2")
	defer nh2.Close()
	require.Equal(t, nh2NodeHostID, nh2.ID(), "NodeHostID changed")
	require.NoError(t, nh2.StartReplica(peers, false, createSM, rc))
	waitForLeaderToBeElected(t, nh2, 1)
	testProposal()
}

type testRegistry struct {
	*registry.Registry

	mu *sync.Mutex // PROTECTS(nodeAddrs)
	// map of nhid -> host:port
	nodeAddrs map[string]string
}

func (tr *testRegistry) Resolve(shardID uint64,
	replicaID uint64) (string, string, error) {
	nhid, ck, err := tr.Registry.Resolve(shardID, replicaID)
	if err != nil {
		return "", "", err
	}
	tr.mu.Lock()
	defer tr.mu.Unlock()
	return tr.nodeAddrs[nhid], ck, nil
}

type testRegistryFactory struct {
	mu sync.Mutex // PROTECTS(nodeAddrs)
	// map of nhid -> host:port
	nodeAddrs map[string]string
}

func (trf *testRegistryFactory) Set(nhid, addr string) {
	trf.mu.Lock()
	defer trf.mu.Unlock()
	trf.nodeAddrs[nhid] = addr
}

func (trf *testRegistryFactory) Create(nhid string,
	streamConnections uint64,
	v config.TargetValidator) (raftio.INodeRegistry, error) {
	return &testRegistry{
		registry.NewNodeRegistry(streamConnections, v),
		&trf.mu,
		trf.nodeAddrs,
	}, nil
}

func TestExternalNodeRegistryFunction(t *testing.T) {
	fs := vfs.GetTestFS()
	datadir1 := fs.PathJoin(singleNodeHostTestDir, "nh1")
	datadir2 := fs.PathJoin(singleNodeHostTestDir, "nh2")
	require.NoError(t, os.RemoveAll(singleNodeHostTestDir))
	defer func() {
		require.NoError(t, os.RemoveAll(singleNodeHostTestDir))
	}()
	addr1 := nodeHostTestAddr1
	addr2 := nodeHostTestAddr2
	nhc1 := config.NodeHostConfig{
		NodeHostDir:    datadir1,
		RTTMillisecond: getRTTMillisecond(fs, datadir1),
		RaftAddress:    addr1,
		Expert: config.ExpertConfig{
			FS: fs,
		},
	}
	nhc2 := config.NodeHostConfig{
		NodeHostDir:    datadir2,
		RTTMillisecond: getRTTMillisecond(fs, datadir2),
		RaftAddress:    addr2,
		Expert: config.ExpertConfig{
			FS: fs,
		},
	}
	nhid1, err := id.NewUUID(testNodeHostID1)
	require.NoError(t, err, "failed to parse nhid")
	nhc1.NodeHostID = nhid1.String()
	nhid2, err := id.NewUUID(testNodeHostID2)
	require.NoError(t, err, "failed to parse nhid")
	nhc2.NodeHostID = nhid2.String()
	testRegistryFactory := &testRegistryFactory{
		nodeAddrs: map[string]string{
			nhc1.NodeHostID: nodeHostTestAddr1,
			nhc2.NodeHostID: nodeHostTestAddr2,
		},
	}
	nhc1.Expert.NodeRegistryFactory = testRegistryFactory
	nhc2.Expert.NodeRegistryFactory = testRegistryFactory

	nh1, err := NewNodeHost(nhc1)
	require.NoError(t, err, "failed to create nh")
	defer nh1.Close()
	nh2, err := NewNodeHost(nhc2)
	require.NoError(t, err, "failed to create nh2")
	nh2NodeHostID := nh2.ID()
	peers := make(map[uint64]string)
	peers[1] = testNodeHostID1
	peers[2] = testNodeHostID2
	createSM := func(uint64, uint64) sm.IStateMachine {
		return &PST{}
	}
	rc := config.Config{
		ShardID:         1,
		ReplicaID:       1,
		ElectionRTT:     3,
		HeartbeatRTT:    1,
		SnapshotEntries: 0,
	}
	require.NoError(t, nh1.StartReplica(peers, false, createSM, rc))
	rc.ReplicaID = 2
	require.NoError(t, nh2.StartReplica(peers, false, createSM, rc))
	waitForLeaderToBeElected(t, nh1, 1)
	waitForLeaderToBeElected(t, nh2, 1)
	pto := lpto(nh1)
	session := nh1.GetNoOPSession(1)
	testProposal := func() {
		done := false
		for i := 0; i < 100; i++ {
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			_, err := nh1.SyncPropose(ctx, session, make([]byte, 0))
			cancel()
			if err != nil {
				time.Sleep(100 * time.Millisecond)
				continue
			}
			done = true
			break
		}
		require.True(t, done, "failed to make proposal")
	}
	testProposal()
	nh2.Close()
	nhc2.RaftAddress = nodeHostTestAddr3
	testRegistryFactory.Set(nh2NodeHostID, nodeHostTestAddr3)
	nh2, err = NewNodeHost(nhc2)
	require.NoError(t, err, "failed to restart nh2")
	defer nh2.Close()
	require.Equal(t, nh2NodeHostID, nh2.ID(), "NodeHostID changed")
	require.NoError(t, nh2.StartReplica(peers, false, createSM, rc))
	waitForLeaderToBeElected(t, nh2, 1)
	testProposal()
}

func TestNewNodeHostReturnErrorOnInvalidConfig(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		updateNodeHostConfig: func(
			nhc *config.NodeHostConfig) *config.NodeHostConfig {
			nhc.RaftAddress = "12345"
			require.Error(t, nhc.Validate(), "config is not considered as invalid")
			return nhc
		},
		newNodeHostToFail: true,
	}
	runNodeHostTest(t, to, fs)
}

func TestDeploymentIDCanBeSetUsingNodeHostConfig(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		updateNodeHostConfig: func(
			nhc *config.NodeHostConfig) *config.NodeHostConfig {
			nhc.DeploymentID = 1000
			return nhc
		},
		tf: func(nh *NodeHost) {
			nhc := nh.NodeHostConfig()
			assert.Equal(t, uint64(1000), nhc.GetDeploymentID())
		},
		noElection: true,
	}
	runNodeHostTest(t, to, fs)
}

var (
	singleNodeHostTestAddr = fmt.Sprintf("localhost:%d", getTestPort())
	nodeHostTestAddr1      = fmt.Sprintf("localhost:%d", getTestPort())
	nodeHostTestAddr2      = fmt.Sprintf("localhost:%d", getTestPort()+1)
	nodeHostTestAddr3      = fmt.Sprintf("localhost:%d", getTestPort()+2)
	singleNodeHostTestDir  = "single_nodehost_test_dir_safe_to_delete"
)

type PST struct {
	mu       sync.Mutex
	stopped  bool
	saved    bool
	restored bool
	slowSave bool
}

func (n *PST) setRestored(v bool) {
	n.mu.Lock()
	defer n.mu.Unlock()
	n.restored = v
}

func (n *PST) getRestored() bool {
	n.mu.Lock()
	defer n.mu.Unlock()
	return n.restored
}

func (n *PST) Close() error { return nil }

// Lookup locally looks up the data.
func (n *PST) Lookup(key interface{}) (interface{}, error) {
	return make([]byte, 1), nil
}

// Update updates the object.
func (n *PST) Update(e sm.Entry) (sm.Result, error) {
	return sm.Result{Value: uint64(len(e.Cmd))}, nil
}

// SaveSnapshot saves the state of the object to the provided io.Writer object.
func (n *PST) SaveSnapshot(w io.Writer,
	fileCollection sm.ISnapshotFileCollection,
	done <-chan struct{}) error {
	n.saved = true
	if !n.slowSave {
		_, err := w.Write([]byte("random-data"))
		if err != nil {
			panic(err)
		}
		return nil
	}
	for {
		time.Sleep(10 * time.Millisecond)
		select {
		case <-done:
			n.stopped = true
			return sm.ErrSnapshotStopped
		default:
		}
	}
}

// RecoverFromSnapshot recovers the object from the snapshot specified by the
// io.Reader object.
func (n *PST) RecoverFromSnapshot(r io.Reader,
	files []sm.SnapshotFile,
	done <-chan struct{}) error {
	n.setRestored(true)
	for {
		time.Sleep(10 * time.Millisecond)
		select {
		case <-done:
			n.stopped = true
			return sm.ErrSnapshotStopped
		default:
		}
	}
}

// GetHash returns a uint64 value representing the current state of the object.
func (n *PST) GetHash() (uint64, error) {
	return 0, nil
}

func createConcurrentTestNodeHost(addr string,
	datadir string,
	snapshotEntry uint64,
	concurrent bool,
	fs vfs.IFS) (*NodeHost, error) {
	// config for raft
	rc := config.Config{
		ReplicaID:          uint64(1),
		ElectionRTT:        3,
		HeartbeatRTT:       1,
		CheckQuorum:        true,
		SnapshotEntries:    snapshotEntry,
		CompactionOverhead: 100,
	}
	peers := make(map[uint64]string)
	peers[1] = addr
	nhc := config.NodeHostConfig{
		WALDir:         datadir,
		NodeHostDir:    datadir,
		RTTMillisecond: getRTTMillisecond(fs, datadir),
		RaftAddress:    peers[1],
		Expert:         getTestExpertConfig(fs),
	}
	nh, err := NewNodeHost(nhc)
	if err != nil {
		return nil, err
	}
	var newConcurrentSM func(uint64, uint64) sm.IConcurrentStateMachine
	var newSM func(uint64, uint64) sm.IStateMachine
	if snapshotEntry == 0 {
		newConcurrentSM = func(uint64, uint64) sm.IConcurrentStateMachine {
			return &tests.ConcurrentUpdate{}
		}
		newSM = func(uint64, uint64) sm.IStateMachine {
			return &tests.TestUpdate{}
		}
	} else {
		newConcurrentSM = func(uint64, uint64) sm.IConcurrentStateMachine {
			return &tests.ConcurrentSnapshot{}
		}
		newSM = func(uint64, uint64) sm.IStateMachine {
			return &tests.TestSnapshot{}
		}
	}
	rc.ShardID = 1 + nhc.Expert.Engine.ApplyShards
	if err := nh.StartConcurrentReplica(peers, false, newConcurrentSM, rc); err != nil {
		return nil, err
	}
	rc.ShardID = 1
	if err := nh.StartReplica(peers, false, newSM, rc); err != nil {
		return nil, err
	}
	return nh, nil
}

func singleConcurrentNodeHostTest(t *testing.T,
	tf func(t *testing.T, nh *NodeHost),
	snapshotEntry uint64,
	concurrent bool,
	fs vfs.IFS) {
	defer func() {
		require.NoError(t, fs.RemoveAll(singleNodeHostTestDir))
	}()
	func() {
		defer leaktest.AfterTest(t)()
		require.NoError(t, fs.RemoveAll(singleNodeHostTestDir))
		nh, err := createConcurrentTestNodeHost(singleNodeHostTestAddr,
			singleNodeHostTestDir, snapshotEntry, concurrent, fs)
		require.NoError(t, err, "failed to create nodehost")
		defer func() {
			nh.Close()
		}()
		nhc := nh.NodeHostConfig()
		waitForLeaderToBeElected(t, nh, 1)
		waitForLeaderToBeElected(t, nh, 1+nhc.Expert.Engine.ApplyShards)
		tf(t, nh)
	}()
	reportLeakedFD(fs, t)
}

func twoFakeDiskNodeHostTest(t *testing.T,
	tf func(t *testing.T, nh1 *NodeHost, nh2 *NodeHost),
	fs vfs.IFS) {
	defer func() {
		require.NoError(t, fs.RemoveAll(singleNodeHostTestDir))
	}()
	func() {
		defer leaktest.AfterTest(t)()
		nh1dir := fs.PathJoin(singleNodeHostTestDir, "nh1")
		nh2dir := fs.PathJoin(singleNodeHostTestDir, "nh2")
		require.NoError(t, fs.RemoveAll(singleNodeHostTestDir))
		nh1, nh2, err := createFakeDiskTwoTestNodeHosts(nodeHostTestAddr1,
			nodeHostTestAddr2, nh1dir, nh2dir, fs)
		require.NoError(t, err, "failed to create nodehost")
		defer func() {
			nh1.Close()
			nh2.Close()
		}()
		tf(t, nh1, nh2)
	}()
	reportLeakedFD(fs, t)
}

func createFakeDiskTwoTestNodeHosts(addr1 string,
	addr2 string,
	datadir1 string,
	datadir2 string,
	fs vfs.IFS) (*NodeHost, *NodeHost, error) {
	peers := make(map[uint64]string)
	peers[1] = addr1
	nhc1 := config.NodeHostConfig{
		WALDir:              datadir1,
		NodeHostDir:         datadir1,
		RTTMillisecond:      getRTTMillisecond(fs, datadir1),
		RaftAddress:         addr1,
		SystemEventListener: &testSysEventListener{},
		Expert:              getTestExpertConfig(fs),
	}
	nhc2 := config.NodeHostConfig{
		WALDir:              datadir2,
		NodeHostDir:         datadir2,
		RTTMillisecond:      getRTTMillisecond(fs, datadir2),
		RaftAddress:         addr2,
		SystemEventListener: &testSysEventListener{},
		Expert:              getTestExpertConfig(fs),
	}
	nh1, err := NewNodeHost(nhc1)
	if err != nil {
		return nil, nil, err
	}
	nh2, err := NewNodeHost(nhc2)
	if err != nil {
		return nil, nil, err
	}
	return nh1, nh2, nil
}

func createRateLimitedTwoTestNodeHosts(addr1 string,
	addr2 string,
	datadir1 string,
	datadir2 string,
	fs vfs.IFS) (*NodeHost, *NodeHost, *tests.NoOP, *tests.NoOP, error) {
	rc := config.Config{
		ShardID:         1,
		ElectionRTT:     3,
		HeartbeatRTT:    1,
		CheckQuorum:     true,
		MaxInMemLogSize: 1024 * 3,
	}
	peers := make(map[uint64]string)
	peers[1] = addr1
	peers[2] = addr2
	nhc1 := config.NodeHostConfig{
		WALDir:         datadir1,
		NodeHostDir:    datadir1,
		RTTMillisecond: getRTTMillisecond(fs, datadir1),
		RaftAddress:    peers[1],
		Expert:         getTestExpertConfig(fs),
	}
	nhc2 := config.NodeHostConfig{
		WALDir:         datadir2,
		NodeHostDir:    datadir2,
		RTTMillisecond: getRTTMillisecond(fs, datadir2),
		RaftAddress:    peers[2],
		Expert:         getTestExpertConfig(fs),
	}
	nh1, err := NewNodeHost(nhc1)
	if err != nil {
		return nil, nil, nil, nil, err
	}
	nh2, err := NewNodeHost(nhc2)
	if err != nil {
		return nil, nil, nil, nil, err
	}
	sm1 := &tests.NoOP{}
	sm2 := &tests.NoOP{}
	newRSM1 := func(shardID uint64, replicaID uint64) sm.IStateMachine {
		return sm1
	}
	newRSM2 := func(shardID uint64, replicaID uint64) sm.IStateMachine {
		return sm2
	}
	rc.ReplicaID = 1
	if err := nh1.StartReplica(peers, false, newRSM1, rc); err != nil {
		return nil, nil, nil, nil, err
	}
	rc.ReplicaID = 2
	if err := nh2.StartReplica(peers, false, newRSM2, rc); err != nil {
		return nil, nil, nil, nil, err
	}
	var leaderNh *NodeHost
	var followerNh *NodeHost

	for i := 0; i < 200; i++ {
		leaderID, _, ready, err := nh1.GetLeaderID(1)
		if err == nil && ready {
			if leaderID == 1 {
				leaderNh = nh1
				followerNh = nh2
				sm2.SetSleepTime(nhc1.RTTMillisecond * 10)
			} else {
				leaderNh = nh2
				followerNh = nh1
				sm1.SetSleepTime(nhc1.RTTMillisecond * 10)
			}
			return leaderNh, followerNh, sm1, sm2, nil
		}
		// wait for leader to be elected
		time.Sleep(100 * time.Millisecond)
	}
	return nil, nil, nil, nil, errors.New("failed to get usable nodehosts")
}

func rateLimitedTwoNodeHostTest(t *testing.T,
	tf func(t *testing.T,
		leaderNh *NodeHost,
		followerNh *NodeHost,
		n1 *tests.NoOP,
		n2 *tests.NoOP),
	fs vfs.IFS) {
	defer func() {
		require.NoError(t, fs.RemoveAll(singleNodeHostTestDir))
	}()
	func() {
		nh1dir := fs.PathJoin(singleNodeHostTestDir, "nh1")
		nh2dir := fs.PathJoin(singleNodeHostTestDir, "nh2")
		defer leaktest.AfterTest(t)()
		require.NoError(t, fs.RemoveAll(singleNodeHostTestDir))
		nh1, nh2, n1, n2, err := createRateLimitedTwoTestNodeHosts(nodeHostTestAddr1,
			nodeHostTestAddr2, nh1dir, nh2dir, fs)
		require.NoError(t, err, "failed to create nodehost2")
		defer func() {
			nh1.Close()
			nh2.Close()
		}()
		tf(t, nh1, nh2, n1, n2)
	}()
	reportLeakedFD(fs, t)
}

func waitForLeaderToBeElected(t *testing.T, nh *NodeHost, shardID uint64) {
	for i := 0; i < 200; i++ {
		_, term, ready, err := nh.GetLeaderID(shardID)
		if err == nil && ready {
			if term == 0 {
				panic("term is 0")
			}
			return
		}
		time.Sleep(100 * time.Millisecond)
	}
	require.Fail(t, "failed to elect leader")
}

func TestJoinedShardCanBeRestartedOrJoinedAgain(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			cfg := getTestConfig()
			peers := make(map[uint64]string)
			newPST := func(uint64, uint64) sm.IStateMachine { return &PST{} }
			require.NoError(t, nh.StopShard(1), "failed to stop the shard")
			for i := 0; i < 1000; i++ {
				err := nh.StartReplica(peers, true, newPST, *cfg)
				if err == nil {
					return
				}
				if err == ErrShardAlreadyExist {
					time.Sleep(5 * time.Millisecond)
					continue
				} else {
					require.NoError(t, err, "failed to join the shard again")
				}
			}
		},
		join:       true,
		noElection: true,
	}
	runNodeHostTest(t, to, fs)
}

func TestCompactionCanBeRequested(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		updateConfig: func(c *config.Config) *config.Config {
			c.SnapshotEntries = 10
			c.CompactionOverhead = 5
			c.DisableAutoCompactions = true
			return c
		},
		tf: func(nh *NodeHost) {
			pto := lpto(nh)
			session := nh.GetNoOPSession(1)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			defer cancel()
			_, err := nh.SyncPropose(ctx, session, []byte("test-data"))
			require.NoError(t, err, "failed to make proposal")
			opt := SnapshotOption{
				OverrideCompactionOverhead: true,
				CompactionOverhead:         0,
			}
			_, err = nh.SyncRequestSnapshot(ctx, 1, opt)
			require.NoError(t, err, "failed to request snapshot")
			for i := 0; i < 100; i++ {
				op, err := nh.RequestCompaction(1, 1)
				if err == ErrRejected {
					time.Sleep(100 * time.Millisecond)
					continue
				}
				require.NoError(t, err, "failed to request compaction")
				select {
				case <-op.ResultC():
				case <-ctx.Done():
					require.FailNow(t, "failed to complete the compaction")
				}
				break
			}
			_, err = nh.RequestCompaction(1, 1)
			require.Equal(t, ErrRejected, err, "not rejected")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestSnapshotCanBeStopped(t *testing.T) {
	fs := vfs.GetTestFS()
	pst := &PST{slowSave: true}
	to := &testOption{
		updateConfig: func(c *config.Config) *config.Config {
			c.SnapshotEntries = 10
			return c
		},
		createSM: func(shardID uint64, replicaID uint64) sm.IStateMachine {
			return pst
		},
		tf: func(nh *NodeHost) {
			createProposalsToTriggerSnapshot(t, nh, 50, true)
		},
		at: func(*NodeHost) {
			assert.True(t, pst.saved && pst.stopped, "snapshot not stopped")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestRecoverFromSnapshotCanBeStopped(t *testing.T) {
	fs := vfs.GetTestFS()
	pst := &PST{slowSave: false}
	to := &testOption{
		updateConfig: func(c *config.Config) *config.Config {
			c.SnapshotEntries = 10
			return c
		},
		createSM: func(shardID uint64, replicaID uint64) sm.IStateMachine {
			return pst
		},
		tf: func(nh *NodeHost) {
			createProposalsToTriggerSnapshot(t, nh, 50, false)
		},
		rf: func(nh *NodeHost) {
			wait := 0
			for !pst.getRestored() {
				time.Sleep(10 * time.Millisecond)
				wait++
				if wait > 1000 {
					break
				}
			}
		},
		at: func(*NodeHost) {
			wait := 0
			for !pst.stopped {
				time.Sleep(10 * time.Millisecond)
				wait++
				if wait > 1000 {
					break
				}
			}
			assert.True(t, pst.getRestored(), "not restored")
			assert.True(t, pst.stopped, "not stopped")
		},
		restartNodeHost: true,
	}
	runNodeHostTest(t, to, fs)
}

func TestGetRequestState(t *testing.T) {
	tests := []struct {
		code RequestResultCode
		err  error
	}{
		{requestCompleted, nil},
		{requestRejected, ErrRejected},
		{requestTimeout, ErrTimeout},
		{requestTerminated, ErrShardClosed},
		{requestDropped, ErrShardNotReady},
		{requestAborted, ErrAborted},
	}

	for _, tt := range tests {
		rs := &RequestState{
			CompletedC: make(chan RequestResult, 1),
		}
		result := RequestResult{code: tt.code}
		rs.notify(result)
		_, err := getRequestState(context.TODO(), rs)
		assert.ErrorIs(t, err, tt.err)
	}
}

func TestGetRequestStateTimeoutAndCancel(t *testing.T) {
	func() {
		ctx, cancel := context.WithTimeout(context.Background(),
			time.Millisecond)
		defer cancel()
		time.Sleep(2 * time.Millisecond)
		rs := &RequestState{
			CompletedC: make(chan RequestResult, 1),
		}
		_, err := getRequestState(ctx, rs)
		assert.ErrorIs(t, err, ErrTimeout)
	}()

	func() {
		ctx, cancel := context.WithTimeout(context.Background(), time.Hour)
		cancel()
		rs := &RequestState{
			CompletedC: make(chan RequestResult, 1),
		}
		_, err := getRequestState(ctx, rs)
		assert.ErrorIs(t, err, ErrCanceled)
	}()
}

func TestNodeHostIDIsStatic(t *testing.T) {
	fs := vfs.GetTestFS()
	id := ""
	to := &testOption{
		restartNodeHost: true,
		noElection:      true,
		tf: func(nh *NodeHost) {
			id = nh.ID()
		},
		rf: func(nh *NodeHost) {
			require.Equal(t, id, nh.ID(), "NodeHost ID value changed")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestNodeHostIDCanBeSet(t *testing.T) {
	fs := vfs.GetTestFS()
	nhidStr := testNodeHostID1
	to := &testOption{
		updateNodeHostConfig: func(c *config.NodeHostConfig) *config.NodeHostConfig {
			c.NodeHostID = nhidStr
			return c
		},
		noElection: true,
		tf: func(nh *NodeHost) {
			nhid, err := id.NewUUID(nhidStr)
			require.NoError(t, err, "failed to create NodeHostID")
			require.Equal(t, nhid.String(), nh.ID(), "failed to set nhid")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestInvalidAddressIsRejected(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := pto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			defer cancel()
			err := nh.SyncRequestAddReplica(ctx, 1, 100, "a1", 0)
			assert.ErrorIs(t, err, ErrInvalidAddress)
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestInvalidContextDeadlineIsReported(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := lpto(nh)
			rctx, rcancel := context.WithTimeout(context.Background(), pto)
			rcs, err := nh.SyncGetSession(rctx, 1)
			rcancel()
			require.NoError(t, err, "failed to get regular session")
			// 8 * time.Millisecond is smaller than the smallest RTTMillisecond
			ctx, cancel := context.WithTimeout(context.Background(),
				8*time.Millisecond)
			defer cancel()
			cs := nh.GetNoOPSession(1)
			_, err = nh.SyncPropose(ctx, cs, make([]byte, 1))
			assert.ErrorIs(t, err, ErrTimeoutTooSmall)
			_, err = nh.SyncRead(ctx, 1, nil)
			assert.ErrorIs(t, err, ErrTimeoutTooSmall)
			_, err = nh.SyncGetSession(ctx, 1)
			assert.ErrorIs(t, err, ErrTimeoutTooSmall)
			err = nh.SyncCloseSession(ctx, rcs)
			assert.ErrorIs(t, err, ErrTimeoutTooSmall)
			_, err = nh.SyncRequestSnapshot(ctx, 1, DefaultSnapshotOption)
			assert.ErrorIs(t, err, ErrTimeoutTooSmall)
			err = nh.SyncRequestDeleteReplica(ctx, 1, 1, 0)
			assert.ErrorIs(t, err, ErrTimeoutTooSmall)
			err = nh.SyncRequestAddReplica(ctx, 1, 100, "a1.com:12345", 0)
			assert.ErrorIs(t, err, ErrTimeoutTooSmall)
			err = nh.SyncRequestAddNonVoting(ctx, 1, 100, "a1.com:12345", 0)
			assert.ErrorIs(t, err, ErrTimeoutTooSmall)
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestErrShardNotFoundCanBeReturned(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := lpto(nh)
			_, _, _, err := nh.GetLeaderID(1234)
			assert.ErrorIs(t, err, ErrShardNotFound)
			_, err = nh.StaleRead(1234, nil)
			assert.ErrorIs(t, err, ErrShardNotFound)
			_, err = nh.RequestSnapshot(1234, DefaultSnapshotOption, pto)
			assert.ErrorIs(t, err, ErrShardNotFound)
			_, err = nh.RequestDeleteReplica(1234, 10, 0, pto)
			assert.ErrorIs(t, err, ErrShardNotFound)
			_, err = nh.RequestAddReplica(1234, 10, "a1", 0, pto)
			assert.ErrorIs(t, err, ErrShardNotFound)
			_, err = nh.RequestAddNonVoting(1234, 10, "a1", 0, pto)
			assert.ErrorIs(t, err, ErrShardNotFound)
			err = nh.RequestLeaderTransfer(1234, 10)
			assert.ErrorIs(t, err, ErrShardNotFound)
			_, err = nh.GetNodeUser(1234)
			assert.ErrorIs(t, err, ErrShardNotFound)
			cs := nh.GetNoOPSession(1234)
			_, err = nh.propose(cs, make([]byte, 1), pto)
			assert.ErrorIs(t, err, ErrShardNotFound)
			_, _, err = nh.readIndex(1234, pto)
			assert.ErrorIs(t, err, ErrShardNotFound)
			err = nh.stopNode(1234, 1, true)
			assert.ErrorIs(t, err, ErrShardNotFound)
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestGetShardMembership(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := pto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			defer cancel()
			_, err := nh.SyncGetShardMembership(ctx, 1)
			require.NoError(t, err, "failed to get shard membership")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestRegisterASessionTwiceWillBeReported(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := lpto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			defer cancel()
			cs, err := nh.SyncGetSession(ctx, 1)
			assert.NoError(t, err, "failed to get client session")
			cs.PrepareForRegister()
			rs, err := nh.ProposeSession(cs, pto)
			assert.NoError(t, err, "failed to propose client session")
			r := <-rs.ResultC()
			assert.True(t, r.Rejected(),
				"failed to reject the cs registeration")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestUnregisterNotRegisterClientSessionWillBeReported(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := lpto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			defer cancel()
			cs, err := nh.SyncGetSession(ctx, 1)
			assert.NoError(t, err, "failed to get client session")
			err = nh.SyncCloseSession(ctx, cs)
			assert.NoError(t, err, "failed to unregister the client session")
			err = nh.SyncCloseSession(ctx, cs)
			assert.ErrorIs(t, err, ErrRejected, "failed to reject the request")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestSnapshotFilePayloadChecksumIsSaved(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		updateConfig: func(c *config.Config) *config.Config {
			c.SnapshotEntries = 10
			return c
		},
		tf: func(nh *NodeHost) {
			cs := nh.GetNoOPSession(1)
			logdb := nh.mu.logdb
			snapshotted := false
			var snapshot pb.Snapshot
			for i := 0; i < 1000; i++ {
				pto := pto(nh)
				ctx, cancel := context.WithTimeout(context.Background(), pto)
				_, err := nh.SyncPropose(ctx, cs, []byte("test-data"))
				cancel()
				if err != nil {
					continue
				}
				ss, err := logdb.GetSnapshot(1, 1)
				require.NoError(t, err, "failed to list snapshots")
				if !pb.IsEmptySnapshot(ss) {
					snapshotted = true
					snapshot = ss
					break
				}
			}
			require.True(t, snapshotted, "snapshot not triggered")
			crc, err := rsm.GetV2PayloadChecksum(snapshot.Filepath, fs)
			require.NoError(t, err, "failed to get payload checksum")
			assert.True(t, bytes.Equal(crc, snapshot.Checksum), "checksum changed")
			ss := pb.Snapshot{}
			err = fileutil.GetFlagFileContent(fs.PathDir(snapshot.Filepath),
				"snapshot.metadata", &ss, fs)
			require.NoError(t, err, "failed to get content")
			assert.Equal(t, &snapshot, &ss, "snapshot record changed")
		},
	}
	runNodeHostTest(t, to, fs)
}

func testZombieSnapshotDirWillBeDeletedDuringAddShard(t *testing.T,
	dirName string,
	fs vfs.IFS) {
	var z1 string
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			did := nh.nhConfig.GetDeploymentID()
			require.NoError(t, nh.env.CreateSnapshotDir(did, 1, 1),
				"failed to get snap dir")
			snapDir := nh.env.GetSnapshotDir(did, 1, 1)
			z1 = fs.PathJoin(snapDir, dirName)
			require.NoError(t, fs.MkdirAll(z1, 0755), "failed to create dir")
		},
		rf: func(nh *NodeHost) {
			_, err := fs.Stat(z1)
			require.True(t, vfs.IsNotExist(err), "failed to delete zombie dir")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestZombieSnapshotDirWillBeDeletedDuringAddShard(t *testing.T) {
	fs := vfs.GetTestFS()
	defer leaktest.AfterTest(t)()
	testZombieSnapshotDirWillBeDeletedDuringAddShard(t,
		"snapshot-AB-01.receiving", fs)
	testZombieSnapshotDirWillBeDeletedDuringAddShard(t,
		"snapshot-AB-10.generating", fs)
}

func TestNodeHostReadIndex(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := lpto(nh)
			rs, err := nh.ReadIndex(1, pto)
			require.NoError(t, err, "failed to read index")
			require.NotNil(t, rs.node, "rs.node not set")
			v := <-rs.ResultC()
			assert.True(t, v.Completed(), "failed to complete read index")
			_, err = nh.ReadLocalNode(rs, make([]byte, 128))
			assert.NoError(t, err, "read local failed")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestNALookupCanReturnErrNotImplemented(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := lpto(nh)
			rs, err := nh.ReadIndex(1, pto)
			require.NoError(t, err, "failed to read index")
			v := <-rs.ResultC()
			require.True(t, v.Completed(), "failed to complete read index")
			_, err = nh.NAReadLocalNode(rs, make([]byte, 128))
			assert.ErrorIs(t, err, sm.ErrNotImplemented)
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestNodeHostSyncIOAPIs(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			cs := nh.GetNoOPSession(1)
			pto := lpto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			defer cancel()
			v, err := nh.SyncPropose(ctx, cs, make([]byte, 128))
			require.NoError(t, err, "make proposal failed")
			assert.Equal(t, uint64(128), v.Value, "unexpected result")
			data, err := nh.SyncRead(ctx, 1, make([]byte, 128))
			require.NoError(t, err, "make linearizable read failed")
			require.NotNil(t, data)
			require.NotEmpty(t, data.([]byte), "failed to get result")
			require.NoError(t, nh.StopShard(1), "failed to stop shard 2")
			listener, ok := nh.events.sys.ul.(*testSysEventListener)
			require.True(t, ok, "failed to get the system event listener")
			waitNodeInfoEvent(t, listener.getNodeReady, 1)
			ni := listener.getNodeReady()[0]
			assert.Equal(t, uint64(1), ni.ShardID, "incorrect node ready info")
			assert.Equal(t, uint64(1), ni.ReplicaID, "incorrect node ready info")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestEntryCompression(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		updateConfig: func(c *config.Config) *config.Config {
			c.EntryCompressionType = config.Snappy
			return c
		},
		tf: func(nh *NodeHost) {
			cs := nh.GetNoOPSession(1)
			pto := pto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			defer cancel()
			_, err := nh.SyncPropose(ctx, cs, make([]byte, 1024))
			require.NoError(t, err, "make proposal failed")
			logdb := nh.mu.logdb
			ents, _, err := logdb.IterateEntries(nil,
				0, 1, 1, 1, 100, math.MaxUint64)
			require.NoError(t, err, "failed to get entries")
			hasEncodedEntry := false
			for _, e := range ents {
				if e.Type == pb.EncodedEntry {
					hasEncodedEntry = true
					payload, err := rsm.GetPayload(e)
					require.NoError(t, err, "failed to get payload")
					assert.Equal(t, make([]byte, 1024), payload, "payload changed")
				}
			}
			assert.True(t, hasEncodedEntry, "failed to locate any encoded entry")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestOrderedMembershipChange(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		updateConfig: func(c *config.Config) *config.Config {
			c.OrderedConfigChange = true
			return c
		},
		tf: func(nh *NodeHost) {
			pto := pto(nh)
			{
				ctx, cancel := context.WithTimeout(context.Background(), 2*pto)
				defer cancel()
				m, err := nh.SyncGetShardMembership(ctx, 1)
				require.NoError(t, err, "get membership failed")
				err = nh.SyncRequestAddReplica(ctx,
					1, 2, "localhost:25000", m.ConfigChangeID+1)
				assert.Error(t, err, "unexpectedly completed")
			}
			{
				ctx, cancel := context.WithTimeout(context.Background(), 2*pto)
				defer cancel()
				m, err := nh.SyncGetShardMembership(ctx, 1)
				require.NoError(t, err, "get membership failed")
				err = nh.SyncRequestAddReplica(ctx,
					1, 2, "localhost:25000", m.ConfigChangeID)
				require.NoError(t, err, "failed to add node")
			}
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestSyncRequestDeleteReplica(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := lpto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			defer cancel()
			err := nh.SyncRequestDeleteReplica(ctx, 1, 2, 0)
			require.NoError(t, err, "failed to delete node")
			listener, ok := nh.events.sys.ul.(*testSysEventListener)
			require.True(t, ok, "failed to get the system event listener")
			waitNodeInfoEvent(t, listener.getMembershipChanged, 2)
			ni := listener.getMembershipChanged()[1]
			assert.Equal(t, uint64(1), ni.ShardID)
			assert.Equal(t, uint64(1), ni.ReplicaID)
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestSyncRequestAddReplica(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := pto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			defer cancel()
			err := nh.SyncRequestAddReplica(ctx, 1, 2, "localhost:25000", 0)
			assert.NoError(t, err, "failed to add node")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestSyncRequestAddNonVoting(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := pto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			defer cancel()
			err := nh.SyncRequestAddNonVoting(ctx, 1, 2, "localhost:25000", 0)
			assert.NoError(t, err, "failed to add nonVoting")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestNodeHostAddReplica(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := pto(nh)
			rs, err := nh.RequestAddReplica(1, 2, "localhost:25000", 0, pto)
			require.NoError(t, err, "failed to add node")
			v := <-rs.ResultC()
			assert.True(t, v.Completed(), "failed to complete add node")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestNodeHostGetNodeUser(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			n, err := nh.GetNodeUser(1)
			assert.NoError(t, err, "failed to get NodeUser")
			assert.NotNil(t, n, "got a nil NodeUser")
			n, err = nh.GetNodeUser(123)
			assert.ErrorIs(t, err, ErrShardNotFound, "didn't return expected err")
			assert.Nil(t, n, "got unexpected node user")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestNodeHostNodeUserPropose(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := pto(nh)
			n, err := nh.GetNodeUser(1)
			require.NoError(t, err, "failed to get NodeUser")
			cs := nh.GetNoOPSession(1)
			rs, err := n.Propose(cs, make([]byte, 16), pto)
			assert.NoError(t, err, "failed to make propose")
			v := <-rs.ResultC()
			assert.True(t, v.Completed(), "failed to complete proposal")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestNodeHostNodeUserRead(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := pto(nh)
			n, err := nh.GetNodeUser(1)
			require.NoError(t, err, "failed to get NodeUser")
			rs, err := n.ReadIndex(pto)
			assert.NoError(t, err, "failed to read index")
			v := <-rs.ResultC()
			assert.True(t, v.Completed(), "failed to complete read index")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestNodeHostAddNonVotingRemoveNode(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := pto(nh)
			rs, err := nh.RequestAddNonVoting(1, 2, "localhost:25000", 0, pto)
			require.NoError(t, err, "failed to add node")
			v := <-rs.ResultC()
			require.True(t, v.Completed(), "failed to complete add node")
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			defer cancel()
			membership, err := nh.SyncGetShardMembership(ctx, 1)
			require.NoError(t, err, "failed to get shard membership")
			require.Equal(t, 1, len(membership.Nodes), "unexpected nodes len")
			require.Equal(t, 0, len(membership.Removed), "unexpected removed len")
			require.Equal(t, 1, len(membership.NonVotings), "unexpected nodes len")
			_, ok := membership.NonVotings[2]
			require.True(t, ok, "node 2 not added")
			// remove it
			rs, err = nh.RequestDeleteReplica(1, 2, 0, pto)
			require.NoError(t, err, "failed to remove node")
			v = <-rs.ResultC()
			require.True(t, v.Completed(), "failed to complete remove node")
			ctx, cancel = context.WithTimeout(context.Background(), pto)
			defer cancel()
			membership, err = nh.SyncGetShardMembership(ctx, 1)
			require.NoError(t, err, "failed to get shard membership")
			require.Equal(t, 1, len(membership.Nodes), "unexpected nodes len")
			require.Equal(t, 1, len(membership.Removed), "unexpected removed len")
			require.Equal(t, 0, len(membership.NonVotings), "unexpected nodes len")
			_, ok = membership.Removed[2]
			require.True(t, ok, "node 2 not removed")
		},
	}
	runNodeHostTest(t, to, fs)
}

// FIXME:
// Leadership transfer is not actually tested
func TestNodeHostLeadershipTransfer(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			err := nh.RequestLeaderTransfer(1, 1)
			assert.NoError(t, err, "leader transfer failed")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestNodeHostHasNodeInfo(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			assert.True(t, nh.HasNodeInfo(1, 1), "node info missing")
			assert.False(t, nh.HasNodeInfo(1, 2), "unexpected node info")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestOnDiskStateMachineDoesNotSupportClientSession(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		fakeDiskNode: true,
		tf: func(nh *NodeHost) {
			require.Panics(t, func() {
				pto := pto(nh)
				ctx, cancel := context.WithTimeout(context.Background(), pto)
				_, err := nh.SyncGetSession(ctx, 1)
				cancel()
				assert.Error(t, err, "managed to get new session")
			}, "no panic when proposing session on disk SM")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestStaleReadOnUninitializedNodeReturnError(t *testing.T) {
	fs := vfs.GetTestFS()
	fakeDiskSM := tests.NewFakeDiskSM(0)
	atomic.StoreUint32(&fakeDiskSM.SlowOpen, 1)
	to := &testOption{
		createOnDiskSM: func(uint64, uint64) sm.IOnDiskStateMachine {
			return fakeDiskSM
		},
		tf: func(nh *NodeHost) {
			n, ok := nh.getShard(1)
			require.True(t, ok, "failed to get the node")
			require.False(t, n.initialized(), "node unexpectedly initialized")
			_, err := nh.StaleRead(1, nil)
			require.ErrorIs(t, err, ErrShardNotInitialized,
				"expected to return ErrShardNotInitialized")
			atomic.StoreUint32(&fakeDiskSM.SlowOpen, 0)
			for !n.initialized() {
				runtime.Gosched()
			}
			v, err := nh.StaleRead(1, nil)
			require.NoError(t, err, "stale read failed")
			require.Len(t, v.([]byte), 8, "unexpected result")
		},
		noElection: true,
	}
	runNodeHostTest(t, to, fs)
}

func TestStartReplicaWaitForReadiness(t *testing.T) {
	fs := vfs.GetTestFS()
	fakeDiskSM := tests.NewFakeDiskSM(0)
	atomic.StoreUint32(&fakeDiskSM.SlowOpen, 1)
	to := &testOption{
		defaultTestNode: false,
		noElection:      true,
		tf: func(nh *NodeHost) {
			cfg := getTestConfig()
			cfg.WaitReady = true

			go func() {
				defer atomic.StoreUint32(&fakeDiskSM.SlowOpen, 0)
				var n *node
				var ok bool

				for i := 0; i < 10; i++ {
					n, ok = nh.getShard(cfg.ShardID)
					if ok {
						break
					}
					time.Sleep(time.Millisecond * 100)
				}
				require.NotNil(t, n, "failed to get the node")
				require.False(t, n.initialized(), "node unexpectedly initialized")
				_, err := nh.StaleRead(1, nil)
				require.ErrorIs(t, err, ErrShardNotInitialized,
					"expected to return ErrShardNotInitialized")
			}()

			initialMembers := map[uint64]Target{
				1: nh.RaftAddress(),
			}

			err := nh.StartOnDiskReplica(initialMembers,
				false,
				func(shardID uint64, replicaID uint64) sm.IOnDiskStateMachine {
					return fakeDiskSM
				},
				*cfg)
			require.NoError(t, err, "failed to StartOnDiskReplica")

			v, err := nh.StaleRead(1, nil)
			require.NoError(t, err, "stale read failed")
			require.Len(t, v.([]byte), 8, "unexpected result")
		},
	}
	runNodeHostTest(t, to, fs)
}

func testOnDiskStateMachineCanTakeDummySnapshot(t *testing.T, compressed bool) {
	fs := vfs.GetTestFS()
	to := &testOption{
		fakeDiskNode: true,
		compressed:   compressed,
		updateConfig: func(c *config.Config) *config.Config {
			c.SnapshotEntries = 30
			c.CompactionOverhead = 30
			return c
		},
		tf: func(nh *NodeHost) {
			session := nh.GetNoOPSession(1)
			logdb := nh.mu.logdb
			snapshotted := false
			var ss pb.Snapshot
			for i := uint64(2); i < 1000; i++ {
				pto := pto(nh)
				ctx, cancel := context.WithTimeout(context.Background(), pto)
				_, err := nh.SyncPropose(ctx, session, []byte("test-data"))
				cancel()
				if err != nil {
					continue
				}
				snapshot, err := logdb.GetSnapshot(1, 1)
				require.NoError(t, err, "list snapshot failed")
				if !pb.IsEmptySnapshot(snapshot) {
					snapshotted = true
					ss = snapshot
					require.True(t, ss.Dummy, "dummy snapshot is not recorded as dummy")
					break
				} else if i%100 == 0 {
					time.Sleep(200 * time.Millisecond)
				}
			}
			require.True(t, snapshotted, "failed to snapshot")
			fi, err := fs.Stat(ss.Filepath)
			require.NoError(t, err, "failed to get file st")
			require.Equal(t, int64(1060), fi.Size(), "unexpected dummy snapshot size")
			reader, h, err := rsm.NewSnapshotReader(ss.Filepath, fs)
			require.NoError(t, err, "failed to read snapshot")
			assert.Equal(t, config.NoCompression, h.CompressionType,
				"dummy snapshot compressed")
			assert.Equal(t, rsm.DefaultVersion,
				rsm.SSVersion(h.Version), "unexpected snapshot version")
			require.NoError(t, reader.Close())
			shrunk, err := rsm.IsShrunkSnapshotFile(ss.Filepath, fs)
			require.NoError(t, err, "failed to check shrunk")
			assert.True(t, shrunk, "not a dummy snapshot")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestOnDiskStateMachineCanTakeDummySnapshot(t *testing.T) {
	testOnDiskStateMachineCanTakeDummySnapshot(t, true)
	testOnDiskStateMachineCanTakeDummySnapshot(t, false)
}

func TestOnDiskSMCanStreamSnapshot(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(t *testing.T, nh1 *NodeHost, nh2 *NodeHost) {
		rc := config.Config{
			ShardID:                 1,
			ReplicaID:               1,
			ElectionRTT:             3,
			HeartbeatRTT:            1,
			CheckQuorum:             true,
			SnapshotEntries:         5,
			CompactionOverhead:      2,
			SnapshotCompressionType: config.Snappy,
			EntryCompressionType:    config.Snappy,
		}
		sm1 := tests.NewFakeDiskSM(0)
		sm1.SetAborted()
		peers := make(map[uint64]string)
		peers[1] = nodeHostTestAddr1
		newSM := func(uint64, uint64) sm.IOnDiskStateMachine {
			return sm1
		}
		require.NoError(t, nh1.StartOnDiskReplica(peers, false, newSM, rc),
			"failed to start shard")
		waitForLeaderToBeElected(t, nh1, 1)
		logdb := nh1.mu.logdb
		snapshotted := false
		session := nh1.GetNoOPSession(1)
		for i := uint64(2); i < 1000; i++ {
			pto := pto(nh1)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			_, err := nh1.SyncPropose(ctx, session, []byte("test-data"))
			cancel()
			if err != nil {
				time.Sleep(100 * time.Millisecond)
				continue
			}
			snapshot, err := logdb.GetSnapshot(1, 1)
			require.NoError(t, err, "list snapshot failed")
			if !pb.IsEmptySnapshot(snapshot) {
				snapshotted = true
				break
			} else if i%50 == 0 {
				time.Sleep(100 * time.Millisecond)
			}
		}
		require.True(t, snapshotted, "failed to take 3 snapshots")
		pto := pto(nh1)
		rs, err := nh1.RequestAddReplica(1, 2, nodeHostTestAddr2, 0, pto)
		require.NoError(t, err, "failed to add node")
		s := <-rs.ResultC()
		require.True(t, s.Completed(), "failed to complete the add node request")
		rc = config.Config{
			ShardID:            1,
			ReplicaID:          2,
			ElectionRTT:        3,
			HeartbeatRTT:       1,
			CheckQuorum:        true,
			SnapshotEntries:    5,
			CompactionOverhead: 2,
		}
		sm2 := tests.NewFakeDiskSM(0)
		sm2.SetAborted()
		newSM2 := func(uint64, uint64) sm.IOnDiskStateMachine {
			return sm2
		}
		sm1.ClearAborted()
		require.NoError(t, nh2.StartOnDiskReplica(nil, true, newSM2, rc),
			"failed to start shard")
		ssIndex := uint64(0)
		logdb = nh2.mu.logdb
		waitForLeaderToBeElected(t, nh2, 1)
		for i := uint64(2); i < 1000; i++ {
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			_, err := nh2.SyncPropose(ctx, session, []byte("test-data"))
			cancel()
			plog.Infof("nh2 proposal result: %v", err)
			if err != nil {
				time.Sleep(100 * time.Millisecond)
				continue
			}
			ss, err := logdb.GetSnapshot(1, 2)
			require.NoError(t, err, "list snapshot failed")
			if !pb.IsEmptySnapshot(ss) {
				require.True(t, sm2.Recovered(), "not recovered")
				require.True(t, sm1.Aborted(), "not aborted")
				require.NotZero(t, ss.OnDiskIndex,
					"on disk index not recorded in ss")
				shrunk, err := rsm.IsShrunkSnapshotFile(ss.Filepath, fs)
				require.NoError(t, err, "failed to check whether snapshot is shrunk")
				require.True(t, shrunk, "snapshot %d is not shrunk", ss.Index)
				if ssIndex == 0 {
					ssIndex = ss.Index
				} else {
					if ssIndex != ss.Index {
						break
					}
				}
			} else if i%50 == 0 {
				time.Sleep(100 * time.Millisecond)
			}
		}
		require.NotZero(t, ssIndex, "failed to take 2 snapshots")
		listener, ok := nh2.events.sys.ul.(*testSysEventListener)
		require.True(t, ok, "failed to get the system event listener")
		require.NotEmpty(t, listener.getSnapshotReceived(),
			"snapshot received not notified")
		require.NotEmpty(t, listener.getSnapshotRecovered(),
			"failed to be notified for recovered snapshot")
		require.NotEmpty(t, listener.getLogCompacted(),
			"log compaction not notified")
		listener, ok = nh1.events.sys.ul.(*testSysEventListener)
		require.True(t, ok, "failed to get the system event listener")
		require.NotEmpty(t, listener.getSendSnapshotStarted(),
			"send snapshot started not notified")
		require.NotEmpty(t, listener.getSendSnapshotCompleted(),
			"send snapshot completed not notified")
		require.NotZero(t, listener.getConnectionEstablished(),
			"connection established not notified")
	}
	twoFakeDiskNodeHostTest(t, tf, fs)
}

func TestConcurrentStateMachineLookup(t *testing.T) {
	fs := vfs.GetTestFS()
	done := uint32(0)
	tf := func(t *testing.T, nh *NodeHost) {
		nhc := nh.NodeHostConfig()
		shardID := 1 + nhc.Expert.Engine.ApplyShards
		count := uint32(0)
		stopper := syncutil.NewStopper()
		pto := pto(nh)
		stopper.RunWorker(func() {
			for i := 0; i < 10000; i++ {
				ctx, cancel := context.WithTimeout(context.Background(), pto)
				session := nh.GetNoOPSession(shardID)
				_, err := nh.SyncPropose(ctx, session, []byte("test"))
				cancel()
				if err == ErrTimeout {
					continue
				}
				require.NoError(t, err, "failed to make proposal")
				if atomic.LoadUint32(&count) > 0 {
					return
				}
			}
		})
		stopper.RunWorker(func() {
			for i := 0; i < 10000; i++ {
				if i%5 == 0 {
					time.Sleep(time.Millisecond)
				}
				rs, err := nh.ReadIndex(shardID, pto)
				if err != nil {
					continue
				}
				s := <-rs.ResultC()
				if !s.Completed() {
					continue
				}
				st := random.LockGuardedRand.Uint64()%7 + 1
				time.Sleep(time.Duration(st) * time.Millisecond)
				result, err := nh.ReadLocalNode(rs, []byte("test"))
				if err != nil {
					continue
				}
				v := binary.LittleEndian.Uint32(result.([]byte))
				if v%2 == 1 {
					atomic.AddUint32(&count, 1)
					atomic.StoreUint32(&done, 1)
					return
				}
			}
		})
		stopper.Stop()
		require.NotZero(t, atomic.LoadUint32(&done),
			"failed to have any concurrent read")
	}
	singleConcurrentNodeHostTest(t, tf, 0, true, fs)
}

func TestConcurrentStateMachineSaveSnapshot(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(t *testing.T, nh *NodeHost) {
		nhc := nh.NodeHostConfig()
		shardID := 1 + nhc.Expert.Engine.ApplyShards
		nhi := nh.GetNodeHostInfo(DefaultNodeHostInfoOption)
		for _, ci := range nhi.ShardInfoList {
			if ci.ShardID == shardID {
				assert.Equal(t, sm.Type(sm.ConcurrentStateMachine), ci.StateMachineType,
					"unexpected state machine type")
			}
			assert.False(t, ci.IsNonVoting, "unexpected IsNonVoting value")
		}
		result := make(map[uint64]struct{})
		session := nh.GetNoOPSession(shardID)
		pto := pto(nh)
		for i := 0; i < 10000; i++ {
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			v, err := nh.SyncPropose(ctx, session, []byte("test"))
			cancel()
			require.NoError(t, err, "failed to make proposal")
			result[v.Value] = struct{}{}
			if len(result) > 1 {
				return
			}
			time.Sleep(time.Millisecond)
		}
		require.FailNow(t, "failed to make proposal when saving snapshots")
	}
	singleConcurrentNodeHostTest(t, tf, 10, true, fs)
}

func TestErrorCanBeReturnedWhenLookingUpConcurrentStateMachine(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(t *testing.T, nh *NodeHost) {
		nhc := nh.NodeHostConfig()
		shardID := 1 + nhc.Expert.Engine.ApplyShards
		for i := 0; i < 100; i++ {
			pto := pto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			_, err := nh.SyncRead(ctx, shardID, []byte("test"))
			cancel()
			require.ErrorIs(t, err, sm.ErrSnapshotStopped, "error not returned")
		}
	}
	singleConcurrentNodeHostTest(t, tf, 10, true, fs)
}

func TestRegularStateMachineDoesNotAllowConucrrentUpdate(t *testing.T) {
	fs := vfs.GetTestFS()
	failed := uint32(0)
	tf := func(t *testing.T, nh *NodeHost) {
		nhi := nh.GetNodeHostInfo(DefaultNodeHostInfoOption)
		for _, ci := range nhi.ShardInfoList {
			if ci.ShardID == 1 {
				assert.Equal(t, sm.Type(sm.RegularStateMachine), ci.StateMachineType,
					"unexpected state machine type")
			}
			assert.False(t, ci.IsNonVoting, "unexpected IsNonVoting value")
		}
		stopper := syncutil.NewStopper()
		pto := pto(nh)
		stopper.RunWorker(func() {
			for i := 0; i < 100; i++ {
				ctx, cancel := context.WithTimeout(context.Background(), pto)
				session := nh.GetNoOPSession(1)
				_, err := nh.SyncPropose(ctx, session, []byte("test"))
				if err != nil {
					plog.Infof("failed to make proposal %v\n", err)
				}
				cancel()
				if atomic.LoadUint32(&failed) == 1 {
					return
				}
			}
		})
		stopper.RunWorker(func() {
			for i := 0; i < 100; i++ {
				ctx, cancel := context.WithTimeout(context.Background(), pto)
				result, err := nh.SyncRead(ctx, 1, []byte("test"))
				cancel()
				if err != nil {
					continue
				}
				v := binary.LittleEndian.Uint32(result.([]byte))
				if v == 1 {
					atomic.StoreUint32(&failed, 1)
					return
				}
			}
		})
		stopper.Stop()
		require.Zero(t, atomic.LoadUint32(&failed),
			"unexpected concurrent update observed")
	}
	singleConcurrentNodeHostTest(t, tf, 0, false, fs)
}

func TestRegularStateMachineDoesNotAllowConcurrentSaveSnapshot(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(t *testing.T, nh *NodeHost) {
		result := make(map[uint64]struct{})
		session := nh.GetNoOPSession(1)
		pto := pto(nh)
		for i := 0; i < 50; i++ {
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			v, err := nh.SyncPropose(ctx, session, []byte("test"))
			cancel()
			if err != nil {
				continue
			}
			result[v.Value] = struct{}{}
			require.LessOrEqual(t, len(result), 1,
				"unexpected concurrent save snapshot observed")
		}
	}
	singleConcurrentNodeHostTest(t, tf, 10, false, fs)
}

func TestLogDBRateLimit(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		updateConfig: func(c *config.Config) *config.Config {
			c.MaxInMemLogSize = 1024 * 3
			return c
		},
		updateNodeHostConfig: func(c *config.NodeHostConfig) *config.NodeHostConfig {
			logDBConfig := config.GetDefaultLogDBConfig()
			logDBConfig.KVMaxWriteBufferNumber = 2
			logDBConfig.KVWriteBufferSize = 1024 * 8
			c.Expert.LogDB = logDBConfig
			return c
		},
		tf: func(nh *NodeHost) {
			if nh.mu.logdb.Name() == "Tan" {
				t.Skip("skipped, using tan logdb")
			}
			rateLimited := false
			for i := 0; i < 10240; i++ {
				pto := pto(nh)
				session := nh.GetNoOPSession(1)
				ctx, cancel := context.WithTimeout(context.Background(), pto)
				_, err := nh.SyncPropose(ctx, session, make([]byte, 512))
				cancel()
				if err == ErrSystemBusy {
					rateLimited = true
					break
				}
			}
			require.True(t, rateLimited, "failed to return ErrSystemBusy")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestTooBigPayloadIsRejectedWhenRateLimited(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		updateConfig: func(c *config.Config) *config.Config {
			c.MaxInMemLogSize = 1024 * 3
			return c
		},
		createSM: func(uint64, uint64) sm.IStateMachine {
			return &tests.NoOP{MillisecondToSleep: 20}
		},
		tf: func(nh *NodeHost) {
			bigPayload := make([]byte, 1024*1024)
			session := nh.GetNoOPSession(1)
			pto := pto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			_, err := nh.SyncPropose(ctx, session, bigPayload)
			cancel()
			require.Equal(t, ErrPayloadTooBig, err,
				"failed to return ErrPayloadTooBig")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestProposalsCanBeMadeWhenRateLimited(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		updateConfig: func(c *config.Config) *config.Config {
			c.MaxInMemLogSize = 1024 * 3
			return c
		},
		createSM: func(uint64, uint64) sm.IStateMachine {
			return &tests.NoOP{MillisecondToSleep: 20}
		},
		tf: func(nh *NodeHost) {
			session := nh.GetNoOPSession(1)
			for i := 0; i < 16; i++ {
				pto := pto(nh)
				ctx, cancel := context.WithTimeout(context.Background(), pto)
				_, err := nh.SyncPropose(ctx, session, make([]byte, 16))
				cancel()
				if err == ErrTimeout {
					continue
				}
				require.NoError(t, err, "failed to make proposal %v", err)
			}
		},
	}
	runNodeHostTest(t, to, fs)
}

func makeTestProposal(nh *NodeHost, count int) bool {
	session := nh.GetNoOPSession(1)
	for i := 0; i < count; i++ {
		pto := pto(nh)
		ctx, cancel := context.WithTimeout(context.Background(), pto)
		_, err := nh.SyncPropose(ctx, session, make([]byte, 1024))
		cancel()
		if err == nil {
			return true
		}
		time.Sleep(20 * time.Millisecond)
	}
	return false
}

func TestRateLimitCanBeTriggered(t *testing.T) {
	fs := vfs.GetTestFS()
	limited := uint32(0)
	stopper := syncutil.NewStopper()
	to := &testOption{
		updateConfig: func(c *config.Config) *config.Config {
			c.MaxInMemLogSize = 1024 * 3
			return c
		},
		createSM: func(uint64, uint64) sm.IStateMachine {
			return &tests.NoOP{MillisecondToSleep: 20}
		},
		tf: func(nh *NodeHost) {
			pto := pto(nh)
			session := nh.GetNoOPSession(1)
			for i := 0; i < 10; i++ {
				stopper.RunWorker(func() {
					for j := 0; j < 16; j++ {
						if atomic.LoadUint32(&limited) == 1 {
							return
						}
						ctx, cancel := context.WithTimeout(context.Background(), pto)
						_, err := nh.SyncPropose(ctx, session, make([]byte, 1024))
						cancel()
						if err == ErrSystemBusy {
							atomic.StoreUint32(&limited, 1)
							return
						}
					}
				})
			}
			stopper.Stop()
			require.Equal(t, uint32(1), atomic.LoadUint32(&limited),
				"failed to observe ErrSystemBusy")
			require.True(t, makeTestProposal(nh, 10000),
				"failed to make proposal again")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestRateLimitCanUseFollowerFeedback(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(t *testing.T, nh1 *NodeHost, nh2 *NodeHost,
		n1 *tests.NoOP, n2 *tests.NoOP) {
		session := nh1.GetNoOPSession(1)
		limited := false
		for i := 0; i < 2000; i++ {
			pto := pto(nh1)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			_, err := nh1.SyncPropose(ctx, session, make([]byte, 1024))
			cancel()
			if err == ErrShardNotReady {
				time.Sleep(20 * time.Millisecond)
			} else if err == ErrSystemBusy {
				limited = true
				break
			}
		}
		require.True(t, limited, "failed to observe rate limited")

		n1.SetSleepTime(0)
		n2.SetSleepTime(0)
		require.True(t, makeTestProposal(nh1, 2000),
			"failed to make proposal again")
		plog.Infof("rate limit lifted, all good")
	}
	rateLimitedTwoNodeHostTest(t, tf, fs)
}

func TestUpdateResultIsReturnedToCaller(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		updateConfig: func(c *config.Config) *config.Config {
			c.MaxInMemLogSize = 1024 * 3
			return c
		},
		createSM: func(uint64, uint64) sm.IStateMachine {
			return &tests.NoOP{MillisecondToSleep: 20}
		},
		tf: func(nh *NodeHost) {
			session := nh.GetNoOPSession(1)
			pto := pto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			cmd := make([]byte, 1518)
			_, err := rand.Read(cmd)
			require.NoError(t, err)
			result, err := nh.SyncPropose(ctx, session, cmd)
			cancel()
			assert.NoError(t, err, "failed to make proposal %v", err)
			assert.Equal(t, uint64(1518), result.Value, "unexpected result value")
			assert.Equal(t, cmd, result.Data, "unexpected result data")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestRaftLogQuery(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			for i := 0; i < 10; i++ {
				makeTestProposal(nh, 10)
			}
			_, err := nh.QueryRaftLog(1, 2, 1, math.MaxUint64)
			assert.Equal(t, ErrInvalidRange, err)

			rs, err := nh.QueryRaftLog(1, 1, 11, math.MaxUint64)
			assert.NoError(t, err)
			ticker := time.NewTicker(2 * time.Second)
			defer ticker.Stop()
			select {
			case v := <-rs.CompletedC:
				assert.True(t, v.Completed())
				entries, logRange := v.RaftLogs()
				assert.Equal(t, 10, len(entries))
				assert.Equal(t, LogRange{FirstIndex: 1, LastIndex: 13}, logRange)
			case <-ticker.C:
				require.Fail(t, "no results")
			}
			rs.Release()

			rs, err = nh.QueryRaftLog(1, 1, 1000, math.MaxUint64)
			assert.NoError(t, err)
			select {
			case v := <-rs.CompletedC:
				assert.True(t, v.Completed())
				entries, logRange := v.RaftLogs()
				assert.Equal(t, 12, len(entries))
				assert.Equal(t, LogRange{FirstIndex: 1, LastIndex: 13}, logRange)
			case <-ticker.C:
				require.Fail(t, "no results")
			}
			rs.Release()

			rs, err = nh.QueryRaftLog(1, 13, 1000, math.MaxUint64)
			assert.NoError(t, err)
			select {
			case v := <-rs.CompletedC:
				assert.True(t, v.RequestOutOfRange())
				entries, logRange := v.RaftLogs()
				assert.Equal(t, 0, len(entries))
				assert.Equal(t, LogRange{FirstIndex: 1, LastIndex: 13}, logRange)
			case <-ticker.C:
				require.Fail(t, "no results")
			}
			rs.Release()

			ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
			defer cancel()
			opts := SnapshotOption{
				CompactionIndex:            10,
				OverrideCompactionOverhead: true,
			}
			_, err = nh.SyncRequestSnapshot(ctx, 1, opts)
			assert.NoError(t, err)
			done := false
			for i := 0; i < 1000; i++ {
				func() {
					rs, err := nh.QueryRaftLog(1, 1, 11, math.MaxUint64)
					assert.NoError(t, err)
					ticker := time.NewTicker(2 * time.Second)
					defer ticker.Stop()
					select {
					case v := <-rs.CompletedC:
						if v.Completed() {
							time.Sleep(10 * time.Millisecond)
							return
						}
						assert.True(t, v.RequestOutOfRange())
						entries, logRange := v.RaftLogs()
						assert.Equal(t, 0, len(entries))
						assert.Equal(t, LogRange{FirstIndex: 11, LastIndex: 13}, logRange)
						done = true
						return
					case <-ticker.C:
						require.Fail(t, "no results")
					}
					rs.Release()
				}()
				if done {
					return
				}
			}
			require.True(t, done, "failed to observe RequestOutOfRange error")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestIsNonVotingIsReturnedWhenNodeIsNonVoting(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(t *testing.T, nh1 *NodeHost, nh2 *NodeHost) {
		rc := config.Config{
			ShardID:                 1,
			ReplicaID:               1,
			ElectionRTT:             3,
			HeartbeatRTT:            1,
			CheckQuorum:             true,
			SnapshotEntries:         5,
			CompactionOverhead:      2,
			SnapshotCompressionType: config.NoCompression,
		}
		newSM := func(uint64, uint64) sm.IOnDiskStateMachine {
			return tests.NewFakeDiskSM(0)
		}
		peers := make(map[uint64]string)
		peers[1] = nodeHostTestAddr1
		err := nh1.StartOnDiskReplica(peers, false, newSM, rc)
		require.NoError(t, err, "failed to start nonVoting %v", err)
		waitForLeaderToBeElected(t, nh1, 1)

		rc = config.Config{
			ShardID:            1,
			ReplicaID:          2,
			ElectionRTT:        3,
			HeartbeatRTT:       1,
			IsNonVoting:        true,
			CheckQuorum:        true,
			SnapshotEntries:    5,
			CompactionOverhead: 2,
		}
		newSM2 := func(uint64, uint64) sm.IOnDiskStateMachine {
			return tests.NewFakeDiskSM(0)
		}
		pto := pto(nh1)
		rs, err := nh1.RequestAddNonVoting(1, 2, nodeHostTestAddr2, 0, pto)
		require.NoError(t, err, "failed to add nonVoting %v", err)
		<-rs.ResultC()

		err = nh2.StartOnDiskReplica(nil, true, newSM2, rc)
		require.NoError(t, err, "failed to start nonVoting %v", err)

		nonVotingReady := false
		for i := 0; i < 10000; i++ {
			nhi := nh2.GetNodeHostInfo(DefaultNodeHostInfoOption)
			for _, ci := range nhi.ShardInfoList {
				if ci.Pending {
					continue
				}
				if ci.IsNonVoting && ci.ReplicaID == 2 {
					nonVotingReady = true
					break
				}
			}
			if nonVotingReady {
				break
			}
			time.Sleep(10 * time.Millisecond)
		}
		require.True(t, nonVotingReady, "failed to get is nonVoting flag")
	}
	twoFakeDiskNodeHostTest(t, tf, fs)
}

func TestSnapshotIndexWillPanicOnRegularRequestResult(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			cs := nh.GetNoOPSession(1)
			pto := pto(nh)
			rs, err := nh.Propose(cs, make([]byte, 1), pto)
			require.NoError(t, err, "propose failed %v", err)

			v := <-rs.ResultC()
			require.Panics(t, func() {
				plog.Infof("%d", v.SnapshotIndex())
			})
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestSyncRequestSnapshot(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			session := nh.GetNoOPSession(1)
			pto := lpto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			cmd := make([]byte, 1518)
			_, err := nh.SyncPropose(ctx, session, cmd)
			cancel()
			require.NoError(t, err, "failed to make proposal %v", err)

			ctx, cancel = context.WithTimeout(context.Background(), pto)
			idx, err := nh.SyncRequestSnapshot(ctx, 1, DefaultSnapshotOption)
			cancel()
			require.NoError(t, err)
			require.NotZero(t, idx, "unexpected index %d", idx)

			listener, ok := nh.events.sys.ul.(*testSysEventListener)
			require.True(t, ok, "failed to get the system event listener")
			waitSnapshotInfoEvent(t, listener.getSnapshotCreated, 1)
			si := listener.getSnapshotCreated()[0]
			assert.Equal(t, uint64(1), si.ShardID, "incorrect shard id")
			assert.Equal(t, uint64(1), si.ReplicaID, "incorrect replica id")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestSnapshotCanBeExportedAfterSnapshotting(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			session := nh.GetNoOPSession(1)
			pto := lpto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			cmd := make([]byte, 1518)
			_, err := nh.SyncPropose(ctx, session, cmd)
			cancel()
			require.NoError(t, err, "failed to make proposal %v", err)

			ctx, cancel = context.WithTimeout(context.Background(), pto)
			idx, err := nh.SyncRequestSnapshot(ctx, 1, DefaultSnapshotOption)
			cancel()
			require.NoError(t, err)
			require.NotZero(t, idx, "unexpected index %d", idx)

			sspath := "exported_snapshot_safe_to_delete"
			require.NoError(t, fs.RemoveAll(sspath))
			require.NoError(t, fs.MkdirAll(sspath, 0755))
			defer func() {
				require.NoError(t, fs.RemoveAll(sspath))
			}()

			opt := SnapshotOption{
				Exported:   true,
				ExportPath: sspath,
			}
			ctx, cancel = context.WithTimeout(context.Background(), pto)
			exportIdx, err := nh.SyncRequestSnapshot(ctx, 1, opt)
			cancel()
			require.NoError(t, err)
			require.Equal(t, idx, exportIdx,
				"unexpected index %d, want %d", exportIdx, idx)
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestCanOverrideSnapshotOverhead(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			session := nh.GetNoOPSession(1)
			cmd := make([]byte, 1)
			pto := pto(nh)
			for i := 0; i < 16; i++ {
				ctx, cancel := context.WithTimeout(context.Background(), pto)
				_, err := nh.SyncPropose(ctx, session, cmd)
				cancel()
				if err != nil {
					if err == ErrTimeout {
						time.Sleep(500 * time.Millisecond)
						continue
					}
					require.NoError(t, err, "failed to make proposal %v", err)
				}
			}

			opt := SnapshotOption{
				OverrideCompactionOverhead: true,
				CompactionOverhead:         0,
			}
			lpto := lpto(nh)
			sr, err := nh.RequestSnapshot(1, opt, lpto)
			require.NoError(t, err, "failed to request snapshot")

			v := <-sr.ResultC()
			require.True(t, v.Completed(), "failed to complete the requested snapshot")
			require.GreaterOrEqual(t, v.SnapshotIndex(), uint64(16),
				"unexpected snapshot index %d", v.SnapshotIndex())

			logdb := nh.mu.logdb
			compacted := false
			for i := 0; i < 1000; i++ {
				time.Sleep(10 * time.Millisecond)
				op, err := nh.RequestCompaction(1, 1)
				if err == nil {
					<-op.ResultC()
				}
				ents, _, err := logdb.IterateEntries(nil, 0, 1, 1, 12, 14, math.MaxUint64)
				require.NoError(t, err, "failed to iterate entries, %v", err)
				if len(ents) == 0 {
					compacted = true
					break
				}
			}
			require.True(t, compacted, "failed to compact the entries")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestSnapshotCanBeRequested(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := lpto(nh)
			session := nh.GetNoOPSession(1)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			cmd := make([]byte, 1518)
			_, err := nh.SyncPropose(ctx, session, cmd)
			cancel()
			require.NoError(t, err, "failed to make proposal %v", err)

			sr, err := nh.RequestSnapshot(1, SnapshotOption{}, pto)
			require.NoError(t, err, "failed to request snapshot")

			var index uint64
			v := <-sr.ResultC()
			require.True(t, v.Completed(),
				"failed to complete the requested snapshot")
			index = v.SnapshotIndex()

			sr, err = nh.RequestSnapshot(1, SnapshotOption{}, pto)
			require.NoError(t, err, "failed to request snapshot")
			v = <-sr.ResultC()
			require.True(t, v.Rejected(), "snapshot request not rejected")

			logdb := nh.mu.logdb
			snapshot, err := logdb.GetSnapshot(1, 1)
			require.NoError(t, err)
			require.False(t, pb.IsEmptySnapshot(snapshot),
				"failed to save snapshots")
			require.Equal(t, index, snapshot.Index, "unexpected index value")

			reader, header, err := rsm.NewSnapshotReader(snapshot.Filepath, fs)
			require.NoError(t, err, "failed to new snapshot reader %v", err)
			defer func() {
				require.NoError(t, reader.Close())
			}()
			require.Equal(t, rsm.V2, rsm.SSVersion(header.Version),
				"unexpected snapshot version")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestClientCanBeNotifiedOnCommittedConfigChange(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		updateNodeHostConfig: func(c *config.NodeHostConfig) *config.NodeHostConfig {
			c.NotifyCommit = true
			return c
		},
		tf: func(nh *NodeHost) {
			pto := pto(nh)
			rs, err := nh.RequestAddReplica(1, 2, "localhost:3456", 0, pto)
			require.NoError(t, err, "failed to request add node")
			require.NotNil(t, rs.committedC, "committedC not set")

			cn := <-rs.ResultC()
			require.True(t, cn.Committed(), "failed to get committed notification")

			cn = <-rs.ResultC()
			require.True(t, cn.Completed(), "failed to get completed notification")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestClientCanBeNotifiedOnCommittedProposals(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		updateNodeHostConfig: func(c *config.NodeHostConfig) *config.NodeHostConfig {
			c.NotifyCommit = true
			return c
		},
		tf: func(nh *NodeHost) {
			pto := pto(nh)
			cmd := make([]byte, 128)
			session := nh.GetNoOPSession(1)
			rs, err := nh.Propose(session, cmd, pto)
			require.NoError(t, err, "failed to make proposal %v", err)
			require.NotNil(t, rs.committedC, "committedC not set")

			cn := <-rs.ResultC()
			require.True(t, cn.Committed(), "failed to get committed notification")

			cn = <-rs.ResultC()
			require.True(t, cn.Completed(), "failed to get completed notification")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestRequestSnapshotTimeoutWillBeReported(t *testing.T) {
	fs := vfs.GetTestFS()
	pst := &PST{slowSave: true}
	to := &testOption{
		createSM: func(uint64, uint64) sm.IStateMachine {
			return pst
		},
		tf: func(nh *NodeHost) {
			pto := pto(nh)
			sr, err := nh.RequestSnapshot(1, SnapshotOption{}, pto)
			require.NoError(t, err, "failed to request snapshot")
			v := <-sr.ResultC()
			require.True(t, v.Timeout(), "failed to report timeout")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestSyncRemoveData(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			err := nh.StopShard(1)
			require.NoError(t, err, "failed to remove shard %v", err)

			pto := lpto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			defer cancel()
			err = nh.SyncRemoveData(ctx, 1, 1)
			require.NoError(t, err, "sync remove data failed: %v", err)

			listener, ok := nh.events.sys.ul.(*testSysEventListener)
			require.True(t, ok, "failed to get the system event listener")
			waitNodeInfoEvent(t, listener.getNodeUnloaded, 1)
			ni := listener.getNodeUnloaded()[0]
			assert.Equal(t, uint64(1), ni.ShardID, "incorrect shard id")
			assert.Equal(t, uint64(1), ni.ReplicaID, "incorrect replica id")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestRemoveNodeDataWillFailWhenNodeIsStillRunning(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			err := nh.RemoveData(1, 1)
			require.Equal(t, ErrShardNotStopped, err, "remove data didn't fail")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestRestartingAnNodeWithRemovedDataWillBeRejected(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			err := nh.StopShard(1)
			require.NoError(t, err, "failed to remove shard %v", err)
			for {
				err := nh.RemoveData(1, 1)
				if err != nil {
					if err == ErrShardNotStopped {
						time.Sleep(100 * time.Millisecond)
						continue
					} else {
						require.NoError(t, err, "remove data failed %v", err)
					}
				}
				break
			}

			rc := getTestConfig()
			peers := make(map[uint64]string)
			peers[1] = nh.RaftAddress()
			newPST := func(shardID uint64, replicaID uint64) sm.IStateMachine {
				return &PST{}
			}
			err = nh.StartReplica(peers, false, newPST, *rc)
			require.Equal(t, ErrReplicaRemoved, err, "start shard failed %v", err)
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestRemoveNodeDataRemovesAllNodeData(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			session := nh.GetNoOPSession(1)
			pto := lpto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			cmd := make([]byte, 1518)
			_, err := nh.SyncPropose(ctx, session, cmd)
			cancel()
			require.NoError(t, err, "failed to make proposal %v", err)

			sr, err := nh.RequestSnapshot(1, SnapshotOption{}, pto)
			require.NoError(t, err, "failed to request snapshot")
			v := <-sr.ResultC()
			require.True(t, v.Completed(), "failed to complete the snapshot")

			require.NoError(t, nh.StopShard(1), "failed to stop shard %v", err)

			logdb := nh.mu.logdb
			snapshot, err := logdb.GetSnapshot(1, 1)
			require.NoError(t, err)
			require.False(t, pb.IsEmptySnapshot(snapshot), "no snapshot saved")

			snapshotDir := nh.env.GetSnapshotDir(nh.nhConfig.GetDeploymentID(), 1, 1)
			exist, err := fileutil.Exist(snapshotDir, fs)
			require.NoError(t, err)
			require.True(t, exist, "snapshot dir %s does not exist", snapshotDir)

			files, err := fs.List(snapshotDir)
			require.NoError(t, err, "failed to read dir %v", err)
			sscount := 0
			for _, fn := range files {
				fi, err := fs.Stat(fs.PathJoin(snapshotDir, fn))
				require.NoError(t, err, "failed to get stat for %s", fn)
				if fi.IsDir() && server.SnapshotDirNameRe.Match([]byte(fi.Name())) {
					sscount++
				}
			}
			require.NotZero(t, sscount, "no snapshot dir found")

			removed := false
			for i := 0; i < 1000; i++ {
				err := nh.RemoveData(1, 1)
				if err == ErrShardNotStopped {
					time.Sleep(100 * time.Millisecond)
					continue
				}
				require.NoError(t, err, "failed to remove data %v", err)
				removed = true
				break
			}
			require.True(t, removed, "failed to remove node data")

			marked, err := fileutil.IsDirMarkedAsDeleted(snapshotDir, fs)
			require.NoError(t, err)
			require.True(t, marked, "snapshot dir %s still exist", snapshotDir)

			files, err = fs.List(snapshotDir)
			require.NoError(t, err, "failed to read dir %v", err)
			for _, fn := range files {
				fi, err := fs.Stat(fs.PathJoin(snapshotDir, fn))
				require.NoError(t, err, "failed to get stat for %s", fn)
				if fi.IsDir() {
					match := server.SnapshotDirNameRe.Match([]byte(fi.Name()))
					require.False(t, match, "snapshot dir %s not deleted", fi.Name())
				}
			}

			bs, err := logdb.GetBootstrapInfo(1, 1)
			require.ErrorIs(t, err, raftio.ErrNoBootstrapInfo,
				"failed to delete bootstrap %v", err)
			require.Equal(t, pb.Bootstrap{}, bs, "bs not nil")

			ents, sz, err := logdb.IterateEntries(nil, 0, 1, 1, 0,
				math.MaxUint64, math.MaxUint64)
			require.NoError(t, err, "failed to get entries %v", err)
			require.Empty(t, ents, "entry returned")
			require.Zero(t, sz, "size not zero")

			snapshot, err = logdb.GetSnapshot(1, 1)
			require.NoError(t, err)
			require.True(t, pb.IsEmptySnapshot(snapshot), "snapshot not deleted")

			_, err = logdb.ReadRaftState(1, 1, 1)
			require.ErrorIs(t, err, raftio.ErrNoSavedLog,
				"raft state not deleted %v", err)

			sysop, err := nh.RequestCompaction(1, 1)
			require.NoError(t, err, "failed to request compaction %v", err)
			<-sysop.ResultC()
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestSnapshotOptionIsChecked(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			opts := SnapshotOption{
				OverrideCompactionOverhead: true,
				CompactionIndex:            100,
				CompactionOverhead:         10,
			}
			assert.Equal(t, ErrInvalidOption, opts.Validate())
			ctx, cancel := context.WithTimeout(context.Background(), time.Second)
			defer cancel()
			_, err := nh.SyncRequestSnapshot(ctx, 1, opts)
			assert.Equal(t, ErrInvalidOption, err)
			rs, err := nh.RequestSnapshot(1, opts, time.Second)
			assert.Equal(t, ErrInvalidOption, err)
			assert.Nil(t, rs)
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestSnapshotCanBeExported(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			sspath := "exported_snapshot_safe_to_delete"
			require.NoError(t, fs.RemoveAll(sspath))
			require.NoError(t, fs.MkdirAll(sspath, 0755))
			defer func() {
				require.NoError(t, fs.RemoveAll(sspath))
			}()

			session := nh.GetNoOPSession(1)
			pto := lpto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			cmd := make([]byte, 1518)
			_, err := nh.SyncPropose(ctx, session, cmd)
			cancel()
			require.NoError(t, err, "failed to make proposal %v", err)

			opt := SnapshotOption{
				Exported:   true,
				ExportPath: sspath,
			}
			sr, err := nh.RequestSnapshot(1, opt, pto)
			require.NoError(t, err, "failed to request snapshot")
			v := <-sr.ResultC()
			require.True(t, v.Completed(), "failed to complete the snapshot")
			index := v.SnapshotIndex()

			logdb := nh.mu.logdb
			snapshot, err := logdb.GetSnapshot(1, 1)
			require.NoError(t, err)
			require.True(t, pb.IsEmptySnapshot(snapshot),
				"snapshot record unexpectedly inserted into the system")

			snapshotDir := fmt.Sprintf("snapshot-%016X", index)
			snapshotFile := fmt.Sprintf("snapshot-%016X.gbsnap", index)
			fp := fs.PathJoin(sspath, snapshotDir, snapshotFile)
			exist, err := fileutil.Exist(fp, fs)
			require.NoError(t, err)
			require.True(t, exist, "snapshot file not saved")

			metafp := fs.PathJoin(sspath, snapshotDir, "snapshot.metadata")
			exist, err = fileutil.Exist(metafp, fs)
			require.NoError(t, err)
			require.True(t, exist, "snapshot metadata not saved")

			var ss pb.Snapshot
			err = fileutil.GetFlagFileContent(fs.PathJoin(sspath, snapshotDir),
				"snapshot.metadata", &ss, fs)
			require.NoError(t, err, "failed to get snapshot from metadata")
			assert.Zero(t, ss.OnDiskIndex, "on disk index is not 0")
			assert.False(t, ss.Imported, "incorrectly recorded as imported")
			assert.Equal(t, pb.RegularStateMachine, ss.Type, "incorrect type")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestOnDiskStateMachineCanExportSnapshot(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		fakeDiskNode: true,
		tf: func(nh *NodeHost) {
			session := nh.GetNoOPSession(1)
			proposed := false
			for i := 0; i < 16; i++ {
				pto := pto(nh)
				ctx, cancel := context.WithTimeout(context.Background(), pto)
				_, err := nh.SyncPropose(ctx, session, []byte("test-data"))
				cancel()
				if err == nil {
					proposed = true
					break
				} else {
					time.Sleep(100 * time.Millisecond)
				}
			}
			require.True(t, proposed, "failed to make proposal")

			sspath := "exported_snapshot_safe_to_delete"
			require.NoError(t, fs.RemoveAll(sspath))
			require.NoError(t, fs.MkdirAll(sspath, 0755))
			defer func() {
				require.NoError(t, fs.RemoveAll(sspath))
			}()

			opt := SnapshotOption{
				Exported:   true,
				ExportPath: sspath,
			}
			aborted := false
			index := uint64(0)
			pto := lpto(nh)
			for {
				sr, err := nh.RequestSnapshot(1, opt, pto)
				if err == ErrRejected {
					continue
				}
				require.NoError(t, err, "failed to request snapshot %v", err)
				v := <-sr.ResultC()
				if v.Aborted() {
					aborted = true
					continue
				}
				if v.code == requestRejected {
					continue
				}
				require.True(t, v.Completed(),
					"failed to complete snapshot, %s", v.code)
				index = v.SnapshotIndex()
				break
			}
			require.True(t, aborted, "never aborted")

			logdb := nh.mu.logdb
			snapshot, err := logdb.GetSnapshot(1, 1)
			require.NoError(t, err)
			require.True(t, pb.IsEmptySnapshot(snapshot), "snapshot inserted")

			snapshotDir := fmt.Sprintf("snapshot-%016X", index)
			snapshotFile := fmt.Sprintf("snapshot-%016X.gbsnap", index)
			fp := fs.PathJoin(sspath, snapshotDir, snapshotFile)
			exist, err := fileutil.Exist(fp, fs)
			require.NoError(t, err)
			require.True(t, exist, "snapshot file not saved")

			metafp := fs.PathJoin(sspath, snapshotDir, "snapshot.metadata")
			exist, err = fileutil.Exist(metafp, fs)
			require.NoError(t, err)
			require.True(t, exist, "snapshot metadata not saved")

			shrunk, err := rsm.IsShrunkSnapshotFile(fp, fs)
			require.NoError(t, err)
			require.False(t, shrunk, "exported snapshot is considered as shrunk")

			var ss pb.Snapshot
			err = fileutil.GetFlagFileContent(fs.PathJoin(sspath, snapshotDir),
				"snapshot.metadata", &ss, fs)
			require.NoError(t, err, "failed to get snapshot from metadata")
			assert.NotZero(t, ss.OnDiskIndex, "on disk index not recorded")
			assert.False(t, ss.Imported, "incorrectly recorded as imported")
			assert.Equal(t, pb.OnDiskStateMachine, ss.Type, "incorrect type")
		},
	}
	runNodeHostTest(t, to, fs)
}

func testImportedSnapshotIsAlwaysRestored(t *testing.T,
	newDir bool, ct config.CompressionType, fs vfs.IFS) {
	tf := func() {
		rc := config.Config{
			ShardID:                 1,
			ReplicaID:               1,
			ElectionRTT:             3,
			HeartbeatRTT:            1,
			CheckQuorum:             true,
			SnapshotEntries:         5,
			CompactionOverhead:      2,
			SnapshotCompressionType: ct,
		}
		peers := make(map[uint64]string)
		peers[1] = nodeHostTestAddr1
		nhc := config.NodeHostConfig{
			NodeHostDir:    singleNodeHostTestDir,
			RTTMillisecond: getRTTMillisecond(fs, singleNodeHostTestDir),
			RaftAddress:    nodeHostTestAddr1,
			Expert:         getTestExpertConfig(fs),
		}
		nh, err := NewNodeHost(nhc)
		require.NoError(t, err, "failed to create node host %v", err)
		pto := lpto(nh)
		newSM := func(uint64, uint64) sm.IOnDiskStateMachine {
			return tests.NewSimDiskSM(0)
		}
		err = nh.StartOnDiskReplica(peers, false, newSM, rc)
		require.NoError(t, err, "failed to start shard %v", err)
		waitForLeaderToBeElected(t, nh, 1)

		makeProposals := func(nn *NodeHost) {
			session := nn.GetNoOPSession(1)
			for i := 0; i < 16; i++ {
				ctx, cancel := context.WithTimeout(context.Background(), pto)
				_, err := nn.SyncPropose(ctx, session, []byte("test-data"))
				cancel()
				require.NoError(t, err, "failed to make proposal %v", err)
			}
		}
		makeProposals(nh)

		sspath := "exported_snapshot_safe_to_delete"
		require.NoError(t, fs.RemoveAll(sspath))
		require.NoError(t, fs.MkdirAll(sspath, 0755))
		defer func() {
			require.NoError(t, fs.RemoveAll(sspath))
		}()

		opt := SnapshotOption{
			Exported:   true,
			ExportPath: sspath,
		}
		var index uint64
		exported := false
		for i := 0; i < 1000; i++ {
			sr, err := nh.RequestSnapshot(1, opt, pto)
			require.NoError(t, err, "failed to request snapshot %v", err)
			v := <-sr.ResultC()
			if v.Rejected() {
				time.Sleep(10 * time.Millisecond)
				continue
			}
			if v.Completed() {
				index = v.SnapshotIndex()
				exported = true
				break
			}
		}
		require.True(t, exported, "failed to export snapshot")
		makeProposals(nh)

		ctx, cancel := context.WithTimeout(context.Background(), pto)
		rv, err := nh.SyncRead(ctx, 1, nil)
		cancel()
		require.NoError(t, err, "failed to read applied value %v", err)
		applied := rv.(uint64)
		require.Greater(t, applied, index, "invalid applied value %d", applied)

		ctx, cancel = context.WithTimeout(context.Background(), pto)
		err = nh.SyncRequestAddReplica(ctx, 1, 2, "noidea:8080", 0)
		require.NoError(t, err, "failed to add node %v", err)
		nh.Close()

		snapshotDir := fmt.Sprintf("snapshot-%016X", index)
		dir := fs.PathJoin(sspath, snapshotDir)
		members := make(map[uint64]string)
		members[1] = nhc.RaftAddress
		if newDir {
			nhc.NodeHostDir = fs.PathJoin(nhc.NodeHostDir, "newdir")
		}
		err = tools.ImportSnapshot(nhc, dir, members, 1)
		require.NoError(t, err, "failed to import snapshot %v", err)

		ok, err := upgrade310.CanUpgradeToV310(nhc)
		require.NoError(t, err, "failed to check whether upgrade is possible")
		require.False(t, ok, "should not be considered as ok to upgrade")

		func() {
			rnh, err := NewNodeHost(nhc)
			require.NoError(t, err, "failed to create node host %v", err)
			defer rnh.Close()
			rnewSM := func(uint64, uint64) sm.IOnDiskStateMachine {
				return tests.NewSimDiskSM(applied)
			}
			err = rnh.StartOnDiskReplica(nil, false, rnewSM, rc)
			require.NoError(t, err, "failed to start shard %v", err)
			waitForLeaderToBeElected(t, rnh, 1)

			ctx, cancel = context.WithTimeout(context.Background(), pto)
			rv, err = rnh.SyncRead(ctx, 1, nil)
			cancel()
			require.NoError(t, err, "failed to read applied value %v", err)
			require.Equal(t, index, rv.(uint64), "invalid returned value %d", rv)
			makeProposals(rnh)
		}()

		ok, err = upgrade310.CanUpgradeToV310(nhc)
		require.NoError(t, err, "failed to check whether upgrade is possible")
		require.True(t, ok, "can not upgrade")
	}
	runNodeHostTestDC(t, tf, true, fs)
}

func TestImportedSnapshotIsAlwaysRestored(t *testing.T) {
	if vfs.GetTestFS() != vfs.DefaultFS {
		t.Skip("not using the default fs")
	} else {
		fs := vfs.GetTestFS()
		testImportedSnapshotIsAlwaysRestored(t, true, config.NoCompression, fs)
		testImportedSnapshotIsAlwaysRestored(t, false, config.NoCompression, fs)
		testImportedSnapshotIsAlwaysRestored(t, false, config.Snappy, fs)
	}
}

func TestShardWithoutQuorumCanBeRestoreByImportingSnapshot(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func() {
		nh1dir := fs.PathJoin(singleNodeHostTestDir, "nh1")
		nh2dir := fs.PathJoin(singleNodeHostTestDir, "nh2")
		rc := config.Config{
			ShardID:            1,
			ReplicaID:          1,
			ElectionRTT:        10,
			HeartbeatRTT:       1,
			CheckQuorum:        true,
			SnapshotEntries:    5,
			CompactionOverhead: 2,
		}
		peers := make(map[uint64]string)
		peers[1] = nodeHostTestAddr1
		nhc1 := config.NodeHostConfig{
			WALDir:         nh1dir,
			NodeHostDir:    nh1dir,
			RTTMillisecond: getRTTMillisecond(fs, nh1dir),
			RaftAddress:    nodeHostTestAddr1,
			Expert:         getTestExpertConfig(fs),
		}
		nhc2 := config.NodeHostConfig{
			WALDir:         nh2dir,
			NodeHostDir:    nh2dir,
			RTTMillisecond: getRTTMillisecond(fs, nh2dir),
			RaftAddress:    nodeHostTestAddr2,
			Expert:         getTestExpertConfig(fs),
		}
		var once sync.Once
		nh1, err := NewNodeHost(nhc1)
		require.NoError(t, err, "failed to create node host %v", err)
		nh2, err := NewNodeHost(nhc2)
		require.NoError(t, err, "failed to create node host %v", err)

		sm1 := tests.NewFakeDiskSM(0)
		sm1.SetAborted()
		newSM := func(uint64, uint64) sm.IOnDiskStateMachine {
			return sm1
		}
		newSM2 := func(uint64, uint64) sm.IOnDiskStateMachine {
			return tests.NewFakeDiskSM(0)
		}
		err = nh1.StartOnDiskReplica(peers, false, newSM, rc)
		require.NoError(t, err, "failed to start shard %v", err)
		waitForLeaderToBeElected(t, nh1, 1)

		defer func() {
			require.NoError(t, fs.RemoveAll(singleNodeHostTestDir))
		}()
		defer once.Do(func() {
			nh1.Close()
			nh2.Close()
		})

		session := nh1.GetNoOPSession(1)
		mkproposal := func(nh *NodeHost) {
			done := false
			pto := pto(nh)
			for i := 0; i < 100; i++ {
				ctx, cancel := context.WithTimeout(context.Background(), pto)
				_, err := nh.SyncPropose(ctx, session, []byte("test-data"))
				cancel()
				if err == nil {
					done = true
					break
				} else {
					time.Sleep(200 * time.Millisecond)
				}
			}
			require.True(t, done, "failed to make proposal on restored shard")
		}
		mkproposal(nh1)

		sspath := "exported_snapshot_safe_to_delete"
		require.NoError(t, fs.RemoveAll(sspath))
		require.NoError(t, fs.MkdirAll(sspath, 0755))
		defer func() {
			require.NoError(t, fs.RemoveAll(sspath))
		}()

		opt := SnapshotOption{
			Exported:   true,
			ExportPath: sspath,
		}
		pto := lpto(nh1)
		sr, err := nh1.RequestSnapshot(1, opt, pto)
		require.NoError(t, err, "failed to request snapshot %v", err)
		v := <-sr.ResultC()
		require.True(t, v.Completed(), "failed to complete snapshot")
		index := v.SnapshotIndex()

		snapshotDir := fmt.Sprintf("snapshot-%016X", index)
		dir := fs.PathJoin(sspath, snapshotDir)
		members := make(map[uint64]string)
		members[1] = nhc1.RaftAddress
		members[10] = nhc2.RaftAddress
		once.Do(func() {
			nh1.Close()
			nh2.Close()
		})
		require.NoError(t, tools.ImportSnapshot(nhc1, dir, members, 1))
		require.NoError(t, tools.ImportSnapshot(nhc2, dir, members, 10))

		rnh1, err := NewNodeHost(nhc1)
		require.NoError(t, err, "failed to create node host %v", err)
		rnh2, err := NewNodeHost(nhc2)
		require.NoError(t, err, "failed to create node host %v", err)
		defer func() {
			rnh1.Close()
			rnh2.Close()
		}()

		require.NoError(t, rnh1.StartOnDiskReplica(nil, false, newSM, rc))
		rc.ReplicaID = 10
		require.NoError(t, rnh2.StartOnDiskReplica(nil, false, newSM2, rc))
		waitForLeaderToBeElected(t, rnh1, 1)
		mkproposal(rnh1)
		mkproposal(rnh2)
	}
	runNodeHostTestDC(t, tf, true, fs)
}

type chunks struct {
	received  uint64
	confirmed uint64
}

var (
	testSnapshotDir = "test_snapshot_dir_safe_to_delete"
)

func (c *chunks) onReceive(pb.MessageBatch) {
	c.received++
}

func (c *chunks) confirm(shardID uint64, replicaID uint64, index uint64) {
	c.confirmed++
}

func (c *chunks) getSnapshotDirFunc(shardID uint64, replicaID uint64) string {
	return testSnapshotDir
}

type testSink2 struct {
	receiver chunkReceiver
}

func (s *testSink2) Receive(chunk pb.Chunk) (bool, bool) {
	s.receiver.Add(chunk)
	return true, false
}

func (s *testSink2) Close() error {
	s.Receive(pb.Chunk{ChunkCount: pb.PoisonChunkCount})
	return nil
}

func (s *testSink2) ShardID() uint64 {
	return 2000
}

func (s *testSink2) ToReplicaID() uint64 {
	return 300
}

type dataCorruptionSink struct {
	receiver chunkReceiver
	enabled  bool
}

func (s *dataCorruptionSink) Receive(chunk pb.Chunk) (bool, bool) {
	if s.enabled && len(chunk.Data) > 0 {
		idx := mathrand.Uint64() % uint64(len(chunk.Data))
		chunk.Data[idx] = chunk.Data[idx] + 1
	}
	s.receiver.Add(chunk)
	return true, false
}

func (s *dataCorruptionSink) Close() error {
	s.Receive(pb.Chunk{ChunkCount: pb.PoisonChunkCount})
	return nil
}

func (s *dataCorruptionSink) ShardID() uint64 {
	return 2000
}

func (s *dataCorruptionSink) ToReplicaID() uint64 {
	return 300
}

type chunkReceiver interface {
	Add(chunk pb.Chunk) bool
}

func getTestSSMeta() rsm.SSMeta {
	return rsm.SSMeta{
		Index: 1000,
		Term:  5,
		From:  150,
	}
}

func testCorruptedChunkWriterOutputCanBeHandledByChunk(t *testing.T,
	enabled bool, exp uint64, fs vfs.IFS) {
	require.NoError(t, fs.RemoveAll(testSnapshotDir))
	c := &chunks{}
	dir := c.getSnapshotDirFunc(0, 0)
	require.NoError(t, fs.MkdirAll(dir, 0755))

	cks := transport.NewChunk(c.onReceive, c.confirm, c.getSnapshotDirFunc, 0, fs)
	sink := &dataCorruptionSink{receiver: cks, enabled: enabled}
	meta := getTestSSMeta()
	cw := rsm.NewChunkWriter(sink, meta)
	defer func() {
		require.NoError(t, fs.RemoveAll(testSnapshotDir))
	}()

	for i := 0; i < 10; i++ {
		data := make([]byte, rsm.ChunkSize)
		_, err := rand.Read(data)
		require.NoError(t, err)
		_, err = cw.Write(data)
		require.NoError(t, err, "failed to write the data %v", err)
	}

	require.NoError(t, cw.Close())
	assert.Equal(t, exp, c.received,
		"unexpected received count: %d, want %d", c.received, exp)
	assert.Equal(t, exp, c.confirmed,
		"unexpected confirmed count: %d, want %d", c.confirmed, exp)
}

func TestCorruptedChunkWriterOutputCanBeHandledByChunk(t *testing.T) {
	fs := vfs.GetTestFS()
	testCorruptedChunkWriterOutputCanBeHandledByChunk(t, false, 1, fs)
	testCorruptedChunkWriterOutputCanBeHandledByChunk(t, true, 0, fs)
}

func TestChunkWriterOutputCanBeHandledByChunk(t *testing.T) {
	fs := vfs.GetTestFS()
	require.NoError(t, fs.RemoveAll(testSnapshotDir))
	c := &chunks{}
	dir := c.getSnapshotDirFunc(0, 0)
	require.NoError(t, fs.MkdirAll(dir, 0755))
	defer func() {
		require.NoError(t, fs.RemoveAll(testSnapshotDir))
	}()

	cks := transport.NewChunk(c.onReceive, c.confirm, c.getSnapshotDirFunc, 0, fs)
	sink := &testSink2{receiver: cks}
	meta := getTestSSMeta()
	cw := rsm.NewChunkWriter(sink, meta)
	_, err := cw.Write(rsm.GetEmptyLRUSession())
	require.NoError(t, err, "write failed %v", err)

	payload := make([]byte, 0)
	payload = append(payload, rsm.GetEmptyLRUSession()...)
	for i := 0; i < 10; i++ {
		data := make([]byte, rsm.ChunkSize)
		_, err := rand.Read(data)
		require.NoError(t, err)
		payload = append(payload, data...)
		_, err = cw.Write(data)
		require.NoError(t, err, "failed to write the data %v", err)
	}
	require.NoError(t, cw.Close(), "failed to flush %v", err)

	require.Equal(t, uint64(1), c.received, "failed to receive the snapshot")
	require.Equal(t, uint64(1), c.confirmed, "failed to confirm")

	fp := fs.PathJoin(testSnapshotDir,
		"snapshot-00000000000003E8", "snapshot-00000000000003E8.gbsnap")
	reader, _, err := rsm.NewSnapshotReader(fp, fs)
	require.NoError(t, err, "failed to get a snapshot reader %v", err)
	defer func() {
		require.NoError(t, reader.Close())
	}()

	got := make([]byte, 0)
	buf := make([]byte, 1024*256)
	for {
		n, err := reader.Read(buf)
		if n > 0 {
			got = append(got, buf[:n]...)
		}
		if err == io.EOF {
			break
		}
		require.NoError(t, err)
	}
	require.Equal(t, payload, got, "snapshot content changed")
}

func TestNodeHostReturnsErrorWhenTransportCanNotBeCreated(t *testing.T) {
	fs := vfs.GetTestFS()
	if fs != vfs.DefaultFS {
		t.Skip("memfs test mode, skipped")
	}
	to := &testOption{
		updateNodeHostConfig: func(c *config.NodeHostConfig) *config.NodeHostConfig {
			c.RaftAddress = "microsoft.com:12345"
			return c
		},
		newNodeHostToFail: true,
	}
	runNodeHostTest(t, to, fs)
}

func TestNodeHostChecksLogDBType(t *testing.T) {
	fs := vfs.GetTestFS()
	ldb := &noopLogDB{}
	to := &testOption{
		updateNodeHostConfig: func(c *config.NodeHostConfig) *config.NodeHostConfig {
			c.Expert.LogDBFactory = &testLogDBFactory{ldb: ldb}
			return c
		},
		at: func(*NodeHost) {
			nhc := getTestNodeHostConfig(fs)
			_, err := NewNodeHost(*nhc)
			require.Equal(t, server.ErrLogDBType, err,
				"didn't report logdb type error %v", err)
		},
		noElection: true,
	}
	runNodeHostTest(t, to, fs)
}

var spawnChild = flag.Bool("spawn-child", false, "spawned child")

func spawn(execName string) ([]byte, error) {
	return exec.Command(execName, "-spawn-child",
		"-test.v", "-test.run=TestNodeHostFileLock$").CombinedOutput()
}

func TestNodeHostFileLock(t *testing.T) {
	fs := vfs.GetTestFS()
	if fs != vfs.DefaultFS {
		t.Skip("not using the default fs, skipped")
	}
	tf := func() {
		child := *spawnChild
		nhc := config.NodeHostConfig{
			NodeHostDir:    singleNodeHostTestDir,
			RTTMillisecond: getRTTMillisecond(fs, singleNodeHostTestDir),
			RaftAddress:    nodeHostTestAddr1,
			Expert:         getTestExpertConfig(fs),
		}
		if !child {
			nh, err := NewNodeHost(nhc)
			require.NoError(t, err, "failed to create nodehost %v", err)
			defer nh.Close()
			out, err := spawn(os.Args[0])
			require.Error(t, err, "file lock didn't prevent start, %s", out)
			require.Contains(t, string(out), "failed to lock data directory",
				"unexpected output: %s", out)
		} else {
			nhc.RaftAddress = nodeHostTestAddr2
			cnh, err := NewNodeHost(nhc)
			require.NotEqual(t, server.ErrLockDirectory, err)
			if err == nil {
				defer cnh.Close()
			}
		}
	}
	runNodeHostTestDC(t, tf, !*spawnChild, fs)
}

func TestChangeNodeHostID(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func() {
		nhc := config.NodeHostConfig{
			NodeHostDir:    singleNodeHostTestDir,
			RTTMillisecond: getRTTMillisecond(fs, singleNodeHostTestDir),
			RaftAddress:    nodeHostTestAddr1,
			Expert:         getTestExpertConfig(fs),
		}
		nh, err := NewNodeHost(nhc)
		require.NoError(t, err, "failed to create nodehost %v", err)
		nh.Close()

		v := id.New()
		nhc.NodeHostID = v.String()
		_, err = NewNodeHost(nhc)
		require.ErrorIs(t, err, server.ErrNodeHostIDChanged,
			"failed to reject changed NodeHostID %v", err)
	}
	runNodeHostTestDC(t, tf, !*spawnChild, fs)
}

type testLogDBFactory2 struct {
	f func(config.NodeHostConfig,
		config.LogDBCallback, []string, []string) (raftio.ILogDB, error)
	name string
}

func (t *testLogDBFactory2) Create(cfg config.NodeHostConfig, cb config.LogDBCallback,
	dirs []string, wals []string) (raftio.ILogDB, error) {
	return t.f(cfg, cb, dirs, wals)
}

func (t *testLogDBFactory2) Name() string {
	return t.name
}

func TestNodeHostReturnsErrLogDBBrokenChangeWhenLogDBTypeChanges(t *testing.T) {
	fs := vfs.GetTestFS()
	bff := func(config config.NodeHostConfig, cb config.LogDBCallback,
		dirs []string, lldirs []string) (raftio.ILogDB, error) {
		return logdb.NewDefaultBatchedLogDB(config, cb, dirs, lldirs)
	}
	nff := func(config config.NodeHostConfig, cb config.LogDBCallback,
		dirs []string, lldirs []string) (raftio.ILogDB, error) {
		return logdb.NewDefaultLogDB(config, cb, dirs, lldirs)
	}
	to := &testOption{
		at: func(*NodeHost) {
			nhc := getTestNodeHostConfig(fs)
			nhc.Expert.LogDBFactory = &testLogDBFactory2{f: nff}
			_, err := NewNodeHost(*nhc)
			require.Equal(t, server.ErrLogDBBrokenChange, err,
				"failed to return ErrLogDBBrokenChange")
		},
		updateNodeHostConfig: func(c *config.NodeHostConfig) *config.NodeHostConfig {
			c.Expert.LogDBFactory = &testLogDBFactory2{f: bff}
			return c
		},
		noElection: true,
	}
	runNodeHostTest(t, to, fs)
}

func TestNodeHostByDefaultUsePlainEntryLogDB(t *testing.T) {
	fs := vfs.GetTestFS()
	bff := func(config config.NodeHostConfig, cb config.LogDBCallback,
		dirs []string, lldirs []string) (raftio.ILogDB, error) {
		return logdb.NewDefaultBatchedLogDB(config, cb, dirs, lldirs)
	}
	nff := func(config config.NodeHostConfig, cb config.LogDBCallback,
		dirs []string, lldirs []string) (raftio.ILogDB, error) {
		return logdb.NewDefaultLogDB(config, cb, dirs, lldirs)
	}
	to := &testOption{
		updateNodeHostConfig: func(c *config.NodeHostConfig) *config.NodeHostConfig {
			c.Expert.LogDBFactory = &testLogDBFactory2{f: nff}
			return c
		},
		noElection: true,
		at: func(*NodeHost) {
			nhc := getTestNodeHostConfig(fs)
			nhc.Expert.LogDBFactory = &testLogDBFactory2{f: bff}
			_, err := NewNodeHost(*nhc)
			require.Equal(t, server.ErrIncompatibleData, err,
				"failed to return server.ErrIncompatibleData")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestNodeHostByDefaultChecksWhetherToUseBatchedLogDB(t *testing.T) {
	fs := vfs.GetTestFS()
	bff := func(config config.NodeHostConfig, cb config.LogDBCallback,
		dirs []string, lldirs []string) (raftio.ILogDB, error) {
		return logdb.NewDefaultBatchedLogDB(config, cb, dirs, lldirs)
	}
	nff := func(config config.NodeHostConfig, cb config.LogDBCallback,
		dirs []string, lldirs []string) (raftio.ILogDB, error) {
		return logdb.NewDefaultLogDB(config, cb, dirs, lldirs)
	}
	to := &testOption{
		updateNodeHostConfig: func(c *config.NodeHostConfig) *config.NodeHostConfig {
			c.Expert.LogDBFactory = &testLogDBFactory2{f: bff}
			return c
		},
		createSM: func(uint64, uint64) sm.IStateMachine {
			return &PST{}
		},
		tf: func(nh *NodeHost) {
			pto := pto(nh)
			cs := nh.GetNoOPSession(1)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			_, err := nh.SyncPropose(ctx, cs, []byte("test-data"))
			cancel()
			require.NoError(t, err, "failed to make proposal %v", err)
		},
		at: func(*NodeHost) {
			nhc := getTestNodeHostConfig(fs)
			nhc.Expert.LogDBFactory = &testLogDBFactory2{f: nff}
			nh, err := NewNodeHost(*nhc)
			require.NoError(t, err, "failed to create node host")
			if nh != nil {
				nh.Close()
			}
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestNodeHostWithUnexpectedDeploymentIDWillBeDetected(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		noElection: true,
		at: func(*NodeHost) {
			nhc := getTestNodeHostConfig(fs)
			nhc.DeploymentID = 200
			_, err := NewNodeHost(*nhc)
			require.Equal(t, server.ErrDeploymentIDChanged, err,
				"failed to return ErrDeploymentIDChanged, got %v", err)
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestGossipInfoIsReported(t *testing.T) {
	fs := vfs.GetTestFS()
	advertiseAddress := "202.96.1.2:12345"
	to := &testOption{
		noElection: true,
		updateNodeHostConfig: func(c *config.NodeHostConfig) *config.NodeHostConfig {
			c.DefaultNodeRegistryEnabled = true
			c.Gossip = config.GossipConfig{
				BindAddress:      "localhost:23001",
				AdvertiseAddress: advertiseAddress,
				Seed:             []string{"localhost:23002"},
			}
			return c
		},
		tf: func(nh *NodeHost) {
			nhi := nh.GetNodeHostInfo(DefaultNodeHostInfoOption)
			assert.Equal(t, advertiseAddress, nhi.Gossip.AdvertiseAddress,
				"unexpected advertise address")
			assert.True(t, nhi.Gossip.Enabled, "gossip info not enabled")
			assert.Equal(t, 1, nhi.Gossip.NumOfKnownNodeHosts,
				"unexpected NumOfKnownNodeHosts")
			assert.Equal(t, nh.ID(), nhi.NodeHostID, "unexpected NodeHostID")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestLeaderInfoIsReported(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			leaderAvailable := false
			for i := 0; i < 500; i++ {
				nhi := nh.GetNodeHostInfo(DefaultNodeHostInfoOption)
				require.Len(t, nhi.ShardInfoList, 1, "unexpected len")
				require.Equal(t, uint64(1), nhi.ShardInfoList[0].ShardID,
					"unexpected shard id")
				if nhi.ShardInfoList[0].LeaderID != 1 {
					time.Sleep(20 * time.Millisecond)
				} else {
					leaderAvailable = true
					break
				}
			}
			require.True(t, leaderAvailable, "failed to get leader info")

			pto := pto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			defer cancel()
			err := nh.SyncRequestAddReplica(ctx, 1, 2, "noidea:8080", 0)
			require.NoError(t, err, "failed to add node %v", err)

			leaderChanged := false
			for i := 0; i < 500; i++ {
				nhi := nh.GetNodeHostInfo(DefaultNodeHostInfoOption)
				require.Len(t, nhi.ShardInfoList, 1, "unexpected len")
				if nhi.ShardInfoList[0].LeaderID == 1 {
					time.Sleep(20 * time.Millisecond)
				} else {
					leaderChanged = true
					break
				}
			}
			require.True(t, leaderChanged, "no leader info change")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestDroppedRequestsAreReported(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			pto := lpto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			defer cancel()
			err := nh.SyncRequestAddReplica(ctx, 1, 2, "noidea:8080", 0)
			require.NoError(t, err, "failed to add node %v", err)

			leaderSteppedDown := false
			for i := 0; i < 1000; i++ {
				_, _, ok, err := nh.GetLeaderID(1)
				require.NoError(t, err, "failed to get leader id %v", err)
				if !ok {
					leaderSteppedDown = true
					break
				}
				time.Sleep(10 * time.Millisecond)
			}
			require.True(t, leaderSteppedDown, "leader failed to step down")

			unlimited := 30 * time.Minute
			func() {
				nctx, ncancel := context.WithTimeout(context.Background(), unlimited)
				defer ncancel()
				cs := nh.GetNoOPSession(1)
				for i := 0; i < 10; i++ {
					_, err := nh.SyncPropose(nctx, cs, make([]byte, 1))
					assert.Equal(t, ErrShardNotReady, err, "unexpected error")
				}
			}()

			func() {
				nctx, ncancel := context.WithTimeout(context.Background(), unlimited)
				defer ncancel()
				for i := 0; i < 10; i++ {
					err := nh.SyncRequestAddReplica(nctx, 1, 3, "noidea:8080", 0)
					assert.Equal(t, ErrShardNotReady, err, "unexpected error")
				}
			}()

			func() {
				nctx, ncancel := context.WithTimeout(context.Background(), unlimited)
				defer ncancel()
				for i := 0; i < 10; i++ {
					_, err := nh.SyncRead(nctx, 1, nil)
					assert.Equal(t, ErrShardNotReady, err, "unexpected error")
				}
			}()
		},
	}
	runNodeHostTest(t, to, fs)
}

type testRaftEventListener struct {
	mu       sync.Mutex
	received []raftio.LeaderInfo
}

func (rel *testRaftEventListener) LeaderUpdated(info raftio.LeaderInfo) {
	rel.mu.Lock()
	defer rel.mu.Unlock()
	rel.received = append(rel.received, info)
}

func (rel *testRaftEventListener) get() []raftio.LeaderInfo {
	rel.mu.Lock()
	defer rel.mu.Unlock()
	r := make([]raftio.LeaderInfo, 0)
	return append(r, rel.received...)
}

func TestRaftEventsAreReported(t *testing.T) {
	fs := vfs.GetTestFS()
	rel := &testRaftEventListener{
		received: make([]raftio.LeaderInfo, 0),
	}
	to := &testOption{
		defaultTestNode: true,
		updateNodeHostConfig: func(nh *config.NodeHostConfig) *config.NodeHostConfig {
			nh.RaftEventListener = rel
			return nh
		},
		tf: func(nh *NodeHost) {
			pto := pto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			err := nh.SyncRequestAddReplica(ctx, 1, 2, "127.0.0.1:8080", 0)
			require.NoError(t, err, "add node failed %v", err)
			cancel()

			var received []raftio.LeaderInfo
			notified := false
			for i := 0; i < 1000; i++ {
				received = rel.get()
				if len(received) >= 4 {
					notified = true
					break
				}
				time.Sleep(10 * time.Millisecond)
			}
			require.True(t, notified, "failed to get LeaderUpdated notification")

			exp0 := raftio.LeaderInfo{ShardID: 1, ReplicaID: 1,
				LeaderID: raftio.NoLeader, Term: 1}
			exp1 := raftio.LeaderInfo{ShardID: 1, ReplicaID: 1,
				LeaderID: raftio.NoLeader, Term: 2}
			exp2 := raftio.LeaderInfo{ShardID: 1, ReplicaID: 1,
				LeaderID: 1, Term: 2}
			exp3 := raftio.LeaderInfo{ShardID: 1, ReplicaID: 1,
				LeaderID: raftio.NoLeader, Term: 2}
			expected := []raftio.LeaderInfo{exp0, exp1, exp2, exp3}
			for idx := range expected {
				assert.Equal(t, expected[idx], received[idx],
					"unexpecded leader info at index %d", idx)
			}
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestV2DataCanBeHandled(t *testing.T) {
	fs := vfs.GetTestFS()
	if vfs.GetTestFS() != vfs.DefaultFS {
		t.Skip("skipped as not using the default fs")
	}
	v2datafp := "internal/logdb/testdata/v2-rocksdb-batched.tar.bz2"
	targetDir := "test-v2-data-safe-to-remove"
	require.NoError(t, fs.RemoveAll(targetDir))
	defer func() {
		require.NoError(t, fs.RemoveAll(targetDir))
	}()

	topDirName := "single_nodehost_test_dir_safe_to_delete"
	testHostname := "lindfield.local"
	require.NoError(t, fileutil.ExtractTarBz2(v2datafp, targetDir, fs))
	hostname, err := os.Hostname()
	require.NoError(t, err, "failed to get hostname %v", err)

	testPath := fs.PathJoin(targetDir, topDirName, testHostname)
	expPath := fs.PathJoin(targetDir, topDirName, hostname)
	if expPath != testPath {
		require.NoError(t, fs.Rename(testPath, expPath),
			"failed to rename the dir %v", err)
	}

	v2dataDir := fs.PathJoin(targetDir, topDirName)
	to := &testOption{
		noElection: true,
		updateNodeHostConfig: func(c *config.NodeHostConfig) *config.NodeHostConfig {
			c.WALDir = v2dataDir
			c.NodeHostDir = v2dataDir
			return c
		},
		tf: func(nh *NodeHost) {
			name := nh.mu.logdb.Name()
			if name != "sharded-pebble" {
				t.Skip("skipped as not using rocksdb compatible logdb")
			}
			logdb := nh.mu.logdb
			rs, err := logdb.ReadRaftState(2, 1, 0)
			require.NoError(t, err, "failed to get raft state %v", err)
			assert.Equal(t, uint64(3), rs.EntryCount, "unexpected entry count")
			assert.Equal(t, uint64(3), rs.State.Commit, "unexpected commit")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestSnapshotCanBeCompressed(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		compressed: true,
		createSM: func(uint64, uint64) sm.IStateMachine {
			return &tests.VerboseSnapshotSM{}
		},
		tf: func(nh *NodeHost) {
			pto := lpto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			_, err := nh.SyncRequestSnapshot(ctx, 1, DefaultSnapshotOption)
			cancel()
			require.NoError(t, err, "failed to request snapshot %v", err)

			logdb := nh.mu.logdb
			ss, err := logdb.GetSnapshot(1, 1)
			require.NoError(t, err, "failed to list snapshots: %v", err)
			require.False(t, pb.IsEmptySnapshot(ss), "failed to get snapshot rec")

			fi, err := fs.Stat(ss.Filepath)
			require.NoError(t, err, "failed to get file path %v", err)
			require.Less(t, fi.Size(), int64(1024*364),
				"snapshot file not compressed, sz %d", fi.Size())
		},
	}
	runNodeHostTest(t, to, fs)
}

func makeProposals(nh *NodeHost) {
	session := nh.GetNoOPSession(1)
	pto := pto(nh)
	for i := 0; i < 16; i++ {
		ctx, cancel := context.WithTimeout(context.Background(), pto)
		_, err := nh.SyncPropose(ctx, session, []byte("test-data"))
		cancel()
		if err != nil {
			time.Sleep(100 * time.Millisecond)
		}
	}
}

func testWitnessIO(t *testing.T,
	witnessTestFunc func(*NodeHost, *NodeHost, *tests.SimDiskSM), fs vfs.IFS) {
	tf := func() {
		rc := config.Config{
			ShardID:      1,
			ReplicaID:    1,
			ElectionRTT:  3,
			HeartbeatRTT: 1,
			CheckQuorum:  true,
		}
		peers := make(map[uint64]string)
		peers[1] = nodeHostTestAddr1
		dir := fs.PathJoin(singleNodeHostTestDir, "nh1")
		nhc1 := config.NodeHostConfig{
			NodeHostDir:    dir,
			RTTMillisecond: getRTTMillisecond(fs, dir),
			RaftAddress:    nodeHostTestAddr1,
			Expert:         getTestExpertConfig(fs),
		}
		nh1, err := NewNodeHost(nhc1)
		require.NoError(t, err, "failed to create node host %v", err)
		defer nh1.Close()

		newSM := func(uint64, uint64) sm.IOnDiskStateMachine {
			return tests.NewSimDiskSM(0)
		}
		require.NoError(t, nh1.StartOnDiskReplica(peers, false, newSM, rc))
		waitForLeaderToBeElected(t, nh1, 1)

		for i := 0; i < 8; i++ {
			makeProposals(nh1)
			pto := lpto(nh1)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			opt := SnapshotOption{
				OverrideCompactionOverhead: true,
				CompactionOverhead:         1,
			}
			_, err := nh1.SyncRequestSnapshot(ctx, 1, opt)
			cancel()
			require.NoError(t, err, "failed to request snapshot %v", err)
		}

		pto := pto(nh1)
		ctx, cancel := context.WithTimeout(context.Background(), pto)
		err = nh1.SyncRequestAddWitness(ctx, 1, 2, nodeHostTestAddr2, 0)
		cancel()
		require.NoError(t, err, "failed to add witness %v", err)

		rc2 := rc
		rc2.ReplicaID = 2
		rc2.IsWitness = true
		nhc2 := nhc1
		nhc2.RaftAddress = nodeHostTestAddr2
		nhc2.NodeHostDir = fs.PathJoin(singleNodeHostTestDir, "nh2")
		nh2, err := NewNodeHost(nhc2)
		require.NoError(t, err, "failed to create node host %v", err)
		defer nh2.Close()

		witness := tests.NewSimDiskSM(0)
		newWitness := func(uint64, uint64) sm.IOnDiskStateMachine {
			return witness
		}
		err = nh2.StartOnDiskReplica(nil, true, newWitness, rc2)
		require.NoError(t, err, "failed to start shard %v", err)
		waitForLeaderToBeElected(t, nh2, 1)
		witnessTestFunc(nh1, nh2, witness)
	}
	runNodeHostTestDC(t, tf, true, fs)
}

func TestWitnessSnapshotIsCorrectlyHandled(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(nh1 *NodeHost, nh2 *NodeHost, witness *tests.SimDiskSM) {
		for {
			require.Zero(t, witness.GetRecovered(),
				"unexpected recovered count %d", witness.GetRecovered())
			snapshot, err := nh2.mu.logdb.GetSnapshot(1, 2)
			require.NoError(t, err)

			if pb.IsEmptySnapshot(snapshot) {
				time.Sleep(100 * time.Millisecond)
			} else {
				require.True(t, snapshot.Witness, "not a witness snapshot")
				return
			}
		}
	}
	testWitnessIO(t, tf, fs)
}

func TestWitnessCanReplicateEntries(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(nh1 *NodeHost, nh2 *NodeHost, witness *tests.SimDiskSM) {
		for i := 0; i < 8; i++ {
			makeProposals(nh1)
		}
		require.Zero(t, witness.GetApplied(),
			"unexpected applied count %d", witness.GetApplied())
	}
	testWitnessIO(t, tf, fs)
}

func TestWitnessCanNotInitiateIORequest(t *testing.T) {
	fs := vfs.GetTestFS()
	tf := func(nh1 *NodeHost, nh2 *NodeHost, witness *tests.SimDiskSM) {
		pto := lpto(nh1)
		opt := SnapshotOption{OverrideCompactionOverhead: true, CompactionOverhead: 1}
		_, err := nh2.RequestSnapshot(1, opt, pto)
		assert.Equal(t, ErrInvalidOperation, err, "snapshot not rejected")

		session := nh2.GetNoOPSession(1)
		_, err = nh2.Propose(session, []byte("test-data"), pto)
		assert.Equal(t, ErrInvalidOperation, err, "proposal not rejected")

		session = client.NewSession(1, nh2.env.GetRandomSource())
		session.PrepareForRegister()
		_, err = nh2.ProposeSession(session, pto)
		assert.Equal(t, ErrInvalidOperation, err, "propose session not rejected")

		_, err = nh2.ReadIndex(1, pto)
		assert.Equal(t, ErrInvalidOperation, err, "sync read not rejected")

		_, err = nh2.RequestAddReplica(1, 3, "a3.com:12345", 0, pto)
		assert.Equal(t, ErrInvalidOperation, err, "add node not rejected")

		_, err = nh2.RequestDeleteReplica(1, 3, 0, pto)
		assert.Equal(t, ErrInvalidOperation, err, "delete node not rejected")

		_, err = nh2.RequestAddNonVoting(1, 3, "a3.com:12345", 0, pto)
		assert.Equal(t, ErrInvalidOperation, err, "add non-voting not rejected")

		_, err = nh2.RequestAddWitness(1, 3, "a3.com:12345", 0, pto)
		assert.Equal(t, ErrInvalidOperation, err, "add witness not rejected")

		err = nh2.RequestLeaderTransfer(1, 3)
		assert.Equal(t, ErrInvalidOperation, err, "leader transfer not rejected")

		_, err = nh2.StaleRead(1, nil)
		assert.Equal(t, ErrInvalidOperation, err, "stale read not rejected")
	}
	testWitnessIO(t, tf, fs)
}

func TestStateMachineIsClosedAfterOffloaded(t *testing.T) {
	fs := vfs.GetTestFS()
	tsm := &TimeoutStateMachine{}
	to := &testOption{
		createSM: func(uint64, uint64) sm.IStateMachine {
			return tsm
		},
		at: func(nh *NodeHost) {
			require.True(t, tsm.closed, "sm not closed")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestTimeoutCanBeReturned(t *testing.T) {
	fs := vfs.GetTestFS()
	rtt := getRTTMillisecond(fs, singleNodeHostTestDir)
	to := &testOption{
		createSM: func(uint64, uint64) sm.IStateMachine {
			return &TimeoutStateMachine{
				updateDelay:   rtt * 10,
				snapshotDelay: rtt * 10,
			}
		},
		tf: func(nh *NodeHost) {
			timeout := time.Duration(rtt*5) * time.Millisecond
			ctx, cancel := context.WithTimeout(context.Background(), timeout)
			session := nh.GetNoOPSession(1)
			_, err := nh.SyncPropose(ctx, session, []byte("test"))
			cancel()
			assert.Equal(t, ErrTimeout, err, "unexpected propose error")

			ctx, cancel = context.WithTimeout(context.Background(), timeout)
			_, err = nh.SyncRequestSnapshot(ctx, 1, SnapshotOption{})
			cancel()
			assert.Equal(t, ErrTimeout, err, "unexpected snapshot error")
		},
	}
	runNodeHostTest(t, to, fs)
}

func testIOErrorIsHandled(t *testing.T, op vfs.Op) {
	inj := vfs.OnIndex(-1, op)
	fs := vfs.Wrap(vfs.GetTestFS(), inj)
	to := &testOption{
		fsErrorInjection: true,
		defaultTestNode:  true,
		tf: func(nh *NodeHost) {
			if nh.mu.logdb.Name() == "Tan" {
				t.Skip("skipped, using tan logdb")
			}
			inj.SetIndex(0)
			pto := pto(nh)
			ctx, cancel := context.WithTimeout(context.Background(), pto)
			session := nh.GetNoOPSession(1)
			_, err := nh.SyncPropose(ctx, session, []byte("test"))
			cancel()
			require.Equal(t, ErrTimeout, err,
				"proposal unexpectedly completed, %v", err)
			select {
			case e := <-nh.engine.ec:
				require.Equal(t, vfs.ErrInjected, e,
					"failed to return the expected error, %v", e)
			default:
				require.Fail(t, "failed to trigger error")
			}
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestIOErrorIsHandled(t *testing.T) {
	testIOErrorIsHandled(t, vfs.OpWrite)
	testIOErrorIsHandled(t, vfs.OpSync)
}

func TestInstallSnapshotMessageIsNeverDropped(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			nh.partitioned = 1
			handler := newNodeHostMessageHandler(nh)
			msg := pb.Message{Type: pb.InstallSnapshot, ShardID: 1, To: 1}
			batch := pb.MessageBatch{Requests: []pb.Message{msg}}
			s, m := handler.HandleMessageBatch(batch)
			assert.Equal(t, uint64(1), s, "snapshot message dropped")
			assert.Equal(t, uint64(0), m, "unexpected message count")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestMessageToUnknownNodeIsIgnored(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			handler := newNodeHostMessageHandler(nh)
			msg1 := pb.Message{Type: pb.Ping, ShardID: 1, To: 1}
			msg2 := pb.Message{Type: pb.Pong, ShardID: 1, To: 1}
			msg3 := pb.Message{Type: pb.Pong, ShardID: 1, To: 2}
			batch := pb.MessageBatch{Requests: []pb.Message{msg1, msg2, msg3}}
			s, m := handler.HandleMessageBatch(batch)
			assert.Equal(t, uint64(0), s, "unexpected snapshot count")
			assert.Equal(t, uint64(2), m, "unexpected message count")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestNodeCanBeUnloadedOnceClosed(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			countNodes := func(nh *NodeHost) uint64 {
				count := uint64(0)
				nh.mu.shards.Range(func(key, value interface{}) bool {
					count++
					return true
				})
				return count
			}
			node, ok := nh.getShard(1)
			require.True(t, ok, "failed to get node")
			node.requestRemoval()

			unloaded := false
			for retry := 0; retry < 1000; retry++ {
				if countNodes(nh) == 0 {
					unloaded = true
					break
				}
				time.Sleep(10 * time.Millisecond)
			}
			require.True(t, unloaded, "failed to unload the node")
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestUsingClosedNodeHostIsNotAllowed(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		at: func(nh *NodeHost) {
			assert.Equal(t, ErrClosed,
				nh.StartReplica(nil, false, nil, config.Config{}))
			assert.Equal(t, ErrClosed,
				nh.StartConcurrentReplica(nil, false, nil, config.Config{}))
			assert.Equal(t, ErrClosed,
				nh.StartOnDiskReplica(nil, false, nil, config.Config{}))
			assert.Equal(t, ErrClosed, nh.StopShard(1))
			assert.Equal(t, ErrClosed, nh.StopReplica(1, 1))
			ctx, cancel := context.WithTimeout(context.Background(), time.Second)
			defer cancel()
			_, err := nh.SyncPropose(ctx, nil, nil)
			assert.Equal(t, ErrClosed, err)
			_, err = nh.SyncRead(ctx, 1, nil)
			assert.Equal(t, ErrClosed, err)
			_, err = nh.SyncGetShardMembership(ctx, 1)
			assert.Equal(t, ErrClosed, err)
			_, _, _, err = nh.GetLeaderID(1)
			assert.Equal(t, ErrClosed, err)
			_, err = nh.Propose(nil, nil, time.Second)
			assert.Equal(t, ErrClosed, err)
			_, err = nh.ReadIndex(1, time.Second)
			assert.Equal(t, ErrClosed, err)
			_, err = nh.ReadLocalNode(nil, nil)
			assert.Equal(t, ErrClosed, err)
			_, err = nh.NAReadLocalNode(nil, nil)
			assert.Equal(t, ErrClosed, err)
			_, err = nh.StaleRead(1, nil)
			assert.Equal(t, ErrClosed, err)
			_, err = nh.SyncRequestSnapshot(ctx, 1, DefaultSnapshotOption)
			assert.Equal(t, ErrClosed, err)
			_, err = nh.RequestSnapshot(1, DefaultSnapshotOption, time.Second)
			assert.Equal(t, ErrClosed, err)
			_, err = nh.RequestCompaction(1, 1)
			assert.Equal(t, ErrClosed, err)
			assert.Equal(t, ErrClosed, nh.SyncRequestDeleteReplica(ctx, 1, 1, 0))
			assert.Equal(t, ErrClosed, nh.SyncRequestAddReplica(ctx, 1, 1, "", 0))
			assert.Equal(t, ErrClosed, nh.SyncRequestAddNonVoting(ctx, 1, 1, "", 0))
			assert.Equal(t, ErrClosed, nh.SyncRequestAddWitness(ctx, 1, 1, "", 0))
			_, err = nh.RequestDeleteReplica(1, 1, 0, time.Second)
			assert.Equal(t, ErrClosed, err)
			_, err = nh.RequestAddReplica(1, 1, "", 0, time.Second)
			assert.Equal(t, ErrClosed, err)
			_, err = nh.RequestAddNonVoting(1, 1, "", 0, time.Second)
			assert.Equal(t, ErrClosed, err)
			_, err = nh.RequestAddWitness(1, 1, "", 0, time.Second)
			assert.Equal(t, ErrClosed, err)
			assert.Equal(t, ErrClosed, nh.RequestLeaderTransfer(1, 2))
			assert.Equal(t, ErrClosed, nh.SyncRemoveData(ctx, 1, 1))
			assert.Equal(t, ErrClosed, nh.RemoveData(1, 1))
			_, err = nh.GetNodeUser(1)
			assert.Equal(t, ErrClosed, err)
			_, err = nh.QueryRaftLog(1, 1, 2, 100)
			assert.Equal(t, ErrClosed, err)
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestContextDeadlineIsChecked(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			ctx := context.Background()
			_, err := nh.SyncPropose(ctx, nil, nil)
			assert.Equal(t, ErrDeadlineNotSet, err)
			_, err = nh.SyncRead(ctx, 1, nil)
			assert.Equal(t, ErrDeadlineNotSet, err)
			_, err = nh.SyncGetShardMembership(ctx, 1)
			assert.Equal(t, ErrDeadlineNotSet, err)
			err = nh.SyncCloseSession(ctx, nil)
			assert.Equal(t, ErrDeadlineNotSet, err)
			_, err = nh.SyncGetSession(ctx, 1)
			assert.Equal(t, ErrDeadlineNotSet, err)
			err = nh.SyncCloseSession(ctx, nil)
			assert.Equal(t, ErrDeadlineNotSet, err)
			_, err = nh.SyncRequestSnapshot(ctx, 1, DefaultSnapshotOption)
			assert.Equal(t, ErrDeadlineNotSet, err)
			err = nh.SyncRequestDeleteReplica(ctx, 1, 1, 0)
			assert.Equal(t, ErrDeadlineNotSet, err)
			err = nh.SyncRequestAddReplica(ctx, 1, 2, "", 0)
			assert.Equal(t, ErrDeadlineNotSet, err)
			err = nh.SyncRequestAddNonVoting(ctx, 1, 2, "", 0)
			assert.Equal(t, ErrDeadlineNotSet, err)
			err = nh.SyncRequestAddWitness(ctx, 1, 2, "", 0)
			assert.Equal(t, ErrDeadlineNotSet, err)
			err = nh.SyncRemoveData(ctx, 1, 1)
			assert.Equal(t, ErrDeadlineNotSet, err)
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestGetTimeoutFromContext(t *testing.T) {
	defer leaktest.AfterTest(t)()
	_, err := getTimeoutFromContext(context.Background())
	require.Equal(t, ErrDeadlineNotSet, err,
		"failed to return ErrDeadlineNotSet, %v", err)
	ctx, cancel := context.WithTimeout(context.Background(), time.Minute)
	defer cancel()
	d, err := getTimeoutFromContext(ctx)
	require.NoError(t, err, "failed to get timeout, %v", err)
	require.True(t, d >= 55*time.Second, "unexpected result")
}

func TestHandleSnapshotStatus(t *testing.T) {
	defer leaktest.AfterTest(t)()
	nh := &NodeHost{stopper: syncutil.NewStopper()}
	engine := newExecEngine(nh, config.GetDefaultEngineConfig(), false, false, nil, nil)
	defer func() {
		require.NoError(t, engine.close())
	}()
	nh.engine = engine
	nh.events.sys = newSysEventListener(nil, nh.stopper.ShouldStop())
	h := messageHandler{nh: nh}
	mq := server.NewMessageQueue(1024, false, lazyFreeCycle, 1024)
	node := &node{shardID: 1, replicaID: 1, mq: mq}
	h.nh.mu.shards.Store(uint64(1), node)
	h.HandleSnapshotStatus(1, 2, true)
	for i := uint64(0); i <= streamPushDelayTick; i++ {
		node.mq.Tick()
	}
	msgs := node.mq.Get()
	require.Len(t, msgs, 1, "no msg")
	assert.Equal(t, pb.SnapshotStatus, msgs[0].Type, "unexpected message type")
	assert.True(t, msgs[0].Reject, "message not rejected")
}

func TestSnapshotReceivedMessageCanBeConverted(t *testing.T) {
	defer leaktest.AfterTest(t)()
	nh := &NodeHost{stopper: syncutil.NewStopper()}
	engine := newExecEngine(nh, config.GetDefaultEngineConfig(), false, false, nil, nil)
	defer func() {
		require.NoError(t, engine.close())
	}()
	nh.engine = engine
	nh.events.sys = newSysEventListener(nil, nh.stopper.ShouldStop())
	h := messageHandler{nh: nh}
	mq := server.NewMessageQueue(1024, false, lazyFreeCycle, 1024)
	node := &node{shardID: 1, replicaID: 1, mq: mq}
	h.nh.mu.shards.Store(uint64(1), node)
	mb := pb.MessageBatch{
		Requests: []pb.Message{{To: 1, From: 2, ShardID: 1, Type: pb.SnapshotReceived}},
	}
	sc, mc := h.HandleMessageBatch(mb)
	require.Zero(t, sc, "snapshot count not zero")
	require.Equal(t, uint64(1), mc, "message count not 1")
	for i := uint64(0); i <= streamConfirmedDelayTick; i++ {
		node.mq.Tick()
	}
	msgs := node.mq.Get()
	require.Len(t, msgs, 1, "no msg")
	assert.Equal(t, pb.SnapshotStatus, msgs[0].Type, "unexpected type")
	assert.False(t, msgs[0].Reject, "message rejected")
}

func TestIncorrectlyRoutedMessagesAreIgnored(t *testing.T) {
	defer leaktest.AfterTest(t)()
	nh := &NodeHost{stopper: syncutil.NewStopper()}
	engine := newExecEngine(nh, config.GetDefaultEngineConfig(), false, false, nil, nil)
	defer func() {
		require.NoError(t, engine.close())
	}()
	nh.engine = engine
	nh.events.sys = newSysEventListener(nil, nh.stopper.ShouldStop())
	h := messageHandler{nh: nh}
	mq := server.NewMessageQueue(1024, false, lazyFreeCycle, 1024)
	node := &node{shardID: 1, replicaID: 1, mq: mq}
	h.nh.mu.shards.Store(uint64(1), node)
	mb := pb.MessageBatch{
		Requests: []pb.Message{{To: 3, From: 2, ShardID: 1, Type: pb.SnapshotReceived}},
	}
	sc, mc := h.HandleMessageBatch(mb)
	assert.Zero(t, sc, "snapshot count not zero")
	assert.Zero(t, mc, "message count not zero")
	msgs := node.mq.Get()
	assert.Empty(t, msgs, "received unexpected message")
}

func TestProposeOnClosedNode(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			u, err := nh.GetNodeUser(1)
			require.NoError(t, err, "failed to get node, %v", err)
			err = nh.StopReplica(1, 1)
			require.NoError(t, err, "failed to stop node, %v", err)
			cs := nh.GetNoOPSession(1)
			_, err = u.Propose(cs, nil, time.Second)
			require.Error(t, err, "propose on closed node didn't cause error")
			plog.Infof("%v returned from closed node", err)
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestReadIndexOnClosedNode(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			u, err := nh.GetNodeUser(1)
			require.NoError(t, err, "failed to get node, %v", err)
			err = nh.StopReplica(1, 1)
			require.NoError(t, err, "failed to stop node, %v", err)
			_, err = u.ReadIndex(time.Second)
			require.Error(t, err, "ReadIndex on closed node didn't cause error")
			plog.Infof("%v returned from closed node", err)
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestNodeCanNotStartWhenStillLoadedInEngine(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			nodes := make(map[uint64]*node)
			nodes[2] = &node{shardID: 2, replicaID: 1}
			nh.engine.loaded.update(1, fromStepWorker, nodes)
			err := nh.startShard(nil, false, nil,
				config.Config{ShardID: 2, ReplicaID: 1}, pb.RegularStateMachine)
			require.Equal(t, ErrShardAlreadyExist, err,
				"failed to return ErrShardAlreadyExist, %v", err)
		},
	}
	runNodeHostTest(t, to, fs)
}

func TestBootstrapInfoIsValidated(t *testing.T) {
	fs := vfs.GetTestFS()
	to := &testOption{
		defaultTestNode: true,
		tf: func(nh *NodeHost) {
			_, _, err := nh.bootstrapShard(nil, false,
				config.Config{ShardID: 1, ReplicaID: 1}, pb.OnDiskStateMachine)
			require.Equal(t, ErrInvalidShardSettings, err,
				"failed to fail the boostrap, %v", err)
		},
	}
	runNodeHostTest(t, to, fs)
}

// slow tests

type stressRSM struct{}

func (s *stressRSM) Update(sm.Entry) (sm.Result, error) {
	plog.Infof("updated")
	return sm.Result{}, nil
}

func (s *stressRSM) Lookup(interface{}) (interface{}, error) {
	return nil, nil
}

func (s *stressRSM) SaveSnapshot(w io.Writer,
	f sm.ISnapshotFileCollection, c <-chan struct{}) error {
	data := make([]byte, settings.SnapshotChunkSize*3)
	_, err := w.Write(data)
	return err
}

func (s *stressRSM) RecoverFromSnapshot(r io.Reader,
	f []sm.SnapshotFile, c <-chan struct{}) error {
	plog.Infof("RecoverFromSnapshot called")
	data := make([]byte, settings.SnapshotChunkSize*3)
	n, err := io.ReadFull(r, data)
	if uint64(n) != settings.SnapshotChunkSize*3 {
		return errors.New("unexpected size")
	}
	return err
}

func (s *stressRSM) Close() error {
	return nil
}

// this test takes around 6 minutes on mbp and 30 seconds on a linux box with
// proper SSD
func TestSlowTestStressedSnapshotWorker(t *testing.T) {
	if len(os.Getenv("SLOW_TEST")) == 0 {
		t.Skip("skipped TestSlowTestStressedSnapshotWorker, SLOW_TEST not set")
	}
	fs := vfs.GetTestFS()
	require.NoError(t, fs.RemoveAll(singleNodeHostTestDir))
	defer func() {
		require.NoError(t, fs.RemoveAll(singleNodeHostTestDir))
	}()

	nh1dir := fs.PathJoin(singleNodeHostTestDir, "nh1")
	nh2dir := fs.PathJoin(singleNodeHostTestDir, "nh2")
	rc := config.Config{
		ShardID:            1,
		ReplicaID:          1,
		ElectionRTT:        10,
		HeartbeatRTT:       1,
		CheckQuorum:        true,
		SnapshotEntries:    5,
		CompactionOverhead: 1,
	}
	peers := make(map[uint64]string)
	peers[1] = nodeHostTestAddr1
	nhc1 := config.NodeHostConfig{
		WALDir:         nh1dir,
		NodeHostDir:    nh1dir,
		RTTMillisecond: 5 * getRTTMillisecond(fs, nh1dir),
		RaftAddress:    nodeHostTestAddr1,
		Expert:         getTestExpertConfig(fs),
	}
	nh1, err := NewNodeHost(nhc1)
	require.NoError(t, err, "failed to create node host %v", err)
	defer nh1.Close()

	newRSM := func(uint64, uint64) sm.IStateMachine {
		return &stressRSM{}
	}
	for i := uint64(1); i <= uint64(96); i++ {
		rc.ShardID = i
		err := nh1.StartReplica(peers, false, newRSM, rc)
		require.NoError(t, err, "failed to start shard %v", err)
		cs := nh1.GetNoOPSession(i)
		total := 20
		for {
			ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second)
			_, err := nh1.SyncPropose(ctx, cs, make([]byte, 1))
			cancel()
			if err == ErrTimeout || err == ErrShardNotReady {
				time.Sleep(100 * time.Millisecond)
				continue
			}
			total--
			if total == 0 {
				break
			}
		}
	}

	for i := uint64(1); i <= uint64(96); i++ {
		for {
			ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second)
			err := nh1.SyncRequestAddReplica(ctx, i, 2, nodeHostTestAddr2, 0)
			cancel()
			if err != nil {
				if err == ErrTimeout || err == ErrShardNotReady {
					time.Sleep(100 * time.Millisecond)
					continue
				}
				require.NoError(t, err, "failed to add node %v", err)
			} else {
				break
			}
		}
	}
	plog.Infof("all nodes added")

	nhc2 := config.NodeHostConfig{
		WALDir:         nh2dir,
		NodeHostDir:    nh2dir,
		RTTMillisecond: 5 * getRTTMillisecond(fs, nh2dir),
		RaftAddress:    nodeHostTestAddr2,
		Expert:         getTestExpertConfig(fs),
	}
	nh2, err := NewNodeHost(nhc2)
	require.NoError(t, err, "failed to create node host 2 %v", err)
	defer nh2.Close()

	for i := uint64(1); i <= uint64(96); i++ {
		rc.ShardID = i
		rc.ReplicaID = 2
		peers := make(map[uint64]string)
		err := nh2.StartReplica(peers, true, newRSM, rc)
		require.NoError(t, err, "failed to start shard %v", err)
	}

	for i := uint64(1); i <= uint64(96); i++ {
		cs := nh2.GetNoOPSession(i)
		total := 1000
		for {
			ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second)
			_, err := nh2.SyncPropose(ctx, cs, make([]byte, 1))
			cancel()
			if err != nil {
				if err == ErrTimeout || err == ErrShardNotReady {
					total--
					require.NotZero(t, total, "failed to make proposal on shard %d", i)
					time.Sleep(100 * time.Millisecond)
					continue
				}
				require.NoError(t, err, "failed to make proposal %v", err)
			} else {
				break
			}
		}
	}
}
````

## File: nodehost.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

/*
Package dragonboat is a feature complete and highly optimized multi-group Raft
implementation for providing consensus in distributed systems.

The NodeHost struct is the facade interface for all features provided by the
dragonboat package. Each NodeHost instance usually runs on a separate server
managing CPU, storage and network resources used for achieving consensus. Each
NodeHost manages Raft nodes from different Raft groups known as Raft shards.
Each Raft shard is identified by its ShardID, it usually consists of
multiple nodes (also known as replicas) each identified by a ReplicaID value.
Nodes from the same Raft shard suppose to be distributed on different NodeHost
instances across the network, this brings fault tolerance for machine and
network failures as application data stored in the Raft shard will be
available as long as the majority of its managing NodeHost instances (i.e. its
underlying servers) are accessible.

Arbitrary number of Raft shards can be launched across the network to
aggregate distributed processing and storage capacities. Users can also make
membership change requests to add or remove nodes from selected Raft shard.

User applications can leverage the power of the Raft protocol by implementing
the IStateMachine or IOnDiskStateMachine component, as defined in
github.com/lni/dragonboat/v4/statemachine. Known as user state machines, each
IStateMachine or IOnDiskStateMachine instance is in charge of updating, querying
and snapshotting application data with minimum exposure to the Raft protocol
itself.

Dragonboat guarantees the linearizability of your I/O when interacting with the
IStateMachine or IOnDiskStateMachine instances. In plain English, writes (via
making proposals) to your Raft shard appears to be instantaneous, once a write
is completed, all later reads (via linearizable read based on Raft's ReadIndex
protocol) should return the value of that write or a later write. Once a value
is returned by a linearizable read, all later reads should return the same value
or the result of a later write.

To strictly provide such guarantee, we need to implement the at-most-once
semantic. For a client, when it retries the proposal that failed to complete by
its deadline, it faces the risk of having the same proposal committed and
applied twice into the user state machine. Dragonboat prevents this by
implementing the client session concept described in Diego Ongaro's PhD thesis.
*/
package dragonboat // github.com/lni/dragonboat/v4

import (
	"context"
	"math"
	"reflect"
	"runtime"
	"sync"
	"sync/atomic"
	"time"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/logutil"
	"github.com/lni/goutils/syncutil"

	"github.com/lni/dragonboat/v4/client"
	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/id"
	"github.com/lni/dragonboat/v4/internal/invariants"
	"github.com/lni/dragonboat/v4/internal/logdb"
	"github.com/lni/dragonboat/v4/internal/registry"
	"github.com/lni/dragonboat/v4/internal/rsm"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/internal/transport"
	"github.com/lni/dragonboat/v4/internal/utils"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
)

const (
	// DragonboatMajor is the major version number
	DragonboatMajor = 4
	// DragonboatMinor is the minor version number
	DragonboatMinor = 0
	// DragonboatPatch is the patch version number
	DragonboatPatch = 0
	// DEVVersion is a boolean flag indicating whether this is a dev version
	DEVVersion = true
)

var (
	receiveQueueLen   = settings.Soft.ReceiveQueueLength
	requestPoolShards = settings.Soft.NodeHostRequestStatePoolShards
	streamConnections = settings.Soft.StreamConnections
)

var (
	// ErrClosed is returned when a request is made on closed NodeHost instance.
	ErrClosed = errors.New("dragonboat: closed")
	// ErrReplicaRemoved indictes that the requested node has been removed.
	ErrReplicaRemoved = errors.New("node removed")
	// ErrShardNotFound indicates that the specified shard is not found.
	ErrShardNotFound = errors.New("shard not found")
	// ErrShardAlreadyExist indicates that the specified shard already exist.
	ErrShardAlreadyExist = errors.New("shard already exist")
	// ErrShardNotStopped indicates that the specified shard is still running
	// and thus prevented the requested operation to be completed.
	ErrShardNotStopped = errors.New("shard not stopped")
	// ErrInvalidShardSettings indicates that shard settings specified for
	// the StartReplica method are invalid.
	ErrInvalidShardSettings = errors.New("shard settings are invalid")
	// ErrShardNotBootstrapped indicates that the specified shard has not
	// been boostrapped yet. When starting this node, depending on whether this
	// node is an initial member of the Raft shard, you must either specify
	// all of its initial members or set the join flag to true.
	// When used correctly, dragonboat only returns this error in the rare
	// situation when you try to restart a node crashed during its previous
	// bootstrap attempt.
	ErrShardNotBootstrapped = errors.New("shard not bootstrapped")
	// ErrDeadlineNotSet indicates that the context parameter provided does not
	// carry a deadline.
	ErrDeadlineNotSet = errors.New("deadline not set")
	// ErrInvalidDeadline indicates that the specified deadline is invalid, e.g.
	// time in the past.
	ErrInvalidDeadline = errors.New("invalid deadline")
	// ErrDirNotExist indicates that the specified dir does not exist.
	ErrDirNotExist = errors.New("specified dir does not exist")
	// ErrLogDBNotCreatedOrClosed indicates that the logdb is not created yet or closed already.
	ErrLogDBNotCreatedOrClosed = errors.New("logdb is not created yet or closed already")
	// ErrInvalidRange indicates that the specified log range is invalid.
	ErrInvalidRange = errors.New("invalid log range")
)

// ShardInfo is a record for representing the state of a Raft shard based
// on the knowledge of the local NodeHost instance.
type ShardInfo = registry.ShardInfo

// ShardView is a record for representing the state of a Raft shard based
// on the knowledge of distributed NodeHost instances as shared by gossip.
type ShardView = registry.ShardView

// GossipInfo contains details of the gossip service.
type GossipInfo struct {
	// AdvertiseAddress is the advertise address used by the gossip service.
	AdvertiseAddress string
	// NumOfKnownNodeHosts is the number of current live NodeHost instances known
	// to the gossip service. Note that the gossip service always knowns the
	// local NodeHost instance itself. When the NumOfKnownNodeHosts value is 1,
	// it means the gossip service doesn't know any other NodeHost instance that
	// is considered as live.
	NumOfKnownNodeHosts int
	// Enabled is a boolean flag indicating whether the gossip service is enabled.
	Enabled bool
}

// NodeHostInfo provides info about the NodeHost, including its managed Raft
// shard nodes and available Raft logs saved in its local persistent storage.
type NodeHostInfo struct {
	// NodeHostID is the unique identifier of the NodeHost instance.
	NodeHostID string
	// RaftAddress is the public address of the NodeHost used for exchanging Raft
	// messages, snapshots and other metadata with other NodeHost instances.
	RaftAddress string
	// Gossip contains gossip service related information.
	Gossip GossipInfo
	// ShardInfo is a list of all Raft shards managed by the NodeHost
	ShardInfoList []ShardInfo
	// LogInfo is a list of raftio.NodeInfo values representing all Raft logs
	// stored on the NodeHost.
	LogInfo []raftio.NodeInfo
}

// NodeHostInfoOption is the option type used when querying NodeHostInfo.
type NodeHostInfoOption struct {
	// SkipLogInfo is the boolean flag indicating whether Raft Log info should be
	// skipped when querying the NodeHostInfo.
	SkipLogInfo bool
}

// DefaultNodeHostInfoOption is the default NodeHostInfoOption value. It
// requests the GetNodeHostInfo method to return all supported info.
var DefaultNodeHostInfoOption NodeHostInfoOption

// SnapshotOption is the options supported when requesting a snapshot to be
// generated.
type SnapshotOption struct {
	// ExportPath is the path where the exported snapshot should be stored, it
	// must point to an existing directory for which the current user has write
	// permission.
	ExportPath string
	// CompactionOverhead is the compaction overhead value to use for the
	// requested snapshot operation when OverrideCompactionOverhead is set to
	// true. This field is ignored when exporting a snapshot. ErrInvalidOption
	// will be returned if both CompactionOverhead and CompactionIndex are set.
	CompactionOverhead uint64
	// CompactionIndex specifies the raft log index before which all log entries
	// can be compacted after creating the snapshot. This option is only considered
	// when OverrideCompactionOverhead is set to true, ErrInvalidOption will be
	// returned if both CompactionOverhead and CompactionIndex are set.
	CompactionIndex uint64
	// Exported is a boolean flag indicating whether to export the requested
	// snapshot. For an exported snapshot, users are responsible for managing the
	// snapshot files. An exported snapshot is usually used to repair the shard
	// when it permanently loses its majority quorum. See the ImportSnapshot method
	// in the tools package for more details.
	Exported bool
	// OverrideCompactionOverhead defines whether the requested snapshot operation
	// should override the compaction overhead setting specified in node's config.
	// This field is ignored when exporting a snapshot.
	OverrideCompactionOverhead bool
}

// Validate checks the SnapshotOption and return error when there is any
// invalid option found.
func (o SnapshotOption) Validate() error {
	if o.OverrideCompactionOverhead {
		if o.CompactionOverhead > 0 && o.CompactionIndex > 0 {
			plog.Errorf("both CompactionOverhead and CompactionIndex are set")
			return ErrInvalidOption
		}
	} else {
		if o.CompactionOverhead > 0 || o.CompactionIndex > 0 {
			plog.Warningf("CompactionOverhead and CompactionIndex will be ignored")
		}
	}
	return nil
}

// ReadonlyLogReader provides safe readonly access to the underlying logdb.
type ReadonlyLogReader interface {
	// GetRange returns the range of the entries in LogDB.
	GetRange() (uint64, uint64)
	// NodeState returns the state of the node persistent in LogDB.
	NodeState() (pb.State, pb.Membership)
	// Term returns the entry term of the specified entry.
	Term(index uint64) (uint64, error)
	// Entries returns entries between [low, high) with total size of entries
	// limited to maxSize bytes.
	Entries(low uint64, high uint64, maxSize uint64) ([]pb.Entry, error)
	// Snapshot returns the metadata for the most recent snapshot known to the
	// LogDB.
	Snapshot() pb.Snapshot
}

// DefaultSnapshotOption is the default SnapshotOption value to use when
// requesting a snapshot to be generated. This default option causes a regular
// snapshot to be generated.
var DefaultSnapshotOption SnapshotOption

// Target is the type used to specify where a node is running. Target is remote
// NodeHost's RaftAddress value when NodeHostConfig.DefaultNodeRegistryEnabled is not
// set. Target will use NodeHost's ID value when
// NodeHostConfig.DefaultNodeRegistryEnabled is set.
type Target = string

// NodeHost manages Raft shards and enables them to share resources such as
// transport and persistent storage etc. NodeHost is also the central thread
// safe access point for accessing Dragonboat functionalities.
type NodeHost struct {
	mu struct {
		sync.RWMutex
		cci    uint64
		cciCh  chan struct{}
		shards sync.Map
		lm     sync.Map
		logdb  raftio.ILogDB
	}
	events struct {
		leaderInfoQ *leaderInfoQueue
		raft        raftio.IRaftEventListener
		sys         *sysEventListener
	}
	registry     INodeHostRegistry
	nodes        raftio.INodeRegistry
	fs           vfs.IFS
	transport    transport.ITransport
	id           *id.UUID
	stopper      *syncutil.Stopper
	msgHandler   *messageHandler
	env          *server.Env
	engine       *engine
	nhConfig     config.NodeHostConfig
	requestPools []*sync.Pool
	partitioned  int32
	closed       int32
}

var _ nodeLoader = (*NodeHost)(nil)

var dn = logutil.DescribeNode

var firstError = utils.FirstError

// NewNodeHost creates a new NodeHost instance. In a typical application, it is
// expected to have one NodeHost on each server.
func NewNodeHost(nhConfig config.NodeHostConfig) (*NodeHost, error) {
	logBuildTagsAndVersion()
	if err := nhConfig.Validate(); err != nil {
		return nil, err
	}
	if err := nhConfig.Prepare(); err != nil {
		return nil, err
	}
	env, err := server.NewEnv(nhConfig, nhConfig.Expert.FS)
	if err != nil {
		return nil, err
	}
	nh := &NodeHost{
		env:      env,
		nhConfig: nhConfig,
		stopper:  syncutil.NewStopper(),
		fs:       nhConfig.Expert.FS,
	}
	// make static check happy
	_ = nh.partitioned
	nh.events.raft = nhConfig.RaftEventListener
	nh.events.sys = newSysEventListener(nhConfig.SystemEventListener,
		nh.stopper.ShouldStop())
	nh.mu.cciCh = make(chan struct{}, 1)
	if nhConfig.RaftEventListener != nil {
		nh.events.leaderInfoQ = newLeaderInfoQueue()
	}
	if nhConfig.RaftEventListener != nil || nhConfig.SystemEventListener != nil {
		nh.stopper.RunWorker(func() {
			nh.handleListenerEvents()
		})
	}
	nh.msgHandler = newNodeHostMessageHandler(nh)
	nh.createPools()
	defer func() {
		if r := recover(); r != nil {
			nh.Close()
			if r, ok := r.(error); ok {
				panicNow(r)
			}
		}
	}()
	did := nh.nhConfig.GetDeploymentID()
	plog.Infof("DeploymentID set to %d", did)
	if err := nh.createLogDB(); err != nil {
		nh.Close()
		return nil, err
	}
	if err := nh.loadNodeHostID(); err != nil {
		nh.Close()
		return nil, err
	}
	plog.Infof("NodeHost ID: %s", nh.id.String())
	if err := nh.createNodeRegistry(); err != nil {
		nh.Close()
		return nil, err
	}
	errorInjection := false
	if nhConfig.Expert.FS != nil {
		_, errorInjection = nhConfig.Expert.FS.(*vfs.ErrorFS)
		plog.Infof("filesystem error injection mode enabled: %t", errorInjection)
	}
	nh.engine = newExecEngine(nh, nhConfig.Expert.Engine,
		nh.nhConfig.NotifyCommit, errorInjection, nh.env, nh.mu.logdb)
	if err := nh.createTransport(); err != nil {
		nh.Close()
		return nil, err
	}
	nh.stopper.RunWorker(func() {
		nh.nodeMonitorMain()
	})
	nh.stopper.RunWorker(func() {
		nh.tickWorkerMain()
	})
	nh.logNodeHostDetails()
	return nh, nil
}

// Close stops all managed Raft nodes and releases all resources owned by the
// NodeHost instance.
func (nh *NodeHost) Close() {
	nh.events.sys.Publish(server.SystemEvent{
		Type: server.NodeHostShuttingDown,
	})
	nh.mu.Lock()
	if atomic.LoadInt32(&nh.closed) != 0 {
		panic("NodeHost.Stop called twice")
	}
	atomic.StoreInt32(&nh.closed, 1)
	nh.mu.Unlock()
	nodes := make([]raftio.NodeInfo, 0)
	nh.forEachShard(func(cid uint64, node *node) bool {
		nodes = append(nodes, raftio.NodeInfo{
			ShardID:   node.shardID,
			ReplicaID: node.replicaID,
		})
		return true
	})
	for _, node := range nodes {
		if err := nh.stopNode(node.ShardID, node.ReplicaID, true); err != nil {
			plog.Errorf("failed to remove shard %s",
				logutil.ShardID(node.ShardID))
		}
	}
	plog.Debugf("%s is stopping the nh stopper", nh.describe())
	nh.stopper.Stop()
	var err error
	plog.Debugf("%s is stopping the tranport module", nh.describe())
	if nh.transport != nil {
		err = firstError(err, nh.transport.Close())
	}
	if nh.nodes != nil {
		err = firstError(err, nh.nodes.Close())
		nh.nodes = nil
	}
	plog.Debugf("%s is stopping the engine module", nh.describe())
	if nh.engine != nil {
		err = firstError(err, nh.engine.close())
		nh.engine = nil
		nh.transport = nil
	}
	plog.Debugf("%s is stopping the logdb module", nh.describe())
	if nh.mu.logdb != nil {
		err = firstError(err, nh.mu.logdb.Close())
		nh.mu.logdb = nil
	}
	plog.Debugf("%s is stopping the env module", nh.describe())
	err = firstError(err, nh.env.Close())
	plog.Debugf("NodeHost %s stopped", nh.describe())
	if err != nil {
		panicNow(err)
	}
}

// NodeHostConfig returns the NodeHostConfig instance used for configuring this
// NodeHost instance.
func (nh *NodeHost) NodeHostConfig() config.NodeHostConfig {
	return nh.nhConfig
}

// RaftAddress returns the Raft address of the NodeHost instance, it is the
// network address by which the NodeHost can be reached by other NodeHost
// instances for exchanging Raft messages, snapshots and other metadata.
func (nh *NodeHost) RaftAddress() string {
	return nh.nhConfig.RaftAddress
}

// ID returns the string representation of the NodeHost ID value. The NodeHost
// ID is assigned to each NodeHost on its initial creation and it can be used
// to uniquely identify the NodeHost instance for its entire life cycle. When
// the system is running in the AddressByNodeHost mode, it is used as the target
// value when calling the StartReplica, RequestAddReplica, RequestAddNonVoting,
// RequestAddWitness methods.
func (nh *NodeHost) ID() string {
	return nh.id.String()
}

// GetNodeHostRegistry returns the NodeHostRegistry instance that can be used
// to query NodeHost details shared between NodeHost instances by gossip.
func (nh *NodeHost) GetNodeHostRegistry() (INodeHostRegistry, bool) {
	return nh.registry, nh.nhConfig.DefaultNodeRegistryEnabled
}

// StartReplica adds the specified Raft replica node to the NodeHost and starts
// the node to make it ready for accepting incoming requests. The node to be
// started is backed by a regular state machine that implements the
// sm.IStateMachine interface.
//
// The input parameter initialMembers is a map of replica ID to replica target for all
// Raft shard's initial member nodes. By default, the target is the
// RaftAddress value of the NodeHost where the node will be running. When running
// in the DefaultNodeRegistryEnabled mode, target should be set to the NodeHostID value
// of the NodeHost where the node will be running. See the godoc of NodeHost's ID
// method for the full definition of NodeHostID. For the same Raft shard, the
// same initialMembers map should be specified when starting its initial member
// nodes on distributed NodeHost instances.
//
// The join flag indicates whether the node is a new node joining an existing
// shard. create is a factory function for creating the IStateMachine instance,
// cfg is the configuration instance that will be passed to the underlying Raft
// node object, the shard ID and replica ID of the involved node are specified in
// the ShardID and ReplicaID fields of the provided cfg parameter.
//
// Note that this method is not for changing the membership of the specified
// Raft shard, it launches a node that is already a member of the Raft shard.
//
// As a summary, when -
//   - starting a brand new Raft shard, set join to false and specify all initial
//     member node details in the initialMembers map.
//   - joining a new node to an existing Raft shard, set join to true and leave
//     the initialMembers map empty. This requires the joining node to have already
//     been added as a member node of the Raft shard.
//   - restarting a crashed or stopped node, set join to false and leave the
//     initialMembers map to be empty. This applies to both initial member nodes
//     and those joined later.
func (nh *NodeHost) StartReplica(initialMembers map[uint64]Target,
	join bool, create sm.CreateStateMachineFunc, cfg config.Config) error {
	cf := func(shardID uint64, replicaID uint64,
		done <-chan struct{}) rsm.IManagedStateMachine {
		sm := create(shardID, replicaID)
		return rsm.NewNativeSM(cfg, rsm.NewInMemStateMachine(sm), done)
	}
	return nh.startShard(initialMembers, join, cf, cfg, pb.RegularStateMachine)
}

// StartConcurrentReplica is similar to the StartReplica method but it is used
// to start a Raft node backed by a concurrent state machine.
func (nh *NodeHost) StartConcurrentReplica(initialMembers map[uint64]Target,
	join bool, create sm.CreateConcurrentStateMachineFunc, cfg config.Config) error {
	cf := func(shardID uint64, replicaID uint64,
		done <-chan struct{}) rsm.IManagedStateMachine {
		sm := create(shardID, replicaID)
		return rsm.NewNativeSM(cfg, rsm.NewConcurrentStateMachine(sm), done)
	}
	return nh.startShard(initialMembers,
		join, cf, cfg, pb.ConcurrentStateMachine)
}

// StartOnDiskReplica is similar to the StartReplica method but it is used to
// start a Raft node backed by an IOnDiskStateMachine.
func (nh *NodeHost) StartOnDiskReplica(initialMembers map[uint64]Target,
	join bool, create sm.CreateOnDiskStateMachineFunc, cfg config.Config) error {
	cf := func(shardID uint64, replicaID uint64,
		done <-chan struct{}) rsm.IManagedStateMachine {
		sm := create(shardID, replicaID)
		return rsm.NewNativeSM(cfg, rsm.NewOnDiskStateMachine(sm), done)
	}
	return nh.startShard(initialMembers,
		join, cf, cfg, pb.OnDiskStateMachine)
}

// StopShard stops the local Raft replica associated with the specified Raft
// shard.
//
// Note that this is not the membership change operation required to remove the
// node from the Raft shard.
func (nh *NodeHost) StopShard(shardID uint64) error {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return ErrClosed
	}
	return nh.stopNode(shardID, 0, false)
}

// StopReplica stops the specified Raft replica.
//
// Note that this is not the membership change operation required to remove the
// node from the Raft shard.
func (nh *NodeHost) StopReplica(shardID uint64, replicaID uint64) error {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return ErrClosed
	}
	return nh.stopNode(shardID, replicaID, true)
}

// SyncPropose makes a synchronous proposal on the Raft shard specified by
// the input client session object. The specified context parameter must have
// the timeout value set.
//
// SyncPropose returns the result returned by IStateMachine or
// IOnDiskStateMachine's Update method, or the error encountered. The input
// byte slice can be reused for other purposes immediate after the return of
// this method.
//
// After calling SyncPropose, unless NO-OP client session is used, it is
// caller's responsibility to update the client session instance accordingly
// based on SyncPropose's outcome. Basically, when a ErrTimeout error is
// returned, application can retry the same proposal without updating the
// client session instance. When ErrInvalidSession error is returned, it
// usually means the session instance has been evicted from the server side,
// the Raft paper recommends to crash the client in this highly unlikely
// event. When the proposal completed successfully, caller must call
// client.ProposalCompleted() to get it ready to be used in future proposals.
func (nh *NodeHost) SyncPropose(ctx context.Context,
	session *client.Session, cmd []byte) (sm.Result, error) {
	timeout, err := getTimeoutFromContext(ctx)
	if err != nil {
		return sm.Result{}, err
	}
	rs, err := nh.Propose(session, cmd, timeout)
	if err != nil {
		return sm.Result{}, err
	}
	result, err := getRequestState(ctx, rs)
	if err != nil {
		return sm.Result{}, err
	}
	rs.Release()
	return result, nil
}

// SyncRead performs a synchronous linearizable read on the specified Raft
// shard. The specified context parameter must have the timeout value set. The
// query interface{} specifies what to query, it will be passed to the Lookup
// method of the IStateMachine or IOnDiskStateMachine after the system
// determines that it is safe to perform the local read. It returns the query
// result from the Lookup method or the error encountered.
func (nh *NodeHost) SyncRead(ctx context.Context, shardID uint64,
	query interface{}) (interface{}, error) {
	v, err := nh.linearizableRead(ctx, shardID,
		func(node *node) (interface{}, error) {
			data, err := node.sm.Lookup(query)
			if errors.Is(err, rsm.ErrShardClosed) {
				return nil, ErrShardClosed
			}
			return data, err
		})
	if err != nil {
		return nil, err
	}
	return v, nil
}

// GetLogReader returns a read-only LogDB reader.
func (nh *NodeHost) GetLogReader(shardID uint64) (ReadonlyLogReader, error) {
	nh.mu.RLock()
	defer nh.mu.RUnlock()
	if nh.mu.logdb == nil {
		return nil, ErrLogDBNotCreatedOrClosed
	}
	n, ok := nh.getShard(shardID)
	if !ok {
		return nil, ErrLogDBNotCreatedOrClosed
	}
	return n.logReader, nil
}

// Membership is the struct used to describe Raft shard membership.
type Membership struct {
	// ConfigChangeID is the Raft entry index of the last applied membership
	// change entry.
	ConfigChangeID uint64
	// Nodes is a map of ReplicaID values to NodeHost Raft addresses for all regular
	// Raft nodes.
	Nodes map[uint64]string
	// NonVotings is a map of ReplicaID values to NodeHost Raft addresses for all
	// nonVotings in the Raft shard.
	NonVotings map[uint64]string
	// Witnesses is a map of ReplicaID values to NodeHost Raft addresses for all
	// witnesses in the Raft shard.
	Witnesses map[uint64]string
	// Removed is a set of ReplicaID values that have been removed from the Raft
	// shard. They are not allowed to be added back to the shard.
	Removed map[uint64]struct{}
}

// SyncGetShardMembership is a synchronous method that queries the membership
// information from the specified Raft shard. The specified context parameter
// must have the timeout value set.
func (nh *NodeHost) SyncGetShardMembership(ctx context.Context,
	shardID uint64) (*Membership, error) {
	v, err := nh.linearizableRead(ctx, shardID,
		func(node *node) (interface{}, error) {
			m := node.sm.GetMembership()
			cm := func(input map[uint64]bool) map[uint64]struct{} {
				result := make(map[uint64]struct{})
				for k := range input {
					result[k] = struct{}{}
				}
				return result
			}
			return &Membership{
				Nodes:          m.Addresses,
				NonVotings:     m.NonVotings,
				Witnesses:      m.Witnesses,
				Removed:        cm(m.Removed),
				ConfigChangeID: m.ConfigChangeId,
			}, nil
		})
	if err != nil {
		return nil, err
	}
	return v.(*Membership), nil
}

// GetLeaderID returns the leader replica ID of the specified Raft shard based
// on local node's knowledge. The returned boolean value indicates whether the
// leader information is available.
func (nh *NodeHost) GetLeaderID(shardID uint64) (uint64, uint64, bool, error) {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return 0, 0, false, ErrClosed
	}
	v, ok := nh.getShard(shardID)
	if !ok {
		return 0, 0, false, ErrShardNotFound
	}
	leaderID, term, valid := v.getLeaderID()
	return leaderID, term, valid, nil
}

// GetNoOPSession returns a NO-OP client session ready to be used for making
// proposals. The NO-OP client session is a dummy client session that will not
// be checked or enforced. Use this No-OP client session when you want to ignore
// features provided by client sessions. A NO-OP client session is not
// registered on the server side and thus not required to be closed at the end
// of its life cycle.
//
// Returned NO-OP client session instance can be concurrently used in multiple
// goroutines.
//
// Use this NO-OP client session when your IStateMachine provides idempotence in
// its own implementation.
//
// NO-OP client session must be used for making proposals on IOnDiskStateMachine
// based user state machines.
func (nh *NodeHost) GetNoOPSession(shardID uint64) *client.Session {
	return client.NewNoOPSession(shardID, nh.env.GetRandomSource())
}

// SyncGetSession starts a synchronous proposal to create, register and return
// a new client session object for the specified Raft shard. The specified
// context parameter must have the timeout value set.
//
// A client session object is used to ensure that a retried proposal, e.g.
// proposal retried after timeout, will not be applied more than once into the
// state machine.
//
// Returned client session instance is not thread safe.
//
// Client session is not supported by IOnDiskStateMachine based user state
// machines. NO-OP client session must be used on IOnDiskStateMachine based
// state machines.
func (nh *NodeHost) SyncGetSession(ctx context.Context,
	shardID uint64) (*client.Session, error) {
	timeout, err := getTimeoutFromContext(ctx)
	if err != nil {
		return nil, err
	}
	cs := client.NewSession(shardID, nh.env.GetRandomSource())
	cs.PrepareForRegister()
	rs, err := nh.ProposeSession(cs, timeout)
	if err != nil {
		return nil, err
	}
	result, err := getRequestState(ctx, rs)
	if err != nil {
		return nil, err
	}
	if result.Value != cs.ClientID {
		plog.Panicf("unexpected result %d, want %d", result.Value, cs.ClientID)
	}
	cs.PrepareForPropose()
	return cs, nil
}

// SyncCloseSession closes the specified client session by unregistering it
// from the system in a synchronous manner. The specified context parameter
// must have the timeout value set.
//
// Closed client session should not be used in future proposals.
func (nh *NodeHost) SyncCloseSession(ctx context.Context,
	cs *client.Session) error {
	timeout, err := getTimeoutFromContext(ctx)
	if err != nil {
		return err
	}
	cs.PrepareForUnregister()
	rs, err := nh.ProposeSession(cs, timeout)
	if err != nil {
		return err
	}
	result, err := getRequestState(ctx, rs)
	if err != nil {
		return err
	}
	if result.Value != cs.ClientID {
		plog.Panicf("unexpected result %d, want %d", result.Value, cs.ClientID)
	}
	return nil
}

// QueryRaftLog starts an asynchronous query for raft logs in the specified
// range [firstIndex, lastIndex) on the given Raft shard. The returned
// raft log entries are limited to maxSize in bytes.
//
// This method returns a RequestState instance or an error immediately. User
// can use the CompletedC channel of the returned RequestState to get notified
// when the query result becomes available.
func (nh *NodeHost) QueryRaftLog(shardID uint64, firstIndex uint64,
	lastIndex uint64, maxSize uint64) (*RequestState, error) {
	return nh.queryRaftLog(shardID, firstIndex, lastIndex, maxSize)
}

// Propose starts an asynchronous proposal on the Raft shard specified by the
// Session object. The input byte slice can be reused for other purposes
// immediate after the return of this method.
//
// This method returns a RequestState instance or an error immediately. User can
// wait on the ResultC() channel of the returned RequestState instance to get
// notified for the outcome of the proposal.
//
// After the proposal is completed, i.e. RequestResult is received from the
// ResultC() channel of the returned RequestState, unless NO-OP client session
// is used, it is caller's responsibility to update the Session instance
// accordingly. Basically, when RequestTimeout is returned, you can retry the
// same proposal without updating your client session instance, when a
// RequestRejected value is returned, it usually means the session instance has
// been evicted from the server side as there are too many ongoing client
// sessions, the Raft paper recommends users to crash the client in such highly
// unlikely event. When the proposal completed successfully with a
// RequestCompleted value, application must call client.ProposalCompleted() to
// get the client session ready to be used in future proposals.
func (nh *NodeHost) Propose(session *client.Session, cmd []byte,
	timeout time.Duration) (*RequestState, error) {
	return nh.propose(session, cmd, timeout)
}

// ProposeSession starts an asynchronous proposal on the specified shard
// for client session related operations. Depending on the state of the specified
// client session object, the supported operations are for registering or
// unregistering a client session. Application can select on the ResultC()
// channel of the returned RequestState instance to get notified for the
// completion (RequestResult.Completed() is true) of the operation.
func (nh *NodeHost) ProposeSession(session *client.Session,
	timeout time.Duration) (*RequestState, error) {
	n, ok := nh.getShard(session.ShardID)
	if !ok {
		return nil, ErrShardNotFound
	}
	// witness node is not expected to propose anything
	if n.isWitness() {
		return nil, ErrInvalidOperation
	}
	if !n.supportClientSession() && !session.IsNoOPSession() {
		plog.Panicf("IOnDiskStateMachine based nodes must use NoOPSession")
	}
	defer nh.engine.setStepReady(session.ShardID)
	return n.proposeSession(session, nh.getTimeoutTick(timeout))
}

// ReadIndex starts the asynchronous ReadIndex protocol used for linearizable
// read on the specified shard. This method returns a RequestState instance
// or an error immediately. Application should wait on the ResultC() channel
// of the returned RequestState object to get notified on the outcome of the
// ReadIndex operation. On a successful completion, the ReadLocalNode method
// can then be invoked to query the state of the IStateMachine or
// IOnDiskStateMachine with linearizability guarantee.
func (nh *NodeHost) ReadIndex(shardID uint64,
	timeout time.Duration) (*RequestState, error) {
	rs, _, err := nh.readIndex(shardID, timeout)
	return rs, err
}

// ReadLocalNode queries the Raft node identified by the input RequestState
// instance. ReadLocalNode is only allowed to be called after receiving a
// RequestCompleted notification from the ReadIndex method.
func (nh *NodeHost) ReadLocalNode(rs *RequestState,
	query interface{}) (interface{}, error) {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return nil, ErrClosed
	}
	rs.mustBeReadyForLocalRead()
	// translate the rsm.ErrShardClosed to ErrShardClosed
	// internally, the IManagedStateMachine might obtain a RLock before performing
	// the local read. The critical section is used to make sure we don't read
	// from a destroyed C++ StateMachine object
	data, err := rs.node.sm.Lookup(query)
	if errors.Is(err, rsm.ErrShardClosed) {
		return nil, ErrShardClosed
	}
	return data, err
}

// NAReadLocalNode is a no extra heap allocation variant of ReadLocalNode, it
// uses byte slice as its input and output data to avoid extra heap allocations
// caused by using interface{}. Users are recommended to use the ReadLocalNode
// method unless performance is the top priority.
//
// As an optional feature of the state machine, NAReadLocalNode returns
// statemachine.ErrNotImplemented if the underlying state machine does not
// implement the statemachine.IExtended interface.
//
// Similar to ReadLocalNode, NAReadLocalNode is only allowed to be called after
// receiving a RequestCompleted notification from the ReadIndex method.
func (nh *NodeHost) NAReadLocalNode(rs *RequestState,
	query []byte) ([]byte, error) {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return nil, ErrClosed
	}
	rs.mustBeReadyForLocalRead()
	data, err := rs.node.sm.NALookup(query)
	if errors.Is(err, rsm.ErrShardClosed) {
		return nil, ErrShardClosed
	}
	return data, err
}

var staleReadCalled uint32

// StaleRead queries the specified Raft node directly without any
// linearizability guarantee.
func (nh *NodeHost) StaleRead(shardID uint64,
	query interface{}) (interface{}, error) {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return nil, ErrClosed
	}
	if atomic.CompareAndSwapUint32(&staleReadCalled, 0, 1) {
		plog.Warningf("StaleRead called, linearizability not guaranteed for stale read")
	}
	n, ok := nh.getShard(shardID)
	if !ok {
		return nil, ErrShardNotFound
	}
	if !n.initialized() {
		return nil, ErrShardNotInitialized
	}
	if n.isWitness() {
		return nil, ErrInvalidOperation
	}
	data, err := n.sm.Lookup(query)
	if errors.Is(err, rsm.ErrShardClosed) {
		return nil, ErrShardClosed
	}
	return data, err
}

// SyncRequestSnapshot is the synchronous variant of the RequestSnapshot
// method. See RequestSnapshot for more details.
//
// The input context object must have deadline set.
//
// SyncRequestSnapshot returns the index of the created snapshot or the error
// encountered.
func (nh *NodeHost) SyncRequestSnapshot(ctx context.Context,
	shardID uint64, opt SnapshotOption) (uint64, error) {
	timeout, err := getTimeoutFromContext(ctx)
	if err != nil {
		return 0, err
	}
	rs, err := nh.RequestSnapshot(shardID, opt, timeout)
	if err != nil {
		return 0, err
	}
	v, err := getRequestState(ctx, rs)
	if err != nil {
		return 0, err
	}
	return v.Value, nil
}

// RequestSnapshot requests a snapshot to be created asynchronously for the
// specified shard node. For each node, only one ongoing snapshot operation
// is allowed.
//
// Each requested snapshot will also trigger Raft log and snapshot compactions
// similar to automatic snapshotting. Users need to subsequently call
// RequestCompaction(), which can be far more I/O intensive, at suitable time to
// actually reclaim disk spaces used by Raft log entries and snapshot metadata
// records.
//
// RequestSnapshot returns a RequestState instance or an error immediately.
// Applications can wait on the ResultC() channel of the returned RequestState
// instance to get notified for the outcome of the create snasphot operation.
// The RequestResult instance returned by the ResultC() channel tells the
// outcome of the snapshot operation, when successful, the SnapshotIndex method
// of the returned RequestResult instance reports the index of the created
// snapshot.
//
// Requested snapshot operation will be rejected if there is already an existing
// snapshot in the system at the same Raft log index.
func (nh *NodeHost) RequestSnapshot(shardID uint64,
	opt SnapshotOption, timeout time.Duration) (*RequestState, error) {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return nil, ErrClosed
	}
	n, ok := nh.getShard(shardID)
	if !ok {
		return nil, ErrShardNotFound
	}
	if err := opt.Validate(); err != nil {
		return nil, err
	}
	defer nh.engine.setStepReady(shardID)
	return n.requestSnapshot(opt, nh.getTimeoutTick(timeout))
}

// RequestCompaction requests a compaction operation to be asynchronously
// executed in the background to reclaim disk spaces used by Raft Log entries
// that have already been marked as removed. This includes Raft Log entries
// that have already been included in created snapshots and Raft Log entries
// that belong to nodes already permanently removed via NodeHost.RemoveData().
//
// By default, compaction is automatically issued after each snapshot is
// captured. RequestCompaction can be used to manually trigger such compaction
// when auto compaction is disabled by the DisableAutoCompactions option in
// config.Config.
//
// The returned *SysOpState instance can be used to get notified when the
// requested compaction is completed. ErrRejected is returned when there is
// nothing to be reclaimed.
func (nh *NodeHost) RequestCompaction(shardID uint64,
	replicaID uint64) (*SysOpState, error) {
	nh.mu.Lock()
	defer nh.mu.Unlock()
	if atomic.LoadInt32(&nh.closed) != 0 {
		return nil, ErrClosed
	}
	n, ok := nh.getShard(shardID)
	if !ok {
		// assume this is a node that has already been removed via RemoveData
		done, err := nh.mu.logdb.CompactEntriesTo(shardID, replicaID, math.MaxUint64)
		if err != nil {
			return nil, err
		}
		return &SysOpState{completedC: done}, nil
	}
	if n.replicaID != replicaID {
		return nil, ErrShardNotFound
	}
	defer nh.engine.setStepReady(shardID)
	return n.requestCompaction()
}

// SyncRequestDeleteReplica is the synchronous variant of the RequestDeleteReplica
// method. See RequestDeleteReplica for more details.
//
// The input context object must have its deadline set.
func (nh *NodeHost) SyncRequestDeleteReplica(ctx context.Context,
	shardID uint64, replicaID uint64, configChangeIndex uint64) error {
	timeout, err := getTimeoutFromContext(ctx)
	if err != nil {
		return err
	}
	rs, err := nh.RequestDeleteReplica(shardID, replicaID, configChangeIndex, timeout)
	if err != nil {
		return err
	}
	_, err = getRequestState(ctx, rs)
	return err
}

// SyncRequestAddReplica is the synchronous variant of the RequestAddReplica method.
// See RequestAddReplica for more details.
//
// The input context object must have its deadline set.
func (nh *NodeHost) SyncRequestAddReplica(ctx context.Context,
	shardID uint64, replicaID uint64,
	target string, configChangeIndex uint64) error {
	timeout, err := getTimeoutFromContext(ctx)
	if err != nil {
		return err
	}
	rs, err := nh.RequestAddReplica(shardID,
		replicaID, target, configChangeIndex, timeout)
	if err != nil {
		return err
	}
	_, err = getRequestState(ctx, rs)
	return err
}

// SyncRequestAddNonVoting is the synchronous variant of the RequestAddNonVoting
// method. See RequestAddNonVoting for more details.
//
// The input context object must have its deadline set.
func (nh *NodeHost) SyncRequestAddNonVoting(ctx context.Context,
	shardID uint64, replicaID uint64,
	target string, configChangeIndex uint64) error {
	timeout, err := getTimeoutFromContext(ctx)
	if err != nil {
		return err
	}
	rs, err := nh.RequestAddNonVoting(shardID,
		replicaID, target, configChangeIndex, timeout)
	if err != nil {
		return err
	}
	_, err = getRequestState(ctx, rs)
	return err
}

// SyncRequestAddWitness is the synchronous variant of the RequestAddWitness
// method. See RequestAddWitness for more details.
//
// The input context object must have its deadline set.
func (nh *NodeHost) SyncRequestAddWitness(ctx context.Context,
	shardID uint64, replicaID uint64,
	target string, configChangeIndex uint64) error {
	timeout, err := getTimeoutFromContext(ctx)
	if err != nil {
		return err
	}
	rs, err := nh.RequestAddWitness(shardID,
		replicaID, target, configChangeIndex, timeout)
	if err != nil {
		return err
	}
	_, err = getRequestState(ctx, rs)
	return err
}

// RequestDeleteReplica is a Raft shard membership change method for requesting
// the specified node to be removed from the specified Raft shard. It starts
// an asynchronous request to remove the node from the Raft shard membership
// list. Application can wait on the ResultC() channel of the returned
// RequestState instance to get notified for the outcome.
//
// It is not guaranteed that deleted node will automatically close itself and
// be removed from its managing NodeHost instance. It is application's
// responsibility to call StopShard on the right NodeHost instance to actually
// have the shard node removed from its managing NodeHost instance.
//
// Once a node is successfully deleted from a Raft shard, it will not be
// allowed to be added back to the shard with the same node identity.
//
// When the Raft shard is created with the OrderedConfigChange config flag
// set as false, the configChangeIndex parameter is ignored. Otherwise, it
// should be set to the most recent Config Change Index value returned by the
// SyncGetShardMembership method. The requested delete node operation will be
// rejected if other membership change has been applied since that earlier call
// to the SyncGetShardMembership method.
func (nh *NodeHost) RequestDeleteReplica(shardID uint64,
	replicaID uint64,
	configChangeIndex uint64, timeout time.Duration) (*RequestState, error) {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return nil, ErrClosed
	}
	n, ok := nh.getShard(shardID)
	if !ok {
		return nil, ErrShardNotFound
	}
	tt := nh.getTimeoutTick(timeout)
	defer nh.engine.setStepReady(shardID)
	return n.requestDeleteNodeWithOrderID(replicaID, configChangeIndex, tt)
}

// RequestAddReplica is a Raft shard membership change method for requesting the
// specified node to be added to the specified Raft shard. It starts an
// asynchronous request to add the node to the Raft shard membership list.
// Application can wait on the ResultC() channel of the returned RequestState
// instance to get notified for the outcome.
//
// If there is already an nonVoting with the same replicaID in the shard, it will
// be promoted to a regular node with voting power. The target parameter of the
// RequestAddReplica call is ignored when promoting an nonVoting to a regular node.
//
// After the node is successfully added to the Raft shard, it is application's
// responsibility to call StartReplica on the target NodeHost instance to
// actually start the Raft shard node.
//
// Requesting a removed node back to the Raft shard will always be rejected.
//
// By default, the target parameter is the RaftAddress of the NodeHost instance
// where the new Raft node will be running. Note that fixed IP or static DNS
// name should be used in RaftAddress in such default mode. When running in the
// DefaultNodeRegistryEnabled mode, target should be set to NodeHost's ID value which
// can be obtained by calling the ID() method.
//
// When the Raft shard is created with the OrderedConfigChange config flag
// set as false, the configChangeIndex parameter is ignored. Otherwise, it
// should be set to the most recent Config Change Index value returned by the
// SyncGetShardMembership method. The requested add node operation will be
// rejected if other membership change has been applied since that earlier call
// to the SyncGetShardMembership method.
func (nh *NodeHost) RequestAddReplica(shardID uint64,
	replicaID uint64, target Target, configChangeIndex uint64,
	timeout time.Duration) (*RequestState, error) {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return nil, ErrClosed
	}
	n, ok := nh.getShard(shardID)
	if !ok {
		return nil, ErrShardNotFound
	}
	defer nh.engine.setStepReady(shardID)
	return n.requestAddNodeWithOrderID(replicaID,
		target, configChangeIndex, nh.getTimeoutTick(timeout))
}

// RequestAddNonVoting is a Raft shard membership change method for requesting
// the specified node to be added to the specified Raft shard as an non-voting
// member without voting power. It starts an asynchronous request to add the
// specified node as an non-voting member.
//
// Such nonVoting is able to receive replicated states from the leader node, but
// it is neither allowed to vote for leader, nor considered as a part of the
// quorum when replicating state. An nonVoting can be promoted to a regular node
// with voting power by making a RequestAddReplica call using its shardID and
// replicaID values. An nonVoting can be removed from the shard by calling
// RequestDeleteReplica with its shardID and replicaID values.
//
// Application should later call StartReplica with config.Config.IsNonVoting
// set to true on the right NodeHost to actually start the nonVoting instance.
//
// See the godoc of the RequestAddReplica method for the details of the target and
// configChangeIndex parameters.
func (nh *NodeHost) RequestAddNonVoting(shardID uint64,
	replicaID uint64, target Target, configChangeIndex uint64,
	timeout time.Duration) (*RequestState, error) {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return nil, ErrClosed
	}
	n, ok := nh.getShard(shardID)
	if !ok {
		return nil, ErrShardNotFound
	}
	defer nh.engine.setStepReady(shardID)
	return n.requestAddNonVotingWithOrderID(replicaID,
		target, configChangeIndex, nh.getTimeoutTick(timeout))
}

// RequestAddWitness is a Raft shard membership change method for requesting
// the specified node to be added as a witness to the given Raft shard. It
// starts an asynchronous request to add the specified node as an witness.
//
// A witness can vote in elections but it doesn't have any Raft log or
// application state machine associated. The witness node can not be used
// to initiate read, write or membership change operations on its Raft shard.
// Section 11.7.2 of Diego Ongaro's thesis contains more info on such witness
// role.
//
// Application should later call StartReplica with config.Config.IsWitness
// set to true on the right NodeHost to actually start the witness node.
//
// See the godoc of the RequestAddReplica method for the details of the target and
// configChangeIndex parameters.
func (nh *NodeHost) RequestAddWitness(shardID uint64,
	replicaID uint64, target Target, configChangeIndex uint64,
	timeout time.Duration) (*RequestState, error) {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return nil, ErrClosed
	}
	n, ok := nh.getShard(shardID)
	if !ok {
		return nil, ErrShardNotFound
	}
	defer nh.engine.setStepReady(shardID)
	return n.requestAddWitnessWithOrderID(replicaID,
		target, configChangeIndex, nh.getTimeoutTick(timeout))
}

// RequestLeaderTransfer makes a request to transfer the leadership of the
// specified Raft shard to the target node identified by targetReplicaID. It
// returns an error if the request fails to be started. There is no guarantee
// that such request can be fulfilled.
func (nh *NodeHost) RequestLeaderTransfer(shardID uint64,
	targetReplicaID uint64) error {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return ErrClosed
	}
	n, ok := nh.getShard(shardID)
	if !ok {
		return ErrShardNotFound
	}
	plog.Debugf("RequestLeaderTransfer called on shard %d target replicaID %d",
		shardID, targetReplicaID)
	defer nh.engine.setStepReady(shardID)
	return n.requestLeaderTransfer(targetReplicaID)
}

// SyncRemoveData is the synchronous variant of the RemoveData. It waits for
// the specified node to be fully offloaded or until the context object instance
// is cancelled or timeout.
//
// Similar to RemoveData, calling SyncRemoveData on a node that is still a Raft
// shard member will corrupt the Raft shard.
func (nh *NodeHost) SyncRemoveData(ctx context.Context,
	shardID uint64, replicaID uint64) error {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return ErrClosed
	}
	if _, ok := ctx.Deadline(); !ok {
		return ErrDeadlineNotSet
	}
	if _, ok := nh.getShard(shardID); ok {
		return ErrShardNotStopped
	}
	if ch := nh.engine.destroyedC(shardID, replicaID); ch != nil {
		select {
		case <-ch:
		case <-ctx.Done():
			if ctx.Err() == context.Canceled {
				return ErrCanceled
			} else if ctx.Err() == context.DeadlineExceeded {
				return ErrTimeout
			}
		}
	}
	err := nh.RemoveData(shardID, replicaID)
	if errors.Is(err, ErrShardNotStopped) {
		panic("node not stopped")
	}
	return err
}

// RemoveData tries to remove all data associated with the specified node. This
// method should only be used after the node has been deleted from its Raft
// shard. Calling RemoveData on a node that is still a Raft shard member
// will corrupt the Raft shard.
//
// RemoveData returns ErrShardNotStopped when the specified node has not been
// fully offloaded from the NodeHost instance.
func (nh *NodeHost) RemoveData(shardID uint64, replicaID uint64) error {
	n, ok := nh.getShard(shardID)
	if ok && n.replicaID == replicaID {
		return ErrShardNotStopped
	}
	nh.mu.Lock()
	defer nh.mu.Unlock()
	if atomic.LoadInt32(&nh.closed) != 0 {
		return ErrClosed
	}
	if nh.engine.nodeLoaded(shardID, replicaID) {
		return ErrShardNotStopped
	}
	plog.Debugf("%s called RemoveData", dn(shardID, replicaID))
	if err := nh.mu.logdb.RemoveNodeData(shardID, replicaID); err != nil {
		panicNow(err)
	}
	// mark the snapshot dir as removed
	did := nh.nhConfig.GetDeploymentID()
	if err := nh.env.RemoveSnapshotDir(did, shardID, replicaID); err != nil {
		panicNow(err)
	}
	return nil
}

// GetNodeUser returns an INodeUser instance ready to be used to directly make
// proposals or read index operations without locating the node repeatedly in
// the NodeHost. A possible use case is when loading a large data set say with
// billions of proposals into the dragonboat based system.
func (nh *NodeHost) GetNodeUser(shardID uint64) (INodeUser, error) {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return nil, ErrClosed
	}
	n, ok := nh.getShard(shardID)
	if !ok {
		return nil, ErrShardNotFound
	}
	return &nodeUser{
		nh:           nh,
		node:         n,
		setStepReady: nh.engine.setStepReady,
	}, nil
}

// HasNodeInfo returns a boolean value indicating whether the specified node
// has been bootstrapped on the current NodeHost instance.
func (nh *NodeHost) HasNodeInfo(shardID uint64, replicaID uint64) bool {
	nh.mu.Lock()
	defer nh.mu.Unlock()
	if atomic.LoadInt32(&nh.closed) != 0 {
		return false
	}
	if _, err := nh.mu.logdb.GetBootstrapInfo(shardID, replicaID); err != nil {
		if errors.Is(err, raftio.ErrNoBootstrapInfo) {
			return false
		}
		panicNow(err)
	}
	return true
}

// GetNodeHostInfo returns a NodeHostInfo instance that contains all details
// of the NodeHost, this includes details of all Raft shards managed by the
// the NodeHost instance.
func (nh *NodeHost) GetNodeHostInfo(opt NodeHostInfoOption) *NodeHostInfo {
	nhi := &NodeHostInfo{
		NodeHostID:    nh.ID(),
		RaftAddress:   nh.RaftAddress(),
		Gossip:        nh.getGossipInfo(),
		ShardInfoList: nh.getShardInfo(),
	}
	nh.mu.Lock()
	defer nh.mu.Unlock()
	if atomic.LoadInt32(&nh.closed) != 0 {
		return nil
	}
	if !opt.SkipLogInfo {
		logInfo, err := nh.mu.logdb.ListNodeInfo()
		if err != nil {
			panicNow(err)
		}
		nhi.LogInfo = logInfo
	}
	return nhi
}

func (nh *NodeHost) getGossipInfo() GossipInfo {
	if r, ok := nh.nodes.(*registry.GossipRegistry); ok {
		return GossipInfo{
			Enabled:             true,
			AdvertiseAddress:    r.AdvertiseAddress(),
			NumOfKnownNodeHosts: r.NumMembers(),
		}
	}
	return GossipInfo{}
}

func (nh *NodeHost) propose(s *client.Session,
	cmd []byte, timeout time.Duration) (*RequestState, error) {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return nil, ErrClosed
	}
	v, ok := nh.getShard(s.ShardID)
	if !ok {
		return nil, ErrShardNotFound
	}
	if !v.supportClientSession() && !s.IsNoOPSession() {
		panic("IOnDiskStateMachine based nodes must use NoOPSession")
	}
	req, err := v.propose(s, cmd, nh.getTimeoutTick(timeout))
	nh.engine.setStepReady(s.ShardID)
	return req, err
}

func (nh *NodeHost) readIndex(shardID uint64,
	timeout time.Duration) (*RequestState, *node, error) {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return nil, nil, ErrClosed
	}
	n, ok := nh.getShard(shardID)
	if !ok {
		return nil, nil, ErrShardNotFound
	}
	req, err := n.read(nh.getTimeoutTick(timeout))
	if err != nil {
		return nil, nil, err
	}
	nh.engine.setStepReady(shardID)
	return req, n, err
}

func (nh *NodeHost) queryRaftLog(shardID uint64,
	firstIndex uint64, lastIndex uint64, maxSize uint64) (*RequestState, error) {
	if atomic.LoadInt32(&nh.closed) != 0 {
		return nil, ErrClosed
	}
	v, ok := nh.getShard(shardID)
	if !ok {
		return nil, ErrShardNotFound
	}
	if lastIndex <= firstIndex {
		return nil, ErrInvalidRange
	}
	req, err := v.queryRaftLog(firstIndex, lastIndex, maxSize)
	nh.engine.setStepReady(shardID)
	return req, err
}

func (nh *NodeHost) linearizableRead(ctx context.Context,
	shardID uint64, f func(n *node) (interface{}, error)) (interface{}, error) {
	timeout, err := getTimeoutFromContext(ctx)
	if err != nil {
		return nil, err
	}
	rs, node, err := nh.readIndex(shardID, timeout)
	if err != nil {
		return nil, err
	}
	if _, err := getRequestState(ctx, rs); err != nil {
		return nil, err
	}
	rs.Release()
	return f(node)
}

func (nh *NodeHost) getShard(shardID uint64) (*node, bool) {
	n, ok := nh.mu.shards.Load(shardID)
	if !ok {
		return nil, false
	}
	return n.(*node), true
}

func (nh *NodeHost) forEachShard(f func(uint64, *node) bool) uint64 {
	nh.mu.RLock()
	defer nh.mu.RUnlock()
	nh.mu.shards.Range(func(k, v interface{}) bool {
		return f(k.(uint64), v.(*node))
	})
	return nh.mu.cci
}

func (nh *NodeHost) getShardSetIndex() uint64 {
	nh.mu.RLock()
	defer nh.mu.RUnlock()
	return nh.mu.cci
}

// there are three major reasons to bootstrap the shard -
//
//  1. when possible, we check whether user incorrectly specified parameters
//     for the startShard method, e.g. call startShard with join=true first,
//     then restart the NodeHost instance and call startShard again with
//     join=false and len(nodes) > 0
//  2. when restarting a node which is a part of the initial shard members,
//     for user convenience, we allow the caller not to provide the details of
//     initial members. when the initial shard member info is required, however
//     we still need to get the initial member info from somewhere. bootstrap is
//     the procedure that records such info.
//  3. the bootstrap record is used as a marker record in our default LogDB
//     implementation to indicate that a certain node exists here
func (nh *NodeHost) bootstrapShard(initialMembers map[uint64]Target,
	join bool, cfg config.Config,
	smType pb.StateMachineType) (map[uint64]string, bool, error) {
	bi, err := nh.mu.logdb.GetBootstrapInfo(cfg.ShardID, cfg.ReplicaID)
	if errors.Is(err, raftio.ErrNoBootstrapInfo) {
		if !join && len(initialMembers) == 0 {
			return nil, false, ErrShardNotBootstrapped
		}
		var members map[uint64]string
		if !join {
			members = initialMembers
		}
		bi = pb.NewBootstrapInfo(join, smType, initialMembers)
		err := nh.mu.logdb.SaveBootstrapInfo(cfg.ShardID, cfg.ReplicaID, bi)
		if err != nil {
			return nil, false, err
		}
		return members, !join, nil
	} else if err != nil {
		return nil, false, err
	}
	if !bi.Validate(initialMembers, join, smType) {
		plog.Errorf("bootstrap info validation failed, %s, %v, %t, %v, %t",
			dn(cfg.ShardID, cfg.ReplicaID),
			bi.Addresses, bi.Join, initialMembers, join)
		return nil, false, ErrInvalidShardSettings
	}
	return bi.Addresses, !bi.Join, nil
}

func (nh *NodeHost) startShard(initialMembers map[uint64]Target,
	join bool, createStateMachine rsm.ManagedStateMachineFactory,
	cfg config.Config, smType pb.StateMachineType) error {
	shardID := cfg.ShardID
	replicaID := cfg.ReplicaID
	validator := nh.nhConfig.GetTargetValidator()
	for _, target := range initialMembers {
		if !validator(target) {
			return ErrInvalidTarget
		}
	}

	doStart := func() (*node, error) {
		nh.mu.Lock()
		defer nh.mu.Unlock()

		if atomic.LoadInt32(&nh.closed) != 0 {
			return nil, ErrClosed
		}
		if _, ok := nh.mu.shards.Load(shardID); ok {
			return nil, ErrShardAlreadyExist
		}
		if nh.engine.nodeLoaded(shardID, replicaID) {
			// node is still loaded in the execution engine, e.g. processing snapshot
			return nil, ErrShardAlreadyExist
		}
		if join && len(initialMembers) > 0 {
			return nil, ErrInvalidShardSettings
		}
		peers, im, err := nh.bootstrapShard(initialMembers, join, cfg, smType)
		if errors.Is(err, ErrInvalidShardSettings) {
			return nil, err
		}
		if err != nil {
			panicNow(err)
		}
		for k, v := range peers {
			if k != replicaID {
				nh.nodes.Add(shardID, k, v)
			}
		}
		did := nh.nhConfig.GetDeploymentID()
		if err := nh.env.CreateSnapshotDir(did, shardID, replicaID); err != nil {
			if errors.Is(err, server.ErrDirMarkedAsDeleted) {
				return nil, ErrReplicaRemoved
			}
			panicNow(err)
		}
		getSnapshotDir := func(cid uint64, nid uint64) string {
			return nh.env.GetSnapshotDir(did, cid, nid)
		}
		logReader := logdb.NewLogReader(shardID, replicaID, nh.mu.logdb)
		ss := newSnapshotter(shardID, replicaID,
			getSnapshotDir, nh.mu.logdb, logReader, nh.fs)
		logReader.SetCompactor(ss)
		if err := ss.processOrphans(); err != nil {
			panicNow(err)
		}
		p := server.NewDoubleFixedPartitioner(nh.nhConfig.Expert.Engine.ExecShards,
			nh.nhConfig.Expert.LogDB.Shards)
		shard := p.GetPartitionID(shardID)
		rn, err := newNode(peers,
			im,
			cfg,
			nh.nhConfig,
			createStateMachine,
			ss,
			logReader,
			nh.engine,
			nh.events.leaderInfoQ,
			nh.transport.GetStreamSink,
			nh.msgHandler.HandleSnapshotStatus,
			nh.sendMessage,
			nh.nodes,
			nh.requestPools[replicaID%requestPoolShards],
			nh.mu.logdb,
			nh.getLogDBMetrics(shard),
			nh.events.sys)
		if err != nil {
			panicNow(err)
		}
		rn.loaded()
		nh.mu.shards.Store(shardID, rn)
		nh.mu.cci++
		nh.cciUpdated()
		nh.engine.setCCIReady(shardID)
		nh.engine.setApplyReady(shardID)

		return rn, nil
	}

	rn, err := doStart()
	if err != nil {
		return err
	}

	if cfg.WaitReady {
		select {
		case <-rn.initializedC:
		case <-rn.stopC:
		}
	}

	return nil
}

func (nh *NodeHost) cciUpdated() {
	select {
	case nh.mu.cciCh <- struct{}{}:
	default:
	}
}

func (nh *NodeHost) loadNodeHostID() error {
	v, err := nh.env.PrepareNodeHostID(nh.nhConfig.NodeHostID)
	if err != nil {
		return err
	}
	nh.id = v
	return nil
}

func (nh *NodeHost) createPools() {
	nh.requestPools = make([]*sync.Pool, requestPoolShards)
	for i := uint64(0); i < requestPoolShards; i++ {
		p := &sync.Pool{}
		p.New = func() interface{} {
			obj := &RequestState{}
			obj.CompletedC = make(chan RequestResult, 1)
			obj.pool = p
			if nh.nhConfig.NotifyCommit {
				obj.committedC = make(chan RequestResult, 1)
			}
			return obj
		}
		nh.requestPools[i] = p
	}
}

func (nh *NodeHost) createLogDB() error {
	did := nh.nhConfig.GetDeploymentID()
	nhDir, walDir, err := nh.env.CreateNodeHostDir(did)
	if err != nil {
		return err
	}
	if err := nh.env.LockNodeHostDir(); err != nil {
		return err
	}
	var lf config.LogDBFactory
	if nh.nhConfig.Expert.LogDBFactory != nil {
		lf = nh.nhConfig.Expert.LogDBFactory
	} else {
		lf = logdb.NewDefaultFactory()
	}
	name := lf.Name()
	if err := nh.env.CheckLogDBType(nh.nhConfig, name); err != nil {
		return err
	}
	ldb, err := lf.Create(nh.nhConfig,
		nh.handleLogDBInfo, []string{nhDir}, []string{walDir})
	if err != nil {
		return err
	}
	nh.mu.logdb = ldb
	ver := ldb.BinaryFormat()
	if err := nh.env.CheckNodeHostDir(nh.nhConfig, ver, name); err != nil {
		return err
	}
	if shardedrdb, ok := ldb.(*logdb.ShardedDB); ok {
		failed, err := shardedrdb.SelfCheckFailed()
		if err != nil {
			return err
		}
		if failed {
			return server.ErrLogDBBrokenChange
		}
	}
	plog.Infof("logdb memory limit: %d MBytes",
		nh.nhConfig.Expert.LogDB.MemorySizeMB())
	return nil
}

func (nh *NodeHost) handleLogDBInfo(info config.LogDBInfo) {
	plog.Infof("LogDB info received, shard %d, busy %t", info.Shard, info.Busy)
	nh.mu.Lock()
	defer nh.mu.Unlock()
	lm := nh.getLogDBMetrics(info.Shard)
	lm.update(info.Busy)
}

func (nh *NodeHost) getLogDBMetrics(shard uint64) *logDBMetrics {
	if v, ok := nh.mu.lm.Load(shard); ok {
		return v.(*logDBMetrics)
	}
	lm := &logDBMetrics{}
	nh.mu.lm.Store(shard, lm)
	return lm
}

type transportEvent struct {
	nh *NodeHost
}

func (te *transportEvent) ConnectionEstablished(addr string, snapshot bool) {
	te.nh.events.sys.Publish(server.SystemEvent{
		Type:               server.ConnectionEstablished,
		Address:            addr,
		SnapshotConnection: snapshot,
	})
}

func (te *transportEvent) ConnectionFailed(addr string, snapshot bool) {
	te.nh.events.sys.Publish(server.SystemEvent{
		Type:               server.ConnectionFailed,
		Address:            addr,
		SnapshotConnection: snapshot,
	})
}

func (nh *NodeHost) createNodeRegistry() error {
	validator := nh.nhConfig.GetTargetValidator()
	// TODO:
	// more tests here required
	if nh.nhConfig.DefaultNodeRegistryEnabled {
		// DefaultNodeRegistryEnabled should not be set if a Expert.NodeRegistryFactory
		// is also set.
		if nh.nhConfig.Expert.NodeRegistryFactory != nil {
			return errors.New("DefaultNodeRegistryEnabled and Expert.NodeRegistryFactory should not both be set")
		}
		plog.Infof("DefaultNodeRegistryEnabled: true, use gossip based node registry")
		r, err := registry.NewGossipRegistry(nh.ID(), nh.getShardInfo,
			nh.nhConfig, streamConnections, validator)
		if err != nil {
			return err
		}
		nh.registry = r.GetNodeHostRegistry()
		nh.nodes = r
	} else if nh.nhConfig.Expert.NodeRegistryFactory != nil {
		plog.Infof("Expert.NodeRegistryFactory was set: using custom registry")
		r, err := nh.nhConfig.Expert.NodeRegistryFactory.Create(nh.ID(), streamConnections, validator)
		if err != nil {
			return err
		}
		nh.nodes = r
	} else {
		plog.Infof("using regular node registry")
		nh.nodes = registry.NewNodeRegistry(streamConnections, validator)
	}
	return nil
}

func (nh *NodeHost) createTransport() error {
	getSnapshotDir := func(cid uint64, nid uint64) string {
		return nh.env.GetSnapshotDir(nh.nhConfig.GetDeploymentID(), cid, nid)
	}
	tsp, err := transport.NewTransport(nh.nhConfig,
		nh.msgHandler, nh.env, nh.nodes, getSnapshotDir,
		&transportEvent{nh: nh}, nh.fs)
	if err != nil {
		return err
	}
	nh.transport = tsp
	return nil
}

func (nh *NodeHost) stopNode(shardID uint64, replicaID uint64, check bool) error {
	nh.mu.Lock()
	defer nh.mu.Unlock()
	v, ok := nh.mu.shards.Load(shardID)
	if !ok {
		return ErrShardNotFound
	}
	n := v.(*node)
	if check && n.replicaID != replicaID {
		return ErrShardNotFound
	}
	nh.mu.shards.Delete(shardID)
	nh.mu.cci++
	nh.cciUpdated()
	nh.engine.setCCIReady(shardID)
	n.close()
	n.offloaded()
	nh.engine.setStepReady(shardID)
	nh.engine.setCommitReady(shardID)
	nh.engine.setApplyReady(shardID)
	nh.engine.setRecoverReady(shardID)
	return nil
}

func (nh *NodeHost) getShardInfo() []ShardInfo {
	shardInfoList := make([]ShardInfo, 0)
	nh.forEachShard(func(cid uint64, node *node) bool {
		shardInfoList = append(shardInfoList, node.getShardInfo())
		return true
	})
	return shardInfoList
}

func (nh *NodeHost) tickWorkerMain() {
	tick := uint64(0)
	idx := uint64(0)
	nodes := make([]*node, 0)
	tf := func() {
		tick++
		if idx != nh.getShardSetIndex() {
			nodes = nodes[:0]
			idx = nh.forEachShard(func(cid uint64, n *node) bool {
				nodes = append(nodes, n)
				return true
			})
		}
		nh.sendTickMessage(nodes, tick)
		nh.engine.setAllStepReady(nodes)
	}
	td := time.Duration(nh.nhConfig.RTTMillisecond) * time.Millisecond
	ticker := time.NewTicker(td)
	defer ticker.Stop()
	for {
		select {
		case <-ticker.C:
			tf()
		case <-nh.stopper.ShouldStop():
			return
		}
	}
}

func (nh *NodeHost) handleListenerEvents() {
	var ch chan struct{}
	if nh.events.leaderInfoQ != nil {
		ch = nh.events.leaderInfoQ.workReady()
	}
	for {
		select {
		case <-nh.stopper.ShouldStop():
			return
		case <-ch:
			for {
				v, ok := nh.events.leaderInfoQ.getLeaderInfo()
				if !ok {
					break
				}
				nh.events.raft.LeaderUpdated(v)
			}
		case e := <-nh.events.sys.events:
			nh.events.sys.handle(e)
		}
	}
}

func (nh *NodeHost) sendMessage(msg pb.Message) {
	if nh.isPartitioned() {
		return
	}
	if msg.Type != pb.InstallSnapshot {
		nh.transport.Send(msg)
	} else {
		witness := msg.Snapshot.Witness
		plog.Debugf("%s is sending snapshot to %s, witness %t, index %d, size %d",
			dn(msg.ShardID, msg.From), dn(msg.ShardID, msg.To),
			witness, msg.Snapshot.Index, msg.Snapshot.FileSize)
		if n, ok := nh.getShard(msg.ShardID); ok {
			if witness || !n.OnDiskStateMachine() {
				nh.transport.SendSnapshot(msg)
			} else {
				n.pushStreamSnapshotRequest(msg.ShardID, msg.To)
			}
		}
		nh.events.sys.Publish(server.SystemEvent{
			Type:      server.SendSnapshotStarted,
			ShardID:   msg.ShardID,
			ReplicaID: msg.To,
			From:      msg.From,
		})
	}
}

func (nh *NodeHost) sendTickMessage(shards []*node, tick uint64) {
	for _, n := range shards {
		m := pb.Message{
			Type: pb.LocalTick,
			To:   n.replicaID,
			From: n.replicaID,
			Hint: tick,
		}
		n.mq.Tick()
		n.mq.Add(m)
	}
}

func (nh *NodeHost) nodeMonitorMain() {
	for {
		nodes := make([]*node, 0)
		nh.forEachShard(func(cid uint64, node *node) bool {
			nodes = append(nodes, node)
			return true
		})
		cases := make([]reflect.SelectCase, len(nodes)+2)
		for i, n := range nodes {
			cases[i] = reflect.SelectCase{
				Dir:  reflect.SelectRecv,
				Chan: reflect.ValueOf(n.ShouldStop()),
			}
		}
		cases[len(nodes)] = reflect.SelectCase{
			Dir:  reflect.SelectRecv,
			Chan: reflect.ValueOf(nh.mu.cciCh),
		}
		cases[len(nodes)+1] = reflect.SelectCase{
			Dir:  reflect.SelectRecv,
			Chan: reflect.ValueOf(nh.stopper.ShouldStop()),
		}
		index, _, ok := reflect.Select(cases)
		if !ok && index < len(nodes) {
			// node closed
			n := nodes[index]
			if err := nh.stopNode(n.shardID, n.replicaID, true); err != nil {
				plog.Debugf("stopNode failed %v", err)
			}
		} else if index == len(nodes) {
			// cci change
			continue
		} else if index == len(nodes)+1 {
			// stopped
			return
		} else {
			plog.Panicf("unknown node list change state, %d, %t", index, ok)
		}
	}
}

func (nh *NodeHost) getTimeoutTick(timeout time.Duration) uint64 {
	return uint64(timeout.Milliseconds()) / nh.nhConfig.RTTMillisecond
}

func (nh *NodeHost) describe() string {
	return nh.RaftAddress()
}

func (nh *NodeHost) logNodeHostDetails() {
	plog.Infof("transport type: %s", nh.transport.Name())
	plog.Infof("logdb type: %s", nh.mu.logdb.Name())
	plog.Infof("nodehost address: %s", nh.nhConfig.RaftAddress)
}

func getRequestState(ctx context.Context, rs *RequestState) (sm.Result, error) {
	select {
	case r := <-rs.AppliedC():
		if r.Completed() {
			return r.GetResult(), nil
		} else if r.Rejected() {
			return sm.Result{}, ErrRejected
		} else if r.Timeout() {
			return sm.Result{}, ErrTimeout
		} else if r.Terminated() {
			return sm.Result{}, ErrShardClosed
		} else if r.Dropped() {
			return sm.Result{}, ErrShardNotReady
		} else if r.Aborted() {
			return sm.Result{}, ErrAborted
		}
		plog.Panicf("unknown v code %v", r)
	case <-ctx.Done():
		if ctx.Err() == context.Canceled {
			return sm.Result{}, ErrCanceled
		} else if ctx.Err() == context.DeadlineExceeded {
			return sm.Result{}, ErrTimeout
		}
	}
	panic("should never reach here")
}

// INodeUser is the interface implemented by a Raft node user type. A Raft node
// user can be used to directly initiate proposals or read index operations
// without locating the Raft node in NodeHost's node list first. It is useful
// when doing bulk load operations on selected shards.
type INodeUser interface {
	// ShardID is the shard ID of the node.
	ShardID() uint64
	// ReplicaID is the replica ID of the node.
	ReplicaID() uint64
	// Propose starts an asynchronous proposal on the Raft shard represented by
	// the INodeUser instance. Its semantics is the same as the Propose() method
	// in NodeHost.
	Propose(s *client.Session,
		cmd []byte, timeout time.Duration) (*RequestState, error)
	// ReadIndex starts the asynchronous ReadIndex protocol used for linearizable
	// reads on the Raft shard represented by the INodeUser instance. Its
	// semantics is the same as the ReadIndex() method in NodeHost.
	ReadIndex(timeout time.Duration) (*RequestState, error)
}

type nodeUser struct {
	nh           *NodeHost
	node         *node
	setStepReady func(shardID uint64)
}

var _ INodeUser = (*nodeUser)(nil)

func (nu *nodeUser) ShardID() uint64 {
	return nu.node.shardID
}

func (nu *nodeUser) ReplicaID() uint64 {
	return nu.node.replicaID
}

func (nu *nodeUser) Propose(s *client.Session,
	cmd []byte, timeout time.Duration) (*RequestState, error) {
	req, err := nu.node.propose(s, cmd, nu.nh.getTimeoutTick(timeout))
	nu.setStepReady(s.ShardID)
	return req, err
}

func (nu *nodeUser) ReadIndex(timeout time.Duration) (*RequestState, error) {
	return nu.node.read(nu.nh.getTimeoutTick(timeout))
}

func getTimeoutFromContext(ctx context.Context) (time.Duration, error) {
	d, ok := ctx.Deadline()
	if !ok {
		return 0, ErrDeadlineNotSet
	}
	now := time.Now()
	if now.After(d) {
		return 0, ErrInvalidDeadline
	}
	return d.Sub(now), nil
}

var (
	streamPushDelayTick      uint64 = 10
	streamConfirmedDelayTick uint64 = 2
)

type messageHandler struct {
	nh *NodeHost
}

var _ transport.IMessageHandler = (*messageHandler)(nil)

func newNodeHostMessageHandler(nh *NodeHost) *messageHandler {
	return &messageHandler{nh: nh}
}

func (h *messageHandler) HandleMessageBatch(msg pb.MessageBatch) (uint64, uint64) {
	nh := h.nh
	snapshotCount := uint64(0)
	msgCount := uint64(0)
	if nh.isPartitioned() {
		keep := false
		// InstallSnapshot is a in-memory local message type that will never be
		// dropped in production as it will never be sent via networks
		for _, req := range msg.Requests {
			if req.Type == pb.InstallSnapshot {
				keep = true
			}
		}
		if !keep {
			return 0, 0
		}
	}
	for _, req := range msg.Requests {
		if req.To == 0 {
			plog.Panicf("to field not set, %s", req.Type)
		}
		if n, ok := nh.getShard(req.ShardID); ok {
			if n.replicaID != req.To {
				plog.Warningf("ignored a %s message sent to %s but received by %s",
					req.Type, dn(req.ShardID, req.To), dn(req.ShardID, n.replicaID))
				continue
			}
			switch req.Type {
			case pb.InstallSnapshot:
				n.mq.MustAdd(req)
				snapshotCount++
			case pb.SnapshotReceived:
				plog.Debugf("SnapshotReceived received, shard id %d, replica id %d",
					req.ShardID, req.From)
				n.mq.AddDelayed(pb.Message{
					Type: pb.SnapshotStatus,
					From: req.From,
				}, streamConfirmedDelayTick)
				msgCount++
			default:
				if added, stopped := n.mq.Add(req); !added || stopped {
					plog.Warningf("dropped an incoming message")
				} else {
					msgCount++
				}
			}
		}
	}
	nh.engine.setStepReadyByMessageBatch(msg)
	return snapshotCount, msgCount
}

func (h *messageHandler) HandleSnapshotStatus(shardID uint64,
	replicaID uint64, failed bool) {
	eventType := server.SendSnapshotCompleted
	if failed {
		eventType = server.SendSnapshotAborted
	}
	h.nh.events.sys.Publish(server.SystemEvent{
		Type:      eventType,
		ShardID:   shardID,
		ReplicaID: replicaID,
	})
	if n, ok := h.nh.getShard(shardID); ok {
		n.mq.AddDelayed(pb.Message{
			Type:   pb.SnapshotStatus,
			From:   replicaID,
			Reject: failed,
		}, streamPushDelayTick)
		h.nh.engine.setStepReady(shardID)
	}
}

func (h *messageHandler) HandleUnreachable(shardID uint64, replicaID uint64) {
	if n, ok := h.nh.getShard(shardID); ok {
		m := pb.Message{
			Type: pb.Unreachable,
			From: replicaID,
			To:   n.replicaID,
		}
		n.mq.MustAdd(m)
		h.nh.engine.setStepReady(shardID)
	}
}

func (h *messageHandler) HandleSnapshot(shardID uint64,
	replicaID uint64, from uint64) {
	m := pb.Message{
		To:      from,
		From:    replicaID,
		ShardID: shardID,
		Type:    pb.SnapshotReceived,
	}
	h.nh.sendMessage(m)
	plog.Debugf("%s sent SnapshotReceived to %d", dn(shardID, replicaID), from)
	h.nh.events.sys.Publish(server.SystemEvent{
		Type:      server.SnapshotReceived,
		ShardID:   shardID,
		ReplicaID: replicaID,
		From:      from,
	})
}

func logBuildTagsAndVersion() {
	devstr := "Rel"
	if DEVVersion {
		devstr = "Dev"
	}
	plog.Infof("go version: %s, %s/%s",
		runtime.Version(), runtime.GOOS, runtime.GOARCH)
	plog.Infof("dragonboat version: %d.%d.%d (%s)",
		DragonboatMajor, DragonboatMinor, DragonboatPatch, devstr)
	if !invariants.IsSupportedOS() || !invariants.IsSupportedArch() {
		plog.Warningf("unsupported OS/ARCH %s/%s, don't use for production",
			runtime.GOOS, runtime.GOARCH)
	}
}

func panicNow(err error) {
	if err == nil {
		panic("panicNow called with nil error")
	}
	plog.Panicf("%+v", err)
	panic(err)
}
````

## File: NOTICE
````
Dragonboat Project (https://github.com/lni/dragonboat)
Copyright 2017-2019 Lei Ni (nilei81@gmail.com)

This product includes software developed by Lei Ni (nilei81@gmail.com).
````

## File: queue_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"testing"

	"github.com/stretchr/testify/assert"

	"github.com/lni/dragonboat/v4/raftio"
	"github.com/lni/dragonboat/v4/raftpb"
)

func TestEntryQueueCanBeCreated(t *testing.T) {
	q := newEntryQueue(5, 0)
	assert.Equal(t, uint64(5), q.size, "size unexpected")
	assert.Len(t, q.left, 5, "left queue size unexpected")
	assert.Len(t, q.right, 5, "right queue size unexpected")
	assert.Equal(t, uint64(0), q.idx, "idx %d, want 0", q.idx)
}

func TestLazyFreeCanBeDisabled(t *testing.T) {
	q := newEntryQueue(5, 0)
	q.add(raftpb.Entry{Cmd: make([]byte, 16)})
	q.add(raftpb.Entry{Cmd: make([]byte, 16)})
	q.add(raftpb.Entry{Cmd: make([]byte, 16)})
	q.get(false)
	q.get(false)
	tq := q.targetQueue()
	for i := 0; i < 3; i++ {
		assert.NotNil(t, tq[i].Cmd, "data unexpectedly freed")
	}
}

func TestLazyFreeCanBeUsed(t *testing.T) {
	q := newEntryQueue(5, 1)
	q.add(raftpb.Entry{Cmd: make([]byte, 16)})
	q.add(raftpb.Entry{Cmd: make([]byte, 16)})
	q.add(raftpb.Entry{Cmd: make([]byte, 16)})
	q.get(false)
	q.get(false)
	tq := q.targetQueue()
	for i := 0; i < 3; i++ {
		assert.Nil(t, tq[i].Cmd, "data unexpectedly not freed")
	}
}

func TestLazyFreeCycleCanBeSet(t *testing.T) {
	q := newEntryQueue(5, 6)
	q.add(raftpb.Entry{Cmd: make([]byte, 16)})
	q.add(raftpb.Entry{Cmd: make([]byte, 16)})
	q.add(raftpb.Entry{Cmd: make([]byte, 16)})
	q.get(false)
	q.get(false)
	tq := q.targetQueue()
	for i := 0; i < 3; i++ {
		assert.NotNil(t, tq[i].Cmd, "data unexpectedly freed")
	}
	q.get(false)
	q.get(false)
	tq = q.targetQueue()
	for i := 0; i < 3; i++ {
		assert.NotNil(t, tq[i].Cmd, "data unexpectedly freed")
	}
	q.get(false)
	q.get(false)
	tq = q.targetQueue()
	for i := 0; i < 3; i++ {
		assert.Nil(t, tq[i].Cmd, "data not freed at the expected cycle")
	}
}

func TestEntryQueueCanBePaused(t *testing.T) {
	q := newEntryQueue(5, 0)
	assert.False(t, q.paused, "entry queue is paused by default")
	for i := 0; i < 5; i++ {
		ok, stopped := q.add(raftpb.Entry{})
		assert.True(t, ok, "failed to add new entry")
		assert.False(t, stopped, "failed to add new entry")
		assert.False(t, q.stopped, "stopped too early")
	}
	v := q.get(true)
	assert.Len(t, v, 5, "failed to get all entries")
	assert.True(t, q.paused, "not paused")
	ok, stopped := q.add(raftpb.Entry{})
	assert.False(t, ok, "entry added to paused queue")
	assert.False(t, stopped, "entry queue unexpectedly stopped")
}

func TestEntryQueueCanBeClosed(t *testing.T) {
	q := newEntryQueue(5, 0)
	assert.False(t, q.stopped, "entry queue is stopped by default")
	for i := 0; i < 5; i++ {
		ok, stopped := q.add(raftpb.Entry{})
		assert.True(t, ok, "failed to add new entry")
		assert.False(t, stopped, "failed to add new entry")
		assert.False(t, q.stopped, "stopped too early")
	}
	ok, _ := q.add(raftpb.Entry{})
	assert.False(t, ok, "not expect to add more")
	q = newEntryQueue(5, 0)
	q.close()
	assert.True(t, q.stopped, "entry queue is not marked as stopped")
	assert.Equal(t, uint64(0), q.idx, "idx %d, want 0", q.idx)
	ok, stopped := q.add(raftpb.Entry{})
	assert.False(t, ok, "not expect to add more")
	assert.True(t, stopped, "stopped flag is not returned")
}

func TestEntryQueueAllowEntriesToBeAdded(t *testing.T) {
	q := newEntryQueue(5, 0)
	for i := uint64(0); i < 5; i++ {
		ok, stopped := q.add(raftpb.Entry{Index: i + 1})
		assert.True(t, ok, "failed to add new entry")
		assert.False(t, stopped, "failed to add new entry")
		assert.Equal(t, i+1, q.idx, "idx %d, want %d", q.idx, i+1)
		var r []raftpb.Entry
		if q.leftInWrite {
			r = q.left
		} else {
			r = q.right
		}
		assert.Equal(t, i+1, r[i].Index, "index %d, want %d",
			r[i].Index, i+1)
	}
}

func TestEntryQueueAllowAddedEntriesToBeReturned(t *testing.T) {
	q := newEntryQueue(5, 0)
	for i := 0; i < 3; i++ {
		ok, stopped := q.add(raftpb.Entry{Index: uint64(i + 1)})
		assert.True(t, ok, "failed to add new entry")
		assert.False(t, stopped, "failed to add new entry")
	}
	r := q.get(false)
	assert.Len(t, r, 3, "len %d, want %d", len(r), 3)
	assert.Equal(t, uint64(0), q.idx, "idx %d, want %d", q.idx, 0)
	// check whether we can keep adding entries as long as we keep getting
	// previously written entries.
	expectedIndex := uint64(1)
	q = newEntryQueue(5, 0)
	for i := 0; i < 1000; i++ {
		ok, stopped := q.add(raftpb.Entry{Index: uint64(i + 1)})
		assert.True(t, ok, "failed to add new entry")
		assert.False(t, stopped, "failed to add new entry")
		if q.idx == q.size {
			r := q.get(false)
			assert.Len(t, r, 5, "len %d, want %d", len(r), 5)
			for _, e := range r {
				assert.Equal(t, expectedIndex, e.Index,
					"index %d, expected %d", e.Index, expectedIndex)
				expectedIndex++
			}
		}
	}
}

func TestShardCanBeSetAsReady(t *testing.T) {
	rc := newReadyShard()
	assert.Empty(t, rc.ready, "ready map not empty")
	rc.setShardReady(1)
	rc.setShardReady(2)
	rc.setShardReady(2)
	assert.Len(t, rc.ready, 2, "ready map sz %d, want 2", len(rc.ready))
	_, ok := rc.ready[1]
	assert.True(t, ok, "shard 1 not set as ready")
	_, ok = rc.ready[2]
	assert.True(t, ok, "shard 2 not set as ready")
}

func TestReadyShardCanBeReturnedAndCleared(t *testing.T) {
	rc := newReadyShard()
	assert.Empty(t, rc.ready, "ready map not empty")
	rc.setShardReady(1)
	rc.setShardReady(2)
	rc.setShardReady(2)
	assert.Len(t, rc.ready, 2, "ready map sz %d, want 2", len(rc.ready))
	r := rc.getReadyShards()
	assert.Len(t, r, 2, "ready map sz %d, want 2", len(r))
	assert.Empty(t, rc.ready, "shard ready map not cleared")
	r = rc.getReadyShards()
	assert.Empty(t, r, "shard ready map not cleared")
	rc.setShardReady(4)
	r = rc.getReadyShards()
	assert.Len(t, r, 1, "shard ready not set")
}

func TestLeaderInfoQueueCanBeCreated(t *testing.T) {
	q := newLeaderInfoQueue()
	assert.Equal(t, 1, cap(q.workCh), "unexpected queue cap")
	assert.Empty(t, q.workCh, "unexpected queue length")
	assert.Empty(t, q.notifications, "unexpected notifications length")
	ch := q.workReady()
	assert.NotNil(t, ch, "failed to return work ready chan")
}

func TestAddToLeaderInfoQueue(t *testing.T) {
	q := newLeaderInfoQueue()
	q.addLeaderInfo(raftio.LeaderInfo{})
	q.addLeaderInfo(raftio.LeaderInfo{})
	assert.Len(t, q.workCh, 1, "unexpected workCh len")
	assert.Len(t, q.notifications, 2, "unexpected notifications len")
}

func TestGetFromLeaderInfoQueue(t *testing.T) {
	q := newLeaderInfoQueue()
	_, ok := q.getLeaderInfo()
	assert.False(t, ok, "unexpectedly returned leader info")
	v1 := raftio.LeaderInfo{ShardID: 101}
	v2 := raftio.LeaderInfo{ShardID: 2002}
	q.addLeaderInfo(v1)
	q.addLeaderInfo(v2)
	rv1, ok1 := q.getLeaderInfo()
	rv2, ok2 := q.getLeaderInfo()
	_, ok3 := q.getLeaderInfo()
	assert.True(t, ok1, "unexpected result")
	assert.Equal(t, v1.ShardID, rv1.ShardID, "unexpected result")
	assert.True(t, ok2, "unexpected result")
	assert.Equal(t, v2.ShardID, rv2.ShardID, "unexpected result")
	assert.False(t, ok3, "unexpectedly return third reader info rec")
}

func TestReadIndexQueueCanHandleAddFailure(t *testing.T) {
	q := newReadIndexQueue(1)
	added, stopped := q.add(&RequestState{})
	assert.True(t, added, "unexpected failure")
	assert.False(t, stopped, "unexpected failure")
	added, stopped = q.add(&RequestState{})
	assert.False(t, added, "unexpectedly added the rs")
	assert.False(t, stopped, "unexpectedly reported state as stopped")
	q.close()
	added, stopped = q.add(&RequestState{})
	assert.False(t, added, "unexpectedly added the rs")
	assert.True(t, stopped, "failed to report state as stopped")
}
````

## File: queue.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"sync"

	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

type entryQueue struct {
	size          uint64
	left          []pb.Entry
	right         []pb.Entry
	leftInWrite   bool
	stopped       bool
	paused        bool
	idx           uint64
	oldIdx        uint64
	cycle         uint64
	lazyFreeCycle uint64
	mu            sync.Mutex
}

func newEntryQueue(size uint64, lazyFreeCycle uint64) *entryQueue {
	return &entryQueue{
		size:          size,
		lazyFreeCycle: lazyFreeCycle,
		left:          make([]pb.Entry, size),
		right:         make([]pb.Entry, size),
	}
}

func (q *entryQueue) close() {
	q.mu.Lock()
	defer q.mu.Unlock()
	q.stopped = true
}

func (q *entryQueue) targetQueue() []pb.Entry {
	if q.leftInWrite {
		return q.left
	}
	return q.right
}

func (q *entryQueue) add(ent pb.Entry) (bool, bool) {
	q.mu.Lock()
	defer q.mu.Unlock()
	if q.paused || q.idx >= q.size {
		return false, q.stopped
	}
	if q.stopped {
		return false, true
	}
	w := q.targetQueue()
	w[q.idx] = ent
	q.idx++
	return true, false
}

func (q *entryQueue) gc() {
	if q.lazyFreeCycle > 0 {
		oldq := q.targetQueue()
		if q.lazyFreeCycle == 1 {
			for i := uint64(0); i < q.oldIdx; i++ {
				oldq[i].Cmd = nil
			}
		} else if q.cycle%q.lazyFreeCycle == 0 {
			for i := uint64(0); i < q.size; i++ {
				oldq[i].Cmd = nil
			}
		}
	}
}

func (q *entryQueue) get(paused bool) []pb.Entry {
	q.mu.Lock()
	defer q.mu.Unlock()
	q.paused = paused
	q.cycle++
	sz := q.idx
	q.idx = 0
	t := q.targetQueue()
	q.leftInWrite = !q.leftInWrite
	q.gc()
	q.oldIdx = sz
	return t[:sz]
}

type readIndexQueue struct {
	size        uint64
	left        []*RequestState
	right       []*RequestState
	leftInWrite bool
	stopped     bool
	idx         uint64
	mu          sync.Mutex
}

func newReadIndexQueue(size uint64) *readIndexQueue {
	return &readIndexQueue{
		size:  size,
		left:  make([]*RequestState, size),
		right: make([]*RequestState, size),
	}
}

func (q *readIndexQueue) pendingSize() uint64 {
	q.mu.Lock()
	defer q.mu.Unlock()
	return q.idx
}

func (q *readIndexQueue) close() {
	q.mu.Lock()
	defer q.mu.Unlock()
	q.stopped = true
}

func (q *readIndexQueue) targetQueue() []*RequestState {
	if q.leftInWrite {
		return q.left
	}
	return q.right
}

func (q *readIndexQueue) add(rs *RequestState) (bool, bool) {
	q.mu.Lock()
	defer q.mu.Unlock()
	if q.idx >= q.size {
		return false, q.stopped
	}
	if q.stopped {
		return false, true
	}
	w := q.targetQueue()
	w[q.idx] = rs
	q.idx++
	return true, false
}

func (q *readIndexQueue) get() []*RequestState {
	q.mu.Lock()
	defer q.mu.Unlock()
	sz := q.idx
	q.idx = 0
	t := q.targetQueue()
	q.leftInWrite = !q.leftInWrite
	return t[:sz]
}

type readyShard struct {
	mu    sync.Mutex
	ready map[uint64]struct{}
	maps  [2]map[uint64]struct{}
	index uint8
}

func newReadyShard() *readyShard {
	r := &readyShard{}
	r.maps[0] = make(map[uint64]struct{})
	r.maps[1] = make(map[uint64]struct{})
	r.ready = r.maps[0]
	return r
}

func (r *readyShard) setShardReady(shardID uint64) {
	r.mu.Lock()
	r.ready[shardID] = struct{}{}
	r.mu.Unlock()
}

func (r *readyShard) getReadyShards() map[uint64]struct{} {
	m := r.maps[(r.index+1)%2]
	for k := range m {
		delete(m, k)
	}
	r.mu.Lock()
	v := r.ready
	r.index++
	r.ready = r.maps[r.index%2]
	r.mu.Unlock()
	return v
}

type leaderInfoQueue struct {
	mu            sync.Mutex
	notifications []raftio.LeaderInfo
	workCh        chan struct{}
}

func newLeaderInfoQueue() *leaderInfoQueue {
	return &leaderInfoQueue{
		workCh:        make(chan struct{}, 1),
		notifications: make([]raftio.LeaderInfo, 0),
	}
}

func (li *leaderInfoQueue) workReady() chan struct{} {
	return li.workCh
}

func (li *leaderInfoQueue) addLeaderInfo(info raftio.LeaderInfo) {
	func() {
		li.mu.Lock()
		defer li.mu.Unlock()
		li.notifications = append(li.notifications, info)
	}()
	select {
	case li.workCh <- struct{}{}:
	default:
	}
}

func (li *leaderInfoQueue) getLeaderInfo() (raftio.LeaderInfo, bool) {
	li.mu.Lock()
	defer li.mu.Unlock()
	if len(li.notifications) > 0 {
		v := li.notifications[0]
		li.notifications = li.notifications[1:]
		return v, true
	}
	return raftio.LeaderInfo{}, false
}
````

## File: quiesce_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"testing"

	pb "github.com/lni/dragonboat/v4/raftpb"
	"github.com/stretchr/testify/assert"
)

func getTestQuiesce() quiesceState {
	return quiesceState{
		electionTick: 10,
		enabled:      true,
	}
}

func TestIncreaseTickCanEnterQuiesce(t *testing.T) {
	q := getTestQuiesce()
	threshold := q.threshold()
	tests := []struct {
		tick     uint64
		quiesced bool
	}{
		{threshold / 2, false},
		{threshold, false},
		{threshold + 1, true},
	}
	for i, tt := range tests {
		q := getTestQuiesce()
		for k := uint64(0); k < tt.tick; k++ {
			q.tick()
		}
		assert.Equal(t, tt.quiesced, q.quiesced(),
			"test case %d", i)
	}
}

func TestQuiesceCanBeDisabled(t *testing.T) {
	q := getTestQuiesce()
	threshold := q.threshold()
	tests := []struct {
		tick     uint64
		quiesced bool
	}{
		{threshold / 2, false},
		{threshold, false},
		{threshold + 1, false},
	}
	for i, tt := range tests {
		q := getTestQuiesce()
		// disable it
		q.enabled = false
		for k := uint64(0); k < tt.tick; k++ {
			q.tick()
		}
		assert.Equal(t, tt.quiesced, q.quiesced(),
			"test case %d", i)
	}
}

func TestExitFromQuiesceWhenActivityIsRecorded(t *testing.T) {
	tests := []pb.MessageType{
		pb.Replicate,
		pb.ReplicateResp,
		pb.RequestVote,
		pb.RequestVoteResp,
		pb.InstallSnapshot,
		pb.Propose,
		pb.ReadIndex,
		pb.ConfigChangeEvent,
	}
	for i, tt := range tests {
		q := getTestQuiesce()
		for k := uint64(0); k < q.threshold()+1; k++ {
			q.tick()
		}
		assert.True(t, q.quiesced(),
			"test case %d: should be quiesced", i)
		q.record(tt)
		assert.False(t, q.quiesced(),
			"test case %d: should not be quiesced", i)
		assert.Equal(t, q.currentTick, q.idleSince,
			"test case %d: idleSince should equal currentTick", i)
	}
}

func TestMsgHeartbeatWillNotStopEnteringQuiesce(t *testing.T) {
	q := getTestQuiesce()
	threshold := q.threshold()
	tests := []struct {
		tick     uint64
		quiesced bool
	}{
		{threshold / 2, false},
		{threshold, false},
		{threshold + 1, true},
	}
	for i, tt := range tests {
		q := getTestQuiesce()
		for k := uint64(0); k < tt.tick; k++ {
			q.tick()
			q.record(pb.Heartbeat)
		}
		assert.Equal(t, tt.quiesced, q.quiesced(),
			"test case %d", i)
	}
}

func TestWillNotExitFromQuiesceForDelayedMsgHeartbeatMsg(t *testing.T) {
	q := getTestQuiesce()
	for k := uint64(0); k < q.threshold()+1; k++ {
		q.tick()
	}
	assert.True(t, q.quiesced(), "should be quiesced")
	assert.True(t, q.newToQuiesce(), "should be new to quiesce")
	for q.newToQuiesce() {
		q.record(pb.Heartbeat)
		assert.True(t, q.quiesced(), "should remain quiesced")
		q.tick()
	}
	// no longer considered as recently entered quiesce
	q.record(pb.Heartbeat)
	assert.False(t, q.quiesced(), "should not be quiesced")
}
````

## File: quiesce.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"sync/atomic"

	pb "github.com/lni/dragonboat/v4/raftpb"
)

type quiesceState struct {
	shardID             uint64
	replicaID           uint64
	currentTick         uint64
	electionTick        uint64
	quiescedSince       uint64
	idleSince           uint64
	exitQuiesceTick     uint64
	enabled             bool
	newQuiesceStateFlag uint32
}

func (q *quiesceState) setNewQuiesceStateFlag() {
	atomic.StoreUint32(&q.newQuiesceStateFlag, 1)
}

func (q *quiesceState) newQuiesceState() bool {
	return atomic.SwapUint32(&q.newQuiesceStateFlag, 0) == 1
}

func (q *quiesceState) tick() uint64 {
	if !q.enabled {
		return 0
	}
	threshold := q.threshold()
	q.currentTick++
	if !q.quiesced() {
		if (q.currentTick - q.idleSince) > threshold {
			q.enterQuiesce()
		}
	}
	return q.currentTick
}

func (q *quiesceState) quiesced() bool {
	return q.enabled && q.quiescedSince > 0
}

func (q *quiesceState) record(msgType pb.MessageType) {
	if !q.enabled {
		return
	}
	if msgType == pb.Heartbeat || msgType == pb.HeartbeatResp {
		if !q.quiesced() {
			return
		}
		if q.newToQuiesce() {
			return
		}
	}
	q.idleSince = q.currentTick
	if q.quiesced() {
		q.exitQuiesce()
		plog.Infof("%s exited from quiesce, msg type %s, current tick %d",
			dn(q.shardID, q.replicaID), msgType, q.currentTick)
	}
}

func (q *quiesceState) threshold() uint64 {
	return q.electionTick * 10
}

func (q *quiesceState) newToQuiesce() bool {
	if !q.quiesced() {
		return false
	}
	return q.currentTick-q.quiescedSince < q.electionTick
}

func (q *quiesceState) justExitedQuiesce() bool {
	if q.quiesced() {
		return false
	}
	return q.currentTick-q.exitQuiesceTick < q.threshold()
}

func (q *quiesceState) tryEnterQuiesce() {
	if q.justExitedQuiesce() {
		return
	}
	if !q.quiesced() {
		plog.Infof("%s going to enter quiesce due to quiesce message",
			dn(q.shardID, q.replicaID))
		q.enterQuiesce()
	}
}

func (q *quiesceState) enterQuiesce() {
	q.quiescedSince = q.currentTick
	q.idleSince = q.currentTick
	q.setNewQuiesceStateFlag()
	plog.Infof("%s entered quiesce", dn(q.shardID, q.replicaID))
}

func (q *quiesceState) exitQuiesce() {
	q.quiescedSince = 0
	q.exitQuiesceTick = q.currentTick
}
````

## File: README.CHS.md
````markdown
![dragonboat](./docs/dragonboat.jpg)
# Dragonboat - Go多组Raft库 #
[![license](http://img.shields.io/badge/license-Apache2-blue.svg)](https://github.com/lni/dragonboat/blob/master/LICENSE)
![Build status](https://github.com/lni/dragonboat/workflows/Test/badge.svg?branch=master)
[![Go Report Card](https://goreportcard.com/badge/github.com/lni/dragonboat)](https://goreportcard.com/report/github.com/lni/dragonboat)
[![codecov](https://codecov.io/gh/lni/dragonboat/branch/master/graph/badge.svg)](https://codecov.io/gh/lni/dragonboat)
[![Godoc](http://img.shields.io/badge/go-documentation-blue.svg)](https://godoc.org/github.com/lni/dragonboat)
[![Ask LLM about dragonboat](https://deepwiki.com/badge.svg)](https://deepwiki.com/lni/dragonboat)

## 项目新闻 ##
* 2025-07-17 加入了一个[DeepWiki链接](https://deepwiki.com/lni/dragonboat)
* 2022-06-03 Dragonboat v4.0 版本正在开发中，master分支已经是v4的API，具体变化请见[CHANGELOG](CHANGELOG.md)。
* 2021-01-20 Dragonboat v3.3 已发布，请查看[CHANGELOG](CHANGELOG.md)获知所有更新情况。

## 关于 ##
Dragonboat是一个高性能纯[Go](https://golang.org)语言实现的多组[Raft](https://raft.github.io/) [共识算法](https://en.wikipedia.org/wiki/Consensus_(computer_science))库。

Raft这样的共识算法使得只要系统中的多数成员在线便可使得系统持续运行。比如，一个拥有5台服务器的Raft集群中即使有两台服务器故障依旧可以工作。它同时向客户端展现一个单机节点，始终提供强一致保证的数据访存。同时，所有在线的成员节点都可用来提供读操作，从而提供更高的读吞吐总和。

所有Raft相关的技术难点都会由Dragonboat来承担，用户从而可以只关注他们各自的应用领域。Dragonboats[使用十分简便](docs/overview.CHS.md)，详细的[例程](https://github.com/lni/dragonboat-example)可使新用户在半小时内完全掌握它。

## 功能 ##
* 便于使用的可构建单组与多组Raft应用的纯Go语言API
* 功能完备的多组Raft协议的实现，同机支持数千Raft组
* 完备的测试确保[正确性](/docs/test.md)，这包括[Jepsen](https://aphyr.com/tags/jepsen)所带的[Knossos](https://github.com/jepsen-io/knossos)强一致性检查，部分测试日志[在此](https://github.com/lni/knossos-data)
* 全流水线设计、TLS支持，适合被部署于跨地域的高网络延时公网环境
* 在中档硬件上即可获得约300万/秒的写或1000万/秒的强一致读的性能
* 支持定制的Raft log存储与通讯模块，可方便整合最新IO类技术产品
* 基于Prometheus的健康度metrics支持
* 内建的用于修复已永久丢失多数派节点的Raft组的工具
* 多Raft组由自带的[Drummer](/drummer)服务器组件管理以获得良好的高可用

Diego Ongaro的[Raft博士学位论文](https://github.com/ongardie/dissertation/blob/master/stanford.pdf)中提及的所有功能都已实现：
* 选主、log复制、状态机快照与log清理
* Raft组成员变更
* Pre-Vote扩展
* 基于ReadIndex协议的只读查询
* 主节点转移
* 无投票权成员
* Witness成员
* 应用透明的幂等更新支持
* 成组处理优化与流水化处理
* 基于磁盘的状态机

## 性能 ##
Dragonboat是目前Github网站上最快的开源多组Raft实现。

在三节点系统上，使用中端硬件（具体信息[在此](/docs/test.md)）与基于内存的状态机，在16字节的荷载下，当使用RocksDB做为存储引擎，Dragonboat可持续每秒900万次写或在9:1的高读写比场景下提供每秒1100万次的混合读写操作。高吞吐在跨地域分布环境依旧被保持，在使用更多的clients的情况下，在RTT为30ms时依旧能实现200万次每秒的IO操作。
![throughput](./docs/throughput.png)

每个服务器上可轻易承载数千Raft组。并发的活跃Raft组数量对吞吐有直接影响，而大量的闲置Raft组对系统性能并无巨大影响。
![nodes](./docs/nodes.png)

下表是毫秒为单位的写延迟数据。Dragonboat可以在处理每秒800万次写（16字节荷载）的时候做到P99的写延迟小于5ms。读延迟低于写延迟，因为用于linearizable读的ReadIndex协议无需对每个读请求做落盘写。

|每秒请求数|荷载大小|99.9% percentile|99% percentile|平均|
|:-:|:----------:|:--:|:-:|:-:|
|100万|16|2.24|1.19|0.79|
|100万|128|11.11|1.37|0.92|
|100万|1024|71.61|25.91|3.75|
|500万|16|4.64|1.95|1.16|
|500万|128|36.61|6.55|1.96|
|800万|16|12.01|4.65|2.13|

当测试单组性能时，Dragonboat可以在16字节负载下持续每秒完成125万次写，此时平均写延迟是1.3毫秒，P99写延迟为2.6毫秒。上述性能是在平均单机占用三个2.8Ghz的核心的情况下实现的。

即使在很高的系统负载下，Go 1.11的GC所带来的Stop-the-World停顿也显著低于1毫秒。在Go 1.12版中，GC的Stop-the-World停顿时间又进一步大幅减低。Golang的runtime.ReadMemStats显示即使在很高的系统负载下，GC也仅占用了少于1%的可利用CPU时间。
![stw](./docs/stw.png)

## 系统需求 ##
* x86_64/Linux, x86_64/MacOS或ARM64/Linux, Go 1.23或更新版本

## 开始使用 ##
__Master是用于开发的非稳定branch。生产环境请使用已发布版本__。如您使用v3.3.x版本，请参考v3.3.x版本的[README.CHS.md](https://github.com/lni/dragonboat/blob/release-3.3/README.CHS.md)。

首先请确保Go 1.14或者更新的版本已被安装以获得[Go module](https://github.com/golang/go/wiki/Modules)支持。

使用下列命令将Dragonboat v3稳定版加入您的项目：

```
go get github.com/lni/dragonboat/v3@latest
```
或者使用下列命令将开发中的Dragonboat v4版加入您的项目：
```
go get github.com/lni/dragonboat/v4@master
```

[Pebble](https://github.com/cockroachdb/pebble)是默认的用于存储Raft Log的存储引擎。RocksDB与自定义存储引擎的使用方法可参考[这里](docs/storage.CHS.md)。

同时可参考[例程](https://github.com/lni/dragonboat-example)以了解更多Dragonboat使用信息。

## 文档与资料 ##

首先建议您阅读项目的[综述文档](docs/overview.CHS.md)与[运维注意事项](docs/devops.CHS.md)。[DeepWiki](https://deepwiki.com/lni/dragonboat)是一个第三方LLM生成的wiki，能帮助深入了解本项目实现，并回答很多技术问题。

欢迎阅读[godoc文档](https://godoc.org/github.com/lni/dragonboat)，[中文例程](https://github.com/lni/dragonboat-example)，[常见问题](https://github.com/lni/dragonboat/wiki/FAQ)和[CHANGELOG](CHANGELOG.md)。

## 中文例程 ##
中文例程在[这里](https://github.com/lni/dragonboat-example)。

## 项目状态 ##
Dragonboat适用于生产环境。

## 参与 ##
报告bugs, 请提交一个[issue](https://github.com/lni/dragonboat/issues/new)。参与贡献改进及新功能, 请提交pull request并创建一个[issue](https://github.com/lni/dragonboat/issues/new)以便讨论与进度追踪。

## 开源许可协议 ##
本项目以Apache License Version 2.0授权开源，请参考LICENSE文件。

本项目所使用的第三方代码与它们的开源许可证信息的列表[在此](docs/COPYRIGHT)
````

## File: README.md
````markdown
![dragonboat](./docs/dragonboat.jpg)
# Dragonboat - A Multi-Group Raft library in Go / [中文版](README.CHS.md) ##
[![license](http://img.shields.io/badge/license-Apache2-blue.svg)](https://github.com/lni/dragonboat/blob/master/LICENSE)
![Build status](https://github.com/lni/dragonboat/workflows/Test/badge.svg?branch=master)
[![Go Report Card](https://goreportcard.com/badge/github.com/lni/dragonboat)](https://goreportcard.com/report/github.com/lni/dragonboat)
[![codecov](https://codecov.io/gh/lni/dragonboat/branch/master/graph/badge.svg)](https://codecov.io/gh/lni/dragonboat)
[![Godoc](http://img.shields.io/badge/go-documentation-blue.svg)](https://godoc.org/github.com/lni/dragonboat/v3)
[![Ask LLM about dragonboat](https://deepwiki.com/badge.svg)](https://deepwiki.com/lni/dragonboat)

## News ##
* 2025-07-17 Added a LLM generated wiki [here](https://deepwiki.com/lni/dragonboat).
* 2022-06-03 We are working towards a v4.0 release which will come with API changes. See [CHANGELOG](CHANGELOG.md) for details. 
* 2021-01-20 Dragonboat v3.3 has been released, please check [CHANGELOG](CHANGELOG.md) for all changes.

## About ##
Dragonboat is a high performance multi-group [Raft](https://raft.github.io/) [consensus](https://en.wikipedia.org/wiki/Consensus_(computer_science)) library in pure [Go](https://golang.org/).

Consensus algorithms such as Raft provides fault-tolerance by alllowing a system continue to operate as long as the majority member servers are available. For example, a Raft shard of 5 servers can make progress even if 2 servers fail. It also appears to clients as a single entity with strong data consistency always provided. All Raft replicas can be used to handle read requests for aggregated read throughput.

Dragonboat handles all technical difficulties associated with Raft to allow users to just focus on their application domains. It is also very easy to use, our step-by-step [examples](https://github.com/lni/dragonboat-example) can help new users to master it in half an hour.

## Features ##
* Easy to use pure-Go APIs for building Raft based applications
* Feature complete and scalable multi-group Raft implementation
* Disk based and memory based state machine support
* Fully pipelined and TLS mutual authentication support, ready for high latency open environment
* Custom Raft log storage and transport support, easy to integrate with latest I/O techs
* Prometheus based health metrics support
* Built-in tool to repair Raft shards that permanently lost the quorum
* [Extensively tested](/docs/test.md) including using [Jepsen](https://aphyr.com/tags/jepsen)'s [Knossos](https://github.com/jepsen-io/knossos) linearizability checker, some results are [here](https://github.com/lni/knossos-data)

All major features covered in Diego Ongaro's [Raft thesis](https://github.com/ongardie/dissertation/blob/master/stanford.pdf) have been supported -
* leader election, log replication, snapshotting and log compaction
* membership change
* pre-vote
* ReadIndex protocol for read-only queries
* leadership transfer
* non-voting member
* witness member
* idempotent update transparent to applications
* batching and pipelining
* disk based state machine

## Performance ##
Dragonboat is the __fastest__ open source multi-group Raft implementation on Github. 

For 3-nodes system using mid-range hardware (details [here](docs/test.md)) and in-memory state machine, when RocksDB is used as the storage engine, Dragonboat can sustain at 9 million writes per second when the payload is 16bytes each or 11 million mixed I/O per second at 9:1 read:write ratio. High throughput is maintained in geographically distributed environment. When the RTT between nodes is 30ms, 2 million I/O per second can still be achieved using a much larger number of clients.
![throughput](./docs/throughput.png)

The number of concurrent active Raft groups affects the overall throughput as requests become harder to be batched. On the other hand, having thousands of idle Raft groups has a much smaller impact on throughput.
![nodes](./docs/nodes.png)

Table below shows write latencies in millisecond, Dragonboat has <5ms P99 write latency when handling 8 million writes per second at 16 bytes each. Read latency is lower than writes as the ReadIndex protocol employed for linearizable reads doesn't require fsync-ed disk I/O.

|Ops|Payload Size|99.9% percentile|99% percentile|AVG|
|:-:|:----------:|:--:|:-:|:-:|
|1m|16|2.24|1.19|0.79|
|1m|128|11.11|1.37|0.92|
|1m|1024|71.61|25.91|3.75|
|5m|16|4.64|1.95|1.16|
|5m|128|36.61|6.55|1.96|
|8m|16|12.01|4.65|2.13|

When tested on a single Raft group, Dragonboat can sustain writes at 1.25 million per second when payload is 16 bytes each, average latency is 1.3ms and the P99 latency is 2.6ms. This is achieved when using an average of 3 cores (2.8GHz) on each server.

As visualized below, Stop-the-World pauses caused by Go1.11's GC are sub-millisecond on highly loaded systems. Such very short Stop-the-World pause time is further significantly reduced in Go 1.12. Golang's runtime.ReadMemStats reports that less than 1% of the available CPU time is used by GC on highly loaded system.
![stw](./docs/stw.png)

## Requirements ##
* x86_64/Linux, x86_64/MacOS or ARM64/Linux, Go 1.23 or above.

## Getting Started ##
__Master is our unstable branch for development, it is current working towards the v4.0 release. Please use the latest released versions for any production purposes.__ For Dragonboat v3.3.x, please follow the instructions in v3.3.x's [README.md](https://github.com/lni/dragonboat/blob/release-3.3/README.md). 

Go 1.17 or above with [Go module](https://github.com/golang/go/wiki/Modules) support is required.

Use the following command to add Dragonboat v3 into your project. 

```
go get github.com/lni/dragonboat/v3@latest
```

Or you can use the following command to start using the development version of the Dragonboat, which is current at v4 for its APIs. 

```
go get github.com/lni/dragonboat/v4@master
```


By default, [Pebble](https://github.com/cockroachdb/pebble) is used for storing Raft Logs in Dragonboat. RocksDB and other storage engines are also supported, more info [here](docs/storage.md).

You can also follow our [examples](https://github.com/lni/dragonboat-example) on how to use Dragonboat. 

## Documents ##
[DeepWiki](https://deepwiki.com/lni/dragonboat) is a LLM generated wiki helping to dive deep into the codebase.

[FAQ](https://github.com/lni/dragonboat/wiki/FAQ), [docs](https://godoc.org/github.com/lni/dragonboat), step-by-step [examples](https://github.com/lni/dragonboat-example), [DevOps doc](docs/devops.md) and [CHANGELOG](CHANGELOG.md).

## Examples ##
Dragonboat examples are [here](https://github.com/lni/dragonboat-example).

## Status ##
Dragonboat is production ready.

## Contributing ##
For reporting bugs, please open an [issue](https://github.com/lni/dragonboat/issues/new). For contributing improvements or new features, please send in the pull request.

## License ##
Dragonboat is licensed under the Apache License Version 2.0. See LICENSE for details.

Third party code used in Dragonboat and their licenses is summarized [here](docs/COPYRIGHT).
````

## File: registry.go
````go
// Copyright 2017-2022 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

// INodeHostRegistry provides APIs for querying data shared between NodeHost
// instances via gossip.
type INodeHostRegistry interface {
	NumOfShards() int
	GetMeta(nhID string) ([]byte, bool)
	GetShardInfo(shardID uint64) (ShardView, bool)
}
````

## File: request_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"crypto/rand"
	"sync"
	"sync/atomic"
	"testing"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/client"
	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/rsm"
	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
	"github.com/lni/goutils/random"
)

func TestIsTempError(t *testing.T) {
	tests := []struct {
		err  error
		temp bool
	}{
		{ErrInvalidOperation, false},
		{ErrInvalidAddress, false},
		{ErrInvalidSession, false},
		{ErrTimeoutTooSmall, false},
		{ErrPayloadTooBig, false},
		{ErrSystemBusy, true},
		{ErrShardClosed, true},
		{ErrShardNotInitialized, true},
		{ErrTimeout, true},
		{ErrCanceled, false},
		{ErrRejected, false},
		{ErrAborted, true},
		{ErrShardNotReady, true},
		{ErrInvalidTarget, false},
		{ErrInvalidRange, false},
	}
	for idx, tt := range tests {
		assert.Equal(t, tt.temp, IsTempError(tt.err), "test %d", idx)
	}
}

func TestRequestCodeName(t *testing.T) {
	code := requestTimeout
	assert.Equal(t, "RequestTimeout", code.String())
}

func TestRequestStateCommitted(t *testing.T) {
	require.Panics(t, func() {
		rs := &RequestState{}
		rs.committed()
	})

	require.Panics(t, func() {
		rs := &RequestState{notifyCommit: true}
		rs.committed()
	})

	require.Panics(t, func() {
		rs := &RequestState{
			notifyCommit: true,
			committedC:   make(chan RequestResult, 1),
		}
		rs.committed()
		rs.committed()
	})

	rs := &RequestState{
		notifyCommit: true,
		committedC:   make(chan RequestResult, 1),
	}
	rs.committed()
	select {
	case cc := <-rs.committedC:
		assert.Equal(t, requestCommitted, cc.code)
	default:
		assert.Fail(t, "nothing in the committedC")
	}
}

func TestRequestStateReuse(t *testing.T) {
	rs := &RequestState{}
	rs.reuse(false)
	require.NotNil(t, rs.CompletedC)
	assert.Equal(t, 1, cap(rs.CompletedC))
	assert.Nil(t, rs.committedC)

	rs = &RequestState{}
	rs.reuse(true)
	require.NotNil(t, rs.committedC)
	assert.Equal(t, 1, cap(rs.committedC))
}

func TestResultCReturnsCompleteCWhenNotifyCommitNotSet(t *testing.T) {
	rs := &RequestState{
		CompletedC: make(chan RequestResult),
	}
	assert.Equal(t, rs.CompletedC, rs.ResultC())
}

func TestResultCPanicWhenCommittedCIsNil(t *testing.T) {
	rs := &RequestState{
		CompletedC:   make(chan RequestResult),
		notifyCommit: true,
	}
	require.Panics(t, func() { rs.ResultC() })
}

func TestResultCCanReceiveRequestResults(t *testing.T) {
	tests := []struct {
		notifyCommit        bool
		hasCommittedCResult bool
		completedCCode      RequestResultCode
		crash               bool
	}{
		{false, false, requestTimeout, false},
		{false, false, requestCompleted, false},
		{false, false, requestTerminated, false},
		{false, false, requestRejected, false},
		{false, false, requestDropped, false},
		{false, false, requestAborted, false},
		{false, false, requestCommitted, false},

		{true, false, requestTimeout, false},
		{true, false, requestCompleted, false},
		{true, false, requestTerminated, false},
		{true, false, requestRejected, false},
		{true, false, requestDropped, false},
		{true, false, requestAborted, true},
		{true, false, requestCommitted, true},

		{true, true, requestTimeout, false},
		{true, true, requestCompleted, false},
		{true, true, requestTerminated, false},
		{true, true, requestRejected, false},
		{true, true, requestDropped, true},
		{true, true, requestAborted, true},
		{true, true, requestCommitted, true},
	}
	for idx, tt := range tests {
		func() {
			rs := &RequestState{
				CompletedC:   make(chan RequestResult, 1),
				notifyCommit: tt.notifyCommit,
			}
			if tt.crash {
				rs.testErr = make(chan struct{})
			}
			if tt.notifyCommit {
				rs.committedC = make(chan RequestResult, 1)
			}
			if tt.hasCommittedCResult {
				rs.committed()
			}
			rs.notify(RequestResult{code: tt.completedCCode})
			ch := rs.ResultC()
			crashed := false
			if tt.hasCommittedCResult {
				select {
				case cc := <-ch:
					assert.Equal(t, requestCommitted, cc.code, "idx %d", idx)
				case <-rs.testErr:
					crashed = true
					cc := <-ch
					assert.Equal(t, requestCommitted, cc.code, "idx %d", idx)
				}
			}
			select {
			case cc := <-ch:
				assert.Equal(t, tt.completedCCode, cc.code, "idx %d", idx)
				if rs.testErr != nil {
					<-rs.testErr
					crashed = true
				}
			case <-rs.testErr:
				crashed = true
			}
			if tt.crash {
				assert.True(t, crashed, "idx %d", idx)
			}
			assert.Equal(t, ch, rs.ResultC(), "idx %d", idx)
		}()
	}
}

func TestPendingLeaderTransferCanBeCreated(t *testing.T) {
	p := newPendingLeaderTransfer()
	require.NotNil(t, p.leaderTransferC)
	assert.Empty(t, p.leaderTransferC)
}

func TestLeaderTransferCanBeRequested(t *testing.T) {
	p := newPendingLeaderTransfer()
	err := p.request(1)
	assert.NoError(t, err, "failed to request leadership transfer")
	assert.Len(t, p.leaderTransferC, 1)
}

func TestInvalidLeaderTransferIsNotAllowed(t *testing.T) {
	p := newPendingLeaderTransfer()
	err := p.request(0)
	assert.Equal(t, ErrInvalidTarget, err)

	err = p.request(1)
	assert.NoError(t, err)

	err = p.request(2)
	assert.Equal(t, ErrSystemBusy, err)
}

func TestCanGetExitingLeaderTransferRequest(t *testing.T) {
	p := newPendingLeaderTransfer()
	_, ok := p.get()
	assert.False(t, ok)

	err := p.request(1)
	assert.NoError(t, err, "failed to request leadership transfer")

	v, ok := p.get()
	assert.True(t, ok)
	assert.Equal(t, uint64(1), v)

	v, ok = p.get()
	assert.False(t, ok)
	assert.Equal(t, uint64(0), v)
}

func TestRequestStatePanicWhenNotReadyForRead(t *testing.T) {
	fn := func(rs *RequestState) {
		require.Panics(t, func() {
			rs.mustBeReadyForLocalRead()
		})
	}
	r1 := &RequestState{}
	r2 := &RequestState{node: &node{}}
	r3 := &RequestState{node: &node{initializedC: make(chan struct{})}}
	r3.node.setInitialized()
	fn(r1)
	fn(r2)
	fn(r3)
	r4 := &RequestState{node: &node{initializedC: make(chan struct{})}}
	r4.node.setInitialized()
	r4.readyToRead.set()
	require.NotPanics(t, func() {
		r4.mustBeReadyForLocalRead()
	})
}

func TestPendingSnapshotCanBeCreatedAndClosed(t *testing.T) {
	snapshotC := make(chan<- rsm.SSRequest, 1)
	ps := newPendingSnapshot(snapshotC)
	assert.Empty(t, ps.snapshotC)
	assert.Nil(t, ps.pending)

	pending := &RequestState{
		CompletedC: make(chan RequestResult, 1),
	}
	ps.pending = pending
	ps.close()
	assert.Nil(t, ps.pending)
	select {
	case v := <-pending.ResultC():
		assert.True(t, v.Terminated())
	default:
		assert.Fail(t, "close() didn't set pending to terminated")
	}
}

func TestPendingSnapshotCanBeRequested(t *testing.T) {
	snapshotC := make(chan rsm.SSRequest, 1)
	ps := newPendingSnapshot(snapshotC)
	ss, err := ps.request(rsm.UserRequested, "", false, 0, 0, 10)
	require.NoError(t, err)
	require.NotNil(t, ss)

	assert.NotNil(t, ps.pending)
	assert.True(t, ss.deadline > ps.getTick())
	select {
	case <-snapshotC:
	default:
		assert.Fail(t, "requested snapshot is not pushed")
	}
}

func TestPendingSnapshotCanReturnBusy(t *testing.T) {
	snapshotC := make(chan rsm.SSRequest, 1)
	ps := newPendingSnapshot(snapshotC)
	_, err := ps.request(rsm.UserRequested, "", false, 0, 0, 10)
	assert.NoError(t, err)
	_, err = ps.request(rsm.UserRequested, "", false, 0, 0, 10)
	assert.Equal(t, ErrSystemBusy, err)
}

func TestTooSmallSnapshotTimeoutIsRejected(t *testing.T) {
	snapshotC := make(chan<- rsm.SSRequest, 1)
	ps := newPendingSnapshot(snapshotC)
	ss, err := ps.request(rsm.UserRequested, "", false, 0, 0, 0)
	assert.Equal(t, ErrTimeoutTooSmall, err)
	assert.Nil(t, ss)
}

func TestMultiplePendingSnapshotIsNotAllowed(t *testing.T) {
	snapshotC := make(chan<- rsm.SSRequest, 1)
	ps := newPendingSnapshot(snapshotC)
	ss, err := ps.request(rsm.UserRequested, "", false, 0, 0, 100)
	require.NoError(t, err)
	require.NotNil(t, ss)

	ss, err = ps.request(rsm.UserRequested, "", false, 0, 0, 100)
	assert.Equal(t, ErrSystemBusy, err)
	assert.Nil(t, ss)
}

func TestPendingSnapshotCanBeGCed(t *testing.T) {
	snapshotC := make(chan rsm.SSRequest, 1)
	ps := newPendingSnapshot(snapshotC)
	ss, err := ps.request(rsm.UserRequested, "", false, 0, 0, 20)
	require.NoError(t, err)
	require.NotNil(t, ss)
	assert.NotNil(t, ps.pending)

	for i := uint64(1); i < 22; i++ {
		ps.tick(i)
		ps.gc()
		assert.NotNil(t, ps.pending)
	}

	ps.tick(uint64(22))
	ps.gc()
	assert.Nil(t, ps.pending)
	select {
	case v := <-ss.ResultC():
		assert.True(t, v.Timeout())
	default:
		assert.Fail(t, "not notify as timed out")
	}
}

func TestPendingSnapshotCanBeApplied(t *testing.T) {
	snapshotC := make(chan rsm.SSRequest, 1)
	ps := newPendingSnapshot(snapshotC)
	ss, err := ps.request(rsm.UserRequested, "", false, 0, 0, 100)
	require.NoError(t, err)
	require.NotNil(t, ss)

	ps.apply(ss.key, false, false, 123)
	select {
	case v := <-ss.ResultC():
		assert.Equal(t, uint64(123), v.SnapshotIndex())
		assert.True(t, v.Completed())
	default:
		assert.Fail(t, "ss is not applied")
	}
}

func TestPendingSnapshotCanBeIgnored(t *testing.T) {
	snapshotC := make(chan rsm.SSRequest, 1)
	ps := newPendingSnapshot(snapshotC)
	ss, err := ps.request(rsm.UserRequested, "", false, 0, 0, 100)
	require.NoError(t, err)
	require.NotNil(t, ss)

	ps.apply(ss.key, true, false, 123)
	select {
	case v := <-ss.ResultC():
		assert.Equal(t, uint64(0), v.SnapshotIndex())
		assert.True(t, v.Rejected())
	default:
		assert.Fail(t, "ss is not applied")
	}
}

func TestPendingSnapshotIsIdentifiedByTheKey(t *testing.T) {
	snapshotC := make(chan rsm.SSRequest, 1)
	ps := newPendingSnapshot(snapshotC)
	ss, err := ps.request(rsm.UserRequested, "", false, 0, 0, 100)
	require.NoError(t, err)
	require.NotNil(t, ss)
	require.NotNil(t, ps.pending)

	ps.apply(ss.key+1, false, false, 123)
	assert.NotNil(t, ps.pending)
	select {
	case <-ss.ResultC():
		assert.Fail(t, "unexpectedly notified")
	default:
	}
}

func TestSnapshotCanNotBeRequestedAfterClose(t *testing.T) {
	snapshotC := make(chan rsm.SSRequest, 1)
	ps := newPendingSnapshot(snapshotC)
	ps.close()
	ss, err := ps.request(rsm.UserRequested, "", false, 0, 0, 100)
	assert.Equal(t, ErrShardClosed, err)
	assert.Nil(t, ss)
}

func TestCompactionOverheadDetailsIsRecorded(t *testing.T) {
	snapshotC := make(chan rsm.SSRequest, 1)
	ps := newPendingSnapshot(snapshotC)
	_, err := ps.request(rsm.UserRequested, "", true, 123, 0, 100)
	assert.NoError(t, err)

	select {
	case req := <-snapshotC:
		assert.True(t, req.OverrideCompaction)
		assert.Equal(t, uint64(123), req.CompactionOverhead)
	default:
		assert.Fail(t, "snapshot request not available")
	}
}

func getPendingConfigChange(notifyCommit bool) (pendingConfigChange,
	chan configChangeRequest) {
	c := make(chan configChangeRequest, 1)
	return newPendingConfigChange(c, notifyCommit), c
}

func TestRequestStateRelease(t *testing.T) {
	rs := RequestState{
		key:         100,
		clientID:    200,
		seriesID:    300,
		respondedTo: 400,
		deadline:    500,
		node:        &node{},
		pool:        &sync.Pool{},
	}
	rs.readyToRead.set()
	rs.readyToRelease.set()
	exp := RequestState{pool: rs.pool}
	rs.Release()
	assert.Equal(t, exp, rs)
}

func TestRequestStateSetToReadyToReleaseOnceNotified(t *testing.T) {
	rs := RequestState{
		CompletedC: make(chan RequestResult, 1),
	}
	assert.False(t, rs.readyToRelease.ready())
	rs.notify(RequestResult{})
	assert.True(t, rs.readyToRelease.ready())
}

func TestReleasingNotReadyRequestStateWillBeIgnored(t *testing.T) {
	rs := RequestState{
		key:         100,
		clientID:    200,
		seriesID:    300,
		respondedTo: 400,
		deadline:    500,
		node:        &node{},
		pool:        &sync.Pool{},
	}
	rsBefore := rs
	rs.Release()
	assert.Equal(t, rsBefore, rs)
}

func TestPendingConfigChangeCanBeCreatedAndClosed(t *testing.T) {
	pcc, c := getPendingConfigChange(false)
	select {
	case <-c:
		assert.Fail(t, "unexpected content in confChangeC")
	default:
	}

	pcc.close()
	select {
	case _, ok := <-c:
		assert.False(t, ok, "channel should be closed")
	default:
		assert.Fail(t, "missing closed signal")
	}
}

func TestCanNotMakeRequestOnClosedPendingConfigChange(t *testing.T) {
	pcc, _ := getPendingConfigChange(false)
	pcc.close()
	_, err := pcc.request(pb.ConfigChange{}, 100)
	assert.Equal(t, ErrShardClosed, err)
}

func TestConfigChangeCanBeRequested(t *testing.T) {
	pcc, c := getPendingConfigChange(false)
	var cc pb.ConfigChange
	rs, err := pcc.request(cc, 100)
	assert.NoError(t, err)
	assert.NotNil(t, rs)
	assert.NotNil(t, pcc.pending)
	assert.Len(t, c, 1)

	_, err = pcc.request(cc, 100)
	assert.Error(t, err)
	assert.Equal(t, ErrSystemBusy, err)

	pcc.close()
	select {
	case v := <-rs.ResultC():
		assert.True(t, v.Terminated())
	default:
		assert.Fail(t, "expect to return something")
	}
}

func TestPendingConfigChangeCanReturnBusy(t *testing.T) {
	pcc, _ := getPendingConfigChange(false)
	var cc pb.ConfigChange
	_, err := pcc.request(cc, 100)
	assert.NoError(t, err)

	_, err = pcc.request(cc, 100)
	assert.Equal(t, ErrSystemBusy, err)
}

func TestConfigChangeCanExpire(t *testing.T) {
	pcc, _ := getPendingConfigChange(false)
	var cc pb.ConfigChange
	tickCount := uint64(100)
	rs, err := pcc.request(cc, tickCount)
	assert.NoError(t, err)

	for i := uint64(0); i < tickCount; i++ {
		pcc.tick(i)
		pcc.gc()
	}
	select {
	case <-rs.ResultC():
		assert.Fail(t, "not suppose to has anything at this stage")
	default:
	}

	for i := uint64(0); i < defaultGCTick+1; i++ {
		pcc.tick(i + tickCount)
		pcc.gc()
	}
	select {
	case v, ok := <-rs.ResultC():
		assert.True(t, ok)
		assert.True(t, v.Timeout())
	default:
		assert.Fail(t, "expect to be expired")
	}
}

func TestCommittedConfigChangeRequestCanBeNotified(t *testing.T) {
	pcc, _ := getPendingConfigChange(true)
	var cc pb.ConfigChange
	rs, err := pcc.request(cc, 100)
	assert.NoError(t, err)
	pcc.committed(rs.key)
	select {
	case <-rs.committedC:
	default:
		assert.Fail(t, "committedC not signalled")
	}
}

func TestCompletedConfigChangeRequestCanBeNotified(t *testing.T) {
	pcc, _ := getPendingConfigChange(false)
	var cc pb.ConfigChange
	rs, err := pcc.request(cc, 100)
	assert.NoError(t, err)

	select {
	case <-rs.ResultC():
		assert.Fail(t, "not suppose to return anything yet")
	default:
	}

	pcc.apply(rs.key, false)
	select {
	case v := <-rs.ResultC():
		assert.True(t, v.Completed())
	default:
		assert.Fail(t, "suppose to return something")
	}
	assert.Nil(t, pcc.pending)
}

func TestConfigChangeRequestCanNotBeNotifiedWithDifferentKey(t *testing.T) {
	pcc, _ := getPendingConfigChange(false)
	var cc pb.ConfigChange
	rs, err := pcc.request(cc, 100)
	assert.NoError(t, err)

	select {
	case <-rs.ResultC():
		assert.Fail(t, "not suppose to return anything yet")
	default:
	}

	pcc.apply(rs.key+1, false)
	select {
	case <-rs.ResultC():
		assert.Fail(t, "unexpectedly notified")
	default:
	}
	assert.NotNil(t, pcc.pending)
}

func TestConfigChangeCanBeDropped(t *testing.T) {
	pcc, _ := getPendingConfigChange(false)
	var cc pb.ConfigChange
	rs, err := pcc.request(cc, 100)
	assert.NoError(t, err)

	select {
	case <-rs.ResultC():
		assert.Fail(t, "not suppose to return anything yet")
	default:
	}

	pcc.dropped(rs.key)
	select {
	case v := <-rs.ResultC():
		assert.True(t, v.Dropped())
	default:
		assert.Fail(t, "not dropped")
	}
	assert.Nil(t, pcc.pending)
}

func TestConfigChangeWithDifferentKeyWillNotBeDropped(t *testing.T) {
	pcc, _ := getPendingConfigChange(false)
	var cc pb.ConfigChange
	rs, err := pcc.request(cc, 100)
	assert.NoError(t, err)

	select {
	case <-rs.ResultC():
		assert.Fail(t, "not suppose to return anything yet")
	default:
	}

	pcc.dropped(rs.key + 1)
	select {
	case <-rs.ResultC():
		assert.Fail(t, "CompletedC unexpectedly set")
	default:
	}
	assert.NotNil(t, pcc.pending)
}

//
// pending proposal
//

func getPendingProposal(notifyCommit bool) (pendingProposal, *entryQueue) {
	c := newEntryQueue(5, 0)
	p := &sync.Pool{}
	p.New = func() interface{} {
		obj := &RequestState{}
		obj.pool = p
		obj.CompletedC = make(chan RequestResult, 1)
		if notifyCommit {
			obj.committedC = make(chan RequestResult, 1)
		}
		return obj
	}
	cfg := config.Config{ShardID: 100, ReplicaID: 120}
	return newPendingProposal(cfg, notifyCommit, p, c), c
}

func getBlankTestSession() *client.Session {
	return &client.Session{}
}

func TestPendingProposalCanBeCreatedAndClosed(t *testing.T) {
	pp, c := getPendingProposal(false)
	assert.Empty(t, c.get(false))
	pp.close()
	assert.True(t, c.stopped)
}

func countPendingProposal(p pendingProposal) int {
	total := 0
	for i := uint64(0); i < p.ps; i++ {
		total += len(p.shards[i].pending)
	}
	return total
}

func TestProposalCanBeProposed(t *testing.T) {
	pp, c := getPendingProposal(false)
	rs, err := pp.propose(getBlankTestSession(), []byte("test data"), 100)
	require.NoError(t, err)
	assert.Equal(t, 1, countPendingProposal(pp))

	select {
	case <-rs.ResultC():
		assert.Fail(t, "not suppose to have anything completed")
	default:
	}

	q := c.get(false)
	assert.Len(t, q, 1)
	pp.close()

	select {
	case v := <-rs.ResultC():
		assert.True(t, v.Terminated())
	default:
		assert.Fail(t, "suppose to return terminated")
	}
}

func TestProposeOnClosedPendingProposalReturnError(t *testing.T) {
	pp, _ := getPendingProposal(false)
	pp.close()
	_, err := pp.propose(getBlankTestSession(), []byte("test data"), 100)
	assert.Equal(t, ErrShardClosed, err)
}

func TestProposalCanBeCompleted(t *testing.T) {
	pp, _ := getPendingProposal(false)
	rs, err := pp.propose(getBlankTestSession(), []byte("test data"), 100)
	require.NoError(t, err)
	pp.applied(rs.clientID, rs.seriesID, rs.key+1, sm.Result{}, false)

	select {
	case <-rs.ResultC():
		assert.Fail(t, "unexpected applied proposal with invalid client ID")
	default:
	}
	assert.NotZero(t, countPendingProposal(pp))

	pp.applied(rs.clientID, rs.seriesID, rs.key, sm.Result{}, false)
	select {
	case v := <-rs.ResultC():
		assert.True(t, v.Completed())
	default:
		assert.Fail(t, "expect to get complete signal")
	}
	assert.Zero(t, countPendingProposal(pp))
}

func TestProposalCanBeDropped(t *testing.T) {
	pp, _ := getPendingProposal(false)
	rs, err := pp.propose(getBlankTestSession(), []byte("test data"), 100)
	require.NoError(t, err)

	pp.dropped(rs.clientID, rs.seriesID, rs.key)
	select {
	case v := <-rs.ResultC():
		assert.True(t, v.Dropped())
	default:
		assert.Fail(t, "not notified")
	}

	for _, shard := range pp.shards {
		assert.Empty(t, shard.pending)
	}
}

func TestProposalResultCanBeObtainedByCaller(t *testing.T) {
	pp, _ := getPendingProposal(false)
	rs, err := pp.propose(getBlankTestSession(), []byte("test data"), 100)
	require.NoError(t, err)

	result := sm.Result{
		Value: 1234,
		Data:  make([]byte, 128),
	}
	_, err = rand.Read(result.Data)
	require.NoError(t, err)
	pp.applied(rs.clientID, rs.seriesID, rs.key, result, false)

	select {
	case v := <-rs.ResultC():
		assert.True(t, v.Completed())
		r := v.GetResult()
		assert.Equal(t, result, r)
	default:
		assert.Fail(t, "expect to get complete signal")
	}
}

func TestClientIDIsCheckedWhenApplyingProposal(t *testing.T) {
	pp, _ := getPendingProposal(false)
	rs, err := pp.propose(getBlankTestSession(), []byte("test data"), 100)
	require.NoError(t, err)

	pp.applied(rs.clientID+1, rs.seriesID, rs.key, sm.Result{}, false)
	select {
	case <-rs.ResultC():
		assert.Fail(t, "unexpected applied proposal with invalid client ID")
	default:
	}
	assert.NotZero(t, countPendingProposal(pp))

	pp.applied(rs.clientID, rs.seriesID, rs.key, sm.Result{}, false)
	select {
	case v := <-rs.ResultC():
		assert.True(t, v.Completed())
	default:
		assert.Fail(t, "expect to get complete signal")
	}
	assert.Zero(t, countPendingProposal(pp))
}

func TestSeriesIDIsCheckedWhenApplyingProposal(t *testing.T) {
	pp, _ := getPendingProposal(false)
	rs, err := pp.propose(getBlankTestSession(), []byte("test data"), 100)
	require.NoError(t, err)

	pp.applied(rs.clientID, rs.seriesID+1, rs.key, sm.Result{}, false)
	select {
	case <-rs.ResultC():
		assert.Fail(t, "unexpected applied proposal with invalid client ID")
	default:
	}
	assert.NotZero(t, countPendingProposal(pp))

	pp.applied(rs.clientID, rs.seriesID, rs.key, sm.Result{}, false)
	select {
	case v := <-rs.ResultC():
		assert.True(t, v.Completed())
	default:
		assert.Fail(t, "expect to get complete signal")
	}
	assert.Zero(t, countPendingProposal(pp))
}

func TestProposalCanBeCommitted(t *testing.T) {
	pp, _ := getPendingProposal(true)
	rs, err := pp.propose(getBlankTestSession(), []byte("test data"), 100)
	require.NoError(t, err)

	pp.committed(rs.clientID, rs.seriesID, rs.key)
	select {
	case <-rs.committedC:
	default:
		assert.Fail(t, "not committed")
	}
	assert.NotZero(t, countPendingProposal(pp))

	pp.applied(rs.clientID, rs.seriesID, rs.key, sm.Result{}, false)
	select {
	case v := <-rs.AppliedC():
		assert.True(t, v.Completed())
	default:
		assert.Fail(t, "expect to get complete signal")
	}
	assert.Zero(t, countPendingProposal(pp))
}

func TestProposalCanBeExpired(t *testing.T) {
	pp, _ := getPendingProposal(false)
	tickCount := uint64(100)
	rs, err := pp.propose(getBlankTestSession(), []byte("test data"), tickCount)
	require.NoError(t, err)

	for i := uint64(0); i < tickCount; i++ {
		pp.tick(i)
		pp.gc()
	}
	select {
	case <-rs.ResultC():
		assert.Fail(t, "not suppose to return anything")
	default:
	}

	for i := uint64(0); i < defaultGCTick+1; i++ {
		pp.tick(i + tickCount)
		pp.gc()
	}
	select {
	case v := <-rs.ResultC():
		assert.True(t, v.Timeout())
	default:
		assert.Fail(t, "result channel is empty")
	}
	assert.Zero(t, countPendingProposal(pp))
}

func TestProposalErrorsAreReported(t *testing.T) {
	pp, c := getPendingProposal(false)
	for i := 0; i < 5; i++ {
		_, err := pp.propose(getBlankTestSession(), []byte("test data"), 100)
		assert.NoError(t, err)
	}

	var cq []pb.Entry
	if c.leftInWrite {
		cq = c.left
	} else {
		cq = c.right
	}
	sz := len(cq)

	_, err := pp.propose(getBlankTestSession(), []byte("test data"), 100)
	assert.Equal(t, ErrSystemBusy, err)

	if c.leftInWrite {
		cq = c.left
	} else {
		cq = c.right
	}
	assert.Len(t, cq, sz)
}

func TestClosePendingProposalIgnoresStepEngineActivities(t *testing.T) {
	pp, _ := getPendingProposal(false)
	session := &client.Session{
		ClientID:    100,
		SeriesID:    200,
		RespondedTo: 199,
	}
	rs, _ := pp.propose(session, nil, 100)
	select {
	case <-rs.ResultC():
		require.FailNow(t, "completedC is already signalled")
	default:
	}

	for i := uint64(0); i < pp.ps; i++ {
		pp.shards[i].stopped = true
	}
	pp.applied(rs.clientID, rs.seriesID, rs.key, sm.Result{Value: 1}, false)

	select {
	case <-rs.ResultC():
		require.FailNow(t, "completedC unexpectedly signaled")
	default:
	}
}

func getPendingReadIndex() (pendingReadIndex, *readIndexQueue) {
	q := newReadIndexQueue(5)
	p := &sync.Pool{}
	p.New = func() interface{} {
		obj := &RequestState{}
		obj.pool = p
		obj.CompletedC = make(chan RequestResult, 1)
		return obj
	}
	return newPendingReadIndex(p, q), q
}

func TestPendingReadIndexCanBeCreatedAndClosed(t *testing.T) {
	pp, c := getPendingReadIndex()
	assert.Empty(t, c.get())
	pp.close()
	assert.True(t, c.stopped)
}

func TestCanNotMakeRequestOnClosedPendingReadIndex(t *testing.T) {
	pp, _ := getPendingReadIndex()
	pp.close()
	_, err := pp.read(100)
	assert.Equal(t, ErrShardClosed, err)
}

func TestPendingReadIndexCanRead(t *testing.T) {
	pp, c := getPendingReadIndex()
	rs, err := pp.read(100)
	require.NoError(t, err)

	select {
	case <-rs.ResultC():
		assert.Fail(t, "not suppose to return anything")
	default:
	}

	var q []*RequestState
	if c.leftInWrite {
		q = c.left[:c.idx]
	} else {
		q = c.right[:c.idx]
	}
	assert.Len(t, q, 1)
	assert.Equal(t, uint64(1), pp.requests.pendingSize())
	assert.Empty(t, pp.batches)

	pp.close()
	select {
	case v := <-rs.ResultC():
		assert.True(t, v.Terminated())
	default:
		assert.Fail(t, "not expected to be signaled")
	}
}

func TestPendingReadIndexCanReturnBusy(t *testing.T) {
	pri, _ := getPendingReadIndex()
	for i := 0; i < 6; i++ {
		_, err := pri.read(100)
		if i < 5 {
			assert.NoError(t, err)
		} else {
			assert.Equal(t, ErrSystemBusy, err)
		}
	}
}

func TestPendingReadIndexCanComplete(t *testing.T) {
	pp, _ := getPendingReadIndex()
	rs, err := pp.read(100)
	require.NoError(t, err)

	s := pp.nextCtx()
	pp.add(s, []*RequestState{rs})
	readState := pb.ReadyToRead{Index: 500, SystemCtx: s}
	pp.addReady([]pb.ReadyToRead{readState})
	pp.applied(499)
	select {
	case <-rs.ResultC():
		assert.Fail(t, "not expected to be signaled")
	default:
	}
	assert.False(t, rs.readyToRead.ready())

	pp.applied(500)
	assert.True(t, rs.readyToRead.ready())
	select {
	case v := <-rs.ResultC():
		assert.True(t, v.Completed())
	default:
		assert.Fail(t, "expect to complete")
	}
	assert.Empty(t, pp.batches)
}

func TestPendingReadIndexCanBeDropped(t *testing.T) {
	pp, _ := getPendingReadIndex()
	rs, err := pp.read(100)
	require.NoError(t, err)

	s := pp.nextCtx()
	pp.add(s, []*RequestState{rs})
	pp.dropped(s)
	select {
	case v := <-rs.ResultC():
		assert.True(t, v.Dropped())
	default:
		assert.Fail(t, "expect to complete")
	}
	assert.Empty(t, pp.batches)
}

func testPendingReadIndexCanExpire(t *testing.T, addReady bool) {
	pp, _ := getPendingReadIndex()
	rs, err := pp.read(100)
	require.NoError(t, err)

	s := pp.nextCtx()
	pp.add(s, []*RequestState{rs})
	if addReady {
		readState := pb.ReadyToRead{Index: 500, SystemCtx: s}
		pp.addReady([]pb.ReadyToRead{readState})
	}

	tickToWait := 100 + defaultGCTick + 1
	for i := uint64(0); i < tickToWait; i++ {
		pp.tick(i)
		pp.applied(499)
	}

	select {
	case v := <-rs.ResultC():
		assert.True(t, v.Timeout())
	default:
		assert.Fail(t, "expect to complete")
	}
	assert.Empty(t, pp.batches)
}

func TestPendingReadIndexCanExpire(t *testing.T) {
	testPendingReadIndexCanExpire(t, true)
}

func TestPendingReadIndexCanExpireWithoutCallingAddReady(t *testing.T) {
	testPendingReadIndexCanExpire(t, false)
}

func TestNonEmptyReadBatchIsNeverExpired(t *testing.T) {
	pp, _ := getPendingReadIndex()
	rs, err := pp.read(10000)
	require.NoError(t, err)

	s := pp.nextCtx()
	pp.add(s, []*RequestState{rs})
	require.Len(t, pp.batches, 1)

	tickToWait := defaultGCTick * 10
	for i := uint64(0); i < tickToWait; i++ {
		pp.tick(i)
		pp.applied(499)
	}
	assert.Len(t, pp.batches, 1)
}

func TestProposalAllocationCount(t *testing.T) {
	sz := 128
	data := make([]byte, sz)
	p := &sync.Pool{}
	p.New = func() interface{} {
		obj := &RequestState{}
		obj.CompletedC = make(chan RequestResult, 1)
		obj.pool = p
		return obj
	}
	total := uint32(0)
	q := newEntryQueue(2048, 0)
	cfg := config.Config{ShardID: 1, ReplicaID: 1}
	pp := newPendingProposal(cfg, false, p, q)
	session := client.NewNoOPSession(1, random.LockGuardedRand)
	ac := testing.AllocsPerRun(10000, func() {
		v := atomic.AddUint32(&total, 1)
		rs, err := pp.propose(session, data, 100)
		require.NoError(t, err)
		if v%128 == 0 {
			atomic.StoreUint32(&total, 0)
			q.get(false)
		}
		pp.applied(rs.key, rs.clientID, rs.seriesID, sm.Result{Value: 1}, false)
		rs.readyToRelease.set()
		rs.Release()
	})
	assert.LessOrEqual(t, ac, float64(1))
}

func TestReadIndexAllocationCount(t *testing.T) {
	p := &sync.Pool{}
	p.New = func() interface{} {
		obj := &RequestState{}
		obj.CompletedC = make(chan RequestResult, 1)
		obj.pool = p
		return obj
	}
	total := uint32(0)
	q := newReadIndexQueue(2048)
	pri := newPendingReadIndex(p, q)
	ac := testing.AllocsPerRun(10000, func() {
		v := atomic.AddUint32(&total, 1)
		rs, err := pri.read(100)
		require.NoError(t, err)
		if v%128 == 0 {
			atomic.StoreUint32(&total, 0)
			q.get()
		}
		rs.readyToRelease.set()
		rs.Release()
	})
	assert.Equal(t, float64(0), ac)
}

func TestPendingRaftLogQueryCanBeCreated(t *testing.T) {
	p := newPendingRaftLogQuery()
	assert.Nil(t, p.mu.pending)
}

func TestPendingRaftLogQueryCanBeClosed(t *testing.T) {
	p := newPendingRaftLogQuery()
	rs, err := p.add(100, 200, 300)
	require.NoError(t, err)
	require.NotNil(t, p.mu.pending)
	p.close()
	assert.Nil(t, p.mu.pending)
	select {
	case v := <-rs.CompletedC:
		assert.True(t, v.Terminated())
	default:
		assert.Fail(t, "not terminated")
	}
}

func TestPendingRaftLogQueryCanAddRequest(t *testing.T) {
	p := newPendingRaftLogQuery()
	rs, err := p.add(100, 200, 300)
	assert.NotNil(t, rs)
	assert.NoError(t, err)

	rs, err = p.add(200, 200, 300)
	assert.Equal(t, ErrSystemBusy, err)
	assert.Nil(t, rs)

	assert.NotNil(t, p.mu.pending)
	expectedRange := LogRange{FirstIndex: 100, LastIndex: 200}
	assert.Equal(t, expectedRange, p.mu.pending.logRange)
	assert.Equal(t, uint64(300), p.mu.pending.maxSize)
	assert.NotNil(t, p.mu.pending.CompletedC)
}

func TestPendingRaftLogQueryGet(t *testing.T) {
	p := newPendingRaftLogQuery()
	assert.Nil(t, p.get())

	rs, err := p.add(100, 200, 300)
	require.NoError(t, err)
	require.NotNil(t, p.mu.pending)

	result := p.get()
	assert.Equal(t, rs, result)
	expectedRange := LogRange{FirstIndex: 100, LastIndex: 200}
	assert.Equal(t, expectedRange, result.logRange)
	assert.Equal(t, uint64(300), result.maxSize)
	assert.NotNil(t, result.CompletedC)
}

func TestPendingRaftLogQueryGetWhenReturnedIsCalledWithoutPendingRequest(
	t *testing.T,
) {
	require.Panics(t, func() {
		p := newPendingRaftLogQuery()
		p.returned(false, LogRange{}, nil)
	})
}

func TestPendingRaftLogQueryCanReturnOutOfRangeError(t *testing.T) {
	p := newPendingRaftLogQuery()
	rs, err := p.add(100, 200, 300)
	require.NoError(t, err)
	lr := LogRange{FirstIndex: 150, LastIndex: 200}
	p.returned(true, lr, nil)
	select {
	case v := <-rs.CompletedC:
		assert.True(t, v.RequestOutOfRange())
		_, rrl := v.RaftLogs()
		assert.Equal(t, lr, rrl)
	default:
		assert.Fail(t, "no result available")
	}
}

func TestPendingRaftLogQueryCanReturnResults(t *testing.T) {
	p := newPendingRaftLogQuery()
	rs, err := p.add(100, 200, 300)
	require.NoError(t, err)
	entries := []pb.Entry{{Index: 1}, {Index: 2}}
	lr := LogRange{FirstIndex: 100, LastIndex: 180}
	p.returned(false, lr, entries)
	select {
	case v := <-rs.CompletedC:
		assert.True(t, v.Completed())
		rentries, rrl := v.RaftLogs()
		assert.Equal(t, lr, rrl)
		assert.Equal(t, entries, rentries)
	default:
		assert.Fail(t, "no result available")
	}
}
````

## File: request.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"crypto/sha512"
	"encoding/binary"
	"fmt"
	"math/rand"
	"os"
	"sync"
	"sync/atomic"
	"time"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/random"

	"github.com/lni/dragonboat/v4/client"
	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/rsm"
	"github.com/lni/dragonboat/v4/internal/settings"
	"github.com/lni/dragonboat/v4/logger"
	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
)

var (
	defaultGCTick         uint64 = 2
	pendingProposalShards        = settings.Soft.PendingProposalShards
)

var (
	plog = logger.GetLogger("dragonboat")
)

var (
	// ErrInvalidOption indicates that the specified option is invalid.
	ErrInvalidOption = errors.New("invalid option")
	// ErrInvalidOperation indicates that the requested operation is not allowed.
	// e.g. making read or write requests on witness node are not allowed.
	ErrInvalidOperation = errors.New("invalid operation")
	// ErrInvalidAddress indicates that the specified address is invalid.
	ErrInvalidAddress = errors.New("invalid address")
	// ErrInvalidSession indicates that the specified client session is invalid.
	ErrInvalidSession = errors.New("invalid session")
	// ErrTimeoutTooSmall indicates that the specified timeout value is too small.
	ErrTimeoutTooSmall = errors.New("specified timeout value is too small")
	// ErrPayloadTooBig indicates that the payload is too big.
	ErrPayloadTooBig = errors.New("payload is too big")
	// ErrSystemBusy indicates that the system is too busy to handle the request.
	// This might be caused when the Raft node reached its MaxInMemLogSize limit
	// or other system limits. For a requested snapshot, leadership transfer or
	// Raft config change operation, ErrSystemBusy means there is already such a
	// request waiting to be processed.
	ErrSystemBusy = errors.New("system is too busy try again later")
	// ErrShardClosed indicates that the requested shard is being shut down.
	ErrShardClosed = errors.New("raft shard already closed")
	// ErrShardNotInitialized indicates that the requested operation can not be
	// completed as the involved raft shard has not been initialized yet.
	ErrShardNotInitialized = errors.New("raft shard not initialized yet")
	// ErrTimeout indicates that the operation timed out.
	ErrTimeout = errors.New("timeout")
	// ErrCanceled indicates that the request has been canceled.
	ErrCanceled = errors.New("request canceled")
	// ErrRejected indicates that the request has been rejected.
	ErrRejected = errors.New("request rejected")
	// ErrAborted indicates that the request has been aborted, usually by user
	// defined behaviours.
	ErrAborted = errors.New("request aborted")
	// ErrShardNotReady indicates that the request has been dropped as the
	// specified raft shard is not ready to handle the request. Unknown leader
	// is the most common cause of this Error, trying to use a shard not fully
	// initialized is another major cause of ErrShardNotReady.
	ErrShardNotReady = errors.New("request dropped as the shard is not ready")
	// ErrInvalidTarget indicates that the specified node id invalid.
	ErrInvalidTarget = errors.New("invalid target node ID")
)

// IsTempError returns a boolean value indicating whether the specified error
// is a temporary error that worth to be retried later with the exact same
// input, potentially on a more suitable NodeHost instance.
func IsTempError(err error) bool {
	return errors.Is(err, ErrSystemBusy) ||
		errors.Is(err, ErrShardClosed) ||
		errors.Is(err, ErrShardNotInitialized) ||
		errors.Is(err, ErrShardNotReady) ||
		errors.Is(err, ErrTimeout) ||
		errors.Is(err, ErrClosed) ||
		errors.Is(err, ErrAborted)
}

// LogRange defines the range [FirstIndex, lastIndex) of the raft log.
type LogRange struct {
	FirstIndex uint64
	LastIndex  uint64
}

// RequestResultCode is the result code returned to the client to indicate the
// outcome of the request.
type RequestResultCode int

// RequestResult is the result struct returned for the request.
type RequestResult struct {
	// code is the result state of the request.
	code RequestResultCode
	// Result is the returned result from the Update method of the state machine
	// instance. Result is only available when making a proposal and the Code
	// value is RequestCompleted.
	result         sm.Result
	entries        []pb.Entry
	logRange       LogRange
	snapshotResult bool
	logQueryResult bool
}

// RequestOutOfRange returns a boolean value indicating whether the request
// is out of range.
func (rr *RequestResult) RequestOutOfRange() bool {
	return rr.code == requestOutOfRange
}

// Timeout returns a boolean value indicating whether the request timed out.
func (rr *RequestResult) Timeout() bool {
	return rr.code == requestTimeout
}

// Committed returns a boolean value indicating whether the request has been
// committed by Raft.
func (rr *RequestResult) Committed() bool {
	return rr.code == requestCompleted || rr.code == requestCommitted
}

// Completed returns a boolean value indicating whether the request completed
// successfully. For proposals, it means the proposal has been committed by the
// Raft shard and applied on the local node. For ReadIndex operation, it means
// the shard is now ready for a local read.
func (rr *RequestResult) Completed() bool {
	return rr.code == requestCompleted
}

// Terminated returns a boolean value indicating the request terminated due to
// the requested Raft shard is being shut down.
func (rr *RequestResult) Terminated() bool {
	return rr.code == requestTerminated
}

// Aborted returns a boolean value indicating the request is aborted.
func (rr *RequestResult) Aborted() bool {
	return rr.code == requestAborted
}

// Rejected returns a boolean value indicating the request is rejected. For a
// proposal, it means that the used client session instance is not registered
// or it has been evicted on the server side. When requesting a client session
// to be registered, Rejected means the another client session with the same
// client ID has already been registered. When requesting a client session to
// be unregistered, Rejected means the specified client session is not found
// on the server side. For a membership change request, it means the request
// is out of order and thus not applied.
func (rr *RequestResult) Rejected() bool {
	return rr.code == requestRejected
}

// Dropped returns a boolean flag indicating whether the request has been
// dropped as the leader is unavailable or not ready yet. Such dropped requests
// can usually be retried once the leader is ready.
func (rr *RequestResult) Dropped() bool {
	return rr.code == requestDropped
}

// SnapshotIndex returns the index of the generated snapshot when the
// RequestResult is from a snapshot related request. Invoking this method on
// RequestResult instances not related to snapshots will cause panic.
func (rr *RequestResult) SnapshotIndex() uint64 {
	if !rr.snapshotResult {
		plog.Panicf("not a snapshot request result")
	}
	return rr.result.Value
}

// RaftLogs returns the raft log query result.
func (rr *RequestResult) RaftLogs() ([]pb.Entry, LogRange) {
	if !rr.logQueryResult {
		panic("not a raft log query result")
	}
	return rr.entries, rr.logRange
}

// GetResult returns the result value of the request. When making a proposal,
// the returned result is the value returned by the Update method of the
// IStateMachine instance. Returned result is only valid if the RequestResultCode
// value is RequestCompleted.
func (rr *RequestResult) GetResult() sm.Result {
	return rr.result
}

const (
	requestTimeout RequestResultCode = iota
	requestCompleted
	requestTerminated
	requestRejected
	requestDropped
	requestAborted
	requestCommitted
	requestOutOfRange
)

var requestResultCodeName = [...]string{
	"RequestTimeout",
	"RequestCompleted",
	"RequestTerminated",
	"RequestRejected",
	"RequestDropped",
	"RequestAborted",
	"RequestCommitted",
	"RequestOutOfRange",
}

func (c RequestResultCode) String() string {
	return requestResultCodeName[uint64(c)]
}

type logicalClock struct {
	ltick      uint64
	lastGcTime uint64
	gcTick     uint64
}

func newLogicalClock() logicalClock {
	if defaultGCTick == 0 || defaultGCTick > 3 {
		plog.Panicf("invalid defaultGCTick %d", defaultGCTick)
	}
	return logicalClock{gcTick: defaultGCTick}
}

func (p *logicalClock) tick(tick uint64) {
	atomic.StoreUint64(&p.ltick, tick)
}

func (p *logicalClock) getTick() uint64 {
	return atomic.LoadUint64(&p.ltick)
}

type ready struct {
	val uint32
}

func (r *ready) ready() bool {
	return atomic.LoadUint32(&r.val) == 1
}

func (r *ready) clear() {
	atomic.StoreUint32(&r.val, 0)
}

func (r *ready) set() {
	atomic.StoreUint32(&r.val, 1)
}

// SysOpState is the object used to provide system maintenance operation result
// to users.
type SysOpState struct {
	completedC <-chan struct{}
}

// CompletedC returns a struct{} chan that is closed when the requested
// operation is completed.
//
// Deprecated: CompletedC() has been deprecated. Use ResultC() instead.
func (o *SysOpState) CompletedC() <-chan struct{} {
	return o.completedC
}

// ResultC returns a struct{} chan that is closed when the requested
// operation is completed.
func (o *SysOpState) ResultC() <-chan struct{} {
	return o.completedC
}

// RequestState is the object used to provide request result to users.
type RequestState struct {
	key            uint64
	clientID       uint64
	seriesID       uint64
	respondedTo    uint64
	deadline       uint64
	logRange       LogRange
	maxSize        uint64
	readyToRead    ready
	readyToRelease ready
	aggrC          chan RequestResult
	committedC     chan RequestResult
	// CompletedC is a channel for delivering request result to users.
	//
	// Deprecated: CompletedC has been deprecated. Use ResultC() or AppliedC()
	// instead.
	CompletedC   chan RequestResult
	node         *node
	pool         *sync.Pool
	notifyCommit bool
	testErr      chan struct{}
}

// AppliedC returns a channel of RequestResult for delivering request result.
// The returned channel reports the final outcomes of proposals and config
// changes, the return value can be of one of the Completed(), Dropped(),
// Timeout(), Rejected(), Terminated() or Aborted() values.
//
// Use ResultC() when the client wants to be notified when proposals or config
// changes are committed.
func (r *RequestState) AppliedC() chan RequestResult {
	return r.CompletedC
}

// ResultC returns a channel of RequestResult for delivering request results to
// users. When NotifyCommit is not enabled, the behaviour of the returned
// channel is the same as the one returned by the AppliedC() method. When
// NotifyCommit is enabled, up to two RequestResult values can be received from
// the returned channel. For example, for a successful proposal that is
// eventually committed and applied, the returned chan RequestResult will return
// a RequestResult value to indicate the proposal is committed first, it will be
// followed by another RequestResult value indicating the proposal has been
// applied into the state machine.
//
// Use AppliedC() when your client don't need extra notification when proposals
// and config changes are committed.
func (r *RequestState) ResultC() chan RequestResult {
	if !r.notifyCommit {
		return r.CompletedC
	}
	if r.committedC == nil {
		plog.Panicf("committedC is nil")
	}
	if r.aggrC != nil {
		return r.aggrC
	}
	r.aggrC = make(chan RequestResult, 2)
	tryBridgeCommittedC := func() {
		select {
		case cn := <-r.committedC:
			if cn.code != requestCommitted {
				plog.Panicf("unexpected requestResult, %s", cn.code)
			}
			r.aggrC <- cn
		default:
		}
	}
	go func() {
		if r.testErr != nil {
			defer func() {
				if rr := recover(); rr != nil {
					close(r.testErr)
				}
			}()
		}
		select {
		case cn := <-r.committedC:
			if cn.code != requestCommitted {
				plog.Panicf("unexpected requestResult, %s", cn.code)
			}
			r.aggrC <- cn
			cc := <-r.CompletedC
			if cc.Dropped() {
				plog.Panicf("committed entry dropped")
			}
			if cc.Aborted() {
				plog.Panicf("committed entry aborted")
			}
			if cc.code == requestCommitted {
				plog.Panicf("entry committed notified twice")
			}
			r.aggrC <- cc
		case cc := <-r.CompletedC:
			if cc.Aborted() {
				// this select is to make the test TestResultCCanReceiveRequestResults
				// easier to implement for the input
				// {true, true, requestAborted, true}
				tryBridgeCommittedC()
				plog.Panicf("requestAborted sent to CompletedC")
			}
			if cc.code == requestCommitted {
				tryBridgeCommittedC()
				plog.Panicf("requestCommitted sent to CompletedC")
			}
			select {
			case ccn := <-r.committedC:
				if cc.Dropped() {
					r.aggrC <- ccn
					plog.Panicf("applied entry dropped")
				}
				r.aggrC <- ccn
			default:
			}
			r.aggrC <- cc
		}
	}()
	return r.aggrC
}

func (r *RequestState) committed() {
	if !r.notifyCommit {
		plog.Panicf("notify commit not allowed")
	}
	if r.committedC == nil {
		plog.Panicf("committedC is nil")
	}
	select {
	case r.committedC <- RequestResult{code: requestCommitted}:
	default:
		plog.Panicf("RequestState.committedC is full")
	}
}

func (r *RequestState) timeout() {
	r.notify(RequestResult{code: requestTimeout})
}

func (r *RequestState) terminated() {
	r.notify(RequestResult{code: requestTerminated})
}

func (r *RequestState) dropped() {
	r.notify(RequestResult{code: requestDropped})
}

func (r *RequestState) notify(result RequestResult) {
	select {
	case r.CompletedC <- result:
		r.readyToRelease.set()
	default:
		plog.Panicf("RequestState.CompletedC is full")
	}
}

// Release puts the RequestState instance back to an internal pool so it can be
// reused. Release is normally called after all RequestResult values have been
// received from the ResultC() channel.
func (r *RequestState) Release() {
	if r.pool != nil {
		if !r.readyToRelease.ready() {
			return
		}
		r.notifyCommit = false
		r.logRange = LogRange{}
		r.maxSize = 0
		r.deadline = 0
		r.key = 0
		r.seriesID = 0
		r.clientID = 0
		r.respondedTo = 0
		r.node = nil
		r.readyToRead.clear()
		r.readyToRelease.clear()
		r.aggrC = nil
		r.pool.Put(r)
	}
}

func (r *RequestState) reuse(notifyCommit bool) {
	if r.aggrC != nil {
		plog.Panicf("aggrC not nil")
	}
	if len(r.CompletedC) > 0 || r.CompletedC == nil {
		r.CompletedC = make(chan RequestResult, 1)
	}
	if notifyCommit {
		if len(r.committedC) > 0 || r.committedC == nil {
			r.committedC = make(chan RequestResult, 1)
		}
	} else {
		r.committedC = nil
	}
}

func (r *RequestState) mustBeReadyForLocalRead() {
	if r.node == nil {
		plog.Panicf("invalid rs")
	}
	if !r.node.initialized() {
		plog.Panicf("%s not initialized", r.node.id())
	}
	if !r.readyToRead.ready() {
		plog.Panicf("not ready for local read")
	}
}

type proposalShard struct {
	mu             sync.Mutex
	proposals      *entryQueue
	pending        map[uint64]*RequestState
	pool           *sync.Pool
	cfg            config.Config
	stopped        bool
	notifyCommit   bool
	expireNotified uint64
	logicalClock
}

type keyGenerator struct {
	randMu sync.Mutex
	rand   *rand.Rand
}

func (k *keyGenerator) nextKey() uint64 {
	k.randMu.Lock()
	v := k.rand.Uint64()
	k.randMu.Unlock()
	return v
}

type pendingProposal struct {
	shards []*proposalShard
	keyg   []*keyGenerator
	ps     uint64
}

type readBatch struct {
	index    uint64
	requests []*RequestState
}

type pendingReadIndex struct {
	mu       sync.Mutex
	batches  map[pb.SystemCtx]readBatch
	requests *readIndexQueue
	stopped  bool
	pool     *sync.Pool
	logicalClock
}

type configChangeRequest struct {
	data []byte
	key  uint64
}

type pendingConfigChange struct {
	mu           sync.Mutex
	pending      *RequestState
	confChangeC  chan<- configChangeRequest
	notifyCommit bool
	logicalClock
}

type pendingSnapshot struct {
	mu        sync.Mutex
	pending   *RequestState
	snapshotC chan<- rsm.SSRequest
	logicalClock
}

type pendingLeaderTransfer struct {
	leaderTransferC chan uint64
}

func newPendingLeaderTransfer() pendingLeaderTransfer {
	return pendingLeaderTransfer{
		leaderTransferC: make(chan uint64, 1),
	}
}

func (l *pendingLeaderTransfer) request(target uint64) error {
	if target == pb.NoNode {
		return ErrInvalidTarget
	}
	select {
	case l.leaderTransferC <- target:
	default:
		return ErrSystemBusy
	}
	return nil
}

func (l *pendingLeaderTransfer) get() (uint64, bool) {
	select {
	case v := <-l.leaderTransferC:
		return v, true
	default:
	}
	return 0, false
}

func newPendingSnapshot(snapshotC chan<- rsm.SSRequest) pendingSnapshot {
	return pendingSnapshot{
		logicalClock: newLogicalClock(),
		snapshotC:    snapshotC,
	}
}

func (p *pendingSnapshot) notify(r RequestResult) {
	r.snapshotResult = true
	p.pending.notify(r)
}

func (p *pendingSnapshot) close() {
	p.mu.Lock()
	defer p.mu.Unlock()
	p.snapshotC = nil
	if p.pending != nil {
		p.pending.terminated()
		p.pending = nil
	}
}

func (p *pendingSnapshot) request(st rsm.SSReqType,
	path string, override bool, overhead uint64, index uint64,
	timeoutTick uint64) (*RequestState, error) {
	if timeoutTick == 0 {
		return nil, ErrTimeoutTooSmall
	}
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.pending != nil {
		return nil, ErrSystemBusy
	}
	if p.snapshotC == nil {
		return nil, ErrShardClosed
	}
	ssreq := rsm.SSRequest{
		Type:               st,
		Path:               path,
		Key:                random.LockGuardedRand.Uint64(),
		OverrideCompaction: override,
		CompactionOverhead: overhead,
		CompactionIndex:    index,
	}
	req := &RequestState{
		key:          ssreq.Key,
		deadline:     p.getTick() + timeoutTick,
		CompletedC:   make(chan RequestResult, 1),
		notifyCommit: false,
	}
	select {
	case p.snapshotC <- ssreq:
		p.pending = req
		return req, nil
	default:
	}
	return nil, ErrSystemBusy
}

func (p *pendingSnapshot) gc() {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.pending == nil {
		return
	}
	now := p.getTick()
	// FIXME:
	// golangci-lint v1.23 complains that lastGcTime is not used unless lastGcTime
	// is accessed as a member of the logicalClock. v1.33's typecheck is pretty
	// broken, preventing us from upgrading.
	if now-p.lastGcTime < p.gcTick {
		return
	}
	p.lastGcTime = now
	if p.pending.deadline < now {
		p.pending.timeout()
		p.pending = nil
	}
}

func (p *pendingSnapshot) apply(key uint64,
	ignored bool, aborted bool, index uint64) {
	if ignored && aborted {
		plog.Panicf("ignored && aborted")
	}
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.pending == nil {
		return
	}
	if p.pending.key == key {
		r := RequestResult{}
		if ignored {
			r.code = requestRejected
		} else if aborted {
			r.code = requestAborted
		} else {
			r.code = requestCompleted
			r.result.Value = index
		}
		p.notify(r)
		p.pending = nil
	}
}

func newPendingConfigChange(confChangeC chan<- configChangeRequest,
	notifyCommit bool) pendingConfigChange {
	return pendingConfigChange{
		confChangeC:  confChangeC,
		logicalClock: newLogicalClock(),
		notifyCommit: notifyCommit,
	}
}

func (p *pendingConfigChange) close() {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.confChangeC != nil {
		if p.pending != nil {
			p.pending.terminated()
			p.pending = nil
		}
		close(p.confChangeC)
		p.confChangeC = nil
	}
}

func (p *pendingConfigChange) request(cc pb.ConfigChange,
	timeoutTick uint64) (*RequestState, error) {
	if timeoutTick == 0 {
		return nil, ErrTimeoutTooSmall
	}
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.pending != nil {
		return nil, ErrSystemBusy
	}
	if p.confChangeC == nil {
		return nil, ErrShardClosed
	}
	data := pb.MustMarshal(&cc)
	ccreq := configChangeRequest{
		key:  random.LockGuardedRand.Uint64(),
		data: data,
	}
	req := &RequestState{
		key:          ccreq.key,
		deadline:     p.getTick() + timeoutTick,
		CompletedC:   make(chan RequestResult, 1),
		notifyCommit: p.notifyCommit,
	}
	if p.notifyCommit {
		req.committedC = make(chan RequestResult, 1)
	}
	select {
	case p.confChangeC <- ccreq:
		p.pending = req
		return req, nil
	default:
	}
	return nil, ErrSystemBusy
}

func (p *pendingConfigChange) gc() {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.pending == nil {
		return
	}
	now := p.getTick()
	if now-p.lastGcTime < p.gcTick {
		return
	}
	p.lastGcTime = now
	if p.pending.deadline < now {
		p.pending.timeout()
		p.pending = nil
	}
}

func (p *pendingConfigChange) committed(key uint64) {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.pending == nil {
		return
	}
	if p.pending.key == key {
		p.pending.committed()
	}
}

func (p *pendingConfigChange) dropped(key uint64) {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.pending == nil {
		return
	}
	if p.pending.key == key {
		p.pending.dropped()
		p.pending = nil
	}
}

func (p *pendingConfigChange) apply(key uint64, rejected bool) {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.pending == nil {
		return
	}
	var v RequestResult
	if rejected {
		v.code = requestRejected
	} else {
		v.code = requestCompleted
	}
	if p.pending.key == key {
		p.pending.notify(v)
		p.pending = nil
	}
}

func newPendingReadIndex(pool *sync.Pool, r *readIndexQueue) pendingReadIndex {
	return pendingReadIndex{
		batches:      make(map[pb.SystemCtx]readBatch),
		requests:     r,
		logicalClock: newLogicalClock(),
		pool:         pool,
	}
}

func (p *pendingReadIndex) close() {
	p.mu.Lock()
	defer p.mu.Unlock()
	p.stopped = true
	if p.requests != nil {
		p.requests.close()
		reqs := p.requests.get()
		for _, rec := range reqs {
			rec.terminated()
		}
	}
	for _, rb := range p.batches {
		for _, req := range rb.requests {
			if req != nil {
				req.terminated()
			}
		}
	}
}

func (p *pendingReadIndex) read(timeoutTick uint64) (*RequestState, error) {
	if timeoutTick == 0 {
		return nil, ErrTimeoutTooSmall
	}
	req := p.pool.Get().(*RequestState)
	req.reuse(false)
	req.notifyCommit = false
	req.deadline = p.getTick() + timeoutTick

	ok, closed := p.requests.add(req)
	if closed {
		return nil, ErrShardClosed
	}
	if !ok {
		return nil, ErrSystemBusy
	}
	return req, nil
}

func (p *pendingReadIndex) genCtx() pb.SystemCtx {
	et := p.getTick() + 30
	for {
		v := pb.SystemCtx{
			Low:  random.LockGuardedRand.Uint64(),
			High: et,
		}
		if v.Low != 0 {
			return v
		}
	}
}

func (p *pendingReadIndex) nextCtx() pb.SystemCtx {
	p.mu.Lock()
	defer p.mu.Unlock()
	return p.genCtx()
}

func (p *pendingReadIndex) addReady(reads []pb.ReadyToRead) {
	if len(reads) == 0 {
		return
	}
	p.mu.Lock()
	defer p.mu.Unlock()
	for _, v := range reads {
		if rb, ok := p.batches[v.SystemCtx]; ok {
			rb.index = v.Index
			p.batches[v.SystemCtx] = rb
		}
	}
}

func (p *pendingReadIndex) add(sys pb.SystemCtx, reqs []*RequestState) {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.stopped {
		return
	}
	if _, ok := p.batches[sys]; ok {
		plog.Panicf("same system ctx added again %v", sys)
	} else {
		rs := make([]*RequestState, len(reqs))
		copy(rs, reqs)
		p.batches[sys] = readBatch{
			requests: rs,
		}
	}
}

func (p *pendingReadIndex) dropped(system pb.SystemCtx) {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.stopped {
		return
	}
	if rb, ok := p.batches[system]; ok {
		for _, req := range rb.requests {
			if req != nil {
				req.dropped()
			}
		}
		delete(p.batches, system)
	}
}

func (p *pendingReadIndex) applied(applied uint64) {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.stopped || len(p.batches) == 0 {
		return
	}
	now := p.getTick()
	for sys, rb := range p.batches {
		if rb.index > 0 && rb.index <= applied {
			for _, req := range rb.requests {
				if req != nil {
					var v RequestResult
					if req.deadline > now {
						req.readyToRead.set()
						v.code = requestCompleted
					} else {
						v.code = requestTimeout
					}
					req.notify(v)
				}
			}
			delete(p.batches, sys)
		}
	}
	if now-p.lastGcTime < p.gcTick {
		return
	}
	p.lastGcTime = now
	p.gc(now)
}

func (p *pendingReadIndex) gc(now uint64) {
	if len(p.batches) == 0 {
		return
	}
	for sys, rb := range p.batches {
		for idx, req := range rb.requests {
			if req != nil && req.deadline < now {
				req.timeout()
				rb.requests[idx] = nil
				p.batches[sys] = rb
			}
		}
	}
	for sys, rb := range p.batches {
		if sys.High < now {
			empty := true
			for _, req := range rb.requests {
				if req != nil {
					empty = false
					break
				}
			}
			if empty {
				delete(p.batches, sys)
			}
		}
	}
}

func getRng(shardID uint64, replicaID uint64, shard uint64) *keyGenerator {
	pid := os.Getpid()
	nano := time.Now().UnixNano()
	seedStr := fmt.Sprintf("%d-%d-%d-%d-%d", pid, nano, shardID, replicaID, shard)
	m := sha512.New()
	fileutil.MustWrite(m, []byte(seedStr))
	sum := m.Sum(nil)
	seed := binary.LittleEndian.Uint64(sum)
	return &keyGenerator{rand: rand.New(rand.NewSource(int64(seed)))}
}

func newPendingProposal(cfg config.Config,
	notifyCommit bool, pool *sync.Pool, proposals *entryQueue) pendingProposal {
	ps := pendingProposalShards
	p := pendingProposal{
		shards: make([]*proposalShard, ps),
		keyg:   make([]*keyGenerator, ps),
		ps:     ps,
	}
	for i := uint64(0); i < ps; i++ {
		p.shards[i] = newPendingProposalShard(cfg, notifyCommit, pool, proposals)
		p.keyg[i] = getRng(cfg.ShardID, cfg.ReplicaID, i)
	}
	return p
}

func (p *pendingProposal) propose(session *client.Session,
	cmd []byte, timeoutTick uint64) (*RequestState, error) {
	key := p.nextKey(session.ClientID)
	pp := p.shards[key%p.ps]
	return pp.propose(session, cmd, key, timeoutTick)
}

func (p *pendingProposal) close() {
	for _, pp := range p.shards {
		pp.close()
	}
}

func (p *pendingProposal) committed(clientID uint64,
	seriesID uint64, key uint64) {
	pp := p.shards[key%p.ps]
	pp.committed(clientID, seriesID, key)
}

func (p *pendingProposal) dropped(clientID uint64,
	seriesID uint64, key uint64) {
	pp := p.shards[key%p.ps]
	pp.dropped(clientID, seriesID, key)
}

func (p *pendingProposal) applied(clientID uint64,
	seriesID uint64, key uint64, result sm.Result, rejected bool) {
	pp := p.shards[key%p.ps]
	pp.applied(clientID, seriesID, key, result, rejected)
}

func (p *pendingProposal) nextKey(clientID uint64) uint64 {
	return p.keyg[clientID%p.ps].nextKey()
}

func (p *pendingProposal) tick(tick uint64) {
	for i := uint64(0); i < p.ps; i++ {
		p.shards[i].tick(tick)
	}
}

func (p *pendingProposal) gc() {
	for i := uint64(0); i < p.ps; i++ {
		pp := p.shards[i]
		pp.gc()
	}
}

func newPendingProposalShard(cfg config.Config,
	notifyCommit bool, pool *sync.Pool, proposals *entryQueue) *proposalShard {
	p := &proposalShard{
		proposals:    proposals,
		pending:      make(map[uint64]*RequestState),
		logicalClock: newLogicalClock(),
		pool:         pool,
		cfg:          cfg,
		notifyCommit: notifyCommit,
	}
	return p
}

func (p *proposalShard) propose(session *client.Session,
	cmd []byte, key uint64, timeoutTick uint64) (*RequestState, error) {
	if timeoutTick == 0 {
		return nil, ErrTimeoutTooSmall
	}
	if rsm.GetMaxBlockSize(p.cfg.EntryCompressionType) < uint64(len(cmd)) {
		return nil, ErrPayloadTooBig
	}
	entry := pb.Entry{
		Key:         key,
		ClientID:    session.ClientID,
		SeriesID:    session.SeriesID,
		RespondedTo: session.RespondedTo,
	}
	if len(cmd) == 0 {
		entry.Type = pb.ApplicationEntry
	} else {
		entry.Type = pb.EncodedEntry
		entry.Cmd = preparePayload(p.cfg.EntryCompressionType, cmd)
	}
	req := p.pool.Get().(*RequestState)
	req.reuse(p.notifyCommit)
	req.clientID = session.ClientID
	req.seriesID = session.SeriesID
	req.key = entry.Key
	req.deadline = p.getTick() + timeoutTick
	req.notifyCommit = p.notifyCommit

	p.mu.Lock()
	p.pending[entry.Key] = req
	p.mu.Unlock()

	added, stopped := p.proposals.add(entry)
	if stopped {
		plog.Warningf("%s dropped proposal, shard stopped",
			dn(p.cfg.ShardID, p.cfg.ReplicaID))
		p.mu.Lock()
		delete(p.pending, entry.Key)
		p.mu.Unlock()
		return nil, ErrShardClosed
	}
	if !added {
		p.mu.Lock()
		delete(p.pending, entry.Key)
		p.mu.Unlock()
		plog.Debugf("%s dropped proposal, overloaded",
			dn(p.cfg.ShardID, p.cfg.ReplicaID))
		return nil, ErrSystemBusy
	}
	return req, nil
}

func (p *proposalShard) close() {
	p.mu.Lock()
	defer p.mu.Unlock()
	p.stopped = true
	if p.proposals != nil {
		p.proposals.close()
	}
	for _, rec := range p.pending {
		rec.terminated()
	}
}

func (p *proposalShard) getProposal(clientID uint64,
	seriesID uint64, key uint64, now uint64) *RequestState {
	return p.takeProposal(clientID, seriesID, key, now, true)
}

func (p *proposalShard) borrowProposal(clientID uint64,
	seriesID uint64, key uint64, now uint64) *RequestState {
	return p.takeProposal(clientID, seriesID, key, now, false)
}

func (p *proposalShard) takeProposal(clientID uint64,
	seriesID uint64, key uint64, now uint64, remove bool) *RequestState {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.stopped {
		return nil
	}
	ps, ok := p.pending[key]
	if ok && ps.deadline >= now {
		if ps.clientID == clientID && ps.seriesID == seriesID {
			if remove {
				delete(p.pending, key)
			}
			return ps
		}
	}
	return nil
}

func (p *proposalShard) committed(clientID uint64, seriesID uint64, key uint64) {
	if ps := p.borrowProposal(clientID, seriesID, key, p.getTick()); ps != nil {
		ps.committed()
	}
}

func (p *proposalShard) dropped(clientID uint64, seriesID uint64, key uint64) {
	if ps := p.getProposal(clientID, seriesID, key, p.getTick()); ps != nil {
		ps.dropped()
	}
}

func (p *proposalShard) applied(clientID uint64,
	seriesID uint64, key uint64, result sm.Result, rejected bool) {
	now := p.getTick()
	var code RequestResultCode
	if rejected {
		code = requestRejected
	} else {
		code = requestCompleted
	}
	if ps := p.getProposal(clientID, seriesID, key, now); ps != nil {
		ps.notify(RequestResult{code: code, result: result})
	}
	if now != p.expireNotified {
		p.gcAt(now)
		p.expireNotified = now
	}
}

func (p *proposalShard) gc() {
	p.gcAt(p.getTick())
}

func (p *proposalShard) gcAt(now uint64) {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.stopped {
		return
	}
	if now-p.lastGcTime < p.gcTick {
		return
	}
	p.lastGcTime = now
	for key, rec := range p.pending {
		if rec.deadline < now {
			rec.timeout()
			delete(p.pending, key)
		}
	}
}

func preparePayload(ct config.CompressionType, cmd []byte) []byte {
	return rsm.GetEncoded(rsm.ToDioType(ct), cmd, nil)
}

type pendingRaftLogQuery struct {
	mu struct {
		sync.Mutex
		pending *RequestState
	}
}

func newPendingRaftLogQuery() pendingRaftLogQuery {
	return pendingRaftLogQuery{}
}

func (p *pendingRaftLogQuery) close() {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.mu.pending != nil {
		p.mu.pending.terminated()
		p.mu.pending = nil
	}
}

func (p *pendingRaftLogQuery) add(firstIndex uint64,
	lastIndex uint64, maxSize uint64) (*RequestState, error) {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.mu.pending != nil {
		return nil, ErrSystemBusy
	}
	req := &RequestState{
		logRange:   LogRange{FirstIndex: firstIndex, LastIndex: lastIndex},
		maxSize:    maxSize,
		CompletedC: make(chan RequestResult, 1),
	}
	p.mu.pending = req

	return req, nil
}

func (p *pendingRaftLogQuery) get() *RequestState {
	p.mu.Lock()
	defer p.mu.Unlock()
	return p.mu.pending
}

func (p *pendingRaftLogQuery) returned(outOfRange bool,
	logRange LogRange, entries []pb.Entry) {
	p.mu.Lock()
	defer p.mu.Unlock()
	if p.mu.pending == nil {
		panic("no pending raft log query")
	}

	req := p.mu.pending
	p.mu.pending = nil

	if outOfRange {
		req.notify(RequestResult{
			logQueryResult: true,
			code:           requestOutOfRange,
			logRange:       logRange,
		})
	} else {
		req.notify(RequestResult{
			logQueryResult: true,
			code:           requestCompleted,
			logRange:       logRange,
			entries:        entries,
		})
	}
}
````

## File: snapshotstate_test.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"reflect"
	"testing"

	"github.com/lni/goutils/leaktest"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/internal/rsm"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

func TestSnapshotTaskCanBeSet(t *testing.T) {
	defer leaktest.AfterTest(t)()
	sr := snapshotTask{}
	rec := rsm.Task{}
	sr.setTask(rec)
	assert.True(t, sr.hasTask, "rec not set")
}

func TestSnapshotTaskCanNotBeSetTwice(t *testing.T) {
	defer leaktest.AfterTest(t)()
	defer func() {
		if r := recover(); r != nil {
			return
		}
		t.Errorf("panic not triggered")
	}()
	sr := snapshotTask{}
	rec := rsm.Task{}
	sr.setTask(rec)
	sr.setTask(rec)
}

func TestCanGetSnapshotTask(t *testing.T) {
	defer leaktest.AfterTest(t)()
	sr := snapshotTask{}
	_, ok := sr.getTask()
	assert.False(t, ok, "unexpected record")

	rec := rsm.Task{}
	sr.setTask(rec)
	r, ok := sr.getTask()
	assert.True(t, ok, "no record to get")
	assert.True(t, reflect.DeepEqual(&rec, &r), "unexpected rec")

	rec, ok = sr.getTask()
	assert.False(t, ok, "record is still available")
}

func TestStreamTaskCanBeSet(t *testing.T) {
	defer leaktest.AfterTest(t)()
	sr := snapshotTask{}
	rec := rsm.Task{}
	fn := func() pb.IChunkSink { return nil }
	sr.setStreamTask(rec, fn)

	assert.True(t, sr.hasTask, "hasTask should be true")
	assert.True(t, reflect.DeepEqual(&(sr.t), &rec),
		"task should be set correctly")
	assert.NotNil(t, sr.getSinkFn, "getSinkFn should not be nil")
}

func TestStreamTaskCanNotBeSetTwice(t *testing.T) {
	defer leaktest.AfterTest(t)()
	sr := snapshotTask{}
	rec := rsm.Task{}
	fn := func() pb.IChunkSink { return nil }
	sr.setStreamTask(rec, fn)
	require.Panics(t, func() {
		sr.setStreamTask(rec, fn)
	})
}
````

## File: snapshotstate.go
````go
// Copyright 2017-2020 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"sync"
	"sync/atomic"

	"github.com/lni/goutils/random"

	"github.com/lni/dragonboat/v4/internal/rsm"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

type getSink func() pb.IChunkSink

type snapshotTask struct {
	mu        sync.Mutex
	getSinkFn getSink
	t         rsm.Task
	hasTask   bool
}

func (sr *snapshotTask) setStreamTask(t rsm.Task, getSinkFn getSink) {
	sr.mu.Lock()
	defer sr.mu.Unlock()
	if sr.hasTask {
		plog.Panicf("setting stream snapshot task again %+v\n%+v", sr.t, t)
	}
	sr.hasTask = true
	sr.t = t
	sr.getSinkFn = getSinkFn
}

func (sr *snapshotTask) setTask(t rsm.Task) {
	sr.mu.Lock()
	defer sr.mu.Unlock()
	if sr.hasTask {
		plog.Panicf("setting snapshot task again %+v\n%+v", sr.t, t)
	}
	sr.hasTask = true
	sr.t = t
}

func (sr *snapshotTask) getTask() (rsm.Task, bool) {
	sr.mu.Lock()
	defer sr.mu.Unlock()
	hasTask := sr.hasTask
	sr.hasTask = false
	return sr.t, hasTask
}

type snapshotState struct {
	snapshotIndex    uint64
	reqSnapshotIndex uint64
	compactLogTo     uint64
	compactedTo      uint64
	savingFlag       uint32
	recoveringFlag   uint32
	streamingFlag    uint32
	recoverReady     snapshotTask
	saveReady        snapshotTask
	streamReady      snapshotTask
	recoverCompleted snapshotTask
	saveCompleted    snapshotTask
	streamCompleted  snapshotTask
}

func (rs *snapshotState) recovering() bool {
	return atomic.LoadUint32(&rs.recoveringFlag) == 1
}

func (rs *snapshotState) setRecovering() {
	atomic.StoreUint32(&rs.recoveringFlag, 1)
}

func (rs *snapshotState) clearRecovering() {
	atomic.StoreUint32(&rs.recoveringFlag, 0)
}

func (rs *snapshotState) streaming() bool {
	return atomic.LoadUint32(&rs.streamingFlag) == 1
}

func (rs *snapshotState) setStreaming() {
	atomic.StoreUint32(&rs.streamingFlag, 1)
}

func (rs *snapshotState) clearStreaming() {
	atomic.StoreUint32(&rs.streamingFlag, 0)
}

func (rs *snapshotState) saving() bool {
	return atomic.LoadUint32(&rs.savingFlag) == 1
}

func (rs *snapshotState) setSaving() {
	atomic.StoreUint32(&rs.savingFlag, 1)
}

func (rs *snapshotState) clearSaving() {
	atomic.StoreUint32(&rs.savingFlag, 0)
}

func (rs *snapshotState) setIndex(index uint64) {
	atomic.StoreUint64(&rs.snapshotIndex, index)
}

func (rs *snapshotState) getIndex() uint64 {
	return atomic.LoadUint64(&rs.snapshotIndex)
}

func (rs *snapshotState) getReqIndex() uint64 {
	return atomic.LoadUint64(&rs.reqSnapshotIndex)
}

func (rs *snapshotState) setReqIndex(idx uint64) {
	atomic.StoreUint64(&rs.reqSnapshotIndex, idx)
}

func (rs *snapshotState) hasCompactLogTo() bool {
	return atomic.LoadUint64(&rs.compactLogTo) > 0
}

func (rs *snapshotState) getCompactLogTo() uint64 {
	return atomic.SwapUint64(&rs.compactLogTo, 0)
}

func (rs *snapshotState) setCompactLogTo(v uint64) {
	atomic.StoreUint64(&rs.compactLogTo, v)
}

func (rs *snapshotState) setCompactedTo(v uint64) {
	atomic.StoreUint64(&rs.compactedTo, v)
}

func (rs *snapshotState) getCompactedTo() uint64 {
	return atomic.SwapUint64(&rs.compactedTo, 0)
}

func (rs *snapshotState) hasCompactedTo() bool {
	return atomic.LoadUint64(&rs.compactedTo) > 0
}

func (rs *snapshotState) setStreamReq(t rsm.Task, fn getSink) {
	rs.streamReady.setStreamTask(t, fn)
}

func (rs *snapshotState) setRecoverReq(t rsm.Task) {
	rs.recoverReady.setTask(t)
}

func (rs *snapshotState) getRecoverReq() (rsm.Task, bool) {
	return rs.recoverReady.getTask()
}

func (rs *snapshotState) setSaveReq(t rsm.Task) {
	rs.saveReady.setTask(t)
}

func (rs *snapshotState) getSaveReq() (rsm.Task, bool) {
	return rs.saveReady.getTask()
}

func (rs *snapshotState) getStreamReq() (rsm.Task, getSink, bool) {
	r, ok := rs.streamReady.getTask()
	if !ok {
		return rsm.Task{}, nil, false
	}
	return r, rs.streamReady.getSinkFn, true
}

func (rs *snapshotState) notifySnapshotStatus(save bool,
	recover bool, stream bool, initial bool, index uint64) {
	count := 0
	if save {
		count++
	}
	if recover {
		count++
	}
	if stream {
		count++
	}
	if count != 1 {
		plog.Panicf("invalid request, save %t, recover %t, stream %t",
			save, recover, stream)
	}
	t := rsm.Task{
		Save:    save,
		Recover: recover,
		Stream:  stream,
		Initial: initial,
		Index:   index,
	}
	if save {
		rs.saveCompleted.setTask(t)
	} else if recover {
		rs.recoverCompleted.setTask(t)
	} else {
		rs.streamCompleted.setTask(t)
	}
}

func (rs *snapshotState) getStreamCompleted() (rsm.Task, bool) {
	return rs.streamCompleted.getTask()
}

func (rs *snapshotState) getRecoverCompleted() (rsm.Task, bool) {
	return rs.recoverCompleted.getTask()
}

func (rs *snapshotState) getSaveCompleted() (rsm.Task, bool) {
	return rs.saveCompleted.getTask()
}

type task struct {
	intervalMs uint64
	lastRun    uint64
}

func newTask(interval uint64) task {
	if interval == 0 {
		panic("invalid interval")
	}
	return task{
		intervalMs: interval,
		lastRun:    random.LockGuardedRand.Uint64() % interval,
	}
}

func (t *task) timeToRun(tt uint64) bool {
	if tt-t.lastRun >= t.intervalMs {
		t.lastRun = tt
		return true
	}
	return false
}
````

## File: snapshotter_test.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"encoding/binary"
	"fmt"
	"reflect"
	"testing"

	"github.com/lni/goutils/leaktest"
	"github.com/stretchr/testify/require"

	"github.com/lni/dragonboat/v4/config"
	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/logdb"
	"github.com/lni/dragonboat/v4/internal/rsm"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
)

const (
	tmpSnapshotDirSuffix = "generating"
	recvTmpDirSuffix     = "receiving"
	rdbTestDirectory     = "rdb_test_dir_safe_to_delete"
)

func getNewTestDB(dir string, lldir string, fs vfs.IFS) raftio.ILogDB {
	d := fs.PathJoin(rdbTestDirectory, dir)
	lld := fs.PathJoin(rdbTestDirectory, lldir)
	if err := fs.MkdirAll(d, 0777); err != nil {
		panic(err)
	}
	if err := fs.MkdirAll(lld, 0777); err != nil {
		panic(err)
	}
	cfg := config.NodeHostConfig{
		Expert: config.GetDefaultExpertConfig(),
	}
	cfg.Expert.FS = fs
	db, err := logdb.NewDefaultLogDB(cfg, nil, []string{d}, []string{lld})
	if err != nil {
		panic(err.Error())
	}
	return db
}

func deleteTestRDB(fs vfs.IFS) {
	if err := fs.RemoveAll(rdbTestDirectory); err != nil {
		panic(err)
	}
}

func getTestSnapshotter(ldb raftio.ILogDB, fs vfs.IFS) *snapshotter {
	fp := fs.PathJoin(rdbTestDirectory, "snapshot")
	if err := fs.MkdirAll(fp, 0777); err != nil {
		panic(err)
	}
	f := func(cid uint64, nid uint64) string {
		return fp
	}
	lr := logdb.NewLogReader(1, 1, ldb)
	return newSnapshotter(1, 1, f, ldb, lr, fs)
}

func runSnapshotterTest(t *testing.T,
	fn func(t *testing.T, logdb raftio.ILogDB, snapshotter *snapshotter),
	fs vfs.IFS) {
	defer leaktest.AfterTest(t)()
	dir := "db-dir"
	lldir := "wal-db-dir"
	deleteTestRDB(fs)
	ldb := getNewTestDB(dir, lldir, fs)
	s := getTestSnapshotter(ldb, fs)
	defer deleteTestRDB(fs)
	defer func() {
		require.NoError(t, ldb.Close())
	}()
	fn(t, ldb, s)
}

func TestFinalizeSnapshotReturnExpectedErrorWhenOutOfDate(t *testing.T) {
	fs := vfs.GetTestFS()
	fn := func(t *testing.T, ldb raftio.ILogDB, s *snapshotter) {
		ss := pb.Snapshot{
			FileSize: 1234,
			Filepath: "f2",
			Index:    100,
			Term:     200,
		}
		env := s.getEnv(ss.Index)
		finalSnapDir := env.GetFinalDir()
		require.NoError(t, fs.MkdirAll(finalSnapDir, 0755))
		require.NoError(t, env.CreateTempDir())
		err := s.Commit(ss, rsm.SSRequest{})
		require.ErrorIs(t, err, errSnapshotOutOfDate)
	}
	runSnapshotterTest(t, fn, fs)
}

func TestSnapshotCanBeFinalized(t *testing.T) {
	fs := vfs.GetTestFS()
	fn := func(t *testing.T, ldb raftio.ILogDB, s *snapshotter) {
		ss := pb.Snapshot{
			FileSize: 1234,
			Filepath: "f2",
			Index:    100,
			Term:     200,
		}
		env := s.getEnv(ss.Index)
		finalSnapDir := env.GetFinalDir()
		tmpDir := env.GetTempDir()
		require.NoError(t, env.CreateTempDir())
		_, err := fs.Stat(tmpDir)
		require.NoError(t, err)
		testfp := fs.PathJoin(tmpDir, "test.data")
		f, err := fs.Create(testfp)
		require.NoError(t, err)
		_, err = f.Write(make([]byte, 12))
		require.NoError(t, err)
		require.NoError(t, f.Close())
		require.NoError(t, s.Commit(ss, rsm.SSRequest{}))
		snapshot, err := ldb.GetSnapshot(1, 1)
		require.NoError(t, err)
		require.False(t, pb.IsEmptySnapshot(snapshot))
		rs, err := s.GetSnapshotFromLogDB()
		require.NoError(t, err)
		require.Equal(t, uint64(100), rs.Index)
		_, err = fs.Stat(tmpDir)
		require.True(t, vfs.IsNotExist(err))
		fi, err := fs.Stat(finalSnapDir)
		require.NoError(t, err)
		require.True(t, fi.IsDir())
		require.False(t, fileutil.HasFlagFile(finalSnapDir,
			fileutil.SnapshotFlagFilename, fs))
		vfp := fs.PathJoin(finalSnapDir, "test.data")
		fi, err = fs.Stat(vfp)
		require.NoError(t, err)
		require.False(t, fi.IsDir())
		require.Equal(t, int64(12), fi.Size())
	}
	runSnapshotterTest(t, fn, fs)
}

func TestSnapshotCanBeSavedToLogDB(t *testing.T) {
	fn := func(t *testing.T, ldb raftio.ILogDB, s *snapshotter) {
		s1 := pb.Snapshot{
			FileSize: 1234,
			Filepath: "f2",
			Index:    1,
			Term:     2,
		}
		require.NoError(t, s.saveSnapshot(s1))
		snapshot, err := ldb.GetSnapshot(1, 1)
		require.NoError(t, err)
		require.True(t, reflect.DeepEqual(s1, snapshot))
	}
	fs := vfs.GetTestFS()
	runSnapshotterTest(t, fn, fs)
}

func TestZombieSnapshotDirsCanBeRemoved(t *testing.T) {
	fs := vfs.GetTestFS()
	fn := func(t *testing.T, ldb raftio.ILogDB, s *snapshotter) {
		env1 := s.getEnv(100)
		env2 := s.getEnv(200)
		fd1 := env1.GetFinalDir()
		fd2 := env2.GetFinalDir()
		fd1 = fd1 + "-100." + tmpSnapshotDirSuffix
		fd2 = fd2 + "-100." + recvTmpDirSuffix
		require.NoError(t, fs.MkdirAll(fd1, 0755))
		require.NoError(t, fs.MkdirAll(fd2, 0755))
		require.NoError(t, s.processOrphans())
		_, err := fs.Stat(fd1)
		require.True(t, vfs.IsNotExist(err))
		_, err = fs.Stat(fd2)
		require.True(t, vfs.IsNotExist(err))
	}
	runSnapshotterTest(t, fn, fs)
}

func TestSnapshotsNotInLogDBAreRemoved(t *testing.T) {
	fs := vfs.GetTestFS()
	fn := func(t *testing.T, ldb raftio.ILogDB, s *snapshotter) {
		env1 := s.getEnv(100)
		env2 := s.getEnv(200)
		fd1 := env1.GetFinalDir()
		fd2 := env2.GetFinalDir()
		require.NoError(t, fs.MkdirAll(fd1, 0755))
		require.NoError(t, fs.MkdirAll(fd2, 0755))
		require.NoError(t, s.processOrphans())
		_, err := fs.Stat(fd1)
		require.True(t, vfs.IsNotExist(err))
		_, err = fs.Stat(fd2)
		require.True(t, vfs.IsNotExist(err))
	}
	runSnapshotterTest(t, fn, fs)
}

func TestOnlyMostRecentSnapshotIsKept(t *testing.T) {
	fs := vfs.GetTestFS()
	fn := func(t *testing.T, ldb raftio.ILogDB, s *snapshotter) {
		env1 := s.getEnv(100)
		env2 := s.getEnv(200)
		env3 := s.getEnv(300)
		s1 := pb.Snapshot{
			FileSize: 1234,
			Filepath: "f2",
			Index:    200,
			Term:     200,
		}
		fd1 := env1.GetFinalDir()
		fd2 := env2.GetFinalDir()
		fd3 := env3.GetFinalDir()
		require.NoError(t, s.saveSnapshot(s1))
		require.NoError(t, fs.MkdirAll(fd1, 0755))
		require.NoError(t, fs.MkdirAll(fd2, 0755))
		require.NoError(t, fs.MkdirAll(fd3, 0755))
		require.NoError(t, s.processOrphans())
		_, err := fs.Stat(fd1)
		require.True(t, vfs.IsNotExist(err))
		_, err = fs.Stat(fd2)
		require.False(t, vfs.IsNotExist(err))
		_, err = fs.Stat(fd3)
		require.True(t, vfs.IsNotExist(err))
	}
	runSnapshotterTest(t, fn, fs)
}

func TestFirstSnapshotBecomeOrphanedIsHandled(t *testing.T) {
	fs := vfs.GetTestFS()
	fn := func(t *testing.T, ldb raftio.ILogDB, s *snapshotter) {
		s1 := pb.Snapshot{
			FileSize: 1234,
			Filepath: "f2",
			Index:    100,
			Term:     200,
		}
		env := s.getEnv(100)
		fd1 := env.GetFinalDir()
		require.NoError(t, fs.MkdirAll(fd1, 0755))
		require.NoError(t, fileutil.CreateFlagFile(fd1,
			fileutil.SnapshotFlagFilename, &s1, fs))
		require.NoError(t, s.processOrphans())
		_, err := fs.Stat(fd1)
		require.True(t, vfs.IsNotExist(err))
	}
	runSnapshotterTest(t, fn, fs)
}

func TestOrphanedSnapshotRecordIsRemoved(t *testing.T) {
	fs := vfs.GetTestFS()
	fn := func(t *testing.T, ldb raftio.ILogDB, s *snapshotter) {
		s1 := pb.Snapshot{
			FileSize: 1234,
			Filepath: "f2",
			Index:    100,
			Term:     200,
		}
		s2 := pb.Snapshot{
			FileSize: 1234,
			Filepath: "f2",
			Index:    200,
			Term:     200,
		}
		env1 := s.getEnv(s1.Index)
		env2 := s.getEnv(s2.Index)
		fd1 := env1.GetFinalDir()
		fd2 := env2.GetFinalDir()
		require.NoError(t, fs.MkdirAll(fd1, 0755))
		require.NoError(t, fs.MkdirAll(fd2, 0755))
		require.NoError(t, fileutil.CreateFlagFile(fd1,
			fileutil.SnapshotFlagFilename, &s1, fs))
		require.NoError(t, fileutil.CreateFlagFile(fd2,
			fileutil.SnapshotFlagFilename, &s2, fs))
		require.NoError(t, s.saveSnapshot(s1))
		require.NoError(t, s.saveSnapshot(s2))
		// two orphane snapshots, kept the most recent one, and remove the older
		// one including its logdb record.
		require.NoError(t, s.processOrphans())
		_, err := fs.Stat(fd1)
		require.False(t, vfs.IsExist(err))
		_, err = fs.Stat(fd2)
		require.False(t, vfs.IsNotExist(err))
		require.False(t, fileutil.HasFlagFile(fd2,
			fileutil.SnapshotFlagFilename, fs))
		snapshot, err := s.logdb.GetSnapshot(1, 1)
		require.NoError(t, err)
		require.Equal(t, uint64(200), snapshot.Index)
	}
	runSnapshotterTest(t, fn, fs)
}

func TestOrphanedSnapshotsCanBeProcessed(t *testing.T) {
	fs := vfs.GetTestFS()
	fn := func(t *testing.T, ldb raftio.ILogDB, s *snapshotter) {
		s1 := pb.Snapshot{
			FileSize: 1234,
			Filepath: "f2",
			Index:    100,
			Term:     200,
		}
		s2 := pb.Snapshot{
			FileSize: 1234,
			Filepath: "f2",
			Index:    200,
			Term:     200,
		}
		s3 := pb.Snapshot{
			FileSize: 1234,
			Filepath: "f2",
			Index:    300,
			Term:     200,
		}
		env1 := s.getEnv(s1.Index)
		env2 := s.getEnv(s2.Index)
		env3 := s.getEnv(s3.Index)
		fd1 := env1.GetFinalDir()
		fd2 := env2.GetFinalDir()
		fd3 := env3.GetFinalDir()
		fd4 := fmt.Sprintf("%s%s", fd3, "xx")
		require.NoError(t, fs.MkdirAll(fd1, 0755))
		require.NoError(t, fs.MkdirAll(fd2, 0755))
		require.NoError(t, fs.MkdirAll(fd4, 0755))
		require.NoError(t, fileutil.CreateFlagFile(fd1,
			fileutil.SnapshotFlagFilename, &s1, fs))
		require.NoError(t, fileutil.CreateFlagFile(fd2,
			fileutil.SnapshotFlagFilename, &s2, fs))
		require.NoError(t, fileutil.CreateFlagFile(fd4,
			fileutil.SnapshotFlagFilename, &s3, fs))
		require.NoError(t, s.saveSnapshot(s1))
		// fd1 has record in logdb. flag file expected to be removed while the fd1
		// foler is expected to be kept
		// fd2 doesn't has its record in logdb, while the most recent snapshot
		// record in logdb is not for fd2, fd2 will be entirely removed
		require.NoError(t, s.processOrphans())
		require.False(t, fileutil.HasFlagFile(fd1,
			fileutil.SnapshotFlagFilename, fs))
		require.False(t, fileutil.HasFlagFile(fd2,
			fileutil.SnapshotFlagFilename, fs))
		require.True(t, fileutil.HasFlagFile(fd4,
			fileutil.SnapshotFlagFilename, fs))
		_, err := fs.Stat(fd1)
		require.False(t, vfs.IsNotExist(err))
		_, err = fs.Stat(fd2)
		require.True(t, vfs.IsNotExist(err))
	}
	runSnapshotterTest(t, fn, fs)
}

func TestSnapshotterCompact(t *testing.T) {
	fs := vfs.GetTestFS()
	fn := func(t *testing.T, ldb raftio.ILogDB, snapshotter *snapshotter) {
		for i := uint64(1); i <= uint64(3); i++ {
			fn := fmt.Sprintf("f%d.data", i)
			s := pb.Snapshot{
				FileSize: 1234,
				Filepath: fn,
				Index:    i,
				Term:     2,
			}
			env := snapshotter.getEnv(s.Index)
			require.NoError(t, env.CreateTempDir())
			require.NoError(t, snapshotter.Commit(s, rsm.SSRequest{}))
			fp := snapshotter.getFilePath(s.Index)
			f, err := fs.Create(fp)
			require.NoError(t, err)
			require.NoError(t, f.Close())
		}
		require.NoError(t, snapshotter.Compact(2))
		check := func(index uint64, exist bool) {
			env := snapshotter.getEnv(index)
			snapDir := env.GetFinalDir()
			_, err := fs.Stat(snapDir)
			if exist {
				require.False(t, vfs.IsNotExist(err))
			} else {
				require.True(t, vfs.IsNotExist(err))
			}
		}
		check(1, true)
		check(2, false)
		check(3, true)
	}
	runSnapshotterTest(t, fn, fs)
}

func TestShrinkSnapshots(t *testing.T) {
	fs := vfs.GetTestFS()
	fn := func(t *testing.T, ldb raftio.ILogDB, snapshotter *snapshotter) {
		for i := uint64(1); i <= 3; i++ {
			index := i * 10
			env := snapshotter.getEnv(index)
			fp := env.GetFilepath()
			s := pb.Snapshot{
				Index:    index,
				Term:     2,
				FileSize: 1234,
				Filepath: fp,
			}
			require.NoError(t, env.CreateTempDir())
			require.NoError(t, snapshotter.Commit(s, rsm.SSRequest{}))
			fp = snapshotter.getFilePath(s.Index)
			writer, err := rsm.NewSnapshotWriter(fp, pb.NoCompression, fs)
			require.NoError(t, err)
			sz := make([]byte, 8)
			binary.LittleEndian.PutUint64(sz, 0)
			_, err = writer.Write(sz)
			require.NoError(t, err)
			for j := 0; j < 10; j++ {
				data := make([]byte, 1024*1024)
				_, err = writer.Write(data)
				require.NoError(t, err)
			}
			require.NoError(t, writer.Close())
		}
		require.NoError(t, snapshotter.Shrink(20))
		env1 := snapshotter.getEnv(10)
		env2 := snapshotter.getEnv(20)
		env3 := snapshotter.getEnv(30)
		cf := func(p string, esz uint64) {
			fi, err := fs.Stat(p)
			require.NoError(t, err)
			require.Equal(t, esz, uint64(fi.Size()))
		}
		cf(env1.GetFilepath(), 10486832)
		cf(env2.GetFilepath(), 1060)
		cf(env3.GetFilepath(), 10486832)
	}
	runSnapshotterTest(t, fn, fs)
}

func TestSnapshotDirNameMatchWorks(t *testing.T) {
	fn := func(t *testing.T, ldb raftio.ILogDB, s *snapshotter) {
		tests := []struct {
			dirName string
			valid   bool
		}{
			{"snapshot-AB", true},
			{"snapshot", false},
			{"xxxsnapshot-AB", false},
			{"snapshot-ABd", false},
			{"snapshot-", false},
		}
		for idx, tt := range tests {
			v := s.dirMatch(tt.dirName)
			require.Equal(t, tt.valid, v,
				fmt.Sprintf("dir name %s (%d) failed to match", tt.dirName, idx))
		}
	}
	fs := vfs.GetTestFS()
	runSnapshotterTest(t, fn, fs)
}

func TestZombieSnapshotDirNameMatchWorks(t *testing.T) {
	fn := func(t *testing.T, ldb raftio.ILogDB, s *snapshotter) {
		tests := []struct {
			dirName string
			valid   bool
		}{
			{"snapshot-AB", false},
			{"snapshot", false},
			{"xxxsnapshot-AB", false},
			{"snapshot-", false},
			{"snapshot-AB-01.receiving", true},
			{"snapshot-AB-1G.receiving", false},
			{"snapshot-AB.receiving", false},
			{"snapshot-XX.receiving", false},
			{"snapshot-AB.receivingd", false},
			{"dsnapshot-AB.receiving", false},
			{"snapshot-AB.generating", false},
			{"snapshot-AB-01.generating", true},
			{"snapshot-AB-0G.generating", false},
			{"snapshot-XX.generating", false},
			{"snapshot-AB.generatingd", false},
			{"dsnapshot-AB.generating", false},
		}
		for idx, tt := range tests {
			v := s.isZombie(tt.dirName)
			require.Equal(t, tt.valid, v,
				fmt.Sprintf("dir name %s (%d) failed to match", tt.dirName, idx))
		}
	}
	fs := vfs.GetTestFS()
	runSnapshotterTest(t, fn, fs)
}
````

## File: snapshotter.go
````go
// Copyright 2017-2021 Lei Ni (nilei81@gmail.com) and other contributors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package dragonboat

import (
	"strconv"

	"github.com/cockroachdb/errors"
	"github.com/lni/goutils/logutil"

	"github.com/lni/dragonboat/v4/internal/fileutil"
	"github.com/lni/dragonboat/v4/internal/logdb"
	"github.com/lni/dragonboat/v4/internal/rsm"
	"github.com/lni/dragonboat/v4/internal/server"
	"github.com/lni/dragonboat/v4/internal/utils/dio"
	"github.com/lni/dragonboat/v4/internal/vfs"
	"github.com/lni/dragonboat/v4/raftio"
	pb "github.com/lni/dragonboat/v4/raftpb"
	sm "github.com/lni/dragonboat/v4/statemachine"
)

func compressionType(ct pb.CompressionType) dio.CompressionType {
	switch ct {
	case pb.NoCompression:
		return dio.NoCompression
	case pb.Snappy:
		return dio.Snappy
	default:
		plog.Panicf("unknown compression type: %d", ct)
	}
	panic("will never reach here")
}

var (
	// ErrNoSnapshot is the error used to indicate that there is no snapshot
	// available.
	ErrNoSnapshot        = errors.New("no snapshot available")
	errSnapshotOutOfDate = errors.New("snapshot being generated is out of date")
)

type snapshotter struct {
	root      server.SnapshotDirFunc
	dir       string
	shardID   uint64
	replicaID uint64
	logdb     raftio.ILogDB
	logReader *logdb.LogReader
	fs        vfs.IFS
}

var _ rsm.ISnapshotter = (*snapshotter)(nil)

func newSnapshotter(shardID uint64, replicaID uint64,
	root server.SnapshotDirFunc, ldb raftio.ILogDB,
	logReader *logdb.LogReader, fs vfs.IFS) *snapshotter {
	return &snapshotter{
		shardID:   shardID,
		replicaID: replicaID,
		root:      root,
		dir:       root(shardID, replicaID),
		logdb:     ldb,
		logReader: logReader,
		fs:        fs,
	}
}

func (s *snapshotter) id() string {
	return dn(s.shardID, s.replicaID)
}

func (s *snapshotter) ssid(index uint64) string {
	return logutil.DescribeSS(s.shardID, s.replicaID, index)
}

func (s *snapshotter) Shrunk(ss pb.Snapshot) (bool, error) {
	return rsm.IsShrunkSnapshotFile(s.getFilePath(ss.Index), s.fs)
}

func (s *snapshotter) Stream(streamable rsm.IStreamable,
	meta rsm.SSMeta, sink pb.IChunkSink) error {
	ct := compressionType(meta.CompressionType)
	cw := dio.NewCompressor(ct, rsm.NewChunkWriter(sink, meta))
	if err := streamable.Stream(meta.Ctx, cw); err != nil {
		if cerr := sink.Close(); cerr != nil {
			plog.Errorf("failed to close the sink %v", cerr)
		}
		return err
	}
	return cw.Close()
}

func (s *snapshotter) Save(savable rsm.ISavable,
	meta rsm.SSMeta) (ss pb.Snapshot, env server.SSEnv, err error) {
	env = s.getCustomEnv(meta)
	if err := env.CreateTempDir(); err != nil {
		return pb.Snapshot{}, env, err
	}
	files := rsm.NewFileCollection()
	fp := env.GetTempFilepath()
	ct := compressionType(meta.CompressionType)
	w, err := rsm.NewSnapshotWriter(fp, meta.CompressionType, s.fs)
	if err != nil {
		return pb.Snapshot{}, env, err
	}
	cw := dio.NewCountedWriter(w)
	sw := dio.NewCompressor(ct, cw)
	defer func() {
		err = firstError(err, sw.Close())
		if ss.Index > 0 {
			total := cw.BytesWritten()
			ss.Checksum = w.GetPayloadChecksum()
			ss.FileSize = w.GetPayloadSize(total) + rsm.HeaderSize
		}
	}()
	session := meta.Session.Bytes()
	dummy, err := savable.Save(meta, sw, session, files)
	if err != nil {
		return pb.Snapshot{}, env, err
	}
	fs, err := files.PrepareFiles(env.GetTempDir(), env.GetFinalDir())
	if err != nil {
		return pb.Snapshot{}, env, err
	}
	return pb.Snapshot{
		ShardID:     s.shardID,
		Filepath:    env.GetFilepath(),
		Membership:  meta.Membership,
		Index:       meta.Index,
		Term:        meta.Term,
		OnDiskIndex: meta.OnDiskIndex,
		Files:       fs,
		Dummy:       dummy,
		Type:        meta.Type,
	}, env, nil
}

func (s *snapshotter) Load(ss pb.Snapshot,
	sessions rsm.ILoadable, asm rsm.IRecoverable) (err error) {
	fp := s.getFilePath(ss.Index)
	fs := make([]sm.SnapshotFile, 0)
	for _, f := range ss.Files {
		fs = append(fs, sm.SnapshotFile{
			FileID:   f.FileId,
			Filepath: f.Filepath,
			Metadata: f.Metadata,
		})
	}
	reader, header, err := rsm.NewSnapshotReader(fp, s.fs)
	if err != nil {
		return err
	}
	ct := compressionType(header.CompressionType)
	cr := dio.NewDecompressor(ct, reader)
	defer func() {
		err = firstError(err, cr.Close())
	}()
	v := rsm.SSVersion(header.Version)
	if err := sessions.LoadSessions(cr, v); err != nil {
		return err
	}
	if err := asm.Recover(cr, fs); err != nil {
		return err
	}
	return nil
}

func (s *snapshotter) GetSnapshot() (pb.Snapshot, error) {
	ss := s.logReader.Snapshot()
	if pb.IsEmptySnapshot(ss) {
		return pb.Snapshot{}, ErrNoSnapshot
	}
	return ss, nil
}

// TODO: update this once the LogDB interface is updated to have the ability to
// query latest snapshot.
func (s *snapshotter) GetSnapshotFromLogDB() (pb.Snapshot, error) {
	snapshot, err := s.logdb.GetSnapshot(s.shardID, s.replicaID)
	if err != nil {
		return pb.Snapshot{}, err
	}
	if !pb.IsEmptySnapshot(snapshot) {
		return snapshot, nil
	}
	return pb.Snapshot{}, ErrNoSnapshot
}

func (s *snapshotter) Shrink(index uint64) error {
	ss, err := s.logdb.GetSnapshot(s.shardID, s.replicaID)
	if err != nil {
		return err
	}
	if ss.Index < index {
		return nil
	}
	if !ss.Dummy && !ss.Witness {
		env := s.getEnv(index)
		fp := env.GetFilepath()
		shrunk := env.GetShrinkedFilepath()
		plog.Infof("%s shrinking %s", s.id(), s.ssid(index))
		if err := rsm.ShrinkSnapshot(fp, shrunk, s.fs); err != nil {
			return err
		}
		return rsm.ReplaceSnapshot(shrunk, fp, s.fs)
	}
	return nil
}

func (s *snapshotter) Compact(index uint64) error {
	ss, err := s.logdb.GetSnapshot(s.shardID, s.replicaID)
	if err != nil {
		return err
	}
	if ss.Index <= index {
		plog.Panicf("%s invalid compaction, LogDB snapshot %d, index %d",
			s.id(), ss.Index, index)
	}
	plog.Debugf("%s called Compact, latest %d, to compact %d",
		s.id(), ss.Index, index)
	if err := s.remove(index); err != nil {
		return err
	}
	return nil
}

func (s *snapshotter) IsNoSnapshotError(err error) bool {
	return errors.Is(err, ErrNoSnapshot)
}

func (s *snapshotter) Commit(ss pb.Snapshot, req rsm.SSRequest) error {
	env := s.getCustomEnv(rsm.SSMeta{
		Index:   ss.Index,
		Request: req,
	})
	if err := env.SaveSSMetadata(&ss); err != nil {
		return err
	}
	if err := env.FinalizeSnapshot(&ss); err != nil {
		if errors.Is(err, server.ErrSnapshotOutOfDate) {
			return errSnapshotOutOfDate
		}
		return err
	}
	if !req.Exported() {
		if err := s.saveSnapshot(ss); err != nil {
			return err
		}
	}
	return env.RemoveFlagFile()
}

func (s *snapshotter) getFilePath(index uint64) string {
	env := s.getEnv(index)
	return env.GetFilepath()
}

func (s *snapshotter) processOrphans() error {
	files, err := s.fs.List(s.dir)
	if err != nil {
		return err
	}
	noss := false
	mrss, err := s.GetSnapshotFromLogDB()
	if err != nil {
		if errors.Is(err, ErrNoSnapshot) {
			noss = true
		} else {
			return err
		}
	}
	removeFolder := func(fdir string) error {
		if err := s.fs.RemoveAll(fdir); err != nil {
			return err
		}
		return fileutil.SyncDir(s.dir, s.fs)
	}
	for _, n := range files {
		fi, err := s.fs.Stat(s.fs.PathJoin(s.dir, n))
		if err != nil {
			return err
		}
		if !fi.IsDir() {
			continue
		}
		fdir := s.fs.PathJoin(s.dir, fi.Name())
		if s.isOrphan(fi.Name()) {
			var ss pb.Snapshot
			if err := fileutil.GetFlagFileContent(fdir,
				fileutil.SnapshotFlagFilename, &ss, s.fs); err != nil {
				return err
			}
			if pb.IsEmptySnapshot(ss) {
				plog.Panicf("empty snapshot found in %s", fdir)
			}
			remove := false
			if noss {
				remove = true
			} else {
				if mrss.Index != ss.Index {
					remove = true
				}
			}
			if remove {
				if err := s.remove(ss.Index); err != nil {
					return err
				}
			} else {
				env := s.getEnv(ss.Index)
				if err := env.RemoveFlagFile(); err != nil {
					return err
				}
			}
		} else if s.isZombie(fi.Name()) {
			if err := removeFolder(fdir); err != nil {
				return err
			}
		} else if s.isSnapshot(fi.Name()) {
			index := s.parseIndex(fi.Name())
			if noss || index != mrss.Index {
				if err := removeFolder(fdir); err != nil {
					return err
				}
			}
		}
	}
	return nil
}

func (s *snapshotter) remove(index uint64) error {
	env := s.getEnv(index)
	return env.RemoveFinalDir()
}

func (s *snapshotter) removeFlagFile(index uint64) error {
	env := s.getEnv(index)
	return env.RemoveFlagFile()
}

func (s *snapshotter) getEnv(index uint64) server.SSEnv {
	return server.NewSSEnv(s.root,
		s.shardID, s.replicaID, index, s.replicaID, server.SnapshotMode, s.fs)
}

func (s *snapshotter) getCustomEnv(meta rsm.SSMeta) server.SSEnv {
	if meta.Request.Exported() {
		if len(meta.Request.Path) == 0 {
			plog.Panicf("Path is empty when exporting snapshot")
		}
		gp := func(shardID uint64, replicaID uint64) string {
			return meta.Request.Path
		}
		return server.NewSSEnv(gp,
			s.shardID, s.replicaID, meta.Index, s.replicaID, server.SnapshotMode, s.fs)
	}
	return s.getEnv(meta.Index)
}

func (s *snapshotter) saveSnapshot(snapshot pb.Snapshot) error {
	return s.logdb.SaveSnapshots([]pb.Update{{
		ShardID:   s.shardID,
		ReplicaID: s.replicaID,
		Snapshot:  snapshot,
	}})
}

func (s *snapshotter) dirMatch(dir string) bool {
	return server.SnapshotDirNameRe.Match([]byte(dir))
}

func (s *snapshotter) parseIndex(dir string) uint64 {
	if parts := server.SnapshotDirNamePartsRe.FindStringSubmatch(dir); len(parts) == 2 {
		index, err := strconv.ParseUint(parts[1], 16, 64)
		if err != nil {
			plog.Panicf("failed to parse index %s", parts[1])
		}
		return index
	}
	plog.Panicf("unknown snapshot fold name: %s", dir)
	return 0
}

func (s *snapshotter) isSnapshot(dir string) bool {
	if !s.dirMatch(dir) {
		return false
	}
	fdir := s.fs.PathJoin(s.dir, dir)
	return !fileutil.HasFlagFile(fdir, fileutil.SnapshotFlagFilename, s.fs)
}

func (s *snapshotter) isZombie(dir string) bool {
	return server.GenSnapshotDirNameRe.Match([]byte(dir)) ||
		server.RecvSnapshotDirNameRe.Match([]byte(dir))
}

func (s *snapshotter) isOrphan(dir string) bool {
	if !s.dirMatch(dir) {
		return false
	}
	fdir := s.fs.PathJoin(s.dir, dir)
	return fileutil.HasFlagFile(fdir, fileutil.SnapshotFlagFilename, s.fs)
}
````
